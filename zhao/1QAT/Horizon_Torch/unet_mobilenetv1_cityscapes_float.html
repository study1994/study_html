<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>unet_mobilenetv1_cityscapes_float</title>
    <style>
      * {
        margin: 0;
        padding: 0;
      }
      #mindmap {
        display: block;
        width: 100vw;
        height: 100vh;
      }
      .hidden-code {
        display: none !important;
      }
    </style>
    <link
      rel="stylesheet"
      href="https://study1994.github.io/study_html/npm/mycss/style.css"
    />
  </head>
  <body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script
      type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"
    ></script>
    <script>
      ((r) => {
        setTimeout(r);
      })(() => {
        const { markmap, mm } = window;
        const toolbar = new markmap.Toolbar();
        toolbar.attach(mm);
        const el = toolbar.render();
        el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
        document.body.append(el);
      });
    </script>
    <script>
      ((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create(
          "svg#mindmap",
          (getOptions || markmap.deriveOptions)(jsonOptions),
          root
        );
      })(() => window.markmap, null, {
        type: "root",
        depth: 0,
        content: "",
        children: [
          {
            type: "heading",
            depth: 1,
            payload: { lines: [0, 1] },
            content:
              "<p style=\"color: blue;font-weight: bold;\">模型配置</p><span class='hidden-code' data-code='float_trainer = dict(\n    type=&amp;#39;distributed_data_parallel_trainer&amp;#39;,\n    model=`model`,\n    data_loader=`data_loader`,\n    optimizer=dict(\n        type=torch.optim.SGD,\n        params={&amp;#39;weight&amp;#39;: dict(weight_decay=weight_decay)},\n        lr=start_lr,\n        momentum=0.9,\n    ),\n    batch_processor=`batch_processor`,       hat_tool.profiler.profilers.SimpleProfiler\n    device=None,\n    num_epochs=train_epochs,\n    callbacks=[\n        `stat_callback`,\n        dict(\n            type=&amp;#39;StepDecayLrUpdater&amp;#39;,\n            lr_decay_id=[200, 240, 280],\n            lr_decay_factor=0.1,\n        ),\n        `metric_updater`,\n        `tb_callback`,\n        `aidi_tb_callback`,\n        `tb_loss_callback`,\n        `aidi_tb_loss_callback`,\n        `float_val_callback`,\n        `ckpt_callback`,\n    ],\n    sync_bn=True,\n)\n'> </span>",
            children: [
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">model</p><span class='hidden-code' data-code='model = dict(\n    type=&amp;#39;Segmentor&amp;#39;,\n    backbone=dict(\n        type=&amp;#39;MobileNetV1&amp;#39;,\n        num_classes=-1,\n        bn_kwargs=bn_kwargs,\n        alpha=alpha,\n        dw_with_relu=True,\n        include_top=False,\n        flat_output=False,\n    ),\n    neck=dict(\n        type=&amp;#39;DwUnet&amp;#39;,\n        base_channels=int(32 * alpha),\n        bn_kwargs=bn_kwargs,\n        act_type=nn.ReLU,\n        use_deconv=False,\n        dw_with_act=True,\n        output_scales=train_scales,\n    ),\n    head=dict(\n        type=&amp;#39;SegHead&amp;#39;,\n        num_classes=num_classes,\n        in_strides=train_scales,\n        out_strides=train_scales,\n        stride2channels={\n            stride: int(stride * 32 * alpha) for stride in train_scales\n        },\n        feat_channels=tuple(np.array(train_scales) * int(32 * alpha)),\n        stacked_convs=0,\n        argmax_output=False,\n        dequant_output=True,              # 表示输出是经过量化处理的 Int8 数据，适用于边缘设备推理优化。\n        int8_output=True,\n        upscale=False,                    # 是否对输出进行上采样到原图大小。\n        output_with_bn=output_with_bn,    # 输出层是否使用 BatchNorm。   True\n        bn_kwargs=bn_kwargs, \n    ),\n    losses=dict(\n        type=&amp;#39;SoftmaxFocalLoss&amp;#39;,\n        loss_name=&amp;#39;Focal&amp;#39;,\n        num_classes=num_classes,\n        weight=tuple(np.array((256, 128, 64, 32, 16)) / 19),\n        reduction=&amp;#39;mean&amp;#39;,\n    ),\n)\n'> </span>",
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">data_loader</p><span class='hidden-code' data-code='data_loader = dict(\n    type=torch.utils.data.DataLoader,\n    dataset=dict(\n        type=&amp;#39;Cityscapes&amp;#39;,\n        data_path=os.path.join(data_rootdir, &amp;#39;train_lmdb&amp;#39;),\n        transforms=[\n            dict(type=&amp;#39;PILToTensor&amp;#39;),        # 图像裁剪、翻转、颜色增强等\n        ],\n    ),\n    sampler=dict(type=torch.utils.data.DistributedSampler),\n    batch_size=batch_size_per_gpu,\n    shuffle=True,\n    num_workers=dataloader_workers,\n    pin_memory=True,\n    collate_fn=collate_2d,\n)\n'> </span>",
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">batch_processor</p><span class='hidden-code' data-code='batch_processor = dict(\n    type=&amp;#39;BasicBatchProcessor&amp;#39;,\n    need_grad_update=True,           # 表示这个 batch processor 会参与梯度更新（即用于训练阶段）。\n    batch_transforms=[               # 这是一个在 batch 层面进行的数据增强与预处理流程，在数据加载后、模型输入前进行处理。 批量归一化、缩放、one-hot、损失计算等\n        dict(type=&amp;#39;LabelRemap&amp;#39;, mapping=CITYSCAPES_LABLE_MAPPINGS),      # 将原始标签（例如 Cityscapes 中的 34 类）映射为训练使用的类别（如 19 类）。\n        dict(type=&amp;#39;SegOneHot&amp;#39;, num_classes=num_classes),         # 将语义分割标签转换为 one-hot 编码格式。输出形状为 (batch_size, num_classes, H, W)，方便后续损失计算。\n        dict(type=&amp;#39;SegResize&amp;#39;, size=data_shape[1:]),             # 将图像和标签统一缩放到指定大小（如 (512, 1024)）。\n        dict(\n            type=&amp;#39;SegRandomAffine&amp;#39;,   # 将图像和标签统一缩放到指定大小（如 (512, 1024)）。\n            degrees=0,\n            scale=(0.5, 2.0),\n            interpolation=InterpolationMode.BILINEAR,\n            label_fill_value=0,\n        ),\n        dict(type=&amp;#39;BgrToYuv444&amp;#39;, rgb_input=True),     # 将 RGB 图像转换为 YUV 格式。\n        dict(\n            type=&amp;#39;TorchVisionAdapter&amp;#39;,                # 图像归一化操作：img = (img - mean) / std\n            interface=&amp;#39;Normalize&amp;#39;,\n            mean=128.0,\n            std=128.0,\n        ),\n        dict(                              # 对输入图像进行缩放，缩放比例为 1 / train_scales。 例如：如果 train_scales = [8, 16, 32]，则 scales = [1/8, 1/16, 1/32]\n            type=&amp;#39;Scale&amp;#39;,\n            scales=tuple(1 / np.array(train_scales)),\n            mode=&amp;#39;bilinear&amp;#39;,\n        ),\n    ],\n    loss_collector=`loss_collector`,\n    enable_amp=enable_amp,                 # False\n)\n'> </span>",
                children: [
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">loss_collector</p><span class='hidden-code' data-code='def loss_collector(model_outs):\n    losses = model_outs[1][&amp;#39;Focal&amp;#39;]\n    return losses\n'> </span>",
                  },
                ],
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">stat_callback</p><span class='hidden-code' data-code='stat_callback = dict(    # 使用的是一个名为 StatsMonitor 的回调（callback）类。损失值（loss）\\准确率（accuracy）\\学习率（learning rate）\\每一步的耗时\\GPU 内存使用情况\\梯度幅值等\n    type=&amp;#39;StatsMonitor&amp;#39;,\n    log_freq=1000,\n)\n'> </span>",
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">MetricUpdater</p><span class='hidden-code' data-code='metric_updater = dict(                # 用于在训练或验证过程中动态更新评估指标（如 mIoU）的模块\n    type=&amp;#39;MetricUpdater&amp;#39;,\n    metrics=[miou_metric],            # miou_metric = MeanIOU(seg_class=[str(i) for i in range(num_classes)])\n    metric_update_func=update_metric, # 每次需要更新指标时调用的函数。这个函数负责从 batch 数据中提取真实标签和预测结果，并更新到指标对象中\n    step_log_freq=100,                # 每隔 100 步输出一次当前指标值（例如 mIoU）\n    epoch_log_freq=1,                 # 每个 epoch 结束后都会输出一次指标值\n    log_prefix=task_name,             # 日志前缀，用于标识不同任务 &amp;#39;unet_mobilenetv1_cityscapes&amp;#39;\n)\n'> </span>",
                children: [
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">update_metric</p><span class='hidden-code' data-code='def update_metric(metrics, batch, model_outs):\n    # 将每个 batch 的真实标签 target 和预测结果 preds 输入到 metric 对象中，进行内部统计（如 TP / FP / FN 等），后续可用于计算 mIoU、准确率等\n    target: Tensor = batch[&amp;#39;gt_seg&amp;#39;][0]\n    ignore_points = target.sum(dim=1) == 0    # # 找出所有类别都为 0 的像素点（即忽略区域）\n    target = target.argmax(dim=1)\n    target[ignore_points] = 255\n    preds = model_outs[0][0]\n    preds = torch.argmax(preds, dim=1, keepdim=False)\n    for metric in metrics:\n        metric.update(target, preds)\n'> </span>",
                  },
                ],
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">tb_callback</p><span class='hidden-code' data-code='tb_callback = dict(\n    type=&amp;#39;TensorBoard&amp;#39;,\n    save_dir=os.path.join(tensorboard_log_path, training_step, &amp;#39;train&amp;#39;),\n    update_freq=1,         # 每隔多少次执行一次更新（这里是每 1 次）\n    update_by=&amp;#39;epoch&amp;#39;,     # 更新频率基于“epoch”而不是“iteration”，即每个 epoch 结束后触发一次更新。\n    tb_update_funcs=[tb_update_func],  # 提供一个或多个函数，这些函数会在每次更新时被调用。这些函数负责向 TensorBoard 中写入具体的指标数据\n)\n'> </span>",
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">aidi_tb_callback</p><span class='hidden-code' data-code='aidi_tb_callback = dict(     # 可能是为了支持在 AI 平台（如 AIDI）上运行时的日志上传机制\n    type=&amp;#39;TensorBoard&amp;#39;,\n    save_dir=os.path.join(\n        os.getenv(&amp;#39;TENSORBOARD_LOG_PATH&amp;#39;)\n        or os.path.join(tensorboard_log_path, &amp;#39;.aidi&amp;#39;),\n        training_step,\n        &amp;#39;train&amp;#39;,\n    ),\n    update_freq=1,\n    update_by=&amp;#39;epoch&amp;#39;,\n    tb_update_funcs=[`tb_update_func`],\n)\n'> </span>",
                children: [
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">tb_update_func</p><span class='hidden-code' data-code='miou_metric = MeanIOU(seg_class=[str(i) for i in range(num_classes)])\ndef tb_update_func(writer, epoch_id, **kwargs):\n    name, value = miou_metric.get()\n    if isinstance(name, str):\n        writer.add_scalar(name, value, global_step=epoch_id)\n    else:\n        for k, v in zip(name, value):\n            writer.add_scalar(k, v, global_step=epoch_id)\n'> </span>",
                  },
                ],
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">tb_loss_callback</p><span class='hidden-code' data-code='tb_loss_callback = dict(\n    type=&amp;#39;TensorBoard&amp;#39;,\n    save_dir=os.path.join(tensorboard_log_path, training_step, &amp;#39;loss&amp;#39;),\n    loss_name_reg=&amp;#39;^.*Focal.*&amp;#39;,\n    update_freq=100,\n    update_by=&amp;#39;step&amp;#39;,\n)\n'> </span>",
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">aidi_tb_loss_callback</p><span class='hidden-code' data-code='aidi_tb_callback = dict(\n    type=&amp;#39;TensorBoard&amp;#39;,\n    save_dir=os.path.join(\n        os.getenv(&amp;#39;TENSORBOARD_LOG_PATH&amp;#39;)\n        or os.path.join(tensorboard_log_path, &amp;#39;.aidi&amp;#39;),\n        training_step,\n        &amp;#39;train&amp;#39;,\n    ),\n    update_freq=1,\n    update_by=&amp;#39;epoch&amp;#39;,\n    tb_update_funcs=[`tb_update_func`],\n)\n'> </span>",
                children: [
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">tb_update_func</p><span class='hidden-code' data-code='def tb_update_func(writer, epoch_id, **kwargs):\n    name, value = miou_metric.get()\n    if isinstance(name, str):\n        writer.add_scalar(name, value, global_step=epoch_id)\n    else:\n        for k, v in zip(name, value):\n            writer.add_scalar(k, v, global_step=epoch_id)\n'> </span>",
                  },
                ],
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">float_val_callback</p><span class='hidden-code' data-code='float_val_callback = dict(\n    type=&amp;#39;Validation&amp;#39;,\n    data_loader=val_data_loader,\n    batch_processor=val_batch_processor,\n    callbacks=[val_metric_updater, val_tb_callback, val_aidi_tb_callback],\n    val_model=deploy_model,\n)\n'> </span>",
              },
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">ckpt_callback</p><span class='hidden-code' data-code='ckpt_callback = dict(\n    type=&amp;#39;Checkpoint&amp;#39;,\n    save_dir=ckpt_dir,\n    name_prefix=training_step + &amp;#39;-&amp;#39;,\n    save_interval=1,\n    strict_match=True,\n    mode=&amp;#39;max&amp;#39;,\n    best_refer_metric=val_miou_metric,\n)\n'> </span>",
              },
            ],
          },
          {
            type: "heading",
            depth: 1,
            payload: { lines: [0, 1] },
            content:
              "<p style=\"color: blue;font-weight: bold;\">模型训练train.py</p><span class='hidden-code' data-code='def train_entrance():\n    4. build and run trainer\n    with DisableLogger(disable_logger, level), RegistryContext():\n        trainer = getattr(cfg, f&amp;#39;{stage}_trainer&amp;#39;)      {&amp;#39;type&amp;#39;: &amp;#39;distributed_data_par...el_trainer&amp;#39;, &amp;#39;model&amp;#39;: {&amp;#39;type&amp;#39;: &amp;#39;Segmentor&amp;#39;, &amp;#39;backbone&amp;#39;:\n        trainer[&amp;#39;device&amp;#39;] = device                      2\n        trainer = build_from_registry(trainer)          `<`hat_tool.engine.ddp_trainer.DistributedDataParallelTrainer`>`\n        trainer.`fit`()\n'> </span>",
            children: [
              {
                type: "heading",
                depth: 2,
                payload: { lines: [0, 1] },
                content:
                  "<p style=\"color: blue;font-weight: bold;\">hat_tool/engine/loop_base.py</p>DistributedDataParallelTrainer-->Trainer-->LoopBase<br>\n<span class='hidden-code' data-code='class LoopBase(PipeBase):\n    # local vars\n    epoch_id = self.start_epoch            # 0\n    global_step_id = self.start_step       # 0\n    end_loop_flag = self._skip_loop        # False\n    # TODO(linkai.liang, 0.5), not pass LoopBase to callback, not do resume in `Checkpoint` callback\n    self.`on_loop_begin`(\n        model=self.model,              # \n        optimizer=self.optimizer,\n        data_loader=self.data_loader,\n        num_epochs=self.num_epochs,\n        num_steps=self.num_steps,\n        loop=self,\n        train_metrics=self.train_metrics,      # [None]\n        val_metrics=self.val_metrics,          # [None]\n        storage=self.storage,                  # hat_tool.core.event.EventStorage\n        profiler=self.profiler,                # hat_tool.profiler.profilers.SimpleProfiler\n    )\n    while not end_loop_flag:\n        self.data_loader_pr = self.profiler.profile_iterable(enumerate(`prefetch_iterator`(self.data_loader)),f&amp;#39;get_{self.name}_batch_data&amp;#39;,)\n        self.`on_epoch_begin`(\n            model=self.model,\n            epoch_id=epoch_id,\n            optimizer=self.optimizer,\n            global_step_id=global_step_id,\n            train_metrics=self.train_metrics,\n            val_metrics=self.val_metrics,\n            storage=self.storage,\n            data_loader=self.data_loader,\n            profiler=self.profiler,\n        )\n        step_id = 0\n        while True:\n            if hasattr(self.profiler, &amp;#39;step&amp;#39;):\n                self.profiler.step()\n            self.`on_step_begin`(\n                model=self.model,\n                optimizer=self.optimizer,\n                epoch_id=epoch_id,\n                step_id=step_id,\n                data_loader=self.data_loader,\n                start_epoch=self.start_epoch,\n                start_step=self.start_step,\n                global_step_id=global_step_id,\n                train_metrics=self.train_metrics,\n                val_metrics=self.val_metrics,\n                storage=self.storage,\n                profiler=self.profiler,\n            )\n            try:\n                _, batch = next(self.data_loader_pr)\n            except StopIteration:\n                break\n            if dist_initialized():\n                with self.profiler.profile(&amp;#39;dataloader_distributed_action_barrier&amp;#39;):\n                    torch.distributed.barrier()\n            if self.log_interval > 0 and step_id % self.log_interval == 0:\n                logger.info(f&amp;#39;{step_id} / {get_dataloader_length(self.data_loader)}&amp;#39;)\n            # <hat_tool.engine.processors.processor.BasicBatchProcessor\n            self.`batch_processor`(step_id,batch,self.model,self.device,optimizer=self.optimizer,storage=self.storage,\n                    batch_begin_callback=partial(\n                        self.`on_batch_begin`,global_step_id=global_step_id,step_id=step_id,epoch_id=epoch_id,train_metrics=self.train_metrics,val_metrics=self.val_metrics,storage=self.storage,\n                    ),\n                    batch_end_callback=partial(\n                        self.`on_batch_end`,global_step_id=global_step_id,step_id=step_id,epoch_id=epoch_id,train_metrics=self.train_metrics,val_metrics=self.val_metrics,storage=self.storage,\n                    ),\n                    backward_begin_callback=partial(\n                        self.`on_backward_begin`,model=self.model,optimizer=self.optimizer,global_step_id=global_step_id,step_id=step_id,epoch_id=epoch_id,\n                    ),\n                    backward_end_callback=partial(\n                        self.`on_backward_end`,model=self.model,optimizer=self.optimizer,global_step_id=global_step_id,step_id=step_id,epoch_id=epoch_id,\n                    ),\n                    optimizer_step_begin_callback=partial(\n                        self.`on_optimizer_step_begin`,model=self.model,optimizer=self.optimizer,epoch_id=epoch_id,step_id=step_id,global_step_id=global_step_id,\n                    ),\n                    optimizer_step_end_callback=partial(\n                        self.`on_optimizer_step_end`,model=self.model,optimizer=self.optimizer,epoch_id=epoch_id,step_id=step_id,global_step_id=global_step_id,\n                    ),\n                    profiler=self.profiler,\n                    forward_begin_callback=partial(\n                        self.`on_forward_begin`,global_step_id=global_step_id,step_id=step_id,epoch_id=epoch_id,\n                    ),\n                    forward_end_callback=partial(\n                        self.`on_forward_end`,global_step_id=global_step_id,step_id=step_id,epoch_id=epoch_id,\n                    ),\n                )\n            self.`on_step_end`(\n                    epoch_id=epoch_id,\n                    step_id=step_id,\n                    global_step_id=global_step_id,\n                    data_loader=self.data_loader,\n                    model=self.model,\n                    ema_model=self.ema_model,\n                    optimizer=self.optimizer,\n                    num_steps=self.num_steps,\n                    device=self.device,\n                    callbacks=self.callbacks,\n                    train_metrics=self.train_metrics,\n                    val_metrics=self.val_metrics,\n                    storage=self.storage,\n                    profiler=self.profiler,\n                )\n                step_id += 1\n                global_step_id += 1\n                if self._stop_by_step and global_step_id >= self.num_steps:\n                    end_loop_flag = True\n                    break\n            self.`on_epoch_end`(\n                epoch_id=epoch_id,\n                global_step_id=global_step_id,\n                model=self.model,\n                ema_model=self.ema_model,\n                optimizer=self.optimizer,\n                num_epochs=self.num_epochs,\n                device=self.device,\n                callbacks=self.callbacks,\n                train_metrics=self.train_metrics,\n                val_metrics=self.val_metrics,\n                storage=self.storage,\n                profiler=self.profiler,\n            )\n            epoch_id += 1\n            if self._stop_by_epoch and epoch_id >= self.num_epochs:\n                end_loop_flag = True\n        self.`on_loop_end`(\n            model=self.model,\n            ema_model=self.ema_model,\n            optimizer=self.optimizer,\n            epoch_id=epoch_id,\n            global_step_id=global_step_id,\n            device=self.device,\n            train_metrics=self.train_metrics,\n            val_metrics=self.val_metrics,\n            callbacks=self.callbacks,\n            storage=self.storage,\n            profiler=self.profiler,\n        )\n        if deterministic_level() == 2:\n            deterministic_summary()\n        self.profiler.describe()\n        self.profiler.teardown()\n        if dist_initialized():\n            torch.distributed.barrier()\n'> </span>",
                children: [
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">hat_tool/engine/trainer.py</p><span class='hidden-code' data-code='class Trainer(LoopBase):  # noqa: D205,D400\n    def on_loop_begin(self, **kwargs):\n        self.model.train()\n        super(Trainer, self).`on_loop_begin`(**kwargs)\n'> </span>",
                    children: [
                      {
                        type: "heading",
                        depth: 4,
                        payload: { lines: [0, 1] },
                        content:
                          "<p style=\"color: blue;font-weight: bold;\">hat_tool/engine/loop_base.py</p><span class='hidden-code' data-code='class PipeBase(ABC):\n    def on_loop_begin(self, **kwargs):\n        with self.profiler.profile(f&amp;#39;on_{self.name}_loop_begin&amp;#39;):\n            for cb in self.callbacks:\n                cb.on_loop_begin(**kwargs)\n'> </span>",
                        children: [
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/monitor.py</p><span class='hidden-code' data-code='class StatsMonitor(CallbackMixin):  # noqa: D205,D400\n    def on_loop_begin(self, num_epochs, num_steps, **kwargs):\n        if num_epochs == 0 and num_steps == 0:\n            logger.warn(&amp;#39;num_epochs and num_steps are both 0, make sure you know what you are doing.&amp;#39;)\n        self.num_epochs = num_epochs         # 300\n        self.num_steps = num_steps           # None\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/lr_updater.py</p><span class='hidden-code' data-code='class LrUpdaterBase(CallbackMixin):\n    # Prepare some vars for lr updater\n    def on_loop_begin(self, optimizer, data_loader, **kwargs):\n        # 1. init warmup_steps\n        if has_len_func(data_loader):\n            self.step_per_epoch = get_dataloader_length(data_loader)      # 1488\n            assert (self.step_per_epoch != float(&amp;#39;inf&amp;#39;) and self.step_per_epoch is not None), err_msg\n            if self.warmup_by == &amp;#39;epoch&amp;#39;:\n                self.warmup_steps = self.warmup_len * self.step_per_epoch   # 0*1488=0\n            else:\n                self.warmup_epochs = ceil(self.warmup_len / self.step_per_epoch)\n        else:\n            assert (self.warmup_by == &amp;#39;step&amp;#39; and self.update_by == &amp;#39;step&amp;#39;), err_msg\n        # 2. backup initial lr of optimizer  when resuming from a checkpoint, if &amp;#39;initial_lr&amp;#39; is not saved, it will be set according to the optimizer params\n        if optimizer is not None:\n            for group in optimizer.param_groups:\n                group.setdefault(&amp;#39;initial_lr&amp;#39;, group[&amp;#39;lr&amp;#39;])\n            self._per_group_init_lr = [group[&amp;#39;initial_lr&amp;#39;] for group in optimizer.param_groups] # 将每个参数组的初始学习率保存到 self._per_group_init_lr 中，供后续使用（比如学习率调度）\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/lr_updater.py</p><span class='hidden-code' data-code='class LrUpdaterBase(CallbackMixin):\n    # Prepare some vars for lr updater\n    def on_loop_begin(self, optimizer, data_loader, **kwargs):     \n        # 1. init warmup_steps\n        if has_len_func(data_loader):\n            self.step_per_epoch = get_dataloader_length(data_loader)      # 1488\n            assert (self.step_per_epoch != float(&amp;#39;inf&amp;#39;) and self.step_per_epoch is not None), err_msg\n            if self.warmup_by == &amp;#39;epoch&amp;#39;:\n                self.warmup_steps = self.warmup_len * self.step_per_epoch   # 0*1488=0\n            else:\n                self.warmup_epochs = ceil(self.warmup_len / self.step_per_epoch)\n        else:\n            assert (self.warmup_by == &amp;#39;step&amp;#39; and self.update_by == &amp;#39;step&amp;#39;), err_msg\n        # 2. backup initial lr of optimizer  when resuming from a checkpoint, if &amp;#39;initial_lr&amp;#39; is not saved, it will be set according to the optimizer params\n        if optimizer is not None:\n            for group in optimizer.param_groups:\n                group.setdefault(&amp;#39;initial_lr&amp;#39;, group[&amp;#39;lr&amp;#39;])\n            self._per_group_init_lr = [group[&amp;#39;initial_lr&amp;#39;] for group in optimizer.param_groups] # 将每个参数组的初始学习率保存到 self._per_group_init_lr 中，供后续使用（比如学习率调度）\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/metric_updater.py</p><span class='hidden-code' data-code='class MetricUpdater(CallbackMixin):\n    def on_loop_begin(self, loop, storage: EventStorage, **kwargs):\n        if self.metrics is not None:     # [MeanIOU()]\n            for m in self.metrics:\n                m.to(loop.device)\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/validation.py</p><span class='hidden-code' data-code='class Validation(CallbackMixin):  # noqa: D400:\n    def on_loop_begin(self, loop, storage: EventStorage, **kwargs):\n        pass\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/checkpoint.py</p><span class='hidden-code' data-code='class Checkpoint(CallbackMixin):  # noqa: D205,D400\n    def on_loop_begin(self, loop, **kwargs):\n        if self.best_refer_metric is not None:    # [MeanIOU()]\n            for m in self.best_refer_metric:\n                m.to(loop.device)\n'> </span>",
                          },
                        ],
                      },
                    ],
                  },
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">hat_tool/utils/generator.py</p><span class='hidden-code' data-code='def prefetch_iterator(iterable: Iterable,) -> Generator[Tuple[Any, bool], None, None]:\n    it = iter(iterable)\n    try:\n        last = next(it)        # # the iterator may be empty from the beginning\n    except StopIteration:\n        return\n    yield last\n    for val in it:\n        last = val\n        yield last\n'> </span>",
                  },
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">hat_tool/engine/loop_base.py</p><span class='hidden-code' data-code='class LoopBase(PipeBase):                            # noqa: D205,D400\n    def on_epoch_begin(self, epoch_id, **kwargs):\n        if hasattr(self.data_loader, &amp;#39;sampler&amp;#39;):\n            sampler = self.data_loader.sampler       # hat_tool.data.samplers.dist_sampler.DistSamplerHook\n            if isinstance(sampler, dict):\n                sampler = sampler.values()\n            else:\n                sampler = _as_list(sampler)          # [`<`hat_tool.data.samplers.dist_sampler.DistSamplerHook`>`]\n            for sa in sampler:\n                if isinstance(sa, DistributedSampler):\n                    sa.set_epoch(epoch_id)\n        if self.device is not None:            # need to set device again in case device has been changed before epoch begins # noqa\n            self.set_device(self.device)\n        super(`LoopBase`, self).`on_epoch_begin`(epoch_id=epoch_id, **kwargs)\n'> </span>",
                    children: [
                      {
                        type: "heading",
                        depth: 4,
                        payload: { lines: [0, 1] },
                        content:
                          "<p style=\"color: blue;font-weight: bold;\">hat_tool/engine/loop_base.py</p><span class='hidden-code' data-code='class PipeBase(ABC):\n    def on_epoch_begin(self, **kwargs):\n        with self.profiler.profile(f&amp;#39;on_{self.name}_epoch_begin&amp;#39;):\n            for cb in self.callbacks:\n                cb.on_epoch_begin(**kwargs)\n'> </span>",
                        children: [
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/monitor.py</p><span class='hidden-code' data-code='class StatsMonitor(CallbackMixin):  # noqa: D205,D400\n    def on_epoch_begin(self, epoch_id, **kwargs):\n        self.epoch_time_begin = time.time()\n        self.epoch_time.reset()\n        self.step_time.reset()\n        self.begin_step = 0\n        self.step_time_begin = None\n        logger.info(&amp;#39;Epoch[%d] Begin &amp;#39; % epoch_id + &amp;#39;=&amp;#39; * 50)\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/lr_updater.py</p><span class='hidden-code' data-code='class LrUpdaterBase(CallbackMixin):\n    def on_epoch_begin(self, optimizer, epoch_id, global_step_id, **kwargs):     # Update formal training lr on each epoch begin if update by &amp;#39;epoch&amp;#39;.\n        assert self.warmup_steps is not None\n        if self.update_by == &amp;#39;epoch&amp;#39; and global_step_id >= self.warmup_steps:    # set lr for each param group among formal training\n            self.set_formal_training_lr(optimizer, num_update=epoch_id)\n        self.log_lr(optimizer,epoch_id=epoch_id,step_id=0,global_step_id=global_step_id,)\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/metric_updater.py</p><span class='hidden-code' data-code='class MetricUpdater(CallbackMixin):\n    def on_epoch_begin(self, train_metrics, **kwargs):\n        metrics = train_metrics if self.metrics is None else self.metrics\n        self.`_reset_metrics`(metrics)              \n'> </span>",
                            children: [
                              {
                                type: "heading",
                                depth: 6,
                                payload: { lines: [0, 1] },
                                content:
                                  "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/metric_updater.py</p><span class='hidden-code' data-code='class MetricUpdater(CallbackMixin):\n    def _reset_metrics(self, metrics):\n        assert metrics is not None\n        for m in metrics:\n            m.reset()          \n'> </span>",
                              },
                            ],
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/validation.py</p><span class='hidden-code' data-code='class Validation(CallbackMixin):  # noqa: D400:\n    pass\n'> </span>",
                          },
                          {
                            type: "heading",
                            depth: 5,
                            payload: { lines: [0, 1] },
                            content:
                              "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/checkpoint.py</p><span class='hidden-code' data-code='class Checkpoint(CallbackMixin):  # noqa: D205,D400\n    pass\n'> </span>",
                          },
                        ],
                      },
                    ],
                  },
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">hat_tool/engine/loop_base.py</p><span class='hidden-code' data-code='class PipeBase(ABC):\n    def on_step_begin(self, **kwargs):\n        with self.profiler.profile(f&amp;#39;on_{self.name}_step_begin&amp;#39;):\n            for cb in self.callbacks:\n                cb.on_step_begin(**kwargs)\n'> </span>",
                    children: [
                      {
                        type: "heading",
                        depth: 4,
                        payload: { lines: [0, 1] },
                        content:
                          "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/monitor.py</p><span class='hidden-code' data-code='class StatsMonitor(CallbackMixin):          # noqa: D205,D400\n    def on_step_begin(self,epoch_id,step_id,global_step_id,data_loader=None,profiler=None,**kwargs):\n        if self.step_time_begin:            # None\n            step_time = time.time() - self.step_time_begin\n            self.step_time_begin = time.time()\n            self.step_time.update(step_time)\n            should_log = (step_id + 1) % self.log_freq == 0\n            if not should_log:\n                return\n            msg = &amp;#39;Epoch[%d] Step[%d-%d] Cost Time: %.3fs&amp;#39; % (epoch_id,self.begin_step,step_id,self.step_time.sum,)\n            if data_loader is not None:\n                speed = self._estimate_speed(data_loader, self.step_time.sum)\n                msg += f&amp;#39; Speed: {speed:.2f} samples/sec&amp;#39;\n                (remain_training_time,remain_step_percent,) = self._estimate_remain_time_and_step(data_loader,epoch_id,step_id,global_step_id,)\n                if remain_training_time >= 0:\n                    remain_training_time = str(datetime.timedelta(seconds=remain_training_time))\n                    msg += f&amp;#39; Remaining Time: {remain_training_time}&amp;#39;\n                if remain_step_percent > 0:\n                    msg += (&amp;#39; Remaining step percent&amp;#39;+ f&amp;#39;: {remain_step_percent * 100:.2f}%&amp;#39;)\n            if (self.log_profiler and profiler is not None and hasattr(profiler, &amp;#39;recorded_durations&amp;#39;)):\n                # only support simple profile\n                recorded_durations = profiler.recorded_durations\n                report = []\n                for a, d in recorded_durations.items():\n                    report.append([a, np.sum(d)])\n                report.sort(key=lambda x: x[1], reverse=True)\n                msg += (f&amp;#39; Most time cost op: {report[0][0]}({report[0][1]:.2f}s);&amp;#39;)\n                msg += f&amp;#39; {report[1][0]}({report[1][1]:.2f}s);&amp;#39;\n                msg += f&amp;#39; {report[2][0]}({report[2][1]:.2f}s);&amp;#39;\n            logger.info(msg)\n            self.step_time.reset()\n            self.begin_step = step_id + 1\n        self.step_time_begin = time.time()\n'> </span>",
                      },
                      {
                        type: "heading",
                        depth: 4,
                        payload: { lines: [0, 1] },
                        content:
                          "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/lr_updater.py</p><span class='hidden-code' data-code='class LrUpdaterBase(CallbackMixin):\n    def on_step_begin(self, optimizer, epoch_id, step_id, global_step_id, **kwargs):\n        # On each step begin, update warmup lr or formal training lr (if update by &amp;#39;step&amp;#39;)\n        assert self.warmup_steps is not None\n        if global_step_id < self.warmup_steps:\n            self.set_warmup_training_lr(optimizer, num_update=global_step_id)\n            if (global_step_id + 1) % self.step_log_interval == 0:\n                self.log_lr(optimizer,epoch_id=epoch_id,step_id=step_id,global_step_id=global_step_id,)\n        elif self.update_by == &amp;#39;step&amp;#39;:\n            # set lr for each param group among formal training\n            self.set_formal_training_lr(optimizer, num_update=global_step_id)\n            if (global_step_id + 1) % self.step_log_interval == 0:\n                self.log_lr(optimizer,epoch_id=epoch_id,step_id=step_id,global_step_id=global_step_id,)\n'> </span>",
                      },
                      {
                        type: "heading",
                        depth: 4,
                        payload: { lines: [0, 1] },
                        content:
                          "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/metric_updater.py</p><span class='hidden-code' data-code='class MetricUpdater(CallbackMixin):\n    pass\n'> </span>",
                      },
                      {
                        type: "heading",
                        depth: 4,
                        payload: { lines: [0, 1] },
                        content:
                          "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/validation.py</p><span class='hidden-code' data-code='class Validation(CallbackMixin):  # noqa: D400:\n    pass\n'> </span>",
                      },
                      {
                        type: "heading",
                        depth: 4,
                        payload: { lines: [0, 1] },
                        content:
                          "<p style=\"color: blue;font-weight: bold;\">hat_tool/callbacks/checkpoint.py</p><span class='hidden-code' data-code='class Checkpoint(CallbackMixin):  # noqa: D205,D400\n    pass\n'> </span>",
                      },
                    ],
                  },
                  {
                    type: "heading",
                    depth: 3,
                    payload: { lines: [0, 1] },
                    content:
                      "<p style=\"color: blue;font-weight: bold;\">hat_tool/engine/processors/processor.py</p><span class='hidden-code' data-code='class BasicBatchProcessor(BatchProcessorMixin):  # noqa: D205,D400\n    @maybe_cast_to_deterministic\n    def __call__(self,step_id,batch,model,device,optimizer=None,storage,batch_begin_callback,batch_end_callback,backward_begin_callback,\n        backward_end_callback,optimizer_step_begin_callback,optimizer_step_end_callback,forward_begin_callback,orward_end_callback,profiler,):   # for torch.compile\n        if OptimizedModule is not None and isinstance(model, OptimizedModule):\n            model_training = model.module.training\n        else:\n            model_training = model.training          # True\n        assert self.need_grad_update == model_training, ( &amp;#39;%s vs. %s, set model to training/eval mode by model.train()/model.eval() when need_grad_update or not&amp;#39;% (self.need_grad_update, modeltraining))\n        if self.use_deepspeed is None and deepspeed is not None:       # None,None\n            self.use_deepspeed = isinstance(model, (deepspeed.DeepSpeedEngine, deepspeed.InferenceEngine))\n            if self.use_deepspeed:\n                self.deepspeed_config_check()\n        if batch_begin_callback is not None:\n            `batch_begin_callback`(batch=batch)\n        if profiler is None:\n            profiler = PassThroughProfiler()\n        # 0. reset grad\n        if (not self.use_deepspeed and self.need_grad_update and step_id % self.ga_step == 0):\n            with profiler.profile(&amp;#39;optimizer_zero_grad&amp;#39;):\n                optimizer.zero_grad(set_to_none=True)\n        if device is not None:\n            batch = to_cuda(batch, device, non_blocking=True)\n        else:\n            # run on cpu\n            pass\n        if self.transforms is not None:\n            with profiler.profile(&amp;#39;batch_transforms&amp;#39;):\n                batch = self.transforms(batch)\n        # 1. forward\n        if forward_begin_callback is not None:\n            forward_begin_callback(batch=batch, model=model)\n        if self.enable_channels_last:\n            batch = convert_memory_format(batch, self.channels_last_keys, torch.channels_last)\n        if (step_id + 1) % self.ga_step != 0 and hasattr(model, &amp;#39;no_sync&amp;#39;):\n            ddp_model_sync = model.no_sync\n        else:\n            ddp_model_sync = localcontext\n        with ddp_model_sync():\n            grad_decorator = (torch.enable_grad if self.need_grad_update else torch.no_grad)\n            if not self.enable_apex:\n                auto_cast = autocast(enabled=self.enable_amp, dtype=self.enable_amp_dtype)\n            else:\n                auto_cast = localcontext()\n            with profiler.profile(&amp;#39;model_forward&amp;#39;):\n                with auto_cast:\n                    with grad_decorator():\n                        if (step_id + 1) % self.ga_step != 0:                      # Only work while using apex.\n                            if hasattr(model, &amp;#39;disable_allreduce&amp;#39;):\n                                model.disable_all_reduce()\n                        else:\n                            if hasattr(model, &amp;#39;enable_allreduce&amp;#39;):\n                                model.enable_allreduce()\n                        # model outputs can be in any format\n                        model_outs = model(*_as_list(batch))\n            if self.inverse_transforms is not None:\n                model_outs = self.inverse_transforms(model_outs, batch)\n            if forward_end_callback is not None:\n                forward_end_callback(model_outs=model_outs)\n            # 2. filter out loss Tensors in model outputs\n            if self.loss_collector is not None:\n                losses = self.loss_collector(model_outs)\n            else:\n                losses = None\n            # 2. backward &amp; step\n            if self.need_grad_update:\n                # Not allow to backward each loss independently, so sum them\n                loss = sum([loss for loss in _as_list(losses) if loss is not None])\n                assert isinstance(loss, torch.Tensor), type(loss)\n                # mean of grad accumulation step\n                loss_scalar = loss.sum() / self.ga_step\n                # when grad_scaler is not enable, equivalent to loss.backward()\n                with profiler.profile(&amp;#39;model_backward&amp;#39;):\n                    if backward_begin_callback:\n                        backward_begin_callback()\n                    # backward\n                    self.backward(loss=loss_scalar,\n                        optimizer=optimizer,\n                        model=model,\n                        step_id=step_id,\n                    )\n                    if backward_end_callback:\n                        backward_end_callback(\n                            batch=batch,\n                            grad_scaler=self.grad_scaler,\n                            non_finite_loss=self.non_finite_loss,\n                        )\n                if (step_id + 1) % self.ga_step == 0:\n                    # when grad_scaler is not enable, equivalent to optimizer.step() # noqa E501\n                    with profiler.profile(&amp;#39;optimizer_step&amp;#39;):\n                        if optimizer_step_begin_callback is not None:\n                            optimizer_step_begin_callback(\n                                grad_scaler=self.grad_scaler\n                            )\n                        # optimizer step\n                        self.step(optimizer=optimizer, model=model)\n                        if optimizer_step_end_callback is not None:\n                            optimizer_step_end_callback(\n                                non_finite_grad=self.non_finite_grad\n                            )\n            if batch_end_callback is not None:\n                batch_end_callback(\n                    batch=batch,\n                    losses=losses,\n                    model_outs=model_outs,\n                )\n            if self.enable_amp:\n                storage.put(\n                    &amp;#39;grad_scaler&amp;#39;,\n                    self.grad_scaler.state_dict(),\n                    always_dict=True,\n                )\n'> </span>",
                  },
                ],
              },
            ],
          },
        ],
      });
    </script>
    <script src="https://study1994.github.io/study_html/npm/myjs/tooltip.js"></script>
  </body>
</html>
