<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>00_QAT快速上手</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型改造</p>在模型输入前插入QuantStub<br>\n在模型输出后插入DequantStub<br>\n模型上板时的输入图像数据一般为centered_yuv444格式<br>\n<span class=\'hidden-code\' data-code=\'对浮点模型做必要的改造\nclass QATReadyMobileNetV2(MobileNetV2):\n    def __init__(...):\n        super().__init__(...)\n        self.quant = QuantStub()        from horizon_plugin_pytorch.quantization import QuantStub,prepare,set_fake_quantize,FakeQuantState\n        self.dequant = DeQuantStub()    from torch.quantization import DeQuantStub\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.quant(x)\n        x = super().forward(x)\n        x = self.dequant(x)\n        return x\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">多输入多输出示例</p><span class=\'hidden-code\' data-code=\'class Net(torch.nn.Module):\n    def __init__(self):\n        self.quantx = QuantStub()\n        self.quanty = QuantStub()\n        self.dequant = DequantStub()\n        ...\n    def forward(self, x, y):\n        x = self.quantx(x)\n        y = self.quanty(y)\n        ...\n        ret_0 = self.dequant(ret_0)\n        ret_1 = self.dequant(ret_1)\n        return ret_0, ret_1\n\'> </span>'}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">Calibration</p>通过在模型中插入Observer的方式，在forward过程中<br>\n统计各处的数据分布情况，从而计算出合理的量化参数,<br>\n执行Calibration过程(不需要 backward)<br>\nset_march+prepare+set_fake_quantize;<br>\n<span class=\'hidden-code\' data-code=\'march = March.NASH_E                                       4.目标硬件平台的代号                             from horizon_plugin_pytorch.march import March, set_march\nexample_input = torch.rand(1, 3, 32, 32, device=device)    5.模型trace和export使用的输入\nset_march(march)                                           在进行模型转化前，必须设置好模型将要执行的硬件平台\n                                                           将模型转化为 Calibration 状态，以统计各处数据的数值分布特征\ncalib_model = prepare(float_model, example_input, default_calibration_qconfig_setter)    from horizon_plugin_pytorch.quantization import QuantStub,prepare,set_fake_quantize,FakeQuantState\n                                                           准备数据集\ncalib_data_loader, eval_data_loader = prepare_data_loaders(data_path, calib_batch_size, eval_batch_size)\n                                                           注意此处对模型状态的控制，模型需要处于 eval 状态以使 Bn 的行为符合要求\ncalib_model.eval()\nset_fake_quantize(calib_model, FakeQuantState.CALIBRATION)\nwith torch.no_grad():\n    cnt = 0\n    for image, target in calib_data_loader:\n        image, target = image.to(device), target.to(device)\n        calib_model(image)\n        print(’.’, end=’’, flush=True)\n        cnt += image.size(0)\n        if cnt >= num_examples:\n            break\n测试伪量化精度;注意此处对模型状态的控制\ncalib_model.eval()\nset_fake_quantize(calib_model, FakeQuantState.VALIDATION)\ntop1, top5 = evaluate(calib_model,eval_data_loader,device)\nprint(’Calibration: evaluation Acc@1 {:.3f} Acc@5 {:.3f}’.format(top1.avg, top5.avg))\n保存 Calibration 模型参数\ntorch.save(calib_model.state_dict(),os.path.join(model_path, ’calib-checkpoint.ckpt’),)\n\'> </span>'}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">量化感知训练</p>通过在模型中插入伪量化节点的方式，在训练过程中使模型感知到量化带来的影响，<br>\n在这种情况下对模型参数进行微调，以提升量化后的精度<br>\nprepare+set_fake_quantize<br>\n<span class=\'hidden-code\' data-code=\'train_batch_size = 256\neval_batch_size = 256\nepoch_num = 3\ntrain_data_loader, eval_data_loader = prepare_data_loaders(data_path, train_batch_size, eval_batch_size)\n将模型转为 QAT 状态\nqat_model = prepare(float_model, example_input, default_qat_qconfig_setter)\n加载 Calibration 模型中的量化参数\nqat_model.load_state_dict(calib_model.state_dict())\n进行量化感知训练----作为一个 filetune 过程，量化感知训练一般需要设定较小的学习率\noptimizer = torch.optim.Adam(qat_model.parameters(), lr=1e-3, weight_decay=1e-4)\nbest_acc = 0\nfor nepoch in range(epoch_num):\n    注意此处对 QAT 模型 training 状态的控制方法\n    qat_model.train()\n    set_fake_quantize(qat_model, FakeQuantState.QAT)\n    train_one_epoch(qat_model,nn.CrossEntropyLoss(),optimizer,None,train_data_loader,device)\n    注意此处对 QAT 模型 eval 状态的控制方法\n    qat_model.eval()\n    set_fake_quantize(qat_model, FakeQuantState.VALIDATION)\n    top1, top5 = evaluate(qat_model,eval_data_loader,device)\n    print(’QAT Epoch {}: evaluation Acc@1 {:.3f} Acc@5 {:.3f}’.format(nepoch, top1.avg, top5.avg))\n    if top1.avg > best_acc:\n        best_acc = top1.avg\n        torch.save(qat_model.state_dict(),os.path.join(model_path, ’qat-checkpoint.ckpt’))\n\'> </span>'}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型部署</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">导出HBIR(Horizon Basic Intermediate Representation)模型(export)</p><span class=\'hidden-code\' data-code=\'# 可根据需要修改以下参数\n# 1. 使用哪个模型作为流程的输入，可以选择 calib_model 或 qat_model\nbase_model = qat_model\nfrom horizon_plugin_pytorch.quantization.hbdk4 import export\nqat_model.eval()\nset_fake_quantize(qat_model, FakeQuantState.VALIDATION)\nhbir_qat_model = export(base_model, (example_input,))\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">batch_size</p>\n<p>模型导出时使用的<code>example_input</code>的<code>batch_size</code>决定了模型仿真和模型上板时的batch_size，<br>\n若需要在仿真和上板使用不同的batch_size，请使用不同的数据分别导出HBIR模型<br>\n测试：直接进行模型部署的流程，以保证模型中不存在无法导出或编译的操作<br></p>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">与torch 模型区别</p>1.torch模型中的大部分非线性elementwise算子，在hbir中转为查表的实现方式。<br>\n2.存在累加计算的算子如reduce_sum, gemm等，会由于累加的顺序不同导致数值波动。<br>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">转定点模型(convert)</p>HBIR模型-->定点模型<br>\n输入仅支持单个Tensor或Tuple[Tensor], 输出仅支持Tuple[Tensor]<br>\n<span class=\'hidden-code\' data-code=\'# 将模型转为定点状态，注意此处的march需要区分 nash-e/m/p\nhbir_quantized_model = hb4.convert(hbir_qat_model,’nash-e’,)\n# hbir 精度测试使用的 dataloader，注意此处的 batch_size 必须和 export hbir 时使用的 example_input 相同\n_, eval_hbir_data_loader = prepare_data_loaders(data_path, train_batch_size, 1)\ndef evaluate_hbir(model: hb4.Module, data_loader: data.DataLoader) -> Tuple[AverageMeter, AverageMeter]:\n    top1 = AverageMeter(’Acc@1’, ’:6.2f’)\n    top5 = AverageMeter(’Acc@5’, ’:6.2f’)\n    for image, target in data_loader:\n        image, target = image.cpu(), target.cpu()\n        # 默认输入输出名字为 _input_{n}, _output_{n} 的形式，可以在 export 时\n        # 通过参数自定义\n        output = model[’forward’].feed({’_input_0’: image})[’_output_0’]\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        top1.update(acc1, image.size(0))\n        top5.update(acc5, image.size(0))\n    return top1, top5\n# 测试定点模型精度\ntop1, top5 = evaluate_hbir(hbir_quantized_model,eval_hbir_data_loader,)\nprint(’Quantized model: evaluation Acc@1 {:.3f} Acc@5 {:.3f}’.format(top1.avg, top5.avg))\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型修改</p>把图像解码、色彩转换、ROI 裁剪、归一化等预处理全部下沉到BPU，减少CPU负担，实现低延迟、高能效的端侧推理<br>\n<span class=\'hidden-code\' data-code=\'原始模型 (PyTorch/NCHW/FP32)\n       ↓ export\nHBIR 模型 (浮点，保留结构)\n       ↓ insert_transpose() → 转 NHWC\n       ↓ insert_image_convert() → NV12→RGB\n       ↓ insert_roi_resize() → 抠图+缩放\n       ↓ insert_image_preprocess() → 归一化\n       ↓ insert_split() → 拆 batch（如需要）\n       ↓ convert\n定点模型 (INT8, 含 Quantize/Dequantize)\n       ↓ remove_io_op()\n干净定点模型（可直接 compile 到 BPU）\n       ↓ compile\n板端可执行模型 (.bin/.xmodel)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">阶段一：export之后、convert之前(修改浮点HBIR模型)</p><span class=\'hidden-code\' data-code=\'1. batch 输入拆分：insert_split()\n目的：某些板端推理场景只支持 batch=1，但训练模型可能是 batch>1。需将 batch 维度拆分为多个单样本输入。\n例子：\n原始模型输入 shape: [4, 3, 224, 224]（batch=4）\n调用 insert_split(axis=0, num_splits=4) 后，模型被改造成接受 4 个独立输入（或通过循环处理），每个 shape 为 [1, 3, 224, 224]。\n适用场景：车载摄像头逐帧处理，不支持多 batch 推理。\n2. 调整排布为 NHWC：insert_transpose()\n目的：地平线 BPU 硬件通常以 NHWC（Batch, Height, Width, Channel）排布运行效率最高，而 PyTorch 默认是 NCHW。\n例子：\n原始输入：[1, 3, 256, 256]（NCHW）\n调用 insert_transpose([0, 2, 3, 1]) → 变为 [1, 256, 256, 3]（NHWC）\n注意：此操作插入一个 Transpose 算子到模型开头，后续所有操作基于 NHWC 进行。\n3. 图像归一化：insert_image_preprocess()\n目的：将原始像素值（如 0~255）转换为模型训练时使用的归一化范围（如 -1~1 或 0~1）。\n例子：\n假设训练时使用：pixel / 255.0\n调用 insert_image_preprocess(scale=1/255.0, mean=0, std=1)，在模型前端插入一个 Scale 算子。\n若使用 ImageNet 归一化：mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]，也可配置。\n好处：归一化由硬件完成，无需 CPU 预处理，降低延迟。\n4. 色彩空间转换（如 NV12 → RGB）：insert_image_convert()\n目的：板端摄像头常输出 NV12/YUV 格式（节省带宽），但模型需要 RGB 输入。\n例子：\n摄像头输出：NV12 格式（Y 分量 + UV interleaved）\n调用 insert_image_convert(input_format=’nv12’, output_format=’rgb’)\n工具链会在模型最前端插入一个 硬件加速的色彩转换算子，直接在 BPU 上完成 YUV→RGB。\n优势：避免 CPU 转换，提升端到端性能。\n5. ROI 抠图与缩放：insert_roi_resize()\n目的：从原始图像中按指定区域（ROI）裁剪并缩放到模型输入尺寸。\n例子：\n原始图像：1920×1080\n模型输入：224×224\nROI 区域：(x=500, y=300, w=800, h=600)\n调用 insert_roi_resize(roi=[500,300,800,600], target_size=(224,224))\n效果：硬件自动完成“抠图 + 双线性插值缩放”，输出 224×224 的 RGB 图像送入模型。\n典型应用：人脸检测后的人脸识别，只需处理人脸区域。\n6. 调整输入/输出排布：insert_transpose()（再次使用）\n目的：有时模型内部是 NHWC，但输出需要转回 NCHW（供后续 CPU 处理），或反之。\n例子：\n模型输出检测框：[1, 100, 4]（NHWC 风格）\n但上层应用期望 [1, 4, 100]（NCHW）\n在输出节点后插入 insert_transpose([0, 2, 1]) 完成重排。\n注意：输入/输出排布需与 Host（CPU）代码约定一致。\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">阶段二：convert之后、compile之前(清理定点模型)</p><span class=\'hidden-code\' data-code=\'7. 删除 I/O 冗余算子：remove_io_op()\n背景：在 convert（量化）过程中，工具链可能自动插入：\nQuantize（浮点→定点）\nDequantize（定点→浮点）\nCast（类型转换） 这些算子在纯板端部署时是多余的，因为：\n输入已是定点（如 INT8 图像），\n输出也应保持定点（由 Host 解释）。\n例子：\n定点模型输入前有一个 Quantize 算子（假设输入是 FP32），\n但实际板端输入是 INT8 NV12 图像 → 此 Quantize 无意义。\n调用 remove_io_op(op_type=[’Quantize’, ’Dequantize’]) 删除这些算子。\n效果：\n减少模型体积；\n避免无效计算；\n使模型真正“端到端定点”。\n\'> </span>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型编译-性能测试-可视化</p>测试定点模型精度并确认符合您的要求后，便可以进行模型编译、性能测试和可视化。<br>\n<span class=\'hidden-code\' data-code=\'# 可根据需要修改以下参数\n# 1. 编译时启用的优化等级，等级越高编译出的模型上板执行速度越快，但编译过程会慢\ncompile_opt = 1\n# 模型编译\nhb4.compile(hbir_quantized_model,os.path.join(model_path, ’model.hbm’),’nash-e’,opt=compile_opt)\n# 模型性能测试\nhb4.hbm_perf(os.path.join(model_path, ’model.hbm’),output_dir=model_path)\n# 模型可视化\nhb4.visualize(hbir_quantized_model, ’mobilenetv2_cifar10.onnx’)\n\'> </span>'}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
