<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>detr训练流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型训练</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/detr.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class DETR(nn.Module):\n    def forward(self, samples: NestedTensor):\n        if isinstance(samples, (list, torch.Tensor)):\n            samples = nested_tensor_from_tensor_list(samples)\n        features, pos = self.backbone(samples)  <span style=\'color: red\'># 先经过backbone得到pos位置嵌入和features是list，包含不同的backbone的layer 每个item是包含mask和tensors的NestedTesnor</span>\n        src, mask = features[-1].decompose()      <span style=\'color: red\'># 分解出feature[bs,2048,h,w]和mask[bs,2048,h,w] 分割任务的mask在不同的特征图尺寸上的插值</span>\n        assert mask is not None\n        <span style=\'color: red\'># self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1) 1x1 衔接 backbone和transformer input_proj [bs,2048,h,w] --input_proj--> [bs,256,h,w]</span>\n        <span style=\'color: red\'># query_embed.weight [100,256]里面[0] hs返回两个返回值，0就是transformer decoder的输出，1是encoder的输出 hs (6,bs,100,256)；用于seg使用</span>\n        hs = self.<span style=\'color: green;font-weight: bold;\'>transformer</span>(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n        outputs_class = self.class_embed(hs)             <span style=\'color: red\'># 类别的输出  outpus_class (6,bs,100,92)    </span>\n        outputs_coord = self.bbox_embed(hs).sigmoid()    <span style=\'color: red\'># bbox的输出,[6,bs,100,4],bbox的值都是相对于图片的hw的因此都是小于1的，因此使用sigmoid将值约束在0-1</span>\n        out = {\'pred_logits\': outputs_class[-1], \'pred_boxes\': outputs_coord[-1]}       <span style=\'color: red\'># 6层decoder,取最后一层的输出</span>\n        if self.aux_loss:\n            out[\'aux_outputs\'] = self._set_aux_loss(outputs_class, outputs_coord)       <span style=\'color: red\'># 计算辅助loss，计算前几层decoder输出的loss</span>\n        return out\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/transformer.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class Transformer(nn.Module):\n    def forward(self, src, mask, query_embed, pos_embed):           <span style=\'color: red\'># self.query_embed = nn.Embedding(num_queries, hidden_dim) 注意与pose_embed区别</span>\n        <span style=\'color: red\'># scr: (bs 256 H W)  mask: (bs H W) query_embed: (100,256) pos_embed: (bs 256 H W)</span>\n        bs, c, h, w = src.shape                                     <span style=\'color: red\'># flatten NxCxHxW to HWxNxC  -> HW,bs,256</span>\n        src = src.flatten(2).permute(2, 0, 1)\n        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)           <span style=\'color: red\'># HW,bs,256</span>\n        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)     <span style=\'color: red\'># query_embed 是给decoder使用的  100， 256 中间加个维度，并将这个维度重复 bs 次 -> 100,bs,256</span>\n        mask = mask.flatten(1)                                      <span style=\'color: red\'># hw 推平     </span>\n        memory = self.<span style=\'color: green;font-weight: bold;\'>encoder</span>(src, src_key_padding_mask=mask, pos=pos_embed)    <span style=\'color: red\'># feature和位置编码 进入encoder网络 输出 memory: HW,bs,256</span>\n        tgt = torch.zeros_like(query_embed)    <span style=\'color: red\'># [100,bs,256]</span>\n        hs = self.<span style=\'color: green;font-weight: bold;\'>decoder</span>(tgt, memory, memory_key_padding_mask=mask,pos=pos_embed, query_pos=query_embed)  <span style=\'color: red\'># tgt, encoder的输出，位置编码，以及query输入到decoder--->hs [6(decoder layer number),100,bs,256]</span>\n        <span style=\'color: red\'># hs  decoder的输出 [6,100,bs,256] --transpose--> [6,bs,100,256]</span>\n        <span style=\'color: red\'># memory  encoder的输出 [hw,bs,256] --permute--> [bs,256,hw] --view--> [bs,256,h,w]</span>\n        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/transformer.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class TransformerEncoder(nn.Module):\n    def forward(self, src, mask, src_key_padding_mask, pos):\n        output = src\n        for layer in self.layers:         <span style=\'color: red\'># 6层</span>\n            output = <span style=\'color: green;font-weight: bold;\'>layer</span>(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos)\n        if self.norm is not None:\n            output = self.norm(output)    <span style=\'color: red\'># 最后经过norm</span>\n        return output\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/transformer.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class TransformerEncoderLayer(nn.Module):\n    def forward_post(self, src, src_mask, src_key_padding_mask, pos):\n        q = k = self.<span style=\'color: green;font-weight: bold;\'>with_pos_embed</span>(src, pos)    <span style=\'color: red\'># src: (hw,3,256) [tensor + pos]   q k 融合位置编码，value不使用位置编码</span>\n        src2 = self.<span style=\'color: green;font-weight: bold;\'>self_attn</span>(q, k, value=src, attn_mask=src_mask,key_padding_mask=src_key_padding_mask)[0]  <span style=\'color: red\'># nn.MultiheadAttention(d_model, nhead, dropout=dropout)</span>\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        <span style=\'color: red\'># FFN</span>\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        <span style=\'color: red\'># 残差的就直接加进去了</span>\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/transformer.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class TransformerDecoder(nn.Module):\n    def forward(self,tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask,\n                memory_key_padding_mask, pos, query_pos):\n        <span style=\'color: red\'># tgt: [100,bs,256] 全0与query_embed大小相同</span>\n        <span style=\'color: red\'># memory: [hw,bs,256] encoder的输出    pos: [hw,bs,256]   query_pos: [100,bs,256]</span>\n        output = tgt\n        intermediate = []      <span style=\'color: red\'># 保留中间层的输出</span>\n        for layer in self.layers:\n            <span style=\'color: red\'># [100,bs,256]</span>\n            output = <span style=\'color: green;font-weight: bold;\'>layer</span>(output, memory, tgt_mask=tgt_mask,memory_mask=memory_mask,tgt_key_padding_mask=tgt_key_padding_mask,\n                           memory_key_padding_mask=memory_key_padding_mask,pos=pos, query_pos=query_pos)\n            if self.return_intermediate:        <span style=\'color: red\'># 每一个中间层计算的结果需要返回</span>\n                intermediate.append(self.norm(output))\n        if self.norm is not None:\n            output = self.norm(output)         <span style=\'color: red\'># 最后经过norm</span>\n            if self.return_intermediate:\n                intermediate.pop()             <span style=\'color: red\'># 弹出最后一个，加上上面经过norm的</span>\n                intermediate.append(output)\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n        return output.unsqueeze(0)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/transformer.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class TransformerDecoderLayer(nn.Module):\n    <span style=\'color: red\'># norm在操作后</span>\n    def forward_post(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask,\n                     pos, query_pos):\n        q = k = self.with_pos_embed(tgt, query_pos)        <span style=\'color: red\'># tensor + pos</span>\n        tgt2 = self.self_attn(q,k,value=tgt,attn_mask=tgt_mask,key_padding_mask=tgt_key_padding_mask)[0]  <span style=\'color: red\'># nn.MultiheadAttention</span>\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)  \n        tgt2 = self.multihead_attn(                         <span style=\'color: red\'># 各项值的组合方式与论文中的图像是一致的</span>\n            query=self.with_pos_embed(tgt, query_pos),      <span style=\'color: red\'># 第二个attention的query是第一个attention的输出</span>\n            key=self.with_pos_embed(memory, pos),           <span style=\'color: red\'># 两个参数，跟上面的都不同 使用了encoder的输出memory，以及位置编码</span>\n            value=memory,                                   <span style=\'color: red\'># 上面是tgt，这个是memory  encoder的输出memory是decoder中第二个attention的value</span>\n            attn_mask=memory_mask,\n            key_padding_mask=memory_key_padding_mask)[0]\n        <span style=\'color: red\'># 第二个dropout  这里的tgt是第一个attention的输出 加上 第二个attention的输出</span>\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        <span style=\'color: red\'># FFN</span>\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        <span style=\'color: red\'># 第三个dropout</span>\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt\n</code></pre></font>'}]}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">loss计算</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/detr.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class SetCriterion(nn.Module):\n    def forward(self, outputs, targets):\n        outputs_without_aux = {k: v for k, v in outputs.items() if k != \'aux_outputs\'}    <span style=\'color: red\'># without 这里排除aux</span>\n        <span style=\'color: red\'># 进行匈牙利匹配</span>\n        indices = self.<span style=\'color: green;font-weight: bold;\'>matcher</span>(outputs_without_aux, targets)    <span style=\'color: red\'># indices是list, 长度是bs, 每个item是一个tuple, 第一个值是100个框的id，第二个值是gt的id</span>\n        num_boxes = sum(len(t["labels"]) for t in targets)      <span style=\'color: red\'># batch中所有的box的数量</span>\n        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n        if is_dist_avail_and_initialized():\n            torch.distributed.all_reduce(num_boxes)                           <span style=\'color: red\'># 所有卡的num_boxes相同</span>\n        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()   <span style=\'color: red\'># 除以卡数</span>\n        <span style=\'color: red\'># 计算后是loss_ce, class_error, loss_bbox, loss_giou, cardinality_error</span>\n        losses = {}\n        for loss in self.losses:                                                           <span style=\'color: red\'># slef.losses 是labels,boxes, cardinality                                        </span>\n            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))       <span style=\'color: red\'># 计算每一项的内容, 如 labels, boxes, cardinality</span>\n        <span style=\'color: red\'># 处理 aux loss</span>\n        if \'aux_outputs\' in outputs:\n            for i, aux_outputs in enumerate(outputs[\'aux_outputs\']):\n                indices = self.matcher(aux_outputs, targets)\n                for loss in self.losses:\n                    if loss == \'masks\':                <span style=\'color: red\'># masks的loss就不计算这个了，计算量大</span>\n                        continue\n                    kwargs = {}\n                    if loss == \'labels\':               <span style=\'color: red\'># Logging is enabled only for the last layer</span>\n                        kwargs = {\'log\': False}\n                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n                    l_dict = {k + f\'_{i}\': v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n        return losses\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/matcher.py</p><font size="0"><pre class="language-python" style="line-height: 0.01; "><code class="language-python">class HungarianMatcher(nn.Module):\n    def forward(self, outputs, targets):  \n        bs, num_queries = outputs["pred_logits"].shape[:2]            <span style=\'color: red\'># bs is batch_size; num_queries 是配置的每张图片中多少个目标=100</span>\n        out_prob = outputs["pred_logits"].flatten(0, 1).softmax(1)    <span style=\'color: red\'># 前两个拉平在一起，最后一个维度是类别，进行softmax  [batch_size * num_queries, num_classes]</span>\n        out_bbox = outputs["pred_boxes"].flatten(0, 1)                <span style=\'color: red\'># [batch_size * num_queries, 4]</span>\n        <span style=\'color: red\'># Also concat the target labels and boxes</span>\n        tgt_ids = torch.cat([v["labels"] for v in targets])           <span style=\'color: red\'># target label</span>\n        tgt_bbox = torch.cat([v["boxes"] for v in targets])           <span style=\'color: red\'># target bbox</span>\n        <span style=\'color: red\'># 取出对应的类别的分数，每个框 的 所有gt的label的分数 [bs*100,92][:,tgt_ids] --> [bs*100, 所有bs中img上的gt的数量和]</span>\n        <span style=\'color: red\'># 一个预测框上的 对应的 在所有的gt上的分数</span>\n        cost_class = -out_prob[:, tgt_ids]\n        <span style=\'color: red\'># out_bbox is [bs*100,4] tgt_box is [all_img_gt_count,4]</span>\n        <span style=\'color: red\'># Compute the L1 cost between boxes -> [bs*100, all_img_gt_count]    p=1 计算l1距离</span>\n        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)              <span style=\'color: red\'># cost_bbox 和 cost_class的维度是一样的</span>\n        <span style=\'color: red\'># 计算的GIoU  先进行中心点宽高变成坐上右下四个坐标值</span>\n        <span style=\'color: red\'># 所有的框，跟gt的giou [bs*100, all_img_gt_count]; Compute the giou cost betwen boxes</span>\n        <span style=\'color: red\'># 完全相同位置的框 giou是1，完全不相交的框，giou是负数  因此这里加了一个负号，完全不相交的框的值就变成了整数，表示了更大的代价</span>\n        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n        <span style=\'color: red\'># 最终的代价矩阵，匈牙利匹配使用的，最终是要总的分配的代价最小  前面都是各个项的权重系数</span>\n        <span style=\'color: red\'># Final cost matrix [bs*100, all_img_gt_count]</span>\n        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n        <span style=\'color: red\'># [bs*100, all_img_gt_count] -> [3,100,all_img_gt_count]    .cpu 为了给scipy计算 维度变为 batch_size , 100, gt的数量</span>\n        C = C.view(bs, num_queries, -1).cpu()\n        <span style=\'color: red\'># 每个图片对应的gt的数量</span>\n        sizes = [len(v["boxes"]) for v in targets]\n        <span style=\'color: red\'># linear_sum_assignment 就是匈牙利算法</span>\n        <span style=\'color: red\'># C.split 按照每个图片的gt的数量进行切分;  第一个值是100内的id，表明100内取哪一个框，第二个应该是对应了哪一个gt的id</span>\n        <span style=\'color: red\'># indices的一个例子[(array([ 0, 51]), array([0, 1])), (array([13, 24, 54, 86]), array([0, 1, 3, 2]))]</span>\n        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n</code></pre></font>'}]}]}]})</script></body>
</html>
