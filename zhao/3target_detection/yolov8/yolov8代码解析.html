<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>yolov8代码解析</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型结构</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/models/yolo/model.py</p><span class=\'hidden-code\' data-code=\'class YOLO(Model):        # YOLO (You Only Look Once) object detection model\n    @property\n    def task_map(self):\n        return {\n            &amp;#39;classify&amp;#39;: {\n                &amp;#39;model&amp;#39;: ClassificationModel,\n                &amp;#39;trainer&amp;#39;: yolo.classify.ClassificationTrainer,\n                &amp;#39;validator&amp;#39;: yolo.classify.ClassificationValidator,\n                &amp;#39;predictor&amp;#39;: yolo.classify.ClassificationPredictor,\n            },\n            &amp;#39;detect&amp;#39;: {\n                &amp;#39;model&amp;#39;: DetectionModel,\n                &amp;#39;trainer&amp;#39;: yolo.detect.DetectionTrainer,\n                &amp;#39;validator&amp;#39;: yolo.detect.DetectionValidator,\n                &amp;#39;predictor&amp;#39;: yolo.detect.DetectionPredictor,\n            },\n        }\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/tasks.py</p><span class=\'hidden-code\' data-code=\'class DetectionModel(BaseModel):           # YOLOv8 detection model\n    def __init__(self, cfg=&amp;#39;yolov8n.yaml&amp;#39;, ch=3, nc=None, verbose=True):    # model, input channels, number of classes\n        super().__init__()\n        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg dict\n        # Define model\n        ch = self.yaml[&amp;#39;ch&amp;#39;] = self.yaml.get(&amp;#39;ch&amp;#39;, ch)  # input channels   3 \n        if nc and nc != self.yaml[&amp;#39;nc&amp;#39;]:\n            LOGGER.info(f&amp;#39;Overriding model.yaml nc={self.yaml[&amp;#39;nc&amp;#39;]} with nc={nc}&amp;#39;)\n            self.yaml[&amp;#39;nc&amp;#39;] = nc                        # override YAML value\n        self.model, self.save = `parse_model`(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\n        self.names = {i: f&amp;#39;{i}&amp;#39; for i in range(self.yaml[&amp;#39;nc&amp;#39;])}  # default names dict\n        self.inplace = self.yaml.get(&amp;#39;inplace&amp;#39;, True)\n        # Build strides\n        m = self.model[-1]  # Detect()\n        if isinstance(m, (Detect, Segment, Pose, OBB)):\n            s = 256  # 2x min stride\n            m.inplace = self.inplace\n            forward = lambda x: self.forward(x)[0] if isinstance(m, (Segment, Pose, OBB)) else self.forward(x)\n            m.stride = torch.tensor([s / x.shape[-2] for x in forward(torch.zeros(1, ch, s, s))])  # forward\n            self.stride = m.stride\n            m.bias_init()  # only run once\n        else:\n            self.stride = torch.Tensor([32])  # default stride for i.e. RTDETR\n        # Init weights, biases\n        initialize_weights(self)\n        if verbose:\n            self.info()\n            LOGGER.info(&amp;#39;&amp;#39;)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/tasks.py</p><span class=\'hidden-code\' data-code=\'def parse_model(d, ch, verbose=True):  # model_dict-配置文件中的字典, input_channels(3)\n    import ast\n    # Args\n    max_channels = float(&amp;#39;inf&amp;#39;)\n    nc, act, scales = (d.get(x) for x in (&amp;#39;nc&amp;#39;, &amp;#39;activation&amp;#39;, &amp;#39;scales&amp;#39;))                                  # 80, None, {&amp;#39;n&amp;#39;:[0.33, 0.25, 1024]} \n    depth, width, kpt_shape = (d.get(x, 1.0) for x in (&amp;#39;depth_multiple&amp;#39;, &amp;#39;width_multiple&amp;#39;, &amp;#39;kpt_shape&amp;#39;))  # 1, 1, 1\n    if scales:\n        scale = d.get(&amp;#39;scale&amp;#39;)                     # &amp;#39;&amp;#39;\n        if not scale:\n            scale = tuple(scales.keys())[0]        # n\n            LOGGER.warning(f&amp;#39;WARNING ⚠️ no model scale passed. Assuming scale=&amp;#39;{scale}&amp;#39;.&amp;#39;)\n        depth, width, max_channels = scales[scale] # 0.33, 0.25, 1024\n    if act:                                         # None\n        Conv.default_act = eval(act)                # redefine default activation, i.e. Conv.default_act = nn.SiLU()\n        if verbose:\n            LOGGER.info(f&amp;#39;{colorstr(&amp;#39;activation:&amp;#39;)} {act}&amp;#39;)  # print\n    if verbose:            # True打印结果\n        LOGGER.info(f&amp;#39;\\n{&amp;#39;&amp;#39;:`>`3}{&amp;#39;from&amp;#39;:`>`20}{&amp;#39;n&amp;#39;:`>`3}{&amp;#39;params&amp;#39;:`>`10}  {&amp;#39;module&amp;#39;:`<`45}{&amp;#39;arguments&amp;#39;:`<`30}&amp;#39;)\n    ch = [ch]                          # [3]\n    layers, save, c2 = [], [], ch[-1]  # layers-运行哪层, savelist, ch out\n    for i, (f, n, m, args) in enumerate(d[&amp;#39;backbone&amp;#39;] + d[&amp;#39;head&amp;#39;]):  # from, number, module, args\n        m = getattr(torch.nn, m[3:]) if &amp;#39;nn.&amp;#39; in m else globals()[m]  # get module\n        for j, a in enumerate(args):\n            if isinstance(a, str):\n                with contextlib.suppress(ValueError):\n                    args[j] = locals()[a] if a in locals() else ast.literal_eval(a)\n        n = n_ = max(round(n * depth), 1) if n > 1 else n  # depth gain\n        if m in (\n            Classify,\n            Conv,\n            ConvTranspose,\n            GhostConv,\n            Bottleneck,\n            GhostBottleneck,\n            SPP,\n            SPPF,\n            DWConv,\n            Focus,\n            BottleneckCSP,\n            C1,\n            C2,\n            C2f,\n            C3,\n            C3TR,\n            C3Ghost,\n            nn.ConvTranspose2d,\n            DWConvTranspose2d,\n            C3x,\n            RepC3,\n        ):\n            c1, c2 = ch[f], args[0]\n            if c2 != nc:  # if c2 not equal to number of classes (i.e. for Classify() output)\n                c2 = make_divisible(min(c2, max_channels) * width, 8)\n            args = [c1, c2, *args[1:]]\n            if m in (BottleneckCSP, C1, C2, C2f, C3, C3TR, C3Ghost, C3x, RepC3):\n                args.insert(2, n)  # number of repeats\n                n = 1\n        elif m is AIFI:\n            args = [ch[f], *args]\n        elif m in (HGStem, HGBlock):\n            c1, cm, c2 = ch[f], args[0], args[1]\n            args = [c1, cm, c2, *args[2:]]\n            if m is HGBlock:\n                args.insert(4, n)  # number of repeats\n                n = 1\n        elif m is ResNetLayer:\n            c2 = args[1] if args[3] else args[1] * 4\n        elif m is nn.BatchNorm2d:\n            args = [ch[f]]\n        elif m is Concat:\n            c2 = sum(ch[x] for x in f)\n        elif m in (Detect, Segment, Pose, OBB):\n            args.append([ch[x] for x in f])\n            if m is Segment:\n                args[2] = make_divisible(min(args[2], max_channels) * width, 8)\n        elif m is RTDETRDecoder:  # special case, channels arg must be passed in index 1\n            args.insert(1, [ch[x] for x in f])\n        else:\n            c2 = ch[f]\n        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n        t = str(m)[8:-2].replace(&amp;#39;__main__.&amp;#39;, &amp;#39;&amp;#39;)  # module type\n        m.np = sum(x.numel() for x in m_.parameters())  # number params\n        m_.i, m_.f, m_.type = i, f, t  # attach index, &amp;#39;from&amp;#39; index, type\n        if verbose:\n            LOGGER.info(f&amp;#39;{i:`>`3}{str(f):`>`20}{n_:`>`3}{m.np:10.0f}  {t:`<`45}{str(args):`<`30}&amp;#39;)  # print\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n        layers.append(m_)\n        if i == 0:\n            ch = []\n        ch.append(c2)\n    return nn.Sequential(*layers), sorted(save)\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型head推理</p>'}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型预测</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/head.py</p><span class=\'hidden-code\' data-code=\'class Detect(nn.Module):\n    def forward(self, x):\n        for i in range(self.nl):         # [torch.Size([1, 128, 80, 80]),[1, 256, 40, 40],[1, 512, 20, 20]]-->[torch.Size([1, 144, 80, 80]),[1, 144, 40, 40],[1, 144, 20, 20]]\n            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)  \n        if self.training:                # Training path\n            return x\n        # Inference path\n        shape = x[0].shape               # BCHW  torch.Size([1, 144, 80, 80])\n        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)       # torch.Size([1, 144, 8400])\n        if self.dynamic or self.shape != shape:                                  # False or [136, 128, 80, 80]!=[1, 144, 80, 80]\n            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))  # torch.Size([2, 8400]) + torch.Size([1, 8400])\n            self.shape = shape \n        if self.export and self.format in {&amp;#39;saved_model&amp;#39;, &amp;#39;pb&amp;#39;, &amp;#39;tflite&amp;#39;, &amp;#39;edgetpu&amp;#39;, &amp;#39;tfjs&amp;#39;}:  # avoid TF FlexSplitV ops\n            ......\n        else:\n            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)               # torch.Size([1, 64, 8400]) + torch.Size([1, 80, 8400])\n        if self.export and self.format in {&amp;#39;tflite&amp;#39;, &amp;#39;edgetpu&amp;#39;}:\n            ......\n        else:\n            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides          # torch.Size([1, 4, 8400])\n        y = torch.cat((dbox, cls.sigmoid()), 1)                                                         # torch.Size([1, 84, 8400])  80个类别\n        return y if self.export else (y, x)            # False,(torch.Size([1, 84, 8400]), torch.Size([1, 144, 80, 80]),[1, 144, 40, 40],[1, 144, 20, 20] )     \n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/models/yolo/detect/predict.py</p><span class=\'hidden-code\' data-code=\'class DetectionPredictor(BasePredictor):\n    def postprocess(self, preds, img, orig_imgs):\n        preds = ops.non_max_suppression(               \n            preds,                              # torch.Size([1, 84, 6300]), [torch.Size([1, 144, 80, 60]), torch.Size([1, 144, 40, 30]), torch.Size([1, 144, 20, 15])]\n            self.args.conf,                     # 0.25\n            self.args.iou,                      # 0.7\n            agnostic=self.args.agnostic_nms,    # False\n            max_det=self.args.max_det,          # 300\n            classes=self.args.classes,          # None\n        )\n        if not isinstance(orig_imgs, list):     # input images are a torch.Tensor, not a list      False [(1080, 810, 3),]\n            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n        results = []\n        for i, pred in enumerate(preds):        # [torch.Size([5, 6])]\n            orig_img = orig_imgs[i]\n            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\n            img_path = self.batch[0][i]\n            results.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\n        return results\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/utils/ops.py</p>'}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
