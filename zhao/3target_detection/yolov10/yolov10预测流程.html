<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>yolov10预测流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/model.py</p><span class=\'hidden-code\' data-code=\'class Model(nn.Module):\n    def predict(self,source=None, stream=False, predictor=None, **kwargs):      &amp;#39;.......jpg&amp;#39;  \n        is_cli = (sys.argv[0].endswith(&amp;#39;yolo&amp;#39;) or sys.argv[0].endswith(&amp;#39;ultralytics&amp;#39;)) and any(x in sys.argv for x in (&amp;#39;predict&amp;#39;,&amp;#39;track&amp;#39;,&amp;#39;mode=predict&amp;#39;,&amp;#39;mode=track&amp;#39;)) False\n        custom = {&amp;#39;conf&amp;#39;: 0.25, &amp;#39;batch&amp;#39;: 1, &amp;#39;save&amp;#39;: is_cli, &amp;#39;mode&amp;#39;: &amp;#39;predict&amp;#39;}  method defaults  {&amp;#39;conf&amp;#39;: 0.25, &amp;#39;batch&amp;#39;: 1, &amp;#39;save&amp;#39;: False, &amp;#39;mode&amp;#39;: &amp;#39;predict&amp;#39;}\n        {&amp;#39;task&amp;#39;: &amp;#39;detect&amp;#39;, &amp;#39;data&amp;#39;: &amp;#39;datasets/side_all/data.yaml&amp;#39;, &amp;#39;imgsz&amp;#39;: 640, &amp;#39;single_cls&amp;#39;: False, &amp;#39;model&amp;#39;: &amp;#39;runs/....../best.pt&amp;#39;, &amp;#39;conf&amp;#39;: 0.25, &amp;#39;batch&amp;#39;: 1, &amp;#39;save&amp;#39;: False, &amp;#39;mode&amp;#39;: &amp;#39;predict&amp;#39;}\n        args = {**self.overrides, **custom, **kwargs}                           args\n        prompts = args.pop(&amp;#39;prompts&amp;#39;, None)                                     for SAM-type models\n        if not self.predictor:                                                  True predictor为None,\n            self.predictor = predictor or self._smart_load(&amp;#39;predictor&amp;#39;)(overrides=args, _callbacks=self.callbacks)  ultralytics.models.yolov10.predict.YOLOv10DetectionPredictor\n            self.predictor.`setup_model`(model=self.model, verbose=is_cli)\n        else:  only update args if predictor is already setup\n            self.predictor.args = get_cfg(self.predictor.args, args)\n            if &amp;#39;project&amp;#39; in args or &amp;#39;name&amp;#39; in args:\n                self.predictor.save_dir = get_save_dir(self.predictor.args)\n        if prompts and hasattr(self.predictor, &amp;#39;set_prompts&amp;#39;):  for SAM-type models\n            self.predictor.set_prompts(prompts)\n        return self.predictor.predict_cli(source=source) if is_cli else self.`predictor`(source=source, stream=stream)   ultralytics.models.yolov10.predict.YOLOv10DetectionPredictor\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/predictor.py</p><span class=\'hidden-code\' data-code=\'class BasePredictor:\n    def setup_model(self, model, verbose=True):          # Initialize YOLO model with given parameters and set it to evaluation mode\n        self.model = AutoBackend(\n            weights=model or self.args.model,\n            device=select_device(self.args.device, verbose=verbose),   # None,False = device(type=&amp;#39;cuda&amp;#39;, index=0)\n            dnn=self.args.dnn,       # False\n            data=self.args.data,     # &amp;#39;datasets/img_2d/data.yaml&amp;#39;\n            fp16=self.args.half,     # False\n            batch=self.args.batch,   # 1\n            fuse=True,\n            verbose=verbose,         # False\n        )\n        self.device = self.model.device   # update device device(type=&amp;#39;cuda&amp;#39;, index=0)\n        self.args.half = self.model.fp16  # update half\n        self.model.eval()\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/predictor.py</p><span class=\'hidden-code\' data-code=\'# YOLOv10DetectionPredictor-->DetectionPredictor-->BasePredictor\nclass BasePredictor:\n    def __call__(self, source=None, model=None, stream=False, *args, **kwargs):       # Performs inference on an image or stream\n        self.stream = stream                               # False\n        if stream:\n            return self.stream_inference(source, model, *args, **kwargs)\n        else:\n            return list(self.`stream_inference`(source, model, *args, **kwargs))  # merge list of Result into one\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/predictor.py</p><span class=\'hidden-code\' data-code=\'class BasePredictor:\n    @smart_inference_mode()\n    def stream_inference(self, source=None, model=None, *args, **kwargs):\n        with self._lock:                                                              # for thread-safe inference\n            self.`setup_source`(source if source is not None else self.args.source)   # Setup source every time predict is called\n            if self.args.save or self.args.save_txt:                                  # Check if save_dir/ label file exists  False\n                (self.save_dir / &amp;#39;labels&amp;#39; if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n            if not self.done_warmup:                                                  # Warmup model  -- False\n                self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))  # imgsz=(1, 3, 640, 640)  AutoBackend.warmup\n                self.done_warmup = True\n            self.seen, self.windows, self.batch = 0, [], None\n            profilers = (ops.Profile(device=self.device),ops.Profile(device=self.device),ops.Profile(device=self.device))\n            self.run_callbacks(&amp;#39;on_predict_start&amp;#39;)\n            for self.batch in self.dataset:\n                self.run_callbacks(&amp;#39;on_predict_batch_start&amp;#39;)\n                paths, im0s, s = self.batch             # [&amp;#39;/sdb/zzhu/code_study...003760.jpg&amp;#39;],[(1080, 1920, 3)],[&amp;#39;image 1/1 /sdb/zzhu/...3760.jpg: &amp;#39;]\n                # Preprocess\n                with profilers[0]:\n                    im = self.`preprocess`(im0s)        # torch.Size([1, 3, 384, 640])\n                # Inference\n                with profilers[1]:\n                    preds = self.`inference`(im, *args, **kwargs)\n                    if self.args.embed:                 # None\n                        yield from [preds] if isinstance(preds, torch.Tensor) else preds  # yield embedding tensors\n                        continue\n                # Postprocess\n                with profilers[2]:\n                    self.results = self.`postprocess`(preds, im, im0s)   # return preds\n                self.run_callbacks(&amp;#39;on_predict_postprocess_end&amp;#39;)\n                # Visualize, save, write results\n                n = len(im0s)\n                for i in range(n):\n                    self.seen += 1\n                    self.results[i].speed = {\n                        &amp;#39;preprocess&amp;#39;: profilers[0].dt * 1e3 / n,\n                        &amp;#39;inference&amp;#39;: profilers[1].dt * 1e3 / n,\n                        &amp;#39;postprocess&amp;#39;: profilers[2].dt * 1e3 / n,\n                    }\n                    if self.args.verbose or self.args.save or self.args.save_txt or self.args.show:\n                        s[i] += self.write_results(i, Path(paths[i]), im, s)\n                # Print batch results\n                if self.args.verbose:\n                    LOGGER.info(&amp;#39;\\n&amp;#39;.join(s))\n                self.run_callbacks(&amp;#39;on_predict_batch_end&amp;#39;)\n                yield from self.results\n        # Release assets\n        for v in self.vid_writer.values():\n            if isinstance(v, cv2.VideoWriter):\n                v.release()\n        # Print final results\n        if self.args.verbose and self.seen:\n            t = tuple(x.t / self.seen * 1e3 for x in profilers)  # speeds per image\n            LOGGER.info(\n                f&amp;#39;Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess per image at shape &amp;#39;\n                f&amp;#39;{(min(self.args.batch, self.seen), 3, *im.shape[2:])}&amp;#39; % t\n            )\n        if self.args.save or self.args.save_txt or self.args.save_crop:\n            nl = len(list(self.save_dir.glob(&amp;#39;labels/*.txt&amp;#39;)))  # number of labels\n            s = f&amp;#39;\\n{nl} label{&amp;#39;s&amp;#39; * (nl > 1)} saved to {self.save_dir / &amp;#39;labels&amp;#39;}&amp;#39; if self.args.save_txt else &amp;#39;&amp;#39;\n            LOGGER.info(f&amp;#39;Results saved to {colorstr(&amp;#39;bold&amp;#39;, self.save_dir)}{s}&amp;#39;)\n        self.run_callbacks(&amp;#39;on_predict_end&amp;#39;)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/predictor.py</p><span class=\'hidden-code\' data-code=\'class BasePredictor:\n    def setup_source(self, source):                  # Sets up source and inference mode\n        self.imgsz = check_imgsz(self.args.imgsz, stride=self.model.stride, min_dim=2)  # check image size  [640, 640]\n        self.transforms = (getattr(self.model.model,&amp;#39;transforms&amp;#39;,classify_transforms(self.imgsz[0], crop_fraction=self.args.crop_fraction),)   # None\n            if self.args.task == &amp;#39;classify&amp;#39; else None)                                  # ultralytics.data.loaders.LoadImagesAndVideos\n        self.dataset = `load_inference_source`(source=source,batch=self.args.batch,vid_stride=self.args.vid_stride,buffer=self.args.stream_buffer) \n        self.source_type = self.dataset.source_type\n        if not getattr(self,&amp;#39;stream&amp;#39;,True) and (self.source_type.stream or self.source_type.screenshot or len(self.dataset)>1000 or any(getattr(self.dataset,&amp;#39;video_flag&amp;#39;,[False]))):\n            LOGGER.warning(STREAM_WARNING)\n        self.vid_writer = {}\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/data/build.py</p><span class=\'hidden-code\' data-code=\'def load_inference_source(source=None, batch=1, vid_stride=1, buffer=False):\n    source, stream, screenshot, from_img, in_memory, tensor = check_source(source)\n    source_type = source.source_type if in_memory else SourceTypes(stream, screenshot, from_img, tensor)\n    if tensor:\n        dataset = LoadTensor(source)\n    elif in_memory:\n        dataset = source\n    elif stream:\n        dataset = LoadStreams(source, vid_stride=vid_stride, buffer=buffer)\n    elif screenshot:\n        dataset = LoadScreenshots(source)\n    elif from_img:\n        dataset = LoadPilAndNumpy(source)\n    else:\n        dataset = `LoadImagesAndVideos`(source, batch=batch, vid_stride=vid_stride)\n    setattr(dataset, &amp;#39;source_type&amp;#39;, source_type)                  # Attach source types to the dataset\n    return dataset\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/data/loaders.py</p><span class=\'hidden-code\' data-code=\'class LoadImagesAndVideos:\n    def __next__(self):                # 正常读取，没有做任何操作\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/predictor.py</p><span class=\'hidden-code\' data-code=\'class BasePredictor:\n    def preprocess(self, im):\n        not_tensor = not isinstance(im, torch.Tensor)      # True\n        if not_tensor:\n            im = np.stack(self.`pre_transform`(im))          # (1, 384, 640, 3)\n            im = im[..., ::-1].transpose((0, 3, 1, 2))     # BGR to RGB, BHWC to BCHW, (n, 3, h, w) (1, 3, 384, 640)\n            im = np.ascontiguousarray(im)                  # contiguous\n            im = torch.from_numpy(im)\n        im = im.to(self.device)\n        im = im.half() if self.model.fp16 else im.float()  # uint8 to fp16/32   torch.float32\n        if not_tensor:                                     # True\n            im /= 255                                      # 0 - 255 to 0.0 - 1.0\n        return im\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/predictor.py</p><span class=\'hidden-code\' data-code=\'class BasePredictor:\n    same_shapes = len({x.shape for x in im}) == 1\n    letterbox = `LetterBox`(self.imgsz, auto=same_shapes and self.model.pt, stride=self.model.stride) # [640, 640],auto=True,32\n    return [`letterbox`(image=x) for x in im]\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/data/augment.py</p><span class=\'hidden-code\' data-code=\'class LetterBox:\n    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n        self.new_shape = new_shape      # [640, 640]\n        self.auto = auto                # True\n        self.scaleFill = scaleFill      # False\n        self.scaleup = scaleup          # True\n        self.stride = stride            # 32\n        self.center = center  # Put the image in the middle or top-left  True\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/data/augment.py</p><span class=\'hidden-code\' data-code=\'class LetterBox:\n    def __call__(self, labels=None, image=None):\n        if labels is None:                # None\n            labels = {}\n        img = labels.get(&amp;#39;img&amp;#39;) if image is None else image       # (1080, 1920, 3) ,值范围0-255\n        shape = img.shape[:2]             # current shape [height, width]   (1080, 1920)\n        new_shape = labels.pop(&amp;#39;rect_shape&amp;#39;, self.new_shape)      # [640, 640]\n        if isinstance(new_shape, int):    # False\n            new_shape = (new_shape, new_shape)\n        # Scale ratio (new / old)\n        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1]) # min(640/1080,640/1920) = 0.33333333333\n        if not self.scaleup:  # only scale down, do not scale up (for better val mAP)  False\n            r = min(r, 1.0)\n        # Compute padding\n        ratio = r, r          # width, height ratios  (0.3333333333333333, 0.3333333333333333)\n        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))     # (640, 360)\n        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding  0,280\n        if self.auto:         # minimum rectangle  True\n            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)      # wh padding (0, 24)\n        elif self.scaleFill:  # stretch\n            dw, dh = 0.0, 0.0\n            new_unpad = (new_shape[1], new_shape[0])\n            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n        if self.center:       # True\n            dw /= 2           # divide padding into 2 sides  0\n            dh /= 2           #                              12.0\n        if shape[::-1] != new_unpad:  # resize  (1080, 1920)[::-1] != (640, 360)\n            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)           # (360, 640, 3)\n        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1)) # 12,12\n        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1)) # 0,0\n        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))  # add border (384, 640, 3)\n        if labels.get(&amp;#39;ratio_pad&amp;#39;):                                   # False\n            labels[&amp;#39;ratio_pad&amp;#39;] = (labels[&amp;#39;ratio_pad&amp;#39;], (left, top))  # for evaluation\n        if len(labels):                                               # False\n            ......\n        else:\n            return img\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/engine/predictor.py</p><span class=\'hidden-code\' data-code=\'class BasePredictor:\n    def inference(self, im, *args, **kwargs):\n        visualize = (increment_path(self.save_dir / Path(self.batch[0][0]).stem, mkdir=True) if self.args.visualize and (not self.source_type.tensor) else False)  # False\n        return self.`model`(im, augment=self.args.augment, visualize=visualize, embed=self.args.embed, *args, **kwargs)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/head.py</p><span class=\'hidden-code\' data-code=\'class v10Detect(Detect):\n    def forward(self, x):\n        one2one = self.`forward_feat`([xi.detach() for xi in x], self.one2one_cv2, self.one2one_cv3)  # # [torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]]\n        if not self.export:\n            one2many = super().`forward`(x)          # (torch.Size([1, 10, 5040]),[torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]])\n        if not self.training:                        # True\n            one2one = self.`inference`(one2one)      # (torch.Size([1, 10, 5040]),[torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]])\n            if not self.export:                      # True\n                return {&amp;#39;one2many&amp;#39;: one2many, &amp;#39;one2one&amp;#39;: one2one}        # 返回\n            else:\n                ......\n        else:\n            return {&amp;#39;one2many&amp;#39;: one2many, &amp;#39;one2one&amp;#39;: one2one}\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/head.py</p><span class=\'hidden-code\' data-code=\'class Detect(nn.Module):\n    def forward_feat(self, x, cv2, cv3):\n        y = []\n        for i in range(self.nl):\n            y.append(torch.cat((cv2[i](x[i]), cv3[i](x[i])), 1))\n        return y          # [torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]]\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/head.py</p><span class=\'hidden-code\' data-code=\'class Detect(nn.Module):\n    def forward(self, x):\n        y = self.forward_feat(x, self.cv2, self.cv3)   # [torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]]\n        if self.training:                              # Fasle\n            return y\n        return self.`inference`(y)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 7, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/head.py</p><span class=\'hidden-code\' data-code=\'class Detect(nn.Module):\n    def inference(self, x):                                                          # [torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]]\n        shape = x[0].shape                                                           # BCHW     torch.Size([1, 70, 48, 80])\n        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)           # torch.Size([1, 70, 5040])\n        if self.dynamic or self.shape != shape:                         # False or (torch.Size([1, 70, 80, 80]) != torch.Size([1, 70, 48, 80]))\n            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))   # torch.Size([2, 5040]),torch.Size([1, 5040])\n            self.shape = shape                                          # torch.Size([1, 70, 48, 80])\n        if self.export and self.format in (&amp;#39;saved_model&amp;#39;, &amp;#39;pb&amp;#39;, &amp;#39;tflite&amp;#39;, &amp;#39;edgetpu&amp;#39;, &amp;#39;tfjs&amp;#39;):  # avoid TF FlexSplitV ops  False\n            box = x_cat[:, : self.reg_max * 4]\n            cls = x_cat[:, self.reg_max * 4 :]\n        else:\n            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)      # torch.Size([1, 64, 5040]),torch.Size([1, 6, 5040])\n        if self.export and self.format in (&amp;#39;tflite&amp;#39;, &amp;#39;edgetpu&amp;#39;):        # False\n            grid_h = shape[2]\n            grid_w = shape[3]\n            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)\n            norm = self.strides / (self.stride[0] * grid_size)\n            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])\n        else:\n            dbox = self.`decode_bboxes`(self.`dfl`(box), self.anchors.unsqueeze(0)) * self.strides     # torch.Size([1, 4, 5040])\n        y = torch.cat((dbox, cls.sigmoid()), 1)    # torch.Size([1, 10, 5040])\n        return y if self.export else (y, x)        # self.export=False  (torch.Size([1, 10, 5040]),[torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]])\n\'> </span>', 'children': [{'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/block.py</p><span class=\'hidden-code\' data-code=\'class DFL(nn.Module):\n    def __init__(self, c1=16):\n        super().__init__()\n        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n        x = torch.arange(c1, dtype=torch.float)\n        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n        self.c1 = c1\n    def forward(self, x):\n        b, _, a = x.shape                        # torch.Size([1, 64, 5040])\n        # [1, 64, 5040]->1,4,16,5040->[1,16,4,5040]-(softmax)>[1,16,4,5040]==(self.conv)==[1, 1, 4, 5040]==>1,4,5040\n        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)  \n\'> </span>'}, {'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/head.py</p><span class=\'hidden-code\' data-code=\'class Detect(nn.Module):\n    def decode_bboxes(self, bboxes, anchors):\n        if self.export:                         # False\n            return dist2bbox(bboxes, anchors, xywh=False, dim=1)\n        return `dist2bbox`(bboxes, anchors, xywh=True, dim=1)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 9, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/utils/tal.py</p><span class=\'hidden-code\' data-code=\'def dist2bbox(distance, anchor_points, xywh=True, dim=-1):    # Transform distance(ltrb) to box(xywh or xyxy)\n    assert(distance.shape[dim] == 4)\n    lt, rb = distance.split([2, 2], dim)\n    x1y1 = anchor_points - lt\n    x2y2 = anchor_points + rb\n    if xywh:\n        c_xy = (x1y1 + x2y2) / 2\n        wh = x2y2 - x1y1\n        return torch.cat((c_xy, wh), dim)  # xywh bbox\n    return torch.cat((x1y1, x2y2), dim)    # xyxy bbox\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/nn/modules/head.py</p><span class=\'hidden-code\' data-code=\'class Detect(nn.Module):\n    def inference(self, x):                                                          # [torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]]\n        shape = x[0].shape                                                           # BCHW     torch.Size([1, 70, 48, 80])\n        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)           # torch.Size([1, 70, 5040])\n        if self.dynamic or self.shape != shape:                         # False or (torch.Size([1, 70, 80, 80]) != torch.Size([1, 70, 48, 80]))\n            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))   # torch.Size([2, 5040]),torch.Size([1, 5040])\n            self.shape = shape                                          # torch.Size([1, 70, 48, 80])\n        if self.export and self.format in (&amp;#39;saved_model&amp;#39;, &amp;#39;pb&amp;#39;, &amp;#39;tflite&amp;#39;, &amp;#39;edgetpu&amp;#39;, &amp;#39;tfjs&amp;#39;):  # avoid TF FlexSplitV ops  False\n            box = x_cat[:, : self.reg_max * 4]\n            cls = x_cat[:, self.reg_max * 4 :]\n        else:\n            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)      # torch.Size([1, 64, 5040]),torch.Size([1, 6, 5040])\n        if self.export and self.format in (&amp;#39;tflite&amp;#39;, &amp;#39;edgetpu&amp;#39;):        # False\n            grid_h = shape[2]\n            grid_w = shape[3]\n            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)\n            norm = self.strides / (self.stride[0] * grid_size)\n            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])\n        else:\n            dbox = self.`decode_bboxes`(self.`dfl`(box), self.anchors.unsqueeze(0)) * self.strides     # torch.Size([1, 4, 5040])\n        y = torch.cat((dbox, cls.sigmoid()), 1)    # torch.Size([1, 10, 5040])\n        return y if self.export else (y, x)        # self.export=False  (torch.Size([1, 10, 5040]),[torch.Size([1, 70, 48, 80]), [1, 70, 24, 40], [1, 70, 12, 20]])\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/models/yolov10/predict.py</p><span class=\'hidden-code\' data-code=\'class YOLOv10DetectionPredictor(DetectionPredictor):\n    def postprocess(self, preds, img, orig_imgs):\n        if isinstance(preds, dict):\n            preds = preds[&amp;#39;one2one&amp;#39;]                      # torch.Size([1, 10, 5040])\n        if isinstance(preds, (list, tuple)):\n            preds = preds[0]\n        if preds.shape[-1] == 6:\n            pass\n        else:\n            preds = preds.transpose(-1, -2)               # torch.Size([1, 5040, 10])\n            bboxes, scores, labels = ops.`v10postprocess`(preds, self.args.max_det, preds.shape[-1]-4)\n            bboxes = ops.`xywh2xyxy`(bboxes)\n            preds = torch.cat([bboxes, scores.unsqueeze(-1), labels.unsqueeze(-1)], dim=-1)    # torch.Size([1, 300, 6])\n        mask = preds[..., 4] > self.args.conf             # 0.25\n        if self.args.classes is not None:                 # False\n            mask = mask &amp; (preds[..., 5:6] == torch.tensor(self.args.classes, device=preds.device).unsqueeze(0)).any(2)\n        preds = [p[mask[idx]] for idx, p in enumerate(preds)]     # [torch.Size([15, 6])]\n        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list\n            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n        results = []\n        for i, pred in enumerate(preds):\n            orig_img = orig_imgs[i]\n            pred[:, :4] = ops.`scale_boxes`(img.shape[2:], pred[:, :4], orig_img.shape)\n            img_path = self.batch[0][i]\n            results.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\n        return results\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/utils/ops.py</p><span class=\'hidden-code\' data-code=\'def v10postprocess(preds, max_det, nc=80):          # torch.Size([1, 5040, 10]),300,10\n    assert(4 + nc == preds.shape[-1])\n    boxes, scores = preds.split([4, nc], dim=-1)    # torch.Size([1, 5040, 4]) + torch.Size([1, 5040, 6])\n    max_scores = scores.amax(dim=-1)                # torch.Size([1, 5040])\n    max_scores, index = torch.topk(max_scores, max_det, dim=-1)                        # torch.Size([1, 300]), torch.Size([1, 300])\n    index = index.unsqueeze(-1)                     #  torch.Size([1, 300, 1]),\n    boxes = torch.gather(boxes, dim=1, index=index.repeat(1, 1, boxes.shape[-1]))      # torch.Size([1, 300, 4])\n    scores = torch.gather(scores, dim=1, index=index.repeat(1, 1, scores.shape[-1]))   # torch.Size([1, 300, 6])\n    scores, index = torch.topk(scores.flatten(1), max_det, dim=-1)        # torch.Size([1, 1800])-->torch.Size([1, 300])+torch.Size([1, 300])  这里同一个框有两个label\n    labels = index % nc             # torch.Size([1, 300])\n    index = index // nc\n    boxes = boxes.gather(dim=1, index=index.unsqueeze(-1).repeat(1, 1, boxes.shape[-1]))\n    return boxes, scores, labels    # torch.Size([1, 300, 4])，torch.Size([1, 300, 1])，torch.Size([1, 300, 1])\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/utils/ops.py</p><span class=\'hidden-code\' data-code=\'def xywh2xyxy(x):\n    assert x.shape[-1] == 4, f&amp;#39;input shape last dimension expected 4 but input shape is {x.shape}&amp;#39;\n    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n    dw = x[..., 2] / 2  # half-width\n    dh = x[..., 3] / 2  # half-height\n    y[..., 0] = x[..., 0] - dw  # top left x\n    y[..., 1] = x[..., 1] - dh  # top left y\n    y[..., 2] = x[..., 0] + dw  # bottom right x\n    y[..., 3] = x[..., 1] + dh  # bottom right y\n    return y\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/utils/ops.py</p><span class=\'hidden-code\' data-code=\'def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None, padding=True, xywh=False):\n    if ratio_pad is None:                                                         # calculate from img0_shape\n        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new  0.3333333333333333\n        pad = (\n            round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1),\n            round((img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1),\n        )                                                                         # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n    if padding:\n        boxes[..., 0] -= pad[0]  # x padding\n        boxes[..., 1] -= pad[1]  # y padding\n        if not xywh:\n            boxes[..., 2] -= pad[0]  # x padding\n            boxes[..., 3] -= pad[1]  # y padding\n    boxes[..., :4] /= gain\n    return `clip_boxes`(boxes, img0_shape)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ultralytics/utils/ops.py</p><span class=\'hidden-code\' data-code=\'def clip_boxes(boxes, shape):\n    if isinstance(boxes, torch.Tensor):  # faster individually (WARNING: inplace .clamp_() Apple MPS bug)\n        boxes[..., 0] = boxes[..., 0].clamp(0, shape[1])  # x1\n        boxes[..., 1] = boxes[..., 1].clamp(0, shape[0])  # y1\n        boxes[..., 2] = boxes[..., 2].clamp(0, shape[1])  # x2\n        boxes[..., 3] = boxes[..., 3].clamp(0, shape[0])  # y2\n    else:  # np.array (faster grouped)\n        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n    return boxes\n\'> </span>'}]}]}]}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
