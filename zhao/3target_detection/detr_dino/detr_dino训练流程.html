<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>detr_dino训练流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">数据处理</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">datasets/coco.py</p><font size="0"><pre class="language-python"><code class="language-python">class CocoDetection(torchvision.datasets.CocoDetection):\n    def __getitem__(self, idx):\n        img, target = super(CocoDetection, self).<span style=\'color: green;font-weight: bold;\'>__getitem__</span>(idx)\n        image_id = self.ids[idx]                                       <span style=\'color: red\'># 435</span>\n        target = {\'image_id\': image_id, \'annotations\': target}\n        img, target = self.<span style=\'color: green;font-weight: bold;\'>prepare</span>(img, target)                      <span style=\'color: red\'># ConvertCocoPolysToMask</span>\n        if self._transforms is not None:\n            img, target = self._transforms(img, target)\n        return img, target                            <span style=\'color: red\'># torch.Size([3, 544, 870])  tensor([ 800, 1280])->tensor([544, 870])</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">site-packages/torchvision/datasets/coco.py</p><font size="0"><pre class="language-python"><code class="language-python">class CocoDetection(VisionDataset):\n    def _load_image(self, id: int) -> Image.Image:\n        path = self.coco.loadImgs(id)[0]["file_name"]\n        return Image.open(os.path.join(self.root, path)).convert("RGB")\n    def _load_target(self, id) -> List[Any]:\n        return self.coco.loadAnns(self.coco.getAnnIds(id))\n    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n        id = self.ids[index]\n        image = self._load_image(id)\n        target = self._load_target(id)    <span style=\'color: red\'># [{\'category_id\':0,\'bbox\':[714.62,164.68,88,38],\'area\':3344,\'image_id\':435,\'id\':4033,\'iscrowd\':0}]</span>\n        if self.transforms is not None:   <span style=\'color: red\'># None    RandomHorizontalFlip + RandomSelect + ToTensor + Normalize</span>\n            image, target = self.transforms(image, target)    \n        return image, target\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">datasets/coco.py</p><font size="0"><pre class="language-python"><code class="language-python">class ConvertCocoPolysToMask(object):\n    def __init__(self, return_masks=False):\n        self.return_masks = return_masks\n    def __call__(self, image, target):\n        w, h = image.size                  <span style=\'color: red\'># 1280,800</span>\n        image_id = target["image_id"]      <span style=\'color: red\'># 435</span>\n        image_id = torch.tensor([image_id])\n        anno = target["annotations"]\n        anno = [obj for obj in anno if \'iscrowd\' not in obj or obj[\'iscrowd\'] == 0]   <span style=\'color: red\'># 过滤</span>\n        boxes = [obj["bbox"] for obj in anno]\n        <span style=\'color: red\'># guard against no boxes via resizing</span>\n        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)  <span style=\'color: red\'># torch.Size([11, 4]) 非归一化的x1,y1,w,h</span>\n        boxes[:, 2:] += boxes[:, :2]\n        boxes[:, 0::2].clamp_(min=0, max=w)\n        boxes[:, 1::2].clamp_(min=0, max=h)\n        classes = [obj["category_id"] for obj in anno]                      <span style=\'color: red\'># [0, 0, 0, 2, 14, 14, 14, 8, 2, 2, 2] 这里要从0开始还是1？</span>\n        classes = torch.tensor(classes, dtype=torch.int64)\n        if self.return_masks:\n            segmentations = [obj["segmentation"] for obj in anno]\n            masks = convert_coco_poly_to_mask(segmentations, h, w)\n        keypoints = None\n        if anno and "keypoints" in anno[0]:               <span style=\'color: red\'># False</span>\n            keypoints = [obj["keypoints"] for obj in anno]\n            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n            num_keypoints = keypoints.shape[0]\n            if num_keypoints:\n                keypoints = keypoints.view(num_keypoints, -1, 3)\n        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n        boxes = boxes[keep]\n        classes = classes[keep]\n        if self.return_masks:                <span style=\'color: red\'># False</span>\n            masks = masks[keep]\n        if keypoints is not None:\n            keypoints = keypoints[keep]\n        target = {}\n        target["boxes"] = boxes\n        target["labels"] = classes\n        if self.return_masks:\n            target["masks"] = masks\n        target["image_id"] = image_id\n        if keypoints is not None:\n            target["keypoints"] = keypoints\n        <span style=\'color: red\'># for conversion to coco api</span>\n        area = torch.tensor([obj["area"] for obj in anno])\n        iscrowd = torch.tensor([obj["iscrowd"] if "iscrowd" in obj else 0 for obj in anno])\n        target["area"] = area[keep]\n        target["iscrowd"] = iscrowd[keep]\n        target["orig_size"] = torch.as_tensor([int(h), int(w)])\n        target["size"] = torch.as_tensor([int(h), int(w)])\n        return image, target\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">util/misc.pyutil/misc.py</p><font size="0"><pre class="language-python"><code class="language-python">def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    if tensor_list[0].ndim == 3:            <span style=\'color: red\'># torch.Size([3, 512, 625])+torch.Size([3, 878, 736])  batch=2</span>\n        if torchvision._is_tracing():\n            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n        <span style=\'color: red\'># TODO make it support different-sized images</span>\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        <span style=\'color: red\'># min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))</span>\n        batch_shape = [len(tensor_list)] + max_size\n        b, c, h, w = batch_shape\n        dtype = tensor_list[0].dtype\n        device = tensor_list[0].device\n        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n        for img, pad_img, m in zip(tensor_list, tensor, mask):\n            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n            m[: img.shape[1], :img.shape[2]] = False\n    else:\n        raise ValueError(\'not supported\')\n    return NestedTensor(tensor, mask)         <span style=\'color: red\'># torch.Size([2, 3, 878, 736])</span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><font size="0"><pre class="language-python"><code class="language-python">class DINO(nn.Module):\n    def forward(self, samples: NestedTensor, targets: List = None):\n        if not isinstance(samples, NestedTensor):\n            samples = nested_tensor_from_tensor_list(samples)     <span style=\'color: red\'># torch.Size([2, 3, 878, 736]) </span>\n        features, poss = self.backbone(samples)                   <span style=\'color: red\'># features的item数量为3，detr仅有一个->(2,384,110,92)-8 + (...,55,46)-16 + (....,28,23)-32</span>\n        srcs = []                                                 <span style=\'color: red\'># poss- (2, 256, 110, 92), (2, 256, 55, 46), (2, 256, 28, 23) 位置编码-根据图像大小hxw来的</span>\n        masks = []\n        for l, feat in enumerate(features):          <span style=\'color: red\'># 遍历每一个特征层</span>\n            src, mask = feat.decompose()             <span style=\'color: red\'># 分解出特征和mask</span>\n            srcs.append(self.input_proj[l](src))     <span style=\'color: red\'># 经过input_proj之后，这里的feature的channel都是256，wh不一样</span>\n            masks.append(mask)\n            assert mask is not None\n        if self.num_feature_levels > len(srcs):                                  <span style=\'color: red\'># 多出backbone特征层的层 4>3</span>\n            _len_srcs = len(srcs)                                                <span style=\'color: red\'># 3</span>\n            for l in range(_len_srcs, self.num_feature_levels):                  <span style=\'color: red\'># 3,4</span>\n                if l == _len_srcs:\n                    src = self.input_proj[l](features[-1].tensors)               <span style=\'color: red\'># 这里的还可以承接backbone的特征  (....,28,23)-32==>torch.Size([2,256,14,12])-约64</span>\n                else:\n                    src = self.input_proj[l](srcs[-1])                           <span style=\'color: red\'># 这里的只能承接上一层的了，跟backbone的特征已经中间有隔阂了</span>\n                m = samples.mask                                                 <span style=\'color: red\'># 这个是gt的mask   torch.Size([2, 878, 736])</span>\n                mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]       <span style=\'color: red\'># 使用插值创建新的mask torch.Size([2, 14, 12])</span>\n                pos_l = self.backbone[1](NestedTensor(src, mask)).to(src.dtype)  <span style=\'color: red\'># backbone[1] 是位置编码，因为这个最后的src，和gt的mask，并没有经过位置编码，这里调用了他们的forward方法</span>\n                srcs.append(src)                    <span style=\'color: red\'># torch.Size([2, 256, 14, 12])</span>\n                masks.append(mask)                  <span style=\'color: red\'># torch.Size([2, 14, 12])</span>\n                poss.append(pos_l)                  <span style=\'color: red\'># 位置编码 torch.Size([2, 256, 14, 12])--64</span>\n        <span style=\'color: red\'># 以上的处理与Deformable DETR相同 ---------------------------------------</span>\n        if self.dn_number > 0 or targets is not None:                        <span style=\'color: red\'># dn_number 100个CDN  生成噪声的代码 ------------------------------------</span>\n            input_query_label, input_query_bbox, attn_mask, dn_meta = <span style=\'color: green;font-weight: bold;\'>prepare_for_cdn</span>(dn_args=(targets, self.dn_number, self.dn_label_noise_ratio, self.dn_box_noise_scale),\n                                training=self.training, num_queries=self.num_queries, num_classes=self.num_classes, hidden_dim=self.hidden_dim, label_enc=self.label_enc)\n            <span style=\'color: red\'># input_query_label (bs,N=11*2*9,256)        input_query_box (bs,N=11*2*9,4)       attn_mask [11*2*9+900,11*2*9+900]  {\'pad_size\': 198, \'num_dn_group\': 9}</span>\n        else:\n            assert targets is None                   <span style=\'color: red\'># 推理模式</span>\n            input_query_bbox = input_query_label = attn_mask = dn_meta = None\n        hs, reference, hs_enc, ref_enc, init_box_proposal = self.<span style=\'color: green;font-weight: bold;\'>transformer</span>(srcs, masks, input_query_bbox, poss, input_query_label, attn_mask)\n        <span style=\'color: red\'># transformer方法的参数还是一样的</span>\n        <span style=\'color: red\'># hs->6*[bs,N(num_queries+2*dn_number),256]     reference->7*[bs,N(num_queries+2*dn_number),4]       enc->这两个是encoder后经过头的输出</span>\n        <span style=\'color: red\'># hs_enc->[1,bs,num_queries,256]                ref_enc->[1,bs,num_queries,4]                        init_box_proposal->[bs,900,4] 最最初始的参考点位</span>\n        hs[0] += self.label_enc.weight[0, 0] * 0.0           <span style=\'color: red\'># In case num object=0    torch.Size([2, 1098, 256])</span>\n        <span style=\'color: red\'># deformable-detr-like anchor update</span>\n        <span style=\'color: red\'># reference_before_sigmoid = inverse_sigmoid(reference[:-1])</span>\n        outputs_coord_list = []\n        <span style=\'color: red\'># 最后一层输出的经过修正后的坐标不使用</span>\n        <span style=\'color: red\'># layer_ref_sig 进入某一层之前的bbox</span>\n        <span style=\'color: red\'># layer_bbox_embed 这一层的bbox头</span>\n        <span style=\'color: red\'># layer_hs 这一层的layer的输出</span>\n        for dec_lid, (layer_ref_sig, layer_bbox_embed, layer_hs) in enumerate(zip(reference[:-1], self.bbox_embed, hs)):\n            layer_delta_unsig = layer_bbox_embed(layer_hs)        <span style=\'color: red\'># 网络输出的偏移量     torch.Size([2, 1098, 4])</span>\n            layer_outputs_unsig = layer_delta_unsig + inverse_sigmoid(layer_ref_sig)     <span style=\'color: red\'># 进行坐标修正  torch.Size([2, 1098, 4])</span>\n            layer_outputs_unsig = layer_outputs_unsig.sigmoid()   <span style=\'color: red\'># 0-1</span>\n            outputs_coord_list.append(layer_outputs_unsig)\n        outputs_coord_list = torch.stack(outputs_coord_list)    <span style=\'color: red\'># torch.Size([6, 2, 1098, 4])</span>\n        <span style=\'color: red\'># [6,bs,N,91] 类别输出</span>\n        outputs_class = torch.stack([layer_cls_embed(layer_hs) for layer_cls_embed, layer_hs in zip(self.class_embed, hs)])   <span style=\'color: red\'># torch.Size([6, 2, 1098, 18])</span>\n        if self.dn_number > 0 and dn_meta is not None:\n            <span style=\'color: red\'># dn的后处理  返回的是匹配部分的结果，去噪部分的不在这之内了  [6,bs,num_queries,91]  [6,bs,num_queries,4]</span>\n            outputs_class, outputs_coord_list = <span style=\'color: green;font-weight: bold;\'>dn_post_process</span>(outputs_class, outputs_coord_list, dn_meta, self.aux_loss, self._set_aux_loss)\n        <span style=\'color: red\'># 这里与DN-DETR相同                                                                   "pred_logits"-[batch_size x num_queries x (num_classes + 1)]; </span>\n        out = {\'pred_logits\': outputs_class[-1], \'pred_boxes\': outputs_coord_list[-1]}     <span style=\'color: red\'># "pred_boxes": (center_x, center_y, height, width).values are normalized in [0, 1],</span>\n        if self.aux_loss:\n            out[\'aux_outputs\'] = self._set_aux_loss(outputs_class, outputs_coord_list)\n        <span style=\'color: red\'># for encoder output</span>\n        if hs_enc is not None:                    <span style=\'color: red\'># torch.Size([1, 2, 900, 256])</span>\n            <span style=\'color: red\'># prepare intermediate outputs  ref_enc 为 [1,bs,900,4] 取-1，其实也只有一个 --> torch.Size([2, 900, 4])</span>\n            interm_coord = ref_enc[-1]\n            interm_class = self.transformer.enc_out_class_embed(hs_enc[-1])        <span style=\'color: red\'># hs_enc [1,bs,900,256]--->torch.Size([2, 900, 18])</span>\n            out[\'interm_outputs\'] = {\'pred_logits\': interm_class, \'pred_boxes\': interm_coord}       <span style=\'color: red\'># encoder输出的经过topk之后的，也放入到out中</span>\n            out[\'interm_outputs_for_matching_pre\'] = {\'pred_logits\': interm_class, \'pred_boxes\': init_box_proposal}   <span style=\'color: red\'># encoder输出的经过topk之后的，也放入到out中，init_box_proposal是在encoder时就生成的最最初始的参考点位</span>\n            if hs_enc.shape[0] > 1:              <span style=\'color: red\'># prepare enc outputs 还有其他的值，可能是应对多个encoder的layer的输出吧</span>\n                enc_outputs_coord = []\n                enc_outputs_class = []\n                for layer_id, (layer_box_embed, layer_class_embed, layer_hs_enc, layer_ref_enc) in enumerate(zip(self.enc_bbox_embed, self.enc_class_embed, hs_enc[:-1], ref_enc[:-1])):\n                    layer_enc_delta_unsig = layer_box_embed(layer_hs_enc)\n                    layer_enc_outputs_coord_unsig = layer_enc_delta_unsig + inverse_sigmoid(layer_ref_enc)\n                    layer_enc_outputs_coord = layer_enc_outputs_coord_unsig.sigmoid()\n                    layer_enc_outputs_class = layer_class_embed(layer_hs_enc)\n                    enc_outputs_coord.append(layer_enc_outputs_coord)\n                    enc_outputs_class.append(layer_enc_outputs_class)\n                out[\'enc_outputs\'] = [{\'pred_logits\': a, \'pred_boxes\': b} for a, b in zip(enc_outputs_class, enc_outputs_coord)]\n        out[\'dn_meta\'] = dn_meta\n        return out\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dn_components.py</p><font size="0"><pre class="language-python"><code class="language-python">def prepare_for_cdn(dn_args, training, num_queries, num_classes, hidden_dim, label_enc):\n    if training:                                                          <span style=\'color: red\'># 在传给transformer之前的预处理</span>\n        targets, dn_number, label_noise_ratio, box_noise_scale = dn_args  <span style=\'color: red\'># dn_number=100,0.5，1</span>\n        dn_number = dn_number * 2                                         <span style=\'color: red\'># positive and negative dn queries正负样本数量相同</span>\n        known = [(torch.ones_like(t[\'labels\'])).cuda() for t in targets]  <span style=\'color: red\'># [tensor([1,1]),tensor([1,1,1]),...]</span>\n        batch_size = len(known)                                           <span style=\'color: red\'># 2</span>\n        known_num = [sum(k) for k in known]                               <span style=\'color: red\'># 各个image gt的数量  [tensor(11, device=\'cuda:0\'), tensor(6, device=\'cuda:0\')]</span>\n        if int(max(known_num)) == 0:\n            dn_number = 1                                                 <span style=\'color: red\'># 没有gt</span>\n        else:\n            if dn_number >= 100:                                          <span style=\'color: red\'># 每组里面会涵盖所有GTbox</span>\n                dn_number = dn_number // (int(max(known_num) * 2))        <span style=\'color: red\'># 有点类似于dn-detr的group的处理 200//(11*2)=9  9组吗？</span>\n            elif dn_number < 1:\n                dn_number = 1\n        if dn_number == 0:\n            dn_number = 1\n        <span style=\'color: red\'># 以下的这些处理与DN-DETR基本一致   假设87个GT</span>\n        unmask_bbox = unmask_label = torch.cat(known)         <span style=\'color: red\'># 所有的1 cat到一起       torch.size=(17,)   最大的是11</span>\n        labels = torch.cat([t[\'labels\'] for t in targets])    <span style=\'color: red\'># 取出所有gt的label值     torch.size=(17,)   这里从0开始？</span>\n        boxes = torch.cat([t[\'boxes\'] for t in targets])      <span style=\'color: red\'># 取出所有gt的box         torch.size=(17,4)</span>\n        batch_idx = torch.cat([torch.full_like(t[\'labels\'].long(), i) for i, t in enumerate(targets)])   <span style=\'color: red\'># 标识属于哪个图片 like tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device=\'cuda:0\') torch.Size([17])</span>\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)         <span style=\'color: red\'># 返回一个二维张量，其中每一行都是非零值的索引 torch.Size([17,1]) tensor([[0],[1],[2],......])</span>\n        known_indice = known_indice.view(-1)                             <span style=\'color: red\'># 拉平 torch.Size([17])</span>\n        known_indice = known_indice.repeat(2 * dn_number, 1).view(-1)    <span style=\'color: red\'># 并没有使用    torch.Size([696])</span>\n        <span style=\'color: red\'># 这三项是gt相关的值</span>\n        known_labels = labels.repeat(2 * dn_number, 1).view(-1)   <span style=\'color: red\'># (all gt*2*dn_number) 这里的N是bs中所有的gt的数量总和 torch.Size([17*2*9=306])</span>\n        known_bid = batch_idx.repeat(2 * dn_number, 1).view(-1)   <span style=\'color: red\'># (all gt*2*dn_number)</span>\n        known_bboxs = boxes.repeat(2 * dn_number, 1)              <span style=\'color: red\'># [all gt*2*dn_number, 4]</span>\n        <span style=\'color: red\'># 这两个是克隆的</span>\n        known_labels_expaned = known_labels.clone()\n        known_bbox_expand = known_bboxs.clone()\n        <span style=\'color: red\'># --------------------------------------------------------------------------------------------------------------</span>\n        if label_noise_ratio > 0:                                                  <span style=\'color: red\'># 相当于对某些类别换成新类别</span>\n            p = torch.rand_like(known_labels_expaned.float())                      <span style=\'color: red\'># 随机值，0-1内                                               torch.Size([306])</span>\n            chosen_indice = torch.nonzero(p < (label_noise_ratio * 0.5)).view(-1)  <span style=\'color: red\'># half of bbox prob </span>\n            new_label = torch.randint_like(chosen_indice, 0, num_classes)          <span style=\'color: red\'># randomly put a new one here </span>\n            known_labels_expaned.scatter_(0, chosen_indice, new_label)             <span style=\'color: red\'># 把上面的值塞进去                                             306个值里面有75个是随机生成得</span>\n        single_pad = int(max(known_num))                          <span style=\'color: red\'># bs中最多的target的数量11</span>\n        pad_size = int(single_pad * 2 * dn_number)                <span style=\'color: red\'># 假设所有的gt=77，但是最多的是11个gt，bs=2   11*2*9=198</span>\n        positive_idx = torch.tensor(range(len(boxes))).long().cuda().unsqueeze(0).repeat(dn_number, 1)    <span style=\'color: red\'># [dn_number组数,gt count所有77]-->torch.Size([9, 17]) 9个0-16</span>\n        <span style=\'color: red\'># 加上了group间的偏移量  torch.Size([9, 1])                </span>\n        positive_idx += (torch.tensor(range(dn_number)) * len(boxes) * 2).long().cuda().unsqueeze(1)\n        positive_idx = positive_idx.flatten()                     <span style=\'color: red\'># torch.Size([153]) [gt count*dn number=17*4]，推平  [0,...,16]+[0+17*2x1=34,...,16+17*2=50]+[0+2*17*2=68,...,16+2*17*2=84]+...</span>\n        negative_idx = positive_idx + len(boxes)                  <span style=\'color: red\'># 再+17  正好剩下的位置是留给negative的 [0+17,...,16+17]+[0+17*2x1=34+17,...,</span>\n        if box_noise_scale > 0:                                   <span style=\'color: red\'># 1</span>\n            known_bbox_ = torch.zeros_like(known_bboxs)           <span style=\'color: red\'># [all gt*2*dn_number, 4]  torch.Size([306, 4])  17x2x9</span>\n            known_bbox_[:, :2] = known_bboxs[:, :2] - known_bboxs[:, 2:] / 2        <span style=\'color: red\'># x1,y1(17*2*4,2)</span>\n            known_bbox_[:, 2:] = known_bboxs[:, :2] + known_bboxs[:, 2:] / 2        <span style=\'color: red\'># x2,y2(17*2*4,2)</span>\n            diff = torch.zeros_like(known_bboxs)\n            diff[:, :2] = known_bboxs[:, 2:] / 2                  <span style=\'color: red\'># (w/2,h/2)</span>\n            diff[:, 2:] = known_bboxs[:, 2:] / 2                  <span style=\'color: red\'># (w/2,h/2)</span>\n            rand_sign = torch.randint_like(known_bboxs, low=0, high=2, dtype=torch.float32) * 2.0 - 1.0   <span style=\'color: red\'># (0,2)->(-1,1)===>(17*2*9=306,4)</span>\n            rand_part = torch.rand_like(known_bboxs)              <span style=\'color: red\'># (17*2*9);0-1内的值</span>\n            rand_part[negative_idx] += 1.0                        <span style=\'color: red\'># 负样本位置的值在1-2</span>\n            rand_part *= rand_sign                                <span style=\'color: red\'># [2*gt count*dn number,4] 坐标位置随机的乘上1 -1  正样本(-1,1)负样本(-2,-1)+(1,2)</span>\n            <span style=\'color: red\'># 加上随机的偏移，左上，右下的点随机的进行了偏移</span>\n            known_bbox_ = known_bbox_ + torch.mul(rand_part,diff).cuda() * box_noise_scale  <span style=\'color: red\'># 顶点左右上下偏移0.2*w或0.2*h属于正样本，偏移0.2*w->0.4*w或0.2*h->0.4*h属于负样本</span>\n            known_bbox_ = known_bbox_.clamp(min=0.0, max=1.0)                               <span style=\'color: red\'># 范围在0,1</span>\n            known_bbox_expand[:, :2] = (known_bbox_[:, :2] + known_bbox_[:, 2:]) / 2        <span style=\'color: red\'># xc,yc</span>\n            known_bbox_expand[:, 2:] = known_bbox_[:, 2:] - known_bbox_[:, :2]              <span style=\'color: red\'># w,h</span>\n        m = known_labels_expaned.long().to(\'cuda\')            <span style=\'color: red\'># 这里的known_labels_expaned 已经被添加过随机的噪声了 (2*gt count*dn_number) 新的label信息 torch.Size([306])</span>\n        input_label_embed = label_enc(m)                      <span style=\'color: red\'># Embedding(19, 256)  num_class=18,所以生成得标签是不是要从1开始，torch.Size([306, 256]) </span>\n        input_bbox_embed = inverse_sigmoid(known_bbox_expand) <span style=\'color: red\'># 对坐标取反函数  对应于特征图上的坐标  x/(1-x)   torch.Size([306, 4])范围从(-0.1146->0.8425)到(-0.1146->0.8425) </span>\n        padding_label = torch.zeros(pad_size, hidden_dim).cuda()       <span style=\'color: red\'># [pad_size=11*2*9, 256]-->torch.Size([198, 256])</span>\n        padding_bbox = torch.zeros(pad_size, 4).cuda()                 <span style=\'color: red\'># [pad_size=11*2*9, 4]     torch.Size([198, 4])</span>\n        input_query_label = padding_label.repeat(batch_size, 1, 1)     <span style=\'color: red\'># [2，11*2*9, 256]   batch=2,里面图片最多11个gt，有9个group分为正负</span>\n        input_query_bbox = padding_bbox.repeat(batch_size, 1, 1)       <span style=\'color: red\'># [2，11*2*9, 4]</span>\n        map_known_indice = torch.tensor([]).to(\'cuda\')\n        if len(known_num):         <span style=\'color: red\'># 如果有gt的话</span>\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])    <span style=\'color: red\'># tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5]) 17</span>\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(2 * dn_number)]).long()  <span style=\'color: red\'># 加上了偏移，这个偏移是这些batch中最大的gt的数量 17*2*9  torch.Size([306])</span>\n                                                                                                                    <span style=\'color: red\'># 第一行+11*0，第二行+11*1</span>\n        if len(known_bid):         <span style=\'color: red\'># known_bid 标识属于哪个图片的 </span>\n            input_query_label[(known_bid.long(), map_known_indice)] = input_label_embed      <span style=\'color: red\'># 替换对应的embed input_query_label第一个维度是bs，known_bid是标识的属于哪一个image  [2，11*2*9, 256]</span>\n            input_query_bbox[(known_bid.long(), map_known_indice)] = input_bbox_embed        <span style=\'color: red\'># [2，11*2*9, 4]</span>\n         \n        tgt_size = pad_size + num_queries         <span style=\'color: red\'># 这里pad_size=198=11*2*9 就是cdn总共的数量，包括了正负样本，num_queries是正常的query的数量----》11*2*9+900</span>\n        attn_mask = torch.ones(tgt_size, tgt_size).to(\'cuda\') < 0  <span style=\'color: red\'># (11*2*9+900,11*2*9+900)-->(1098,1098)的False</span>\n        attn_mask[pad_size:, :pad_size] = True          <span style=\'color: red\'># match query cannot see the reconstruct  :198,:198</span>\n        for i in range(dn_number):                <span style=\'color: red\'># 各个组的掩码  reconstruct cannot see each other</span>\n            if i == 0:                            <span style=\'color: red\'># 第一组</span>\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), single_pad * 2 * (i + 1):pad_size] = True      <span style=\'color: red\'># 看不到他后面的所有 0:22,22:198</span>\n            if i == dn_number - 1:                <span style=\'color: red\'># 最后一组</span>\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), :single_pad * i * 2] = True                    <span style=\'color: red\'># 看不到他前面的所有</span>\n            else:                                 <span style=\'color: red\'># 中间组 看不到他后面的 也看不到他前面的</span>\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), single_pad * 2 * (i + 1):pad_size] = True\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), :single_pad * 2 * i] = True\n        dn_meta = {\'pad_size\': pad_size,\'num_dn_group\': dn_number}         <span style=\'color: red\'># pad_size=198=11*2*9 就是cdn总共的数量  +  dn_number--->多少组 9   </span>\n    else:\n        input_query_label = None\n        input_query_bbox = None\n        attn_mask = None\n        dn_meta = None\n    <span style=\'color: red\'># label加入些噪声取随机值，bbox划分正负样本，[2，11*2*9, 256]  [2，11*2*9, 4]  (1098,1098)</span>\n    return input_query_label, input_query_bbox, attn_mask, dn_meta         <span style=\'color: red\'># 这里并不包含正常match部分的tgt，这点与DN-DETR的实现不同了    </span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class DeformableTransformer(nn.Module):\n    def forward(self, srcs, masks, refpoint_embed, pos_embeds, tgt, attn_mask=None): \n        <span style=\'color: red\'># [bs, ci, hi, wi];  [bs, hi, wi]; refpoint_embed-torch.Size([2, 198, 4]);  位置编码-encode;  tgt-torch.Size([2, 198, 256]);   torch.Size([1098, 1098])</span>\n        <span style=\'color: red\'># 先处理一下输入特征 --------------------------------------prepare input for encoder-------------------------------------------------------</span>\n        src_flatten = []\n        mask_flatten = []\n        lvl_pos_embed_flatten = []\n        spatial_shapes = []\n        for lvl, (src, mask, pos_embed) in enumerate(zip(srcs, masks, pos_embeds)):     <span style=\'color: red\'># 这地方的处理与Deformable DETR是相同的  src mask pos_embed 尺寸是相同的  </span>\n            bs, c, h, w = src.shape                             <span style=\'color: red\'># 特征图的高宽</span>\n            spatial_shape = (h, w)\n            spatial_shapes.append(spatial_shape)                <span style=\'color: red\'># 假设两个h,w得到[[20,30],[40,60]]</span>\n            src = src.flatten(2).transpose(1, 2)                <span style=\'color: red\'># bs, hw, c</span>\n            mask = mask.flatten(1)                              <span style=\'color: red\'># bs, hw</span>\n            pos_embed = pos_embed.flatten(2).transpose(1, 2)    <span style=\'color: red\'># bs, hw, c</span>\n            if self.num_feature_levels > 1 and self.level_embed is not None:      <span style=\'color: red\'># 这个是Deformable DETR论文中提到的Level embed</span>\n                lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)  <span style=\'color: red\'># pos_embed和level_embed相加  self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))</span>\n            else:\n                lvl_pos_embed = pos_embed\n            lvl_pos_embed_flatten.append(lvl_pos_embed)\n            src_flatten.append(src)  \n            mask_flatten.append(mask)\n        src_flatten = torch.cat(src_flatten, 1)                      <span style=\'color: red\'># bs, \\sum{hxw}, c   ==[bs,all hw,256] 所有特征层的拼在一起 torch.Size([2, 13462, 256])</span>\n        mask_flatten = torch.cat(mask_flatten, 1)                    <span style=\'color: red\'># bs, \\sum{hxw}                                           torch.Size([2, 13462])</span>\n        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)  <span style=\'color: red\'># bs, \\sum{hxw}, c                                        torch.Size([2, 13462, 256])</span>\n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)           <span style=\'color: red\'># [特征层的数量，2] tensor([[110,  92], [ 55,  46], [ 28,  23], [ 14,  12]], device=\'cuda:0\')</span>\n        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))  <span style=\'color: red\'># 各个src层 起始的位置 tensor([0,h1*w1,h2*w2,...,hn-1*wn-1])</span>\n        valid_ratios = torch.stack([self.<span style=\'color: green;font-weight: bold;\'>get_valid_ratio</span>(m) for m in masks], 1)                                 <span style=\'color: red\'># 有效高宽占总的batch高宽的比率 [bs,4,2]</span>\n        <span style=\'color: red\'># -----------------------------------------------------------以上的处理与Deformable DETR基本一致---------------------------------- </span>\n        enc_topk_proposals = enc_refpoint_embed = None               <span style=\'color: red\'># two stage</span>\n        <span style=\'color: red\'># 先经过encoder处理后两个输出值是encoder的中间层的输出            Begin Encoder</span>\n        <span style=\'color: red\'># -----------------------------------------------------------</span>\n        memory, enc_intermediate_output, enc_intermediate_refpoints = self.<span style=\'color: green;font-weight: bold;\'>encoder</span>(       <span style=\'color: red\'># torch.Size([2, 13462, 256])  +  None  +  None</span>\n            src_flatten,                                 <span style=\'color: red\'># 图像特征            torch.Size([2, 13462, 256]) </span>\n            pos=lvl_pos_embed_flatten,                   <span style=\'color: red\'># 空间位置编码        torch.Size([2, 13462, 256])</span>\n            level_start_index=level_start_index,         <span style=\'color: red\'># 每个特征图开始的位置索引  </span>\n            spatial_shapes=spatial_shapes,               <span style=\'color: red\'># 特征空间大小        tensor([[110,  92], [ 55,  46],[ 28,  23], [ 14,  12]], device=\'cuda:0\')</span>\n            valid_ratios=valid_ratios,\n            key_padding_mask=mask_flatten,               <span style=\'color: red\'># 以上这些参数是上面处理的 (bs,\\sum{hw})</span>\n            ref_token_index=enc_topk_proposals,  <span style=\'color: red\'># bs, nq     </span>\n            ref_token_coord=enc_refpoint_embed,  <span style=\'color: red\'># bs, nq, 4</span>\n        )  <span style=\'color: red\'># ---->(bs,\\sum{hw},c) 【None or (nenc+1, bs, nq, c) or (nenc, bs, nq, c)】【None or (nenc+1, bs, nq, c) or (nenc, bs, nq, c)】</span>\n        <span style=\'color: red\'># -----------------------------------------------------------</span>\n        if self.two_stage_type == \'standard\':    <span style=\'color: red\'># True</span>\n            if self.two_stage_learn_wh:          <span style=\'color: red\'># False</span>\n                input_hw = self.two_stage_wh_embedding.weight[0]\n            else:\n                input_hw = None\n            <span style=\'color: red\'># output_memory 是memory 经过填充inf，经过一层全连接后的结果 [bs,all hw,256]    output_proposals 是制作的proposals，非法的位置填充了inf [bs,all hw,4]</span>\n            output_memory, output_proposals = <span style=\'color: green;font-weight: bold;\'>gen_encoder_output_proposals</span>(memory, mask_flatten, spatial_shapes,input_hw)\n            <span style=\'color: red\'># 这一行在Deformable DETR中是在gen_encoder_output_proposals方法中的，这里是放到了这里</span>\n            output_memory = self.enc_output_norm(self.enc_output(output_memory))       <span style=\'color: red\'># nn.Linear(d_model, d_model) + nn.LayerNorm(d_model)  torch.Size([2, 13462, 256])</span>\n            if self.two_stage_pat_embed > 0:                                                  <span style=\'color: red\'># 0  False</span>\n                bs, nhw, _ = output_memory.shape\n                output_memory = output_memory.repeat(1, self.two_stage_pat_embed, 1)          <span style=\'color: red\'># output_memory: bs, n, 256; self.pat_embed_for_2stage: k, 256</span>\n                _pats = self.pat_embed_for_2stage.repeat_interleave(nhw, 0)\n                output_memory = output_memory + _pats\n                output_proposals = output_proposals.repeat(1, self.two_stage_pat_embed, 1)\n            if self.two_stage_add_query_num > 0:                                               <span style=\'color: red\'># 0  False</span>\n                assert refpoint_embed is not None\n                output_memory = torch.cat((output_memory, tgt), dim=1)\n                output_proposals = torch.cat((output_proposals, refpoint_embed), dim=1)\n            enc_outputs_class_unselected = self.enc_out_class_embed(output_memory)                    <span style=\'color: red\'># Linear(in_features=256, out_features=num_class, bias=True) 经过分类头 [bs,sum(hw),91]</span>\n            enc_outputs_coord_unselected = self.enc_out_bbox_embed(output_memory) + output_proposals  <span style=\'color: red\'># 经过box头 [bs,sum(hw),4] +output_proposals 是因为经过网络头的结果是修正</span>\n            topk = self.num_queries                                                                   <span style=\'color: red\'># 900</span>\n            topk_proposals = torch.topk(enc_outputs_class_unselected.max(-1)[0], topk, dim=1)[1]      <span style=\'color: red\'># bs, nq  torch.Size([2, 900])</span>\n            refpoint_embed_undetach = torch.gather(enc_outputs_coord_unselected, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, 4))  <span style=\'color: red\'># unsigmoid  取出对应index的四个坐标值出来  [bs,900,4]</span>\n            refpoint_embed_ = refpoint_embed_undetach.detach()                                        <span style=\'color: red\'># 参考点位脱离  torch.Size([2, 900, 4])</span>\n            init_box_proposal = torch.gather(output_proposals, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, 4)).sigmoid()          <span style=\'color: red\'># 同样的取出对应的初始点位的值-->sigmoid  torch.Size([2, 900, 4])</span>\n            tgt_undetach = torch.gather(output_memory, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, self.d_model))                 <span style=\'color: red\'># gather tgt 同样的取出对应的memory torch.Size([2, 900, 256])</span>\n            if self.embed_init_tgt:                                                                   <span style=\'color: red\'># 对应于论文中图5 c的 static content queries     True</span>\n                tgt_ = self.tgt_embed.weight[:, None, :].repeat(1, bs, 1).transpose(0, 1)             <span style=\'color: red\'># [900,256] -> [900,bs,256] -> [bs,900,256], 这里依然使用的是网络的参数 一个mebedding</span>\n            else:\n                tgt_ = tgt_undetach.detach()\n            <span style=\'color: red\'># refpoint_embed 是prepare_for_cdn生成的, 带有噪声的gt, tgt也是prepare_for_cdn生成的 带 _ 后缀的这两个是上面处理的，利用encoder的output topk筛选的</span>\n            if refpoint_embed is not None:\n                <span style=\'color: red\'># cat prepare_for_cdn生成的去噪的，加上match部分使用的</span>\n                refpoint_embed = torch.cat([refpoint_embed, refpoint_embed_], dim=1)     <span style=\'color: red\'># torch.Size([2, 198, 4])+torch.Size([2, 900, 4])-->torch.Size([2, 1098, 4])</span>\n                <span style=\'color: red\'># cat prepare_for_cdn生成的去噪的，加上match部分使用的</span>\n                tgt = torch.cat([tgt, tgt_], dim=1)                                      <span style=\'color: red\'># torch.Size([2, 198, 256])+torch.Size([2, 900, 256])-->torch.Size([2, 1098, 256])</span>\n            else:\n                <span style=\'color: red\'># 这种是推理模式，没有去噪的内容, 传入的参数refpoint_embed和tgt也就没有使用</span>\n                refpoint_embed, tgt = refpoint_embed_, tgt_\n        elif self.two_stage_type == \'no\':\n            tgt_ = self.tgt_embed.weight[:, None, :].repeat(1, bs, 1).transpose(0, 1)  <span style=\'color: red\'># nq, bs, d_model</span>\n            refpoint_embed_ = self.refpoint_embed.weight[:, None, :].repeat(1, bs, 1).transpose(0, 1)  <span style=\'color: red\'># nq, bs, 4</span>\n            if refpoint_embed is not None:\n                refpoint_embed = torch.cat([refpoint_embed, refpoint_embed_], dim=1)\n                tgt = torch.cat([tgt, tgt_], dim=1)\n            else:\n                refpoint_embed, tgt = refpoint_embed_, tgt_\n            if self.num_patterns > 0:\n                tgt_embed = tgt.repeat(1, self.num_patterns, 1)\n                refpoint_embed = refpoint_embed.repeat(1, self.num_patterns, 1)\n                tgt_pat = self.patterns.weight[None, :, :].repeat_interleave(self.num_queries, 1)  <span style=\'color: red\'># 1, n_q*n_pat, d_model</span>\n                tgt = tgt_embed + tgt_pat\n            init_box_proposal = refpoint_embed_.sigmoid()\n        else:\n            raise NotImplementedError("unknown two_stage_type {}".format(self.two_stage_type))\n        <span style=\'color: red\'># references比hs多一个，hs是6，references是7   hs [bs,all, 256]->[torch.Size([2, 1098, 256]),...,torch.Size([2, 1098, 256])] + references [bs,all, 4]  [torch.Size([2, 1098, 4]), ..., torch.Size([2, 1098, 4])]</span>\n        hs, references = self.<span style=\'color: green;font-weight: bold;\'>decoder</span>(\n            tgt=tgt.transpose(0, 1),                    <span style=\'color: red\'># torch.Size([1098, 2, 256])</span>\n            memory=memory.transpose(0, 1),              <span style=\'color: red\'># torch.Size([2, 13462, 256])</span>\n            memory_key_padding_mask=mask_flatten,       <span style=\'color: red\'># torch.Size([2, 13462])</span>\n            pos=lvl_pos_embed_flatten.transpose(0, 1),  <span style=\'color: red\'># torch.Size([13462, 2, 256])</span>\n            <span style=\'color: red\'># 参考点位</span>\n            refpoints_unsigmoid=refpoint_embed.transpose(0, 1),     <span style=\'color: red\'># torch.Size([1098, 2, 4])</span>\n            level_start_index=level_start_index,        <span style=\'color: red\'># tensor([    0, 10120, 12650, 13294], device=\'cuda:0\')</span>\n            spatial_shapes=spatial_shapes,              <span style=\'color: red\'># tensor([[110,  92], [ 55,  46], [ 28,  23], [ 14,  12]], device=\'cuda:0\')</span>\n            valid_ratios=valid_ratios,                  <span style=\'color: red\'># torch.Size([2, 4, 2])</span>\n            <span style=\'color: red\'># prepare_for_cdn时生成的</span>\n            tgt_mask=attn_mask)                         <span style=\'color: red\'># torch.Size([1098, 1098])</span>\n        <span style=\'color: red\'># Begin postprocess</span>\n        <span style=\'color: red\'># -------------------------------------------------------     </span>\n        if self.two_stage_type == \'standard\':\n            if self.two_stage_keep_all_tokens:\n                hs_enc = output_memory.unsqueeze(0)      <span style=\'color: red\'># 没有经过topk筛选的</span>\n                ref_enc = enc_outputs_coord_unselected.unsqueeze(0)\n                init_box_proposal = output_proposals\n            else:\n                <span style=\'color: red\'># [1,bs,900,256] tgt_undetach是encoder的输出memory经过topk选取后的</span>\n                hs_enc = tgt_undetach.unsqueeze(0)       <span style=\'color: red\'># torch.Size([2, 900, 256])-->torch.Size([1, 2, 900, 256])</span>\n                <span style=\'color: red\'># [1,bs,900,4] refpoint_embed_undetach是encoder的输出经过topk选取后的</span>\n                ref_enc = refpoint_embed_undetach.sigmoid().unsqueeze(0)  <span style=\'color: red\'># torch.Size([2, 900, 4])--->torch.Size([1, 2, 900, 4])</span>\n        else:\n            hs_enc = ref_enc = None\n        <span style=\'color: red\'># init_box_proposal 最最初始的参考点位    hs_enc, ref_enc 这两个是encoder的输出经过topk筛选的</span>\n        return hs, references, hs_enc, ref_enc, init_box_proposal\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class DeformableTransformer(nn.Module):\n    def get_valid_ratio(self, mask):\n        _, H, W = mask.shape\n        valid_H = torch.sum(~mask[:, :, 0], 1)\n        valid_W = torch.sum(~mask[:, 0, :], 1)\n        valid_ratio_h = valid_H.float() / H\n        valid_ratio_w = valid_W.float() / W\n        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n        return valid_ratio\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransformerEncoder(nn.Module):\n    def forward(self,src,pos,spatial_shapes,                  <span style=\'color: red\'># [bs, sum(hi*wi), 256]     [bs, sum(hi*wi), 256]pos embed for src.  spatial_shapes: h,w of each level [num_level, 2]  </span>\n            level_start_index,valid_ratios,key_padding_mask,  <span style=\'color: red\'># [num_level] start point of level in sum(hi*wi)    [bs, num_level, 2]    [bs, sum(hi*wi)]</span>\n            ref_token_index,ref_token_coord):                 <span style=\'color: red\'># None,None</span>\n        if self.two_stage_type in [\'no\', \'standard\', \'enceachlayer\', \'enclayer1\']:\n            assert ref_token_index is None\n        output = src                                          <span style=\'color: red\'># 图像特征   torch.Size([2, 13462, 256])</span>\n        if self.num_layers > 0:                               <span style=\'color: red\'># preparation and reshape   6</span>\n            if self.deformable_encoder:                       <span style=\'color: red\'># True</span>\n                reference_points = self.<span style=\'color: green;font-weight: bold;\'>get_reference_points</span>(spatial_shapes, valid_ratios, device=src.device)     <span style=\'color: red\'># encoder的参考点是生成的grid，Deformable DETR中的方法  torch.Size([2, 13462, 4, 2])</span>\n        intermediate_output = []\n        intermediate_ref = [] \n        if ref_token_index is not None:                       <span style=\'color: red\'># 这个传进来的值就是None</span>\n            out_i = torch.gather(output, 1, ref_token_index.unsqueeze(-1).repeat(1, 1, self.d_model))\n            intermediate_output.append(out_i)\n            intermediate_ref.append(ref_token_coord)\n        for layer_id, layer in enumerate(self.layers):        <span style=\'color: red\'># main process    encoder layer的循环</span>\n            dropflag = False\n            if self.enc_layer_dropout_prob is not None:       <span style=\'color: red\'># 默认是None</span>\n                prob = random.random()\n                if prob < self.enc_layer_dropout_prob[layer_id]:\n                    dropflag = True\n            if not dropflag:\n                if self.deformable_encoder:\n                    output = <span style=\'color: green;font-weight: bold;\'>layer</span>(src=output, pos=pos, reference_points=reference_points,         <span style=\'color: red\'># encoder layer 是与Deformable DETR相同的 -----> torch.Size([2, 13462, 256])</span>\n                                   spatial_shapes=spatial_shapes, level_start_index=level_start_index, key_padding_mask=key_padding_mask)\n                else:\n                    output = layer(src=output.transpose(0, 1), pos=pos.transpose(0, 1),key_padding_mask=key_padding_mask).transpose(0, 1)  <span style=\'color: red\'># 正常的attention</span>\n            if ((layer_id==0 and self.two_stage_type in [\'enceachlayer\',\'enclayer1\']) or (self.two_stage_type==\'enceachlayer\')) and (layer_id!=self.num_layers-1): <span style=\'color: red\'># two_stage_type默认是standard</span>\n                output_memory, output_proposals = <span style=\'color: green;font-weight: bold;\'>gen_encoder_output_proposals</span>(output, key_padding_mask, spatial_shapes)                   <span style=\'color: red\'># 在每一层encoder都进行topk proposal的选择</span>\n                output_memory = self.enc_norm[layer_id](self.enc_proj[layer_id](output_memory))  \n                topk = self.num_queries                                                                                    <span style=\'color: red\'># gather boxes</span>\n                enc_outputs_class = self.class_embed[layer_id](output_memory)\n                ref_token_index = torch.topk(enc_outputs_class.max(-1)[0], topk, dim=1)[1]                                 <span style=\'color: red\'># bs, nq</span>\n                ref_token_coord = torch.gather(output_proposals, 1, ref_token_index.unsqueeze(-1).repeat(1, 1, 4))\n                output = output_memory\n            \n            if (layer_id != self.num_layers - 1) and ref_token_index is not None:                                          <span style=\'color: red\'># ref_token_index 默认是None  aux loss</span>\n                out_i = torch.gather(output, 1, ref_token_index.unsqueeze(-1).repeat(1, 1, self.d_model))\n                intermediate_output.append(out_i)\n                intermediate_ref.append(ref_token_coord)\n        if self.norm is not None:\n            output = self.norm(output)\n        if ref_token_index is not None:\n            intermediate_output = torch.stack(intermediate_output)                                                          <span style=\'color: red\'># n_enc/n_enc-1, bs, \\sum{hw}, d_model</span>\n            intermediate_ref = torch.stack(intermediate_ref)\n        else:\n            intermediate_output = intermediate_ref = None\n        return output, intermediate_output, intermediate_ref      <span style=\'color: red\'># torch.Size([2, 13462, 256]),None,None</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransformerEncoder(nn.Module):\n    def get_reference_points(spatial_shapes, valid_ratios, device):       <span style=\'color: red\'># spatial_shapes[特征层数,2]=[[h1,w1],[h2,w2],...,[hn,wn]] valid_ratios [bs,特征层数,2]</span>\n        reference_points_list = []\n        for lvl, (H_, W_) in enumerate(spatial_shapes):\n            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),  <span style=\'color: red\'># 生成网格点,从0.5开始 到 减掉一个0.5</span>\n                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)      <span style=\'color: red\'># [bs,hw]</span>\n            ref = torch.stack((ref_x, ref_y), -1)                                       <span style=\'color: red\'># [bs,hw,2]</span>\n            reference_points_list.append(ref)\n        reference_points = torch.cat(reference_points_list, 1)                          <span style=\'color: red\'># 所有特征层的参考点拼在一起 [bs,all hw,2]</span>\n        reference_points = reference_points[:, :, None] * valid_ratios[:, None]         <span style=\'color: red\'># reference_points[:,:,None] -> [2,all hw,1,2] + valid_ratios[:,None] -> [bs,1,特征层数量,2]</span>\n        return reference_points                                                         <span style=\'color: red\'># [2,all hw,4,2]</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class DeformableTransformerEncoderLayer(nn.Module):\n    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, key_padding_mask=None):\n        src2 = self.<span style=\'color: green;font-weight: bold;\'>self_attn</span>(self.with_pos_embed(src, pos), reference_points, src, spatial_shapes, level_start_index, key_padding_mask)     <span style=\'color: red\'># self attention</span>\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src = self.forward_ffn(src)                            <span style=\'color: red\'># ffn</span>\n        <span style=\'color: red\'># 比Deformable DETR的encoder多的部分 channel attn</span>\n        if self.add_channel_attention:\n            src = self.norm_channel(src + self.activ_channel(src))\n        return src\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/ops/modules/ms_deform_attn.py</p><font size="0"><pre class="language-python"><code class="language-python">class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):   <span style=\'color: red\'># 维数 + 特征层的数量 + head的数量 + 每个特征层每个注意力头的采样点数量</span>\n        super().__init__()\n        _d_per_head = d_model // n_heads                                  <span style=\'color: red\'># 32  这个要为2的倍数</span>\n        self.im2col_step = 64\n        self.d_model = d_model\n        self.n_levels = n_levels\n        self.n_heads = n_heads\n        self.n_points = n_points\n        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)  <span style=\'color: red\'># 采样点的偏移量 论文中的2MK 就是n_heads*n_points 因为多层特征因此还有个n_levels</span>\n        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)     <span style=\'color: red\'># 权重矩阵，论文中的MK 就是n_heads*n_points 因为多层特征因此还有个n_levels</span>\n        self.value_proj = nn.Linear(d_model, d_model)                     <span style=\'color: red\'># 图2中左下区域的那个Linear</span>\n        self.output_proj = nn.Linear(d_model, d_model)                    <span style=\'color: red\'># 图2中右下区域的那个Linear  Deformable-DETR论文里面的  deformable attention module.</span>\n        self._reset_parameters()\n    def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):    \n        N, Len_q, _ = query.shape               <span style=\'color: red\'># query 是src+pos，query下面通过全连接层变成了attention_weights</span>\n        N, Len_in, _ = input_flatten.shape      <span style=\'color: red\'># input_flatten - [bs,all hw,256] 是src 对应了V</span>\n        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in         <span style=\'color: red\'># (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]</span>\n        value = self.value_proj(input_flatten)  <span style=\'color: red\'># 图二中左下的全连接Linear-》[bs,all hw,256]</span>\n        if input_padding_mask is not None:\n            value = value.masked_fill(input_padding_mask[..., None], float(0))                   <span style=\'color: red\'># 在mask的地方填充0 [bs, all hw,256]</span>\n        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)                <span style=\'color: red\'># 分成多头，拆分的是最后的256 [bs,all hw,256] -> [bs,all hw, 8, 32]</span>\n        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)   <span style=\'color: red\'># sampling_offsets全连接--> (bs, all hw,8,4,4,2) 8个头，4个特征层，4个采样点 2个偏移量坐标</span>\n        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)   <span style=\'color: red\'># attention_weights 是一个全连接 like (bs, all hw,8,16)</span>\n        attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points) <span style=\'color: red\'># 经过softmax 保证权重和为1，然后在拆分成4 4  like (bs,all hw,8,4,4)</span>\n        \n        if reference_points.shape[-1] == 2:                                                                        <span style=\'color: red\'># N, Len_q, n_heads, n_levels, n_points, 2</span>\n            offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)      <span style=\'color: red\'># input_spatial_shapes 换位置，高宽 变成 宽高</span>\n            <span style=\'color: red\'># reference_points  [bs,all hw,4,2] -> [bs,all hw,1,4,1,2] + sampling_offsets  [bs,all hw,8,4,4,2] / offset_normalizer [4,2] -> [1,1,1,4,1,2]</span>\n            <span style=\'color: red\'># 采样点加上偏移量 sampling_offsets / offset_normalizer 表示相对的偏移量   ----->   like (bs, hw,8,4,4,2)</span>\n            sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n        elif reference_points.shape[-1] == 4:\n            sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n        else:\n            raise ValueError(\'Last dim of reference_points must be 2 or 4, but get {} instead.\'.format(reference_points.shape[-1]))\n        \n        if value.dtype == torch.float16:                     <span style=\'color: red\'># for amp</span>\n            output = MSDeformAttnFunction.apply(value.to(torch.float32), input_spatial_shapes, input_level_start_index, sampling_locations.to(torch.float32), attention_weights, self.im2col_step)  <span style=\'color: red\'># for mixed precision</span>\n            output = output.to(torch.float16)\n            output = self.output_proj(output)\n            return output        \n        output = MSDeformAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights,self.im2col_step)  <span style=\'color: red\'># 这里调用了cuda实现  </span>\n        output = self.output_proj(output)   <span style=\'color: red\'># 图二中右下的全连接Linear</span>\n        return output\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/utils.py</p><font size="0"><pre class="language-python"><code class="language-python">def gen_encoder_output_proposals(memory, memory_padding_mask, spatial_shapes, learnedwh=None):      <span style=\'color: red\'># 与Deformable DETR相同的</span>\n    N_, S_, C_ = memory.shape\n    base_scale = 4.0\n    proposals = []\n    _cur = 0                                                              <span style=\'color: red\'># 定位在memory_padding_mask中的位置，memory_padding_mask这里是所有特征层的内容在一起了</span>\n    for lvl, (H_, W_) in enumerate(spatial_shapes):                                         <span style=\'color: red\'># 某个特征层的 高 宽</span>\n        mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H_ * W_)].view(N_, H_, W_, 1)   <span style=\'color: red\'># 获取mask, memory_padding_mask is[bs,all hw] [bs,H_,W_,1]</span>\n        valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)                <span style=\'color: red\'># 有效的高度</span>\n        valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)                <span style=\'color: red\'># 有效的宽度</span>\n        grid_y, grid_x = torch.meshgrid(torch.linspace(0, H_ - 1, H_, dtype=torch.float32, device=memory.device),   <span style=\'color: red\'># 生成网格点，这里没有0.5的做法 get_reference_points这个方法中是从0.5开始，这里是从0开始</span>\n                                        torch.linspace(0, W_ - 1, W_, dtype=torch.float32, device=memory.device))\n        grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)                      <span style=\'color: red\'># 生成网格二维点 H_, W_, 2    </span>\n        scale = torch.cat([valid_W.unsqueeze(-1), valid_H.unsqueeze(-1)], 1).view(N_, 1, 1, 2)  <span style=\'color: red\'># scale [bs,1,1,2]</span>\n        grid = (grid.unsqueeze(0).expand(N_, -1, -1, -1) + 0.5) / scale   <span style=\'color: red\'># 获取相对值 [bs,h,w,2]</span>\n        if learnedwh is not None:\n            wh = torch.ones_like(grid) * learnedwh.sigmoid() * (2.0 ** lvl)\n        else:\n            wh = torch.ones_like(grid) * 0.05 * (2.0 ** lvl)              <span style=\'color: red\'># [bs,h,w,2] 宽高的相对值，不同的特征层 wh不同,层级越高，wh相对也会更大</span>\n        proposal = torch.cat((grid, wh), -1).view(N_, -1, 4)              <span style=\'color: red\'># [bs,h,w,4] -> [bs,hw,4]</span>\n        proposals.append(proposal)\n        _cur += (H_ * W_)\n    \n    output_proposals = torch.cat(proposals, 1)                                                         <span style=\'color: red\'># [bs,all hw,4]</span>\n    output_proposals_valid = ((output_proposals > 0.01) & (output_proposals < 0.99)).all(-1, keepdim=True)    <span style=\'color: red\'># [bs,all hw,1] proposals 不需要太靠近边界</span>\n    output_proposals = torch.log(output_proposals / (1 - output_proposals))                            <span style=\'color: red\'># unsigmoid--->sigmoid的反函数 (-4.5951->4.5951)</span>\n    output_proposals = output_proposals.masked_fill(memory_padding_mask.unsqueeze(-1), float(\'inf\'))   <span style=\'color: red\'># 对于mask部分，填充inf</span>\n    output_proposals = output_proposals.masked_fill(~output_proposals_valid, float(\'inf\'))             <span style=\'color: red\'># 靠近边界的部分，填充inf</span>\n    output_memory = memory\n    output_memory = output_memory.masked_fill(memory_padding_mask.unsqueeze(-1), float(0))            <span style=\'color: red\'># 对于memory的输出进行同样的填充</span>\n    output_memory = output_memory.masked_fill(~output_proposals_valid, float(0))                      <span style=\'color: red\'># [bs,all hw,256]</span>\n    return output_memory, output_proposals\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/utils.py</p><font size="0"><pre class="language-python"><code class="language-python">def gen_encoder_output_proposals(memory:Tensor, memory_padding_mask:Tensor, spatial_shapes:Tensor, learnedwh=None):\n    <span style=\'color: red\'># torch.Size([2, 13462, 256])  torch.Size([2, 13462])  torch.Size([4, 2])   None</span>\n    N_, S_, C_ = memory.shape\n    base_scale = 4.0\n    proposals = []\n    _cur = 0\n    for lvl, (H_, W_) in enumerate(spatial_shapes):\n        mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H_ * W_)].view(N_, H_, W_, 1)       <span style=\'color: red\'># torch.Size([2, 110, 92, 1])</span>\n        valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)          <span style=\'color: red\'># tensor([ 65, 110], device=\'cuda:0\')</span>\n        valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)          <span style=\'color: red\'># tensor([79, 92], device=\'cuda:0\')</span>\n        grid_y, grid_x = torch.meshgrid(torch.linspace(0, H_ - 1, H_, dtype=torch.float32, device=memory.device),   <span style=\'color: red\'># torch.Size([110, 92])</span>\n                                        torch.linspace(0, W_ - 1, W_, dtype=torch.float32, device=memory.device))   <span style=\'color: red\'># torch.Size([110, 92])</span>\n        grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1) <span style=\'color: red\'># H_, W_, 2  torch.Size([110, 92, 2])</span>\n        scale = torch.cat([valid_W.unsqueeze(-1), valid_H.unsqueeze(-1)], 1).view(N_, 1, 1, 2)\n        grid = (grid.unsqueeze(0).expand(N_, -1, -1, -1) + 0.5) / scale\n        if learnedwh is not None:\n            wh = torch.ones_like(grid) * learnedwh.sigmoid() * (2.0 ** lvl)\n        else:\n            wh = torch.ones_like(grid) * 0.05 * (2.0 ** lvl)\n        proposal = torch.cat((grid, wh), -1).view(N_, -1, 4)\n        proposals.append(proposal)\n        _cur += (H_ * W_)\n    output_proposals = torch.cat(proposals, 1)                      <span style=\'color: red\'># torch.Size([2, 13462, 4])</span>\n    output_proposals_valid = ((output_proposals > 0.01) & (output_proposals < 0.99)).all(-1, keepdim=True)  <span style=\'color: red\'># torch.Size([2, 13462, 1])</span>\n    output_proposals = torch.log(output_proposals / (1 - output_proposals)) <span style=\'color: red\'># unsigmoid</span>\n    output_proposals = output_proposals.masked_fill(memory_padding_mask.unsqueeze(-1), float(\'inf\'))\n    output_proposals = output_proposals.masked_fill(~output_proposals_valid, float(\'inf\'))\n    output_memory = memory\n    output_memory = output_memory.masked_fill(memory_padding_mask.unsqueeze(-1), float(0))\n    output_memory = output_memory.masked_fill(~output_proposals_valid, float(0))\n    return output_memory, output_proposals     <span style=\'color: red\'># torch.Size([2, 13462, 256])   torch.Size([2, 13462, 4])</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class DeformableTransformer(nn.Module):\n    def forward(self, srcs, masks, refpoint_embed, pos_embeds, tgt, attn_mask=None):\n        output = tgt                                             <span style=\'color: red\'># torch.Size([1098, 2, 256])</span>\n        intermediate = []                                        <span style=\'color: red\'># 每一层decoder计算后的结果</span>\n        reference_points = refpoints_unsigmoid.sigmoid()         <span style=\'color: red\'># [900+2*9*11,bs,4] 限制在0-1</span>\n        ref_points = [reference_points]                          <span style=\'color: red\'># 有一个初始的点位，以及每一层decoder计算后，进行修正后的结果</span>\n        for layer_id, layer in enumerate(self.layers):\n            <span style=\'color: red\'># preprocess ref points 对box添加随机的扰动噪声</span>\n            if self.training and self.decoder_query_perturber is not None and layer_id != 0:\n                reference_points = self.decoder_query_perturber(reference_points)\n            if self.deformable_decoder:\n                if reference_points.shape[-1] == 4:            <span style=\'color: red\'># [N,bs,1,4]*[1,bs,4,4] -> [N,bs,4,4]  torch.Size([1098, 2, 4])-->torch.Size([1098, 2, 4, 4])</span>\n                    reference_points_input = reference_points[:, :, None] * torch.cat([valid_ratios, valid_ratios], -1)[None, :]  <span style=\'color: red\'># nq, bs, nlevel, 4</span>\n                else:\n                    assert reference_points.shape[-1] == 2\n                    reference_points_input = reference_points[:, :, None] * valid_ratios[None, :]\n                <span style=\'color: red\'># get sine embedding for the query vector -> [N,bs,512] xywh的高频位置编码</span>\n                query_sine_embed = gen_sineembed_for_position(reference_points_input[:, :, 0, :])  <span style=\'color: red\'># nq, bs, 256*2    torch.Size([1098, 2, 512])</span>\n            else:\n                query_sine_embed = gen_sineembed_for_position(reference_points)  <span style=\'color: red\'># nq, bs, 256*2</span>\n                reference_points_input = None\n            <span style=\'color: red\'># conditional query [N,bs,256]</span>\n            raw_query_pos = self.ref_point_head(query_sine_embed)                <span style=\'color: red\'># torch.Size([1098, 2, 512])-->torch.Size([1098, 2, 256])</span>\n            pos_scale = self.query_scale(output) if self.query_scale is not None else 1            <span style=\'color: red\'># None-->1</span>\n            <span style=\'color: red\'># conditional detr的做法</span>\n            query_pos = pos_scale * raw_query_pos        <span style=\'color: red\'># torch.Size([1098, 2, 256])</span>\n            if not self.deformable_decoder:              <span style=\'color: red\'># 如果不使用变形attention</span>\n                query_sine_embed = query_sine_embed[..., :self.d_model] * self.query_pos_sine_scale(output)      <span style=\'color: red\'># 这里就是conditional detr的那个乘法</span>\n            <span style=\'color: red\'># 如果使用了变形attention 就不执行hw的调制了，todo 论文中好像没有提到过 modulated HW attentions</span>\n            if not self.deformable_decoder and self.modulate_hw_attn:\n                <span style=\'color: red\'># DAB-DETR的部分; 结构图中第二行的MLP的右边那个MLP; 公式7的Wref,Href [300,bs,2]</span>\n                refHW_cond = self.ref_anchor_head(output).sigmoid()  <span style=\'color: red\'># nq, bs, 2</span>\n                <span style=\'color: red\'># 公式6的Xref</span>\n                query_sine_embed[..., self.d_model <span style=\'color: red\'>// 2:] *= (refHW_cond[..., 0] / reference_points[..., 2]).unsqueeze(-1)</span>\n                <span style=\'color: red\'># 公式6的Yref</span>\n                query_sine_embed[..., :self.d_model <span style=\'color: red\'>// 2] *= (refHW_cond[..., 1] / reference_points[..., 3]).unsqueeze(-1)</span>\n            <span style=\'color: red\'># 随机的跨过某些decoder layer，并不处理，直接进入下一层random drop some layers if needed</span>\n            dropflag = False\n            if self.dec_layer_dropout_prob is not None:\n                prob = random.random()\n                if prob < self.dec_layer_dropout_prob[layer_id]:\n                    dropflag = True\n            if not dropflag:\n                <span style=\'color: red\'># [N,bs,256]</span>\n                output = layer(\n                    tgt=output,                <span style=\'color: red\'># [N,bs,256]     torch.Size([1098, 2, 256])</span>\n                    tgt_query_pos=query_pos,   <span style=\'color: red\'># [N,bs,256]     torch.Size([1098, 2, 256])</span>\n                    tgt_query_sine_embed=query_sine_embed,        <span style=\'color: red\'># [N,bs,512]   torch.Size([1098, 2, 512])</span>\n                    tgt_key_padding_mask=tgt_key_padding_mask,    <span style=\'color: red\'># None</span>\n                    tgt_reference_points=reference_points_input,  <span style=\'color: red\'># [N,bs,4,4]   torch.Size([1098, 2, 4, 4])</span>\n                    memory=memory,             <span style=\'color: red\'># [sum(hw),bs,256]                torch.Size([13462, 2, 256])</span>\n                    memory_key_padding_mask=memory_key_padding_mask,            <span style=\'color: red\'># torch.Size([2, 13462])</span>\n                    memory_level_start_index=level_start_index,                 <span style=\'color: red\'># tensor([    0, 10120, 12650, 13294], device=\'cuda:0\')</span>\n                    memory_spatial_shapes=spatial_shapes,         <span style=\'color: red\'># [level,2]  torch.Size([4, 2])</span>\n                    memory_pos=pos,            <span style=\'color: red\'># [sum(hw),bs,256]</span>\n                    self_attn_mask=tgt_mask,   <span style=\'color: red\'># prepare_for_cdn时生成的  torch.Size([1098, 1098])</span>\n                    cross_attn_mask=memory_mask                   <span style=\'color: red\'># 默认为None，调用时并未传入  None</span>\n                )\n            <span style=\'color: red\'># iter update</span>\n            if self.bbox_embed is not None:\n                reference_before_sigmoid = inverse_sigmoid(reference_points)    <span style=\'color: red\'># 得到特征图上的值</span>\n                delta_unsig = self.bbox_embed[layer_id](output)                 <span style=\'color: red\'># 得到网络输出的修正值</span>\n                outputs_unsig = delta_unsig + reference_before_sigmoid          <span style=\'color: red\'># 进行修正</span>\n                new_reference_points = outputs_unsig.sigmoid()                  <span style=\'color: red\'># 限制在0-1</span>\n                <span style=\'color: red\'># select</span>\n                if self.dec_layer_number is not None and layer_id != self.num_layers - 1:\n                    nq_now = new_reference_points.shape[0]\n                    select_number = self.dec_layer_number[layer_id + 1]\n                    if nq_now != select_number:\n                        class_unselected = self.class_embed[layer_id](output)  <span style=\'color: red\'># nq, bs, 91</span>\n                        topk_proposals = torch.topk(class_unselected.max(-1)[0], select_number, dim=0)[1]  <span style=\'color: red\'># new_nq, bs</span>\n                        new_reference_points = torch.gather(new_reference_points, 0,topk_proposals.unsqueeze(-1).repeat(1, 1, 4))  <span style=\'color: red\'># unsigmoid</span>\n                if self.rm_detach and \'dec\' in self.rm_detach:\n                    reference_points = new_reference_points\n                else:\n                    reference_points = new_reference_points.detach()      <span style=\'color: red\'># 脱离</span>\n                if self.use_detached_boxes_dec_out:\n                    ref_points.append(reference_points)                   <span style=\'color: red\'># 这里是脱离的</span>\n                else:\n                    ref_points.append(new_reference_points)               <span style=\'color: red\'># 这个是没有脱离的，这个地方的处理就是论文钟的look forward twice</span>\n            <span style=\'color: red\'># 中间层的值</span>\n            intermediate.append(self.norm(output))\n            if self.dec_layer_number is not None and layer_id != self.num_layers - 1:\n                if nq_now != select_number:\n                    output = torch.gather(output, 0,topk_proposals.unsqueeze(-1).repeat(1, 1, self.d_model))  <span style=\'color: red\'># unsigmoid</span>\n        return [\n            [itm_out.transpose(0, 1) for itm_out in intermediate],  <span style=\'color: red\'># 将bs维度放在前面</span>\n            [itm_refpoint.transpose(0, 1) for itm_refpoint in ref_points]  <span style=\'color: red\'># 将bs维度放在前面</span>\n        ]\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dn_components.py</p><font size="0"><pre class="language-python"><code class="language-python"><span style=\'color: red\'># 这里与DN-DETR的处理基本一致，只多了一个aux_loss的处理</span>\ndef dn_post_process(outputs_class, outputs_coord, dn_meta, aux_loss, _set_aux_loss):        <span style=\'color: red\'># transformer处理之后的后处理</span>\n    if dn_meta and dn_meta[\'pad_size\'] > 0:                                   <span style=\'color: red\'># dn_meta[\'pad_size\']=198</span>\n        output_known_class = outputs_class[:, :, :dn_meta[\'pad_size\'], :]     <span style=\'color: red\'># 前面的这些是去噪的部分  torch.Size([6, 2, 1098, 18])-->torch.Size([6, 2, 198, 18])</span>\n        output_known_coord = outputs_coord[:, :, :dn_meta[\'pad_size\'], :]     <span style=\'color: red\'># torch.Size([6, 2, 1098, 4])-->torch.Size([6, 2, 198, 4])</span>\n        outputs_class = outputs_class[:, :, dn_meta[\'pad_size\']:, :]          <span style=\'color: red\'># 后面这些是正常的匹配预测部分  torch.Size([6, 2, 900, 18])</span>\n        outputs_coord = outputs_coord[:, :, dn_meta[\'pad_size\']:, :]          <span style=\'color: red\'>#                             torch.Size([6, 2, 900, 4])</span>\n        out = {\'pred_logits\': output_known_class[-1], \'pred_boxes\': output_known_coord[-1]}\n        if aux_loss:                                                          <span style=\'color: red\'># True</span>\n            out[\'aux_outputs\'] = _set_aux_loss(output_known_class, output_known_coord)       <span style=\'color: red\'># [{\'pred_logits\': a, \'pred_boxes\': b}]*6</span>\n        dn_meta[\'output_known_lbs_bboxes\'] = out                              <span style=\'color: red\'># output_known_lbs_bboxes 内容是去噪部分的</span>\n    return outputs_class, outputs_coord                                       <span style=\'color: red\'># 返回这俩还是网络自己预测的，不包括去噪部分的</span>\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">计算损失</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><font size="0"><pre class="language-python"><code class="language-python">class SetCriterion(nn.Module):                    <span style=\'color: red\'># forward方法与DN-DETR不同</span>\n    def forward(self, outputs, targets, return_indices=False):\n        outputs_without_aux = {k: v for k, v in outputs.items() if k != \'aux_outputs\'}      \n        <span style=\'color: red\'># outputs_without_aux={\'pred_logits\':[2, 900, 18], \'pred_boxes\':[2, 900, 4], \'interm_outputs\':{\'pred_logits\':[2, 900, 18],\'pred_boxes\':[2, 900, 4]}, \'interm_outputs_for_matching_pre\':{\'pred_logits\':[2, 900, 18],\'pred_boxes\':[2, 900, 4]}, </span>\n        <span style=\'color: red\'># \'dn_meta\':{\'pad_size\':198,\'num_dn_group\':9,\'output_known_lbs_bboxes\':{}}</span>\n        device = next(iter(outputs.values())).device\n        indices = self.matcher(outputs_without_aux, targets)  <span style=\'color: red\'># 匈牙利匹配  indices是list, 长度是bs, 每个item是一个tuple, 第一个值是框的id，第二个值是gt的id</span>\n        <span style=\'color: red\'># [(tensor([322, 482, 54...824, 851]), tensor([ 1,  5,  8, ...,  9,  2])),(tensor([414, 674, 76...832, 876]), tensor([4, 3, 1, 2, 0, 5]))]</span>\n        if return_indices:\n            indices0_copy = indices\n            indices_list = []\n        <span style=\'color: red\'># bs中所有GT的数量</span>\n        <span style=\'color: red\'># Compute the average number of target boxes accross all nodes, for normalization purposes</span>\n        num_boxes = sum(len(t["labels"]) for t in targets)                       <span style=\'color: red\'># 17</span>\n        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=device)\n        if is_dist_avail_and_initialized():\n            torch.distributed.all_reduce(num_boxes) \n        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()      <span style=\'color: red\'># 17.0</span>\n        <span style=\'color: red\'># Compute all the requested losses</span>\n        losses = {}\n        <span style=\'color: red\'># dn_meta 这个地方与DN-DETR不同了   prepare for dn loss {pad_size,num_dn_group,output_known_lbs_bboxes{pred_logits,pred_boxes,aux_outputs} }</span>\n        dn_meta = outputs[\'dn_meta\']\n        if self.training and dn_meta and \'output_known_lbs_bboxes\' in dn_meta:\n            <span style=\'color: red\'># single_pad bs最大的gt的数量x2 (包括了正负样本的总数量)，scalar可以认为是dn-detr中的group的概念</span>\n            <span style=\'color: red\'># output_known_lbs_bboxes的内容     pred_logits,pred_boxes,aux_outputs pred_logits [bs,2*dn_number,91]   pred_boxes [bs,2*dn_number,4]</span>\n            output_known_lbs_bboxes, single_pad, scalar = self.<span style=\'color: green;font-weight: bold;\'>prep_for_dn</span>(dn_meta)     <span style=\'color: red\'># ...,   22,  9</span>\n            dn_pos_idx = []\n            dn_neg_idx = []\n            for i in range(len(targets)):\n                if len(targets[i][\'labels\']) > 0:\n                    t = torch.range(0, len(targets[i][\'labels\']) - 1).long().cuda()  <span style=\'color: red\'># 0 --> gt的数量-1   tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], device=\'cuda:0\')</span>\n                    t = t.unsqueeze(0).repeat(scalar, 1)                             <span style=\'color: red\'># [scalar, gt num]   torch.Size([9, 11])</span>\n                    tgt_idx = t.flatten()                                            <span style=\'color: red\'># (scalar*gt num)  torch.Size([99])</span>\n                    <span style=\'color: red\'># (torch.tensor(range(scalar)) * single_pad).long().cuda().unsqueeze(1) ---> torch.Size([9, 1])</span>\n                    <span style=\'color: red\'># tensor([[  0],</span>\n                    <span style=\'color: red\'>#         [ 22],</span>\n                    <span style=\'color: red\'>#         [ 44],</span>\n                    <span style=\'color: red\'>#         [ 66],</span>\n                    <span style=\'color: red\'>#         [ 88],</span>\n                    <span style=\'color: red\'>#         [110],</span>\n                    <span style=\'color: red\'>#         [132],</span>\n                    <span style=\'color: red\'>#         [154],</span>\n                    <span style=\'color: red\'>#         [176]], device=\'cuda:0\')</span>\n                    <span style=\'color: red\'># *single_pad, 就是乘上gap</span>\n                    output_idx = (torch.tensor(range(scalar)) * single_pad).long().cuda().unsqueeze(1) + t\n                    output_idx = output_idx.flatten()           <span style=\'color: red\'># torch.Size([99])  tensor([  0,   1,...,  10,  22,  23,...,  32,  44,  45, ..., 54,  66,  67, ...,  76,  88,  89,....], device=\'cuda:0\')</span>\n                else:\n                    output_idx = tgt_idx = torch.tensor([]).long().cuda()\n                <span style=\'color: red\'># 这里第一个参数就是提议框的id，第二个参数就是gt的id的意思</span>\n                dn_pos_idx.append((output_idx, tgt_idx))\n                <span style=\'color: red\'># 这个变量并没有使用</span>\n                dn_neg_idx.append((output_idx + single_pad <span style=\'color: red\'>// 2, tgt_idx))</span>\n            output_known_lbs_bboxes = dn_meta[\'output_known_lbs_bboxes\']\n            l_dict = {}\n            for loss in self.losses:                   <span style=\'color: red\'># [\'labels\', \'boxes\', \'cardinality\']</span>\n                kwargs = {}\n                if \'labels\' in loss:\n                    kwargs = {\'log\': False}\n                l_dict.update(self.<span style=\'color: green;font-weight: bold;\'>get_loss</span>(loss, output_known_lbs_bboxes, targets, dn_pos_idx, num_boxes * scalar, **kwargs))           \n            l_dict = {k + f\'_dn\': v for k, v in l_dict.items()}\n            losses.update(l_dict)\n        else:\n            l_dict = dict()              <span style=\'color: red\'># 没有去噪训练，这些loss就都是0</span>\n            l_dict[\'loss_bbox_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n            l_dict[\'loss_giou_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n            l_dict[\'loss_ce_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n            l_dict[\'loss_xy_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n            l_dict[\'loss_hw_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n            l_dict[\'cardinality_error_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n            losses.update(l_dict)\n        for loss in self.losses:                       <span style=\'color: red\'># [\'labels\', \'boxes\', \'cardinality\']</span>\n            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n        <span style=\'color: red\'># In case of auxiliary losses, we repeat this process with the output of each intermediate layer.</span>\n        if \'aux_outputs\' in outputs:\n            for idx, aux_outputs in enumerate(outputs[\'aux_outputs\']):\n                indices = self.matcher(aux_outputs, targets)\n                if return_indices:\n                    indices_list.append(indices)\n                for loss in self.losses:\n                    if loss == \'masks\':\n                        <span style=\'color: red\'># Intermediate masks losses are too costly to compute, we ignore them.</span>\n                        continue\n                    kwargs = {}\n                    if loss == \'labels\':\n                        <span style=\'color: red\'># Logging is enabled only for the last layer</span>\n                        kwargs = {\'log\': False}\n                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n                    l_dict = {k + f\'_{idx}\': v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n                if self.training and dn_meta and \'output_known_lbs_bboxes\' in dn_meta:\n                    aux_outputs_known = output_known_lbs_bboxes[\'aux_outputs\'][idx]\n                    l_dict = {}\n                    for loss in self.losses:\n                        kwargs = {}\n                        if \'labels\' in loss:\n                            kwargs = {\'log\': False}\n                        <span style=\'color: red\'># 这里的第四个参数是indices, 在这里是dn_pos_idx</span>\n                        <span style=\'color: red\'># indices的内容是bs中的每个img的匹配，第一个参数是提议框的id，第二个参数是gt的id</span>\n                        l_dict.update(self.get_loss(loss, aux_outputs_known, targets, dn_pos_idx, num_boxes * scalar,\n                                                    **kwargs))\n                    l_dict = {k + f\'_dn_{idx}\': v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n                else:\n                    l_dict = dict()\n                    l_dict[\'loss_bbox_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n                    l_dict[\'loss_giou_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n                    l_dict[\'loss_ce_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n                    l_dict[\'loss_xy_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n                    l_dict[\'loss_hw_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n                    l_dict[\'cardinality_error_dn\'] = torch.as_tensor(0.).to(\'cuda\')\n                    l_dict = {k + f\'_{idx}\': v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n        <span style=\'color: red\'># 最后encoder的输出经过topk的</span>\n        <span style=\'color: red\'># interm_outputs loss</span>\n        if \'interm_outputs\' in outputs:\n            interm_outputs = outputs[\'interm_outputs\']\n            indices = self.matcher(interm_outputs, targets)\n            if return_indices:\n                indices_list.append(indices)\n            for loss in self.losses:\n                if loss == \'masks\':\n                    <span style=\'color: red\'># Intermediate masks losses are too costly to compute, we ignore them.</span>\n                    continue\n                kwargs = {}\n                if loss == \'labels\':\n                    <span style=\'color: red\'># Logging is enabled only for the last layer</span>\n                    kwargs = {\'log\': False}\n                l_dict = self.get_loss(loss, interm_outputs, targets, indices, num_boxes, **kwargs)\n                l_dict = {k + f\'_interm\': v for k, v in l_dict.items()}\n                losses.update(l_dict)\n        <span style=\'color: red\'># 其他encoder layer的输出</span>\n        <span style=\'color: red\'># enc output loss</span>\n        if \'enc_outputs\' in outputs:\n            for i, enc_outputs in enumerate(outputs[\'enc_outputs\']):\n                indices = self.matcher(enc_outputs, targets)\n                if return_indices:\n                    indices_list.append(indices)\n                for loss in self.losses:\n                    if loss == \'masks\':\n                        <span style=\'color: red\'># Intermediate masks losses are too costly to compute, we ignore them.</span>\n                        continue\n                    kwargs = {}\n                    if loss == \'labels\':\n                        <span style=\'color: red\'># Logging is enabled only for the last layer</span>\n                        kwargs = {\'log\': False}\n                    l_dict = self.get_loss(loss, enc_outputs, targets, indices, num_boxes, **kwargs)\n                    l_dict = {k + f\'_enc_{i}\': v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n        if return_indices:\n            indices_list.append(indices0_copy)\n            return losses, indices_list\n        return losses\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">计算损失models/dino/dino.py</p><font size="0"><pre class="language-python"><code class="language-python">class SetCriterion(nn.Module):                    <span style=\'color: red\'># forward方法与DN-DETR不同</span>\n    def prep_for_dn(self,dn_meta):\n        output_known_lbs_bboxes = dn_meta[\'output_known_lbs_bboxes\']\n        num_dn_groups,pad_size=dn_meta[\'num_dn_group\'],dn_meta[\'pad_size\']\n        assert pad_size % num_dn_groups==0\n        single_pad=pad_size//num_dn_groups\n        return output_known_lbs_bboxes,single_pad,num_dn_groups\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><font size="0"><pre class="language-python"><code class="language-python">class SetCriterion(nn.Module):                        <span style=\'color: red\'># 与DN-DETR相同</span>\n    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n        assert \'pred_logits\' in outputs\n        src_logits = outputs[\'pred_logits\']            <span style=\'color: red\'># torch.Size([2, 198, 18]) / torch.Size([2, 198, 19])</span>\n        <span style=\'color: red\'># 是两个tensor，第一个tensor是哪个图片的id，第二个tensor是分配的预测框的id 这里只有正样本对应的id</span>\n        idx = self._get_src_permutation_idx(indices)   <span style=\'color: red\'># () 153</span>\n        target_classes_o = torch.cat([t["labels"][J] for t,  (_, J) in zip(targets, indices)])                           <span style=\'color: red\'># torch.Size([153])  范围从0-17  /  1-18</span>\n        target_classes = torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device) <span style=\'color: red\'># torch.Size([2, 198]) 全部填充为18  / 19</span>\n        target_classes[idx] = target_classes_o            \n        <span style=\'color: red\'># 这里对于对比去噪的负样本，它的对应的就是no object</span>\n        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],\n                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)   <span style=\'color: red\'># torch.Size([2, 198, 19])  / torch.Size([2, 198, 20])</span>\n        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)       <span style=\'color: red\'># target_classes.unsqueeze(-1).shape=torch.Size([2, 198, 1])，背景的话第19个为1</span>\n        target_classes_onehot = target_classes_onehot[:, :, :-1]                 <span style=\'color: red\'># torch.Size([2, 198, 18])    这样导致背景的都是0000 /torch.Size([2, 198, 19]</span>\n        loss_ce = sigmoid_focal_loss(src_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2) * src_logits.shape[1]\n        losses = {\'loss_ce\': loss_ce}\n        if log:                                                                  <span style=\'color: red\'># TODO this should probably be a separate loss, not hacked in this one here</span>\n            losses[\'class_error\'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n        return losses\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><font size="0"><pre class="language-python"><code class="language-python">class SetCriterion(nn.Module):\n    <span style=\'color: red\'># 与DN-DETR相同</span>\n    def loss_boxes(self, outputs, targets, indices, num_boxes):\n        """Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss targets dicts must contain the key "boxes" containing a tensor of dim [nb_target_boxes, 4]\n           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.这里仅有正样本\n        """\n        assert \'pred_boxes\' in outputs\n        idx = self._get_src_permutation_idx(indices)                  <span style=\'color: red\'># 两个tensor，第一个tensor是哪个图片的id，第二个tensor是分配的预测框的id</span>\n        src_boxes = outputs[\'pred_boxes\'][idx]\n        target_boxes = torch.cat([t[\'boxes\'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction=\'none\')\n        losses = {}\n        losses[\'loss_bbox\'] = loss_bbox.sum() / num_boxes\n        loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(box_ops.box_cxcywh_to_xyxy(src_boxes),box_ops.box_cxcywh_to_xyxy(target_boxes)))\n        losses[\'loss_giou\'] = loss_giou.sum() / num_boxes\n        <span style=\'color: red\'># calculate the x,y and h,w loss</span>\n        with torch.no_grad():\n            losses[\'loss_xy\'] = loss_bbox[..., :2].sum() / num_boxes\n            losses[\'loss_hw\'] = loss_bbox[..., 2:].sum() / num_boxes\n        return losses\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><font size="0"><pre class="language-python"><code class="language-python">class SetCriterion(nn.Module):\n    @torch.no_grad()\n    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n        """ Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n        This is not really a loss, it is intended for logging purposes only. It doesn\'t propagate gradients\n        """\n        pred_logits = outputs[\'pred_logits\']         <span style=\'color: red\'># torch.Size([2, 198, 18])</span>\n        device = pred_logits.device\n        tgt_lengths = torch.as_tensor([len(v["labels"]) for v in targets], device=device)    <span style=\'color: red\'># tensor([11,  6], device=\'cuda:0\')</span>\n        <span style=\'color: red\'># Count the number of predictions that are NOT "no-object" (which is the last class)</span>\n        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n        losses = {\'cardinality_error\': card_err}\n        return losses\n</code></pre></font>'}]}]}]})</script></body>
</html>
