<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>detr_dino训练流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">数据处理</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">datasets/coco.py</p><span class=\'hidden-code\' data-code=\'class CocoDetection(torchvision.datasets.CocoDetection):\n    def __getitem__(self, idx):\n        img, target = super(CocoDetection, self).`__getitem__`(idx)\n        image_id = self.ids[idx]                                       # 435\n        target = {&amp;#39;image_id&amp;#39;: image_id, &amp;#39;annotations&amp;#39;: target}\n        img, target = self.`prepare`(img, target)                      # ConvertCocoPolysToMask\n        if self._transforms is not None:\n            img, target = self._transforms(img, target)\n        return img, target                            # torch.Size([3, 544, 870])  tensor([ 800, 1280])->tensor([544, 870])\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">site-packages/torchvision/datasets/coco.py</p><span class=\'hidden-code\' data-code=\'class CocoDetection(VisionDataset):\n    def _load_image(self, id: int) -> Image.Image:\n        path = self.coco.loadImgs(id)[0][&amp;#39;file_name&amp;#39;]\n        return Image.open(os.path.join(self.root, path)).convert(&amp;#39;RGB&amp;#39;)\n    def _load_target(self, id) -> List[Any]:\n        return self.coco.loadAnns(self.coco.getAnnIds(id))\n    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n        id = self.ids[index]\n        image = self._load_image(id)\n        target = self._load_target(id)    # [{&amp;#39;category_id&amp;#39;:0,&amp;#39;bbox&amp;#39;:[714.62,164.68,88,38],&amp;#39;area&amp;#39;:3344,&amp;#39;image_id&amp;#39;:435,&amp;#39;id&amp;#39;:4033,&amp;#39;iscrowd&amp;#39;:0}]\n        if self.transforms is not None:   # None    RandomHorizontalFlip + RandomSelect + ToTensor + Normalize\n            image, target = self.transforms(image, target)    \n        return image, target\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">datasets/coco.py</p><span class=\'hidden-code\' data-code=\'class ConvertCocoPolysToMask(object):\n    def __init__(self, return_masks=False):\n        self.return_masks = return_masks\n    def __call__(self, image, target):\n        w, h = image.size                  # 1280,800\n        image_id = target[&amp;#39;image_id&amp;#39;]      # 435\n        image_id = torch.tensor([image_id])\n        anno = target[&amp;#39;annotations&amp;#39;]\n        anno = [obj for obj in anno if &amp;#39;iscrowd&amp;#39; not in obj or obj[&amp;#39;iscrowd&amp;#39;] == 0]   # 过滤\n        boxes = [obj[&amp;#39;bbox&amp;#39;] for obj in anno]\n        # guard against no boxes via resizing\n        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)  # torch.Size([11, 4]) 非归一化的x1,y1,w,h\n        boxes[:, 2:] += boxes[:, :2]\n        boxes[:, 0::2].clamp_(min=0, max=w)\n        boxes[:, 1::2].clamp_(min=0, max=h)\n        classes = [obj[&amp;#39;category_id&amp;#39;] for obj in anno]                      # [0, 0, 0, 2, 14, 14, 14, 8, 2, 2, 2] 这里要从0开始还是1？\n        classes = torch.tensor(classes, dtype=torch.int64)\n        if self.return_masks:\n            segmentations = [obj[&amp;#39;segmentation&amp;#39;] for obj in anno]\n            masks = convert_coco_poly_to_mask(segmentations, h, w)\n        keypoints = None\n        if anno and &amp;#39;keypoints&amp;#39; in anno[0]:               # False\n            keypoints = [obj[&amp;#39;keypoints&amp;#39;] for obj in anno]\n            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n            num_keypoints = keypoints.shape[0]\n            if num_keypoints:\n                keypoints = keypoints.view(num_keypoints, -1, 3)\n        keep = (boxes[:, 3] > boxes[:, 1]) &amp; (boxes[:, 2] > boxes[:, 0])\n        boxes = boxes[keep]\n        classes = classes[keep]\n        if self.return_masks:                # False\n            masks = masks[keep]\n        if keypoints is not None:\n            keypoints = keypoints[keep]\n        target = {}\n        target[&amp;#39;boxes&amp;#39;] = boxes\n        target[&amp;#39;labels&amp;#39;] = classes\n        if self.return_masks:\n            target[&amp;#39;masks&amp;#39;] = masks\n        target[&amp;#39;image_id&amp;#39;] = image_id\n        if keypoints is not None:\n            target[&amp;#39;keypoints&amp;#39;] = keypoints\n        # for conversion to coco api\n        area = torch.tensor([obj[&amp;#39;area&amp;#39;] for obj in anno])\n        iscrowd = torch.tensor([obj[&amp;#39;iscrowd&amp;#39;] if &amp;#39;iscrowd&amp;#39; in obj else 0 for obj in anno])\n        target[&amp;#39;area&amp;#39;] = area[keep]\n        target[&amp;#39;iscrowd&amp;#39;] = iscrowd[keep]\n        target[&amp;#39;orig_size&amp;#39;] = torch.as_tensor([int(h), int(w)])\n        target[&amp;#39;size&amp;#39;] = torch.as_tensor([int(h), int(w)])\n        return image, target\n\'> </span>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">util/misc.pyutil/misc.py</p><span class=\'hidden-code\' data-code=\'def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    if tensor_list[0].ndim == 3:            # torch.Size([3, 512, 625])+torch.Size([3, 878, 736])  batch=2\n        if torchvision._is_tracing():\n            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        batch_shape = [len(tensor_list)] + max_size\n        b, c, h, w = batch_shape\n        dtype = tensor_list[0].dtype\n        device = tensor_list[0].device\n        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n        for img, pad_img, m in zip(tensor_list, tensor, mask):\n            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n            m[: img.shape[1], :img.shape[2]] = False\n    else:\n        raise ValueError(&amp;#39;not supported&amp;#39;)\n    return NestedTensor(tensor, mask)         # torch.Size([2, 3, 878, 736])\n\'> </span>'}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><span class=\'hidden-code\' data-code=\'class DINO(nn.Module):\n    def forward(self, samples: NestedTensor, targets: List = None):\n        if not isinstance(samples, NestedTensor):\n            samples = nested_tensor_from_tensor_list(samples)     # torch.Size([2, 3, 878, 736]) \n        features, poss = self.backbone(samples)                   # features的item数量为3，detr仅有一个->(2,384,110,92)-8 + (...,55,46)-16 + (....,28,23)-32\n        srcs = []                                                 # poss- (2, 256, 110, 92), (2, 256, 55, 46), (2, 256, 28, 23) 位置编码-根据图像大小hxw来的\n        masks = []\n        for l, feat in enumerate(features):          # 遍历每一个特征层\n            src, mask = feat.decompose()             # 分解出特征和mask\n            srcs.append(self.input_proj[l](src))     # 经过input_proj之后，这里的feature的channel都是256，wh不一样\n            masks.append(mask)\n            assert mask is not None\n        if self.num_feature_levels > len(srcs):                                  # 多出backbone特征层的层 4>3\n            _len_srcs = len(srcs)                                                # 3\n            for l in range(_len_srcs, self.num_feature_levels):                  # 3,4\n                if l == _len_srcs:\n                    src = self.input_proj[l](features[-1].tensors)               # 这里的还可以承接backbone的特征  (....,28,23)-32==>torch.Size([2,256,14,12])-约64\n                else:\n                    src = self.input_proj[l](srcs[-1])                           # 这里的只能承接上一层的了，跟backbone的特征已经中间有隔阂了\n                m = samples.mask                                                 # 这个是gt的mask   torch.Size([2, 878, 736])\n                mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]       # 使用插值创建新的mask torch.Size([2, 14, 12])\n                pos_l = self.backbone[1](NestedTensor(src, mask)).to(src.dtype)  # backbone[1] 是位置编码，因为这个最后的src，和gt的mask，并没有经过位置编码，这里调用了他们的forward方法\n                srcs.append(src)                    # torch.Size([2, 256, 14, 12])\n                masks.append(mask)                  # torch.Size([2, 14, 12])\n                poss.append(pos_l)                  # 位置编码 torch.Size([2, 256, 14, 12])--64\n        # 以上的处理与Deformable DETR相同 ---------------------------------------\n        if self.dn_number > 0 or targets is not None:                        # dn_number 100个CDN  生成噪声的代码 ------------------------------------\n            input_query_label, input_query_bbox, attn_mask, dn_meta = `prepare_for_cdn`(dn_args=(targets, self.dn_number, self.dn_label_noise_ratio, self.dn_box_noise_scale),\n                                training=self.training, num_queries=self.num_queries, num_classes=self.num_classes, hidden_dim=self.hidden_dim, label_enc=self.label_enc)\n            # input_query_label (bs,N=11*2*9,256)        input_query_box (bs,N=11*2*9,4)       attn_mask [11*2*9+900,11*2*9+900]  {&amp;#39;pad_size&amp;#39;: 198, &amp;#39;num_dn_group&amp;#39;: 9}\n        else:\n            assert targets is None                   # 推理模式\n            input_query_bbox = input_query_label = attn_mask = dn_meta = None\n        hs, reference, hs_enc, ref_enc, init_box_proposal = self.`transformer`(srcs, masks, input_query_bbox, poss, input_query_label, attn_mask)\n        # transformer方法的参数还是一样的\n        # hs->6*[bs,N(num_queries+2*dn_number),256]     reference->7*[bs,N(num_queries+2*dn_number),4]       enc->这两个是encoder后经过头的输出\n        # hs_enc->[1,bs,num_queries,256]                ref_enc->[1,bs,num_queries,4]                        init_box_proposal->[bs,900,4] 最最初始的参考点位\n        hs[0] += self.label_enc.weight[0, 0] * 0.0           # In case num object=0    torch.Size([2, 1098, 256])\n        # deformable-detr-like anchor update\n        # reference_before_sigmoid = inverse_sigmoid(reference[:-1]) # n_dec, bs, nq, 4\n        outputs_coord_list = []\n        # 最后一层输出的经过修正后的坐标不使用\n        # layer_ref_sig 进入某一层之前的bbox\n        # layer_bbox_embed 这一层的bbox头\n        # layer_hs 这一层的layer的输出\n        for dec_lid, (layer_ref_sig, layer_bbox_embed, layer_hs) in enumerate(zip(reference[:-1], self.bbox_embed, hs)):\n            layer_delta_unsig = layer_bbox_embed(layer_hs)        # 网络输出的偏移量     torch.Size([2, 1098, 4])\n            layer_outputs_unsig = layer_delta_unsig + inverse_sigmoid(layer_ref_sig)     # 进行坐标修正  torch.Size([2, 1098, 4])\n            layer_outputs_unsig = layer_outputs_unsig.sigmoid()   # 0-1\n            outputs_coord_list.append(layer_outputs_unsig)\n        outputs_coord_list = torch.stack(outputs_coord_list)    # torch.Size([6, 2, 1098, 4])\n        # [6,bs,N,91] 类别输出\n        outputs_class = torch.stack([layer_cls_embed(layer_hs) for layer_cls_embed, layer_hs in zip(self.class_embed, hs)])   # torch.Size([6, 2, 1098, 18])\n        if self.dn_number > 0 and dn_meta is not None:\n            # dn的后处理  返回的是匹配部分的结果，去噪部分的不在这之内了  [6,bs,num_queries,91]  [6,bs,num_queries,4]\n            outputs_class, outputs_coord_list = `dn_post_process`(outputs_class, outputs_coord_list, dn_meta, self.aux_loss, self._set_aux_loss)\n        # 这里与DN-DETR相同                                                                   &amp;#39;pred_logits&amp;#39;-[batch_size x num_queries x (num_classes + 1)]; \n        out = {&amp;#39;pred_logits&amp;#39;: outputs_class[-1], &amp;#39;pred_boxes&amp;#39;: outputs_coord_list[-1]}     # &amp;#39;pred_boxes&amp;#39;: (center_x, center_y, height, width).values are normalized in [0, 1],\n        if self.aux_loss:\n            out[&amp;#39;aux_outputs&amp;#39;] = self._set_aux_loss(outputs_class, outputs_coord_list)\n        # for encoder output\n        if hs_enc is not None:                    # torch.Size([1, 2, 900, 256])\n            # prepare intermediate outputs  ref_enc 为 [1,bs,900,4] 取-1，其实也只有一个 --> torch.Size([2, 900, 4])\n            interm_coord = ref_enc[-1]\n            interm_class = self.transformer.enc_out_class_embed(hs_enc[-1])        # hs_enc [1,bs,900,256]--->torch.Size([2, 900, 18])\n            out[&amp;#39;interm_outputs&amp;#39;] = {&amp;#39;pred_logits&amp;#39;: interm_class, &amp;#39;pred_boxes&amp;#39;: interm_coord}       # encoder输出的经过topk之后的，也放入到out中\n            out[&amp;#39;interm_outputs_for_matching_pre&amp;#39;] = {&amp;#39;pred_logits&amp;#39;: interm_class, &amp;#39;pred_boxes&amp;#39;: init_box_proposal}   # encoder输出的经过topk之后的，也放入到out中，init_box_proposal是在encoder时就生成的最最初始的参考点位\n            if hs_enc.shape[0] > 1:              # prepare enc outputs 还有其他的值，可能是应对多个encoder的layer的输出吧\n                enc_outputs_coord = []\n                enc_outputs_class = []\n                for layer_id, (layer_box_embed, layer_class_embed, layer_hs_enc, layer_ref_enc) in enumerate(zip(self.enc_bbox_embed, self.enc_class_embed, hs_enc[:-1], ref_enc[:-1])):\n                    layer_enc_delta_unsig = layer_box_embed(layer_hs_enc)\n                    layer_enc_outputs_coord_unsig = layer_enc_delta_unsig + inverse_sigmoid(layer_ref_enc)\n                    layer_enc_outputs_coord = layer_enc_outputs_coord_unsig.sigmoid()\n                    layer_enc_outputs_class = layer_class_embed(layer_hs_enc)\n                    enc_outputs_coord.append(layer_enc_outputs_coord)\n                    enc_outputs_class.append(layer_enc_outputs_class)\n                out[&amp;#39;enc_outputs&amp;#39;] = [{&amp;#39;pred_logits&amp;#39;: a, &amp;#39;pred_boxes&amp;#39;: b} for a, b in zip(enc_outputs_class, enc_outputs_coord)]\n        out[&amp;#39;dn_meta&amp;#39;] = dn_meta\n        return out\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dn_components.py</p><span class=\'hidden-code\' data-code=\'def prepare_for_cdn(dn_args, training, num_queries, num_classes, hidden_dim, label_enc):\n    if training:                                                          # 在传给transformer之前的预处理\n        targets, dn_number, label_noise_ratio, box_noise_scale = dn_args  # dn_number=100,0.5，1\n        dn_number = dn_number * 2                                         # positive and negative dn queries正负样本数量相同\n        known = [(torch.ones_like(t[&amp;#39;labels&amp;#39;])).cuda() for t in targets]  # [tensor([1,1]),tensor([1,1,1]),...]\n        batch_size = len(known)                                           # 2\n        known_num = [sum(k) for k in known]                               # 各个image gt的数量  [tensor(11, device=&amp;#39;cuda:0&amp;#39;), tensor(6, device=&amp;#39;cuda:0&amp;#39;)]\n        if int(max(known_num)) == 0:\n            dn_number = 1                                                 # 没有gt\n        else:\n            if dn_number >= 100:                                          # 每组里面会涵盖所有GTbox\n                dn_number = dn_number // (int(max(known_num) * 2))        # 有点类似于dn-detr的group的处理 200//(11*2)=9  9组吗？\n            elif dn_number < 1:\n                dn_number = 1\n        if dn_number == 0:\n            dn_number = 1\n        # 以下的这些处理与DN-DETR基本一致   假设87个GT\n        unmask_bbox = unmask_label = torch.cat(known)         # 所有的1 cat到一起       torch.size=(17,)   最大的是11\n        labels = torch.cat([t[&amp;#39;labels&amp;#39;] for t in targets])    # 取出所有gt的label值     torch.size=(17,)   这里从0开始？\n        boxes = torch.cat([t[&amp;#39;boxes&amp;#39;] for t in targets])      # 取出所有gt的box         torch.size=(17,4)\n        batch_idx = torch.cat([torch.full_like(t[&amp;#39;labels&amp;#39;].long(), i) for i, t in enumerate(targets)])   # 标识属于哪个图片 like tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device=&amp;#39;cuda:0&amp;#39;) torch.Size([17])\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)         # 返回一个二维张量，其中每一行都是非零值的索引 torch.Size([17,1]) tensor([[0],[1],[2],......])\n        known_indice = known_indice.view(-1)                             # 拉平 torch.Size([17])\n        known_indice = known_indice.repeat(2 * dn_number, 1).view(-1)    # 并没有使用    torch.Size([696])\n        # 这三项是gt相关的值\n        known_labels = labels.repeat(2 * dn_number, 1).view(-1)   # (all gt*2*dn_number) 这里的N是bs中所有的gt的数量总和 torch.Size([17*2*9=306])\n        known_bid = batch_idx.repeat(2 * dn_number, 1).view(-1)   # (all gt*2*dn_number)\n        known_bboxs = boxes.repeat(2 * dn_number, 1)              # [all gt*2*dn_number, 4]\n        # 这两个是克隆的\n        known_labels_expaned = known_labels.clone()\n        known_bbox_expand = known_bboxs.clone()\n        # --------------------------------------------------------------------------------------------------------------\n        if label_noise_ratio > 0:                                                  # 相当于对某些类别换成新类别\n            p = torch.rand_like(known_labels_expaned.float())                      # 随机值，0-1内                                               torch.Size([306])\n            chosen_indice = torch.nonzero(p < (label_noise_ratio * 0.5)).view(-1)  # half of bbox prob  # 被选择的id 这里比dn-detr 多了一个 1/2   torch.Size([75]) 306里面选择75个\n            new_label = torch.randint_like(chosen_indice, 0, num_classes)          # randomly put a new one here  # 给被选择的gt 一个随机的label id \n            known_labels_expaned.scatter_(0, chosen_indice, new_label)             # 把上面的值塞进去                                             306个值里面有75个是随机生成得\n        single_pad = int(max(known_num))                          # bs中最多的target的数量11\n        pad_size = int(single_pad * 2 * dn_number)                # 假设所有的gt=77，但是最多的是11个gt，bs=2   11*2*9=198\n        positive_idx = torch.tensor(range(len(boxes))).long().cuda().unsqueeze(0).repeat(dn_number, 1)    # [dn_number组数,gt count所有77]-->torch.Size([9, 17]) 9个0-16\n        # 加上了group间的偏移量  torch.Size([9, 1])                 # tensor([[17x2x0,17x2x1,68,102,136,170,204,238,272]], device=&amp;#39;cuda:0&amp;#39;)-->torch.Size([9, 17]) 第一个0-16加0；第二个0-14加17x2x1\n        positive_idx += (torch.tensor(range(dn_number)) * len(boxes) * 2).long().cuda().unsqueeze(1)\n        positive_idx = positive_idx.flatten()                     # torch.Size([153]) [gt count*dn number=17*4]，推平  [0,...,16]+[0+17*2x1=34,...,16+17*2=50]+[0+2*17*2=68,...,16+2*17*2=84]+...\n        negative_idx = positive_idx + len(boxes)                  # 再+17  正好剩下的位置是留给negative的 [0+17,...,16+17]+[0+17*2x1=34+17,...,\n        if box_noise_scale > 0:                                   # 1\n            known_bbox_ = torch.zeros_like(known_bboxs)           # [all gt*2*dn_number, 4]  torch.Size([306, 4])  17x2x9\n            known_bbox_[:, :2] = known_bboxs[:, :2] - known_bboxs[:, 2:] / 2        # x1,y1(17*2*4,2)\n            known_bbox_[:, 2:] = known_bboxs[:, :2] + known_bboxs[:, 2:] / 2        # x2,y2(17*2*4,2)\n            diff = torch.zeros_like(known_bboxs)\n            diff[:, :2] = known_bboxs[:, 2:] / 2                  # (w/2,h/2)\n            diff[:, 2:] = known_bboxs[:, 2:] / 2                  # (w/2,h/2)\n            rand_sign = torch.randint_like(known_bboxs, low=0, high=2, dtype=torch.float32) * 2.0 - 1.0   # (0,2)->(-1,1)===>(17*2*9=306,4)\n            rand_part = torch.rand_like(known_bboxs)              # (17*2*9);0-1内的值\n            rand_part[negative_idx] += 1.0                        # 负样本位置的值在1-2\n            rand_part *= rand_sign                                # [2*gt count*dn number,4] 坐标位置随机的乘上1 -1  正样本(-1,1)负样本(-2,-1)+(1,2)\n            # 加上随机的偏移，左上，右下的点随机的进行了偏移\n            known_bbox_ = known_bbox_ + torch.mul(rand_part,diff).cuda() * box_noise_scale  # 顶点左右上下偏移0.2*w或0.2*h属于正样本，偏移0.2*w->0.4*w或0.2*h->0.4*h属于负样本\n            known_bbox_ = known_bbox_.clamp(min=0.0, max=1.0)                               # 范围在0,1\n            known_bbox_expand[:, :2] = (known_bbox_[:, :2] + known_bbox_[:, 2:]) / 2        # xc,yc\n            known_bbox_expand[:, 2:] = known_bbox_[:, 2:] - known_bbox_[:, :2]              # w,h\n        m = known_labels_expaned.long().to(&amp;#39;cuda&amp;#39;)            # 这里的known_labels_expaned 已经被添加过随机的噪声了 (2*gt count*dn_number) 新的label信息 torch.Size([306])\n        input_label_embed = label_enc(m)                      # Embedding(19, 256)  num_class=18,所以生成得标签是不是要从1开始，torch.Size([306, 256]) \n        input_bbox_embed = inverse_sigmoid(known_bbox_expand) # 对坐标取反函数  对应于特征图上的坐标  x/(1-x)   torch.Size([306, 4])范围从(-0.1146->0.8425)到(-0.1146->0.8425) \n        padding_label = torch.zeros(pad_size, hidden_dim).cuda()       # [pad_size=11*2*9, 256]-->torch.Size([198, 256])\n        padding_bbox = torch.zeros(pad_size, 4).cuda()                 # [pad_size=11*2*9, 4]     torch.Size([198, 4])\n        input_query_label = padding_label.repeat(batch_size, 1, 1)     # [2，11*2*9, 256]   batch=2,里面图片最多11个gt，有9个group分为正负\n        input_query_bbox = padding_bbox.repeat(batch_size, 1, 1)       # [2，11*2*9, 4]\n        map_known_indice = torch.tensor([]).to(&amp;#39;cuda&amp;#39;)\n        if len(known_num):         # 如果有gt的话\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])    # tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5]) 17\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(2 * dn_number)]).long()  # 加上了偏移，这个偏移是这些batch中最大的gt的数量 17*2*9  torch.Size([306])\n                                                                                                                    # 第一行+11*0，第二行+11*1\n        if len(known_bid):         # known_bid 标识属于哪个图片的  # (all gt*2*dn_number)\n            input_query_label[(known_bid.long(), map_known_indice)] = input_label_embed      # 替换对应的embed input_query_label第一个维度是bs，known_bid是标识的属于哪一个image  [2，11*2*9, 256]\n            input_query_bbox[(known_bid.long(), map_known_indice)] = input_bbox_embed        # [2，11*2*9, 4]\n         \n        tgt_size = pad_size + num_queries         # 这里pad_size=198=11*2*9 就是cdn总共的数量，包括了正负样本，num_queries是正常的query的数量----》11*2*9+900\n        attn_mask = torch.ones(tgt_size, tgt_size).to(&amp;#39;cuda&amp;#39;) `<` 0  # (11*2*9+900,11*2*9+900)--`>`(1098,1098)的False\n        attn_mask[pad_size:, :pad_size] = True          # match query cannot see the reconstruct  :198,:198\n        for i in range(dn_number):                # 各个组的掩码  reconstruct cannot see each other\n            if i == 0:                            # 第一组\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), single_pad * 2 * (i + 1):pad_size] = True      # 看不到他后面的所有 0:22,22:198\n            if i == dn_number - 1:                # 最后一组\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), :single_pad * i * 2] = True                    # 看不到他前面的所有\n            else:                                 # 中间组 看不到他后面的 也看不到他前面的\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), single_pad * 2 * (i + 1):pad_size] = True\n                attn_mask[single_pad * 2 * i:single_pad * 2 * (i + 1), :single_pad * 2 * i] = True\n        dn_meta = {&amp;#39;pad_size&amp;#39;: pad_size,&amp;#39;num_dn_group&amp;#39;: dn_number}         # pad_size=198=11*2*9 就是cdn总共的数量  +  dn_number--->多少组 9   \n    else:\n        input_query_label = None\n        input_query_bbox = None\n        attn_mask = None\n        dn_meta = None\n    # label加入些噪声取随机值，bbox划分正负样本，[2，11*2*9, 256]  [2，11*2*9, 4]  (1098,1098)\n    return input_query_label, input_query_bbox, attn_mask, dn_meta         # 这里并不包含正常match部分的tgt，这点与DN-DETR的实现不同了    \n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><span class=\'hidden-code\' data-code=\'class DeformableTransformer(nn.Module):\n    def forward(self, srcs, masks, refpoint_embed, pos_embeds, tgt, attn_mask=None): \n        # [bs, ci, hi, wi];  [bs, hi, wi]; refpoint_embed-torch.Size([2, 198, 4]);  位置编码-encode;  tgt-torch.Size([2, 198, 256]);   torch.Size([1098, 1098])\n        # 先处理一下输入特征 --------------------------------------prepare input for encoder-------------------------------------------------------\n        src_flatten = []\n        mask_flatten = []\n        lvl_pos_embed_flatten = []\n        spatial_shapes = []\n        for lvl, (src, mask, pos_embed) in enumerate(zip(srcs, masks, pos_embeds)):     # 这地方的处理与Deformable DETR是相同的  src mask pos_embed 尺寸是相同的  \n            bs, c, h, w = src.shape                             # 特征图的高宽\n            spatial_shape = (h, w)\n            spatial_shapes.append(spatial_shape)                # 假设两个h,w得到[[20,30],[40,60]]\n            src = src.flatten(2).transpose(1, 2)                # bs, hw, c\n            mask = mask.flatten(1)                              # bs, hw\n            pos_embed = pos_embed.flatten(2).transpose(1, 2)    # bs, hw, c\n            if self.num_feature_levels > 1 and self.level_embed is not None:      # 这个是Deformable DETR论文中提到的Level embed\n                lvl_pos_embed = pos_embed + self.level_embed[lvl].view(1, 1, -1)  # pos_embed和level_embed相加  self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))\n            else:\n                lvl_pos_embed = pos_embed\n            lvl_pos_embed_flatten.append(lvl_pos_embed)\n            src_flatten.append(src)  \n            mask_flatten.append(mask)\n        src_flatten = torch.cat(src_flatten, 1)                      # bs, \\sum{hxw}, c   ==[bs,all hw,256] 所有特征层的拼在一起 torch.Size([2, 13462, 256])\n        mask_flatten = torch.cat(mask_flatten, 1)                    # bs, \\sum{hxw}                                           torch.Size([2, 13462])\n        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)  # bs, \\sum{hxw}, c                                        torch.Size([2, 13462, 256])\n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)           # [特征层的数量，2] tensor([[110,  92], [ 55,  46], [ 28,  23], [ 14,  12]], device=&amp;#39;cuda:0&amp;#39;)\n        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))  # 各个src层 起始的位置 tensor([0,h1*w1,h2*w2,...,hn-1*wn-1])\n        valid_ratios = torch.stack([self.`get_valid_ratio`(m) for m in masks], 1)                                 # 有效高宽占总的batch高宽的比率 [bs,4,2]\n        # -----------------------------------------------------------以上的处理与Deformable DETR基本一致---------------------------------- \n        enc_topk_proposals = enc_refpoint_embed = None               # two stage\n        # 先经过encoder处理后两个输出值是encoder的中间层的输出            Begin Encoder\n        # -----------------------------------------------------------\n        memory, enc_intermediate_output, enc_intermediate_refpoints = self.`encoder`(       # torch.Size([2, 13462, 256])  +  None  +  None\n            src_flatten,                                 # 图像特征            torch.Size([2, 13462, 256]) \n            pos=lvl_pos_embed_flatten,                   # 空间位置编码        torch.Size([2, 13462, 256])\n            level_start_index=level_start_index,         # 每个特征图开始的位置索引  \n            spatial_shapes=spatial_shapes,               # 特征空间大小        tensor([[110,  92], [ 55,  46],[ 28,  23], [ 14,  12]], device=&amp;#39;cuda:0&amp;#39;)\n            valid_ratios=valid_ratios,\n            key_padding_mask=mask_flatten,               # 以上这些参数是上面处理的 (bs,\\sum{hw})\n            ref_token_index=enc_topk_proposals,  # bs, nq      # 这两个参数是Deformable DETR中没有的，其他的是相同的, 不过这两个参数在上面设定为了None\n            ref_token_coord=enc_refpoint_embed,  # bs, nq, 4\n        )  # ---->(bs,\\sum{hw},c) 【None or (nenc+1, bs, nq, c) or (nenc, bs, nq, c)】【None or (nenc+1, bs, nq, c) or (nenc, bs, nq, c)】\n        # -----------------------------------------------------------\n        if self.two_stage_type == &amp;#39;standard&amp;#39;:    # True\n            if self.two_stage_learn_wh:          # False\n                input_hw = self.two_stage_wh_embedding.weight[0]\n            else:\n                input_hw = None\n            # output_memory 是memory 经过填充inf，经过一层全连接后的结果 [bs,all hw,256]    output_proposals 是制作的proposals，非法的位置填充了inf [bs,all hw,4]\n            output_memory, output_proposals = `gen_encoder_output_proposals`(memory, mask_flatten, spatial_shapes,input_hw)\n            # 这一行在Deformable DETR中是在gen_encoder_output_proposals方法中的，这里是放到了这里\n            output_memory = self.enc_output_norm(self.enc_output(output_memory))       # nn.Linear(d_model, d_model) + nn.LayerNorm(d_model)  torch.Size([2, 13462, 256])\n            if self.two_stage_pat_embed > 0:                                                  # 0  False\n                bs, nhw, _ = output_memory.shape\n                output_memory = output_memory.repeat(1, self.two_stage_pat_embed, 1)          # output_memory: bs, n, 256; self.pat_embed_for_2stage: k, 256\n                _pats = self.pat_embed_for_2stage.repeat_interleave(nhw, 0)\n                output_memory = output_memory + _pats\n                output_proposals = output_proposals.repeat(1, self.two_stage_pat_embed, 1)\n            if self.two_stage_add_query_num > 0:                                               # 0  False\n                assert refpoint_embed is not None\n                output_memory = torch.cat((output_memory, tgt), dim=1)\n                output_proposals = torch.cat((output_proposals, refpoint_embed), dim=1)\n            enc_outputs_class_unselected = self.enc_out_class_embed(output_memory)                    # Linear(in_features=256, out_features=num_class, bias=True) 经过分类头 [bs,sum(hw),91]\n            enc_outputs_coord_unselected = self.enc_out_bbox_embed(output_memory) + output_proposals  # 经过box头 [bs,sum(hw),4] +output_proposals 是因为经过网络头的结果是修正\n            topk = self.num_queries                                                                   # 900\n            topk_proposals = torch.topk(enc_outputs_class_unselected.max(-1)[0], topk, dim=1)[1]      # bs, nq  torch.Size([2, 900])\n            refpoint_embed_undetach = torch.gather(enc_outputs_coord_unselected, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, 4))  # unsigmoid  取出对应index的四个坐标值出来  [bs,900,4]\n            refpoint_embed_ = refpoint_embed_undetach.detach()                                        # 参考点位脱离  torch.Size([2, 900, 4])\n            init_box_proposal = torch.gather(output_proposals, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, 4)).sigmoid()          # 同样的取出对应的初始点位的值-->sigmoid  torch.Size([2, 900, 4])\n            tgt_undetach = torch.gather(output_memory, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, self.d_model))                 # gather tgt 同样的取出对应的memory torch.Size([2, 900, 256])\n            if self.embed_init_tgt:                                                                   # 对应于论文中图5 c的 static content queries     True\n                tgt_ = self.tgt_embed.weight[:, None, :].repeat(1, bs, 1).transpose(0, 1)             # [900,256] -> [900,bs,256] -> [bs,900,256], 这里依然使用的是网络的参数 一个mebedding\n            else:\n                tgt_ = tgt_undetach.detach()\n            # refpoint_embed 是prepare_for_cdn生成的, 带有噪声的gt, tgt也是prepare_for_cdn生成的 带 _ 后缀的这两个是上面处理的，利用encoder的output topk筛选的\n            if refpoint_embed is not None:\n                # cat prepare_for_cdn生成的去噪的，加上match部分使用的\n                refpoint_embed = torch.cat([refpoint_embed, refpoint_embed_], dim=1)     # torch.Size([2, 198, 4])+torch.Size([2, 900, 4])-->torch.Size([2, 1098, 4])\n                # cat prepare_for_cdn生成的去噪的，加上match部分使用的\n                tgt = torch.cat([tgt, tgt_], dim=1)                                      # torch.Size([2, 198, 256])+torch.Size([2, 900, 256])-->torch.Size([2, 1098, 256])\n            else:\n                # 这种是推理模式，没有去噪的内容, 传入的参数refpoint_embed和tgt也就没有使用\n                refpoint_embed, tgt = refpoint_embed_, tgt_\n        elif self.two_stage_type == &amp;#39;no&amp;#39;:\n            tgt_ = self.tgt_embed.weight[:, None, :].repeat(1, bs, 1).transpose(0, 1)  # nq, bs, d_model\n            refpoint_embed_ = self.refpoint_embed.weight[:, None, :].repeat(1, bs, 1).transpose(0, 1)  # nq, bs, 4\n            if refpoint_embed is not None:\n                refpoint_embed = torch.cat([refpoint_embed, refpoint_embed_], dim=1)\n                tgt = torch.cat([tgt, tgt_], dim=1)\n            else:\n                refpoint_embed, tgt = refpoint_embed_, tgt_\n            if self.num_patterns > 0:\n                tgt_embed = tgt.repeat(1, self.num_patterns, 1)\n                refpoint_embed = refpoint_embed.repeat(1, self.num_patterns, 1)\n                tgt_pat = self.patterns.weight[None, :, :].repeat_interleave(self.num_queries, 1)  # 1, n_q*n_pat, d_model\n                tgt = tgt_embed + tgt_pat\n            init_box_proposal = refpoint_embed_.sigmoid()\n        else:\n            raise NotImplementedError(&amp;#39;unknown two_stage_type {}&amp;#39;.format(self.two_stage_type))\n        # references比hs多一个，hs是6，references是7   hs [bs,all, 256]->[torch.Size([2, 1098, 256]),...,torch.Size([2, 1098, 256])] + references [bs,all, 4]  [torch.Size([2, 1098, 4]), ..., torch.Size([2, 1098, 4])]\n        hs, references = self.`decoder`(\n            tgt=tgt.transpose(0, 1),                    # torch.Size([1098, 2, 256])\n            memory=memory.transpose(0, 1),              # torch.Size([2, 13462, 256])\n            memory_key_padding_mask=mask_flatten,       # torch.Size([2, 13462])\n            pos=lvl_pos_embed_flatten.transpose(0, 1),  # torch.Size([13462, 2, 256])\n            # 参考点位\n            refpoints_unsigmoid=refpoint_embed.transpose(0, 1),     # torch.Size([1098, 2, 4])\n            level_start_index=level_start_index,        # tensor([    0, 10120, 12650, 13294], device=&amp;#39;cuda:0&amp;#39;)\n            spatial_shapes=spatial_shapes,              # tensor([[110,  92], [ 55,  46], [ 28,  23], [ 14,  12]], device=&amp;#39;cuda:0&amp;#39;)\n            valid_ratios=valid_ratios,                  # torch.Size([2, 4, 2])\n            # prepare_for_cdn时生成的\n            tgt_mask=attn_mask)                         # torch.Size([1098, 1098])\n        # Begin postprocess\n        # -------------------------------------------------------     \n        if self.two_stage_type == &amp;#39;standard&amp;#39;:\n            if self.two_stage_keep_all_tokens:\n                hs_enc = output_memory.unsqueeze(0)      # 没有经过topk筛选的\n                ref_enc = enc_outputs_coord_unselected.unsqueeze(0)\n                init_box_proposal = output_proposals\n            else:\n                # [1,bs,900,256] tgt_undetach是encoder的输出memory经过topk选取后的\n                hs_enc = tgt_undetach.unsqueeze(0)       # torch.Size([2, 900, 256])-->torch.Size([1, 2, 900, 256])\n                # [1,bs,900,4] refpoint_embed_undetach是encoder的输出经过topk选取后的\n                ref_enc = refpoint_embed_undetach.sigmoid().unsqueeze(0)  # torch.Size([2, 900, 4])--->torch.Size([1, 2, 900, 4])\n        else:\n            hs_enc = ref_enc = None\n        # init_box_proposal 最最初始的参考点位    hs_enc, ref_enc 这两个是encoder的输出经过topk筛选的\n        return hs, references, hs_enc, ref_enc, init_box_proposal\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><span class=\'hidden-code\' data-code=\'class DeformableTransformer(nn.Module):\n    def get_valid_ratio(self, mask):\n        _, H, W = mask.shape\n        valid_H = torch.sum(~mask[:, :, 0], 1)\n        valid_W = torch.sum(~mask[:, 0, :], 1)\n        valid_ratio_h = valid_H.float() / H\n        valid_ratio_w = valid_W.float() / W\n        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)\n        return valid_ratio\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><span class=\'hidden-code\' data-code=\'class TransformerEncoder(nn.Module):\n    def forward(self,src,pos,spatial_shapes,                  # [bs, sum(hi*wi), 256]     [bs, sum(hi*wi), 256]pos embed for src.  spatial_shapes: h,w of each level [num_level, 2]  \n            level_start_index,valid_ratios,key_padding_mask,  # [num_level] start point of level in sum(hi*wi)    [bs, num_level, 2]    [bs, sum(hi*wi)]\n            ref_token_index,ref_token_coord):                 # None,None\n        if self.two_stage_type in [&amp;#39;no&amp;#39;, &amp;#39;standard&amp;#39;, &amp;#39;enceachlayer&amp;#39;, &amp;#39;enclayer1&amp;#39;]:\n            assert ref_token_index is None\n        output = src                                          # 图像特征   torch.Size([2, 13462, 256])\n        if self.num_layers > 0:                               # preparation and reshape   6\n            if self.deformable_encoder:                       # True\n                reference_points = self.`get_reference_points`(spatial_shapes, valid_ratios, device=src.device)     # encoder的参考点是生成的grid，Deformable DETR中的方法  torch.Size([2, 13462, 4, 2])\n        intermediate_output = []\n        intermediate_ref = [] \n        if ref_token_index is not None:                       # 这个传进来的值就是None\n            out_i = torch.gather(output, 1, ref_token_index.unsqueeze(-1).repeat(1, 1, self.d_model))\n            intermediate_output.append(out_i)\n            intermediate_ref.append(ref_token_coord)\n        for layer_id, layer in enumerate(self.layers):        # main process    encoder layer的循环\n            dropflag = False\n            if self.enc_layer_dropout_prob is not None:       # 默认是None\n                prob = random.random()\n                if prob < self.enc_layer_dropout_prob[layer_id]:\n                    dropflag = True\n            if not dropflag:\n                if self.deformable_encoder:\n                    output = `layer`(src=output, pos=pos, reference_points=reference_points,         # encoder layer 是与Deformable DETR相同的 -----> torch.Size([2, 13462, 256])\n                                   spatial_shapes=spatial_shapes, level_start_index=level_start_index, key_padding_mask=key_padding_mask)\n                else:\n                    output = layer(src=output.transpose(0, 1), pos=pos.transpose(0, 1),key_padding_mask=key_padding_mask).transpose(0, 1)  # 正常的attention\n            if ((layer_id==0 and self.two_stage_type in [&amp;#39;enceachlayer&amp;#39;,&amp;#39;enclayer1&amp;#39;]) or (self.two_stage_type==&amp;#39;enceachlayer&amp;#39;)) and (layer_id!=self.num_layers-1): # two_stage_type默认是standard\n                output_memory, output_proposals = `gen_encoder_output_proposals`(output, key_padding_mask, spatial_shapes)                   # 在每一层encoder都进行topk proposal的选择\n                output_memory = self.enc_norm[layer_id](self.enc_proj[layer_id](output_memory))  \n                topk = self.num_queries                                                                                    # gather boxes\n                enc_outputs_class = self.class_embed[layer_id](output_memory)\n                ref_token_index = torch.topk(enc_outputs_class.max(-1)[0], topk, dim=1)[1]                                 # bs, nq\n                ref_token_coord = torch.gather(output_proposals, 1, ref_token_index.unsqueeze(-1).repeat(1, 1, 4))\n                output = output_memory\n            \n            if (layer_id != self.num_layers - 1) and ref_token_index is not None:                                          # ref_token_index 默认是None  aux loss\n                out_i = torch.gather(output, 1, ref_token_index.unsqueeze(-1).repeat(1, 1, self.d_model))\n                intermediate_output.append(out_i)\n                intermediate_ref.append(ref_token_coord)\n        if self.norm is not None:\n            output = self.norm(output)\n        if ref_token_index is not None:\n            intermediate_output = torch.stack(intermediate_output)                                                          # n_enc/n_enc-1, bs, \\sum{hw}, d_model\n            intermediate_ref = torch.stack(intermediate_ref)\n        else:\n            intermediate_output = intermediate_ref = None\n        return output, intermediate_output, intermediate_ref      # torch.Size([2, 13462, 256]),None,None\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><span class=\'hidden-code\' data-code=\'class TransformerEncoder(nn.Module):\n    def get_reference_points(spatial_shapes, valid_ratios, device):       # spatial_shapes[特征层数,2]=[[h1,w1],[h2,w2],...,[hn,wn]] valid_ratios [bs,特征层数,2]\n        reference_points_list = []\n        for lvl, (H_, W_) in enumerate(spatial_shapes):\n            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),  # 生成网格点,从0.5开始 到 减掉一个0.5\n                                          torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)\n            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W_)      # [bs,hw]\n            ref = torch.stack((ref_x, ref_y), -1)                                       # [bs,hw,2]\n            reference_points_list.append(ref)\n        reference_points = torch.cat(reference_points_list, 1)                          # 所有特征层的参考点拼在一起 [bs,all hw,2]\n        reference_points = reference_points[:, :, None] * valid_ratios[:, None]         # reference_points[:,:,None] -> [2,all hw,1,2] + valid_ratios[:,None] -> [bs,1,特征层数量,2]\n        return reference_points                                                         # [2,all hw,4,2]\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><span class=\'hidden-code\' data-code=\'class DeformableTransformerEncoderLayer(nn.Module):\n    def forward(self, src, pos, reference_points, spatial_shapes, level_start_index, key_padding_mask=None):\n        src2 = self.`self_attn`(self.with_pos_embed(src, pos), reference_points, src, spatial_shapes, level_start_index, key_padding_mask)     # self attention\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src = self.forward_ffn(src)                            # ffn\n        # 比Deformable DETR的encoder多的部分 channel attn\n        if self.add_channel_attention:\n            src = self.norm_channel(src + self.activ_channel(src))\n        return src\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/ops/modules/ms_deform_attn.py</p><span class=\'hidden-code\' data-code=\'class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):   # 维数 + 特征层的数量 + head的数量 + 每个特征层每个注意力头的采样点数量\n        super().__init__()\n        _d_per_head = d_model // n_heads                                  # 32  这个要为2的倍数\n        self.im2col_step = 64\n        self.d_model = d_model\n        self.n_levels = n_levels\n        self.n_heads = n_heads\n        self.n_points = n_points\n        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)  # 采样点的偏移量 论文中的2MK 就是n_heads*n_points 因为多层特征因此还有个n_levels\n        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)     # 权重矩阵，论文中的MK 就是n_heads*n_points 因为多层特征因此还有个n_levels\n        self.value_proj = nn.Linear(d_model, d_model)                     # 图2中左下区域的那个Linear\n        self.output_proj = nn.Linear(d_model, d_model)                    # 图2中右下区域的那个Linear  Deformable-DETR论文里面的  deformable attention module.\n        self._reset_parameters()\n    def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):    \n        N, Len_q, _ = query.shape               # query 是src+pos，query下面通过全连接层变成了attention_weights\n        N, Len_in, _ = input_flatten.shape      # input_flatten - [bs,all hw,256] 是src 对应了V\n        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in         # (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n        value = self.value_proj(input_flatten)  # 图二中左下的全连接Linear-》[bs,all hw,256]\n        if input_padding_mask is not None:\n            value = value.masked_fill(input_padding_mask[..., None], float(0))                   # 在mask的地方填充0 [bs, all hw,256]\n        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)                # 分成多头，拆分的是最后的256 [bs,all hw,256] -> [bs,all hw, 8, 32]\n        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)   # sampling_offsets全连接--> (bs, all hw,8,4,4,2) 8个头，4个特征层，4个采样点 2个偏移量坐标\n        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)   # attention_weights 是一个全连接 like (bs, all hw,8,16)\n        attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points) # 经过softmax 保证权重和为1，然后在拆分成4 4  like (bs,all hw,8,4,4)\n        \n        if reference_points.shape[-1] == 2:                                                                        # N, Len_q, n_heads, n_levels, n_points, 2\n            offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)      # input_spatial_shapes 换位置，高宽 变成 宽高\n            # reference_points  [bs,all hw,4,2] -> [bs,all hw,1,4,1,2] + sampling_offsets  [bs,all hw,8,4,4,2] / offset_normalizer [4,2] -> [1,1,1,4,1,2]\n            # 采样点加上偏移量 sampling_offsets / offset_normalizer 表示相对的偏移量   ----->   like (bs, hw,8,4,4,2)\n            sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n        elif reference_points.shape[-1] == 4:\n            sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n        else:\n            raise ValueError(&amp;#39;Last dim of reference_points must be 2 or 4, but get {} instead.&amp;#39;.format(reference_points.shape[-1]))\n        \n        if value.dtype == torch.float16:                     # for amp\n            output = MSDeformAttnFunction.apply(value.to(torch.float32), input_spatial_shapes, input_level_start_index, sampling_locations.to(torch.float32), attention_weights, self.im2col_step)  # for mixed precision\n            output = output.to(torch.float16)\n            output = self.output_proj(output)\n            return output        \n        output = MSDeformAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights,self.im2col_step)  # 这里调用了cuda实现  \n        output = self.output_proj(output)   # 图二中右下的全连接Linear\n        return output\n\'> </span>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/utils.py</p><span class=\'hidden-code\' data-code=\'def gen_encoder_output_proposals(memory, memory_padding_mask, spatial_shapes, learnedwh=None):      # 与Deformable DETR相同的\n    N_, S_, C_ = memory.shape\n    base_scale = 4.0\n    proposals = []\n    _cur = 0                                                              # 定位在memory_padding_mask中的位置，memory_padding_mask这里是所有特征层的内容在一起了\n    for lvl, (H_, W_) in enumerate(spatial_shapes):                                         # 某个特征层的 高 宽\n        mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H_ * W_)].view(N_, H_, W_, 1)   # 获取mask, memory_padding_mask is[bs,all hw] [bs,H_,W_,1]\n        valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)                # 有效的高度\n        valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)                # 有效的宽度\n        grid_y, grid_x = torch.meshgrid(torch.linspace(0, H_ - 1, H_, dtype=torch.float32, device=memory.device),   # 生成网格点，这里没有0.5的做法 get_reference_points这个方法中是从0.5开始，这里是从0开始\n                                        torch.linspace(0, W_ - 1, W_, dtype=torch.float32, device=memory.device))\n        grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)                      # 生成网格二维点 H_, W_, 2    \n        scale = torch.cat([valid_W.unsqueeze(-1), valid_H.unsqueeze(-1)], 1).view(N_, 1, 1, 2)  # scale [bs,1,1,2]\n        grid = (grid.unsqueeze(0).expand(N_, -1, -1, -1) + 0.5) / scale   # 获取相对值 [bs,h,w,2]\n        if learnedwh is not None:\n            wh = torch.ones_like(grid) * learnedwh.sigmoid() * (2.0 ** lvl)\n        else:\n            wh = torch.ones_like(grid) * 0.05 * (2.0 ** lvl)              # [bs,h,w,2] 宽高的相对值，不同的特征层 wh不同,层级越高，wh相对也会更大\n        proposal = torch.cat((grid, wh), -1).view(N_, -1, 4)              # [bs,h,w,4] -> [bs,hw,4]\n        proposals.append(proposal)\n        _cur += (H_ * W_)\n    \n    output_proposals = torch.cat(proposals, 1)                                                         # [bs,all hw,4]\n    output_proposals_valid = ((output_proposals `>` 0.01) &amp; (output_proposals `<` 0.99)).all(-1, keepdim=True)    # [bs,all hw,1] proposals 不需要太靠近边界\n    output_proposals = torch.log(output_proposals / (1 - output_proposals))                            # unsigmoid--->sigmoid的反函数 (-4.5951->4.5951)\n    output_proposals = output_proposals.masked_fill(memory_padding_mask.unsqueeze(-1), float(&amp;#39;inf&amp;#39;))   # 对于mask部分，填充inf\n    output_proposals = output_proposals.masked_fill(~output_proposals_valid, float(&amp;#39;inf&amp;#39;))             # 靠近边界的部分，填充inf\n    output_memory = memory\n    output_memory = output_memory.masked_fill(memory_padding_mask.unsqueeze(-1), float(0))            # 对于memory的输出进行同样的填充\n    output_memory = output_memory.masked_fill(~output_proposals_valid, float(0))                      # [bs,all hw,256]\n    return output_memory, output_proposals\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/utils.py</p><span class=\'hidden-code\' data-code=\'def gen_encoder_output_proposals(memory:Tensor, memory_padding_mask:Tensor, spatial_shapes:Tensor, learnedwh=None):\n    # torch.Size([2, 13462, 256])  torch.Size([2, 13462])  torch.Size([4, 2])   None\n    N_, S_, C_ = memory.shape\n    base_scale = 4.0\n    proposals = []\n    _cur = 0\n    for lvl, (H_, W_) in enumerate(spatial_shapes):\n        mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H_ * W_)].view(N_, H_, W_, 1)       # torch.Size([2, 110, 92, 1])\n        valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)          # tensor([ 65, 110], device=&amp;#39;cuda:0&amp;#39;)\n        valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)          # tensor([79, 92], device=&amp;#39;cuda:0&amp;#39;)\n        grid_y, grid_x = torch.meshgrid(torch.linspace(0, H_ - 1, H_, dtype=torch.float32, device=memory.device),   # torch.Size([110, 92])\n                                        torch.linspace(0, W_ - 1, W_, dtype=torch.float32, device=memory.device))   # torch.Size([110, 92])\n        grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1) # H_, W_, 2  torch.Size([110, 92, 2])\n        scale = torch.cat([valid_W.unsqueeze(-1), valid_H.unsqueeze(-1)], 1).view(N_, 1, 1, 2)\n        grid = (grid.unsqueeze(0).expand(N_, -1, -1, -1) + 0.5) / scale\n        if learnedwh is not None:\n            wh = torch.ones_like(grid) * learnedwh.sigmoid() * (2.0 ** lvl)\n        else:\n            wh = torch.ones_like(grid) * 0.05 * (2.0 ** lvl)\n        proposal = torch.cat((grid, wh), -1).view(N_, -1, 4)\n        proposals.append(proposal)\n        _cur += (H_ * W_)\n    output_proposals = torch.cat(proposals, 1)                      # torch.Size([2, 13462, 4])\n    output_proposals_valid = ((output_proposals `>` 0.01) &amp; (output_proposals `<` 0.99)).all(-1, keepdim=True)  # torch.Size([2, 13462, 1])\n    output_proposals = torch.log(output_proposals / (1 - output_proposals)) # unsigmoid\n    output_proposals = output_proposals.masked_fill(memory_padding_mask.unsqueeze(-1), float(&amp;#39;inf&amp;#39;))\n    output_proposals = output_proposals.masked_fill(~output_proposals_valid, float(&amp;#39;inf&amp;#39;))\n    output_memory = memory\n    output_memory = output_memory.masked_fill(memory_padding_mask.unsqueeze(-1), float(0))\n    output_memory = output_memory.masked_fill(~output_proposals_valid, float(0))\n    return output_memory, output_proposals     # torch.Size([2, 13462, 256])   torch.Size([2, 13462, 4])\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/deformable_transformer.py</p><span class=\'hidden-code\' data-code=\'class TransformerDecoder(nn.Module):\n    def forward(self, tgt, memory,tgt_mask,memory_mask,tgt_key_padding_mask,memory_key_padding_mask,pos,refpoints_unsigmoid,level_start_index,spatial_shapes,valid_ratios):\n        output = tgt\n        intermediate = []                                  # 每一层decoder计算后的结果\n        reference_points = refpoints_unsigmoid.sigmoid()   # [900+2*100,bs,4] 限制在0-1\n        ref_points = [reference_points]                    # 有一个初始的点位，以及每一层decoder计算后，进行修正后的结果\n        for layer_id, layer in enumerate(self.layers):\n            if self.training and self.decoder_query_perturber is not None and layer_id != 0:      # preprocess ref points 对box添加随机的扰动噪声\n                reference_points = self.`decoder_query_perturber`(reference_points)\n            if self.deformable_decoder:\n                if reference_points.shape[-1] == 4:                                                                               # [N,bs,1,4]*[1,bs,4,4] -> [N,bs,4,4]\n                    reference_points_input = reference_points[:, :, None] * torch.cat([valid_ratios, valid_ratios], -1)[None, :]  # nq, bs, nlevel, 4 \n                else:\n                    assert reference_points.shape[-1] == 2\n                    reference_points_input = reference_points[:, :, None] * valid_ratios[None, :]\n                query_sine_embed = gen_sineembed_for_position(reference_points_input[:, :, 0, :])  # get sine embedding for the query vector -> [N,bs,512] xywh的高频位置编码\n            else:\n                query_sine_embed = gen_sineembed_for_position(reference_points)                    # nq, bs, 256*2\n                reference_points_input = None\n            raw_query_pos = self.ref_point_head(query_sine_embed)                                  # conditional query [N,bs,256]  nq, bs, 256\n            pos_scale = self.query_scale(output) if self.query_scale is not None else 1\n            query_pos = pos_scale * raw_query_pos                       # conditional detr的做法\n            if not self.deformable_decoder:                             # 如果不使用变形attention\n                query_sine_embed = query_sine_embed[..., :self.d_model] * self.query_pos_sine_scale(output)        # 这里就是conditional detr的那个乘法\n            if not self.deformable_decoder and self.modulate_hw_attn:   # 如果使用了变形attention 就不执行hw的调制了，todo 论文中好像没有提到过        modulated HW attentions\n                refHW_cond = self.ref_anchor_head(output).sigmoid()     # DAB-DETR的部分 结构图中第二行的MLP的右边那个MLP 公式7的Wref,Href [300,bs,2] nq, bs, 2\n                query_sine_embed[..., self.d_model // 2:] *= (refHW_cond[..., 0] / reference_points[..., 2]).unsqueeze(-1)      # 公式6的Xref    \n                query_sine_embed[..., :self.d_model // 2] *= (refHW_cond[..., 1] / reference_points[..., 3]).unsqueeze(-1)      # 公式6的Yref\n            dropflag = False                                            # 随机的跨过某些decoder layer，并不处理，直接进入下一层 random drop some layers if needed\n            if self.dec_layer_dropout_prob is not None:\n                prob = random.random()\n                if prob < self.dec_layer_dropout_prob[layer_id]:\n                    dropflag = True\n            if not dropflag:\n                output = layer(                                          # [N,bs,256]\n                    tgt=output,                                          # [N,bs,256]\n                    tgt_query_pos=query_pos,                             # [N,bs,256]\n                    tgt_query_sine_embed=query_sine_embed,               # [N,bs,512]\n                    tgt_key_padding_mask=tgt_key_padding_mask,           # None\n                    tgt_reference_points=reference_points_input,         # [N,bs,4,4]\n                    memory=memory,                                       # [sum(hw),bs,256]\n                    memory_key_padding_mask=memory_key_padding_mask,\n                    memory_level_start_index=level_start_index,\n                    memory_spatial_shapes=spatial_shapes,                # [level,2]\n                    memory_pos=pos,                                      # [sum(hw),bs,256]\n                    self_attn_mask=tgt_mask,                             # prepare_for_cdn时生成的 [N,N]\n                    cross_attn_mask=memory_mask                          # 默认为None，调用时并未传入\n                )\n            if self.bbox_embed is not None:                                     # iter update\n                reference_before_sigmoid = inverse_sigmoid(reference_points)    # 得到特征图上的值\n                delta_unsig = self.bbox_embed[layer_id](output)                 # 得到网络输出的修正值\n                outputs_unsig = delta_unsig + reference_before_sigmoid          # 进行修正\n                new_reference_points = outputs_unsig.sigmoid()                  # 限制在0-1\n                if self.dec_layer_number is not None and layer_id != self.num_layers - 1:            # select # ref points\n                    nq_now = new_reference_points.shape[0]\n                    select_number = self.dec_layer_number[layer_id + 1]\n                    if nq_now != select_number:\n                        class_unselected = self.class_embed[layer_id](output)                              # nq, bs, 91\n                        topk_proposals = torch.topk(class_unselected.max(-1)[0], select_number, dim=0)[1]  # new_nq, bs\n                        new_reference_points = torch.gather(new_reference_points, 0, topk_proposals.unsqueeze(-1).repeat(1, 1, 4))  # unsigmoid\n                if self.rm_detach and &amp;#39;dec&amp;#39; in self.rm_detach:\n                    reference_points = new_reference_points\n                else:\n                    reference_points = new_reference_points.detach()       # 脱离   \n                if self.use_detached_boxes_dec_out:\n                    ref_points.append(reference_points)                    # 这里是脱离的\n                else:\n                    ref_points.append(new_reference_points)                # 这个是没有脱离的，这个地方的处理就是论文钟的look forward twice\n            intermediate.append(self.norm(output))                         # 中间层的值\n            if self.dec_layer_number is not None and layer_id != self.num_layers - 1:\n                if nq_now != select_number:\n                    output = torch.gather(output, 0, topk_proposals.unsqueeze(-1).repeat(1, 1, self.d_model))  # unsigmoid\n        return [\n            [itm_out.transpose(0, 1) for itm_out in intermediate],         # 将bs维度放在前面\n            [itm_refpoint.transpose(0, 1) for itm_refpoint in ref_points]  # 将bs维度放在前面\n        ]\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dn_components.py</p><span class=\'hidden-code\' data-code=\'# 这里与DN-DETR的处理基本一致，只多了一个aux_loss的处理\ndef dn_post_process(outputs_class, outputs_coord, dn_meta, aux_loss, _set_aux_loss):        # transformer处理之后的后处理\n    if dn_meta and dn_meta[&amp;#39;pad_size&amp;#39;] > 0:                                   # dn_meta[&amp;#39;pad_size&amp;#39;]=198\n        output_known_class = outputs_class[:, :, :dn_meta[&amp;#39;pad_size&amp;#39;], :]     # 前面的这些是去噪的部分  torch.Size([6, 2, 1098, 18])-->torch.Size([6, 2, 198, 18])\n        output_known_coord = outputs_coord[:, :, :dn_meta[&amp;#39;pad_size&amp;#39;], :]     # torch.Size([6, 2, 1098, 4])-->torch.Size([6, 2, 198, 4])\n        outputs_class = outputs_class[:, :, dn_meta[&amp;#39;pad_size&amp;#39;]:, :]          # 后面这些是正常的匹配预测部分  torch.Size([6, 2, 900, 18])\n        outputs_coord = outputs_coord[:, :, dn_meta[&amp;#39;pad_size&amp;#39;]:, :]          #                             torch.Size([6, 2, 900, 4])\n        out = {&amp;#39;pred_logits&amp;#39;: output_known_class[-1], &amp;#39;pred_boxes&amp;#39;: output_known_coord[-1]}\n        if aux_loss:                                                          # True\n            out[&amp;#39;aux_outputs&amp;#39;] = _set_aux_loss(output_known_class, output_known_coord)       # [{&amp;#39;pred_logits&amp;#39;: a, &amp;#39;pred_boxes&amp;#39;: b}]*6\n        dn_meta[&amp;#39;output_known_lbs_bboxes&amp;#39;] = out                              # output_known_lbs_bboxes 内容是去噪部分的\n    return outputs_class, outputs_coord                                       # 返回这俩还是网络自己预测的，不包括去噪部分的\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">计算损失</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><span class=\'hidden-code\' data-code=\'class SetCriterion(nn.Module):                    # forward方法与DN-DETR不同\n    def forward(self, outputs, targets, return_indices=False):\n        outputs_without_aux = {k: v for k, v in outputs.items() if k != &amp;#39;aux_outputs&amp;#39;}      \n        # outputs_without_aux={&amp;#39;pred_logits&amp;#39;:[2, 900, 18], &amp;#39;pred_boxes&amp;#39;:[2, 900, 4], &amp;#39;interm_outputs&amp;#39;:{&amp;#39;pred_logits&amp;#39;:[2, 900, 18],&amp;#39;pred_boxes&amp;#39;:[2, 900, 4]}, &amp;#39;interm_outputs_for_matching_pre&amp;#39;:{&amp;#39;pred_logits&amp;#39;:[2, 900, 18],&amp;#39;pred_boxes&amp;#39;:[2, 900, 4]}, \n        # &amp;#39;dn_meta&amp;#39;:{&amp;#39;pad_size&amp;#39;:198,&amp;#39;num_dn_group&amp;#39;:9,&amp;#39;output_known_lbs_bboxes&amp;#39;:{}}\n        device = next(iter(outputs.values())).device\n        indices = self.matcher(outputs_without_aux, targets)  # 匈牙利匹配  indices是list, 长度是bs, 每个item是一个tuple, 第一个值是框的id，第二个值是gt的id\n        # [(tensor([322, 482, 54...824, 851]), tensor([ 1,  5,  8, ...,  9,  2])),(tensor([414, 674, 76...832, 876]), tensor([4, 3, 1, 2, 0, 5]))]\n        if return_indices:\n            indices0_copy = indices\n            indices_list = []\n        # bs中所有GT的数量\n        # Compute the average number of target boxes accross all nodes, for normalization purposes\n        num_boxes = sum(len(t[&amp;#39;labels&amp;#39;]) for t in targets)                       # 17\n        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=device)\n        if is_dist_avail_and_initialized():\n            torch.distributed.all_reduce(num_boxes) \n        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()      # 17.0\n        # Compute all the requested losses\n        losses = {}\n        # dn_meta 这个地方与DN-DETR不同了   prepare for dn loss {pad_size,num_dn_group,output_known_lbs_bboxes{pred_logits,pred_boxes,aux_outputs} }\n        dn_meta = outputs[&amp;#39;dn_meta&amp;#39;]\n        if self.training and dn_meta and &amp;#39;output_known_lbs_bboxes&amp;#39; in dn_meta:\n            # single_pad bs最大的gt的数量x2 (包括了正负样本的总数量)，scalar可以认为是dn-detr中的group的概念\n            # output_known_lbs_bboxes的内容     pred_logits,pred_boxes,aux_outputs pred_logits [bs,2*dn_number,91]   pred_boxes [bs,2*dn_number,4]\n            output_known_lbs_bboxes, single_pad, scalar = self.`prep_for_dn`(dn_meta)     # ...,   22,  9\n            dn_pos_idx = []\n            dn_neg_idx = []\n            for i in range(len(targets)):\n                if len(targets[i][&amp;#39;labels&amp;#39;]) > 0:\n                    t = torch.range(0, len(targets[i][&amp;#39;labels&amp;#39;]) - 1).long().cuda()  # 0 --> gt的数量-1   tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], device=&amp;#39;cuda:0&amp;#39;)\n                    t = t.unsqueeze(0).repeat(scalar, 1)                             # [scalar, gt num]   torch.Size([9, 11])\n                    tgt_idx = t.flatten()                                            # (scalar*gt num)  torch.Size([99])\n                    # (torch.tensor(range(scalar)) * single_pad).long().cuda().unsqueeze(1) ---> torch.Size([9, 1])\n                    # tensor([[  0],\n                    #         [ 22],\n                    #         [ 44],\n                    #         [ 66],\n                    #         [ 88],\n                    #         [110],\n                    #         [132],\n                    #         [154],\n                    #         [176]], device=&amp;#39;cuda:0&amp;#39;)\n                    # *single_pad, 就是乘上gap\n                    output_idx = (torch.tensor(range(scalar)) * single_pad).long().cuda().unsqueeze(1) + t\n                    output_idx = output_idx.flatten()           # torch.Size([99])  tensor([  0,   1,...,  10,  22,  23,...,  32,  44,  45, ..., 54,  66,  67, ...,  76,  88,  89,....], device=&amp;#39;cuda:0&amp;#39;)\n                else:\n                    output_idx = tgt_idx = torch.tensor([]).long().cuda()\n                # 这里第一个参数就是提议框的id，第二个参数就是gt的id的意思\n                dn_pos_idx.append((output_idx, tgt_idx))\n                # 这个变量并没有使用\n                dn_neg_idx.append((output_idx + single_pad // 2, tgt_idx))\n            output_known_lbs_bboxes = dn_meta[&amp;#39;output_known_lbs_bboxes&amp;#39;]\n            l_dict = {}\n            for loss in self.losses:                   # [&amp;#39;labels&amp;#39;, &amp;#39;boxes&amp;#39;, &amp;#39;cardinality&amp;#39;]\n                kwargs = {}\n                if &amp;#39;labels&amp;#39; in loss:\n                    kwargs = {&amp;#39;log&amp;#39;: False}\n                l_dict.update(self.`get_loss`(loss, output_known_lbs_bboxes, targets, dn_pos_idx, num_boxes * scalar, **kwargs))           \n            l_dict = {k + f&amp;#39;_dn&amp;#39;: v for k, v in l_dict.items()}\n            losses.update(l_dict)\n        else:\n            l_dict = dict()              # 没有去噪训练，这些loss就都是0\n            l_dict[&amp;#39;loss_bbox_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n            l_dict[&amp;#39;loss_giou_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n            l_dict[&amp;#39;loss_ce_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n            l_dict[&amp;#39;loss_xy_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n            l_dict[&amp;#39;loss_hw_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n            l_dict[&amp;#39;cardinality_error_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n            losses.update(l_dict)\n        for loss in self.losses:                       # [&amp;#39;labels&amp;#39;, &amp;#39;boxes&amp;#39;, &amp;#39;cardinality&amp;#39;]\n            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n        if &amp;#39;aux_outputs&amp;#39; in outputs:\n            for idx, aux_outputs in enumerate(outputs[&amp;#39;aux_outputs&amp;#39;]):\n                indices = self.matcher(aux_outputs, targets)\n                if return_indices:\n                    indices_list.append(indices)\n                for loss in self.losses:\n                    if loss == &amp;#39;masks&amp;#39;:\n                        # Intermediate masks losses are too costly to compute, we ignore them.\n                        continue\n                    kwargs = {}\n                    if loss == &amp;#39;labels&amp;#39;:\n                        # Logging is enabled only for the last layer\n                        kwargs = {&amp;#39;log&amp;#39;: False}\n                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n                    l_dict = {k + f&amp;#39;_{idx}&amp;#39;: v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n                if self.training and dn_meta and &amp;#39;output_known_lbs_bboxes&amp;#39; in dn_meta:\n                    aux_outputs_known = output_known_lbs_bboxes[&amp;#39;aux_outputs&amp;#39;][idx]\n                    l_dict = {}\n                    for loss in self.losses:\n                        kwargs = {}\n                        if &amp;#39;labels&amp;#39; in loss:\n                            kwargs = {&amp;#39;log&amp;#39;: False}\n                        # 这里的第四个参数是indices, 在这里是dn_pos_idx\n                        # indices的内容是bs中的每个img的匹配，第一个参数是提议框的id，第二个参数是gt的id\n                        l_dict.update(self.get_loss(loss, aux_outputs_known, targets, dn_pos_idx, num_boxes * scalar,\n                                                    **kwargs))\n                    l_dict = {k + f&amp;#39;_dn_{idx}&amp;#39;: v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n                else:\n                    l_dict = dict()\n                    l_dict[&amp;#39;loss_bbox_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n                    l_dict[&amp;#39;loss_giou_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n                    l_dict[&amp;#39;loss_ce_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n                    l_dict[&amp;#39;loss_xy_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n                    l_dict[&amp;#39;loss_hw_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n                    l_dict[&amp;#39;cardinality_error_dn&amp;#39;] = torch.as_tensor(0.).to(&amp;#39;cuda&amp;#39;)\n                    l_dict = {k + f&amp;#39;_{idx}&amp;#39;: v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n        # 最后encoder的输出经过topk的\n        # interm_outputs loss\n        if &amp;#39;interm_outputs&amp;#39; in outputs:\n            interm_outputs = outputs[&amp;#39;interm_outputs&amp;#39;]\n            indices = self.matcher(interm_outputs, targets)\n            if return_indices:\n                indices_list.append(indices)\n            for loss in self.losses:\n                if loss == &amp;#39;masks&amp;#39;:\n                    # Intermediate masks losses are too costly to compute, we ignore them.\n                    continue\n                kwargs = {}\n                if loss == &amp;#39;labels&amp;#39;:\n                    # Logging is enabled only for the last layer\n                    kwargs = {&amp;#39;log&amp;#39;: False}\n                l_dict = self.get_loss(loss, interm_outputs, targets, indices, num_boxes, **kwargs)\n                l_dict = {k + f&amp;#39;_interm&amp;#39;: v for k, v in l_dict.items()}\n                losses.update(l_dict)\n        # 其他encoder layer的输出\n        # enc output loss\n        if &amp;#39;enc_outputs&amp;#39; in outputs:\n            for i, enc_outputs in enumerate(outputs[&amp;#39;enc_outputs&amp;#39;]):\n                indices = self.matcher(enc_outputs, targets)\n                if return_indices:\n                    indices_list.append(indices)\n                for loss in self.losses:\n                    if loss == &amp;#39;masks&amp;#39;:\n                        # Intermediate masks losses are too costly to compute, we ignore them.\n                        continue\n                    kwargs = {}\n                    if loss == &amp;#39;labels&amp;#39;:\n                        # Logging is enabled only for the last layer\n                        kwargs = {&amp;#39;log&amp;#39;: False}\n                    l_dict = self.get_loss(loss, enc_outputs, targets, indices, num_boxes, **kwargs)\n                    l_dict = {k + f&amp;#39;_enc_{i}&amp;#39;: v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n        if return_indices:\n            indices_list.append(indices0_copy)\n            return losses, indices_list\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">计算损失models/dino/dino.py</p><span class=\'hidden-code\' data-code=\'class SetCriterion(nn.Module):                    # forward方法与DN-DETR不同\n    def prep_for_dn(self,dn_meta):\n        output_known_lbs_bboxes = dn_meta[&amp;#39;output_known_lbs_bboxes&amp;#39;]\n        num_dn_groups,pad_size=dn_meta[&amp;#39;num_dn_group&amp;#39;],dn_meta[&amp;#39;pad_size&amp;#39;]\n        assert pad_size % num_dn_groups==0\n        single_pad=pad_size//num_dn_groups\n        return output_known_lbs_bboxes,single_pad,num_dn_groups\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><span class=\'hidden-code\' data-code=\'class SetCriterion(nn.Module):                        # 与DN-DETR相同\n    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n        assert &amp;#39;pred_logits&amp;#39; in outputs\n        src_logits = outputs[&amp;#39;pred_logits&amp;#39;]            # torch.Size([2, 198, 18]) / torch.Size([2, 198, 19])\n        # 是两个tensor，第一个tensor是哪个图片的id，第二个tensor是分配的预测框的id 这里只有正样本对应的id\n        idx = self._get_src_permutation_idx(indices)   # () 153\n        target_classes_o = torch.cat([t[&amp;#39;labels&amp;#39;][J] for t,  (_, J) in zip(targets, indices)])                           # torch.Size([153])  范围从0-17  /  1-18\n        target_classes = torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device) # torch.Size([2, 198]) 全部填充为18  / 19\n        target_classes[idx] = target_classes_o            \n        # 这里对于对比去噪的负样本，它的对应的就是no object\n        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],\n                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)   # torch.Size([2, 198, 19])  / torch.Size([2, 198, 20])\n        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)       # target_classes.unsqueeze(-1).shape=torch.Size([2, 198, 1])，背景的话第19个为1\n        target_classes_onehot = target_classes_onehot[:, :, :-1]                 # torch.Size([2, 198, 18])    这样导致背景的都是0000 /torch.Size([2, 198, 19]\n        loss_ce = sigmoid_focal_loss(src_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2) * src_logits.shape[1]\n        losses = {&amp;#39;loss_ce&amp;#39;: loss_ce}\n        if log:                                                                  # TODO this should probably be a separate loss, not hacked in this one here\n            losses[&amp;#39;class_error&amp;#39;] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n        return losses\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><span class=\'hidden-code\' data-code=\'class SetCriterion(nn.Module):\n    # 与DN-DETR相同\n    def loss_boxes(self, outputs, targets, indices, num_boxes):\n        &amp;#39;&amp;#39;&amp;#39;Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss targets dicts must contain the key &amp;#39;boxes&amp;#39; containing a tensor of dim [nb_target_boxes, 4]\n           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.这里仅有正样本\n        &amp;#39;&amp;#39;&amp;#39;\n        assert &amp;#39;pred_boxes&amp;#39; in outputs\n        idx = self._get_src_permutation_idx(indices)                  # 两个tensor，第一个tensor是哪个图片的id，第二个tensor是分配的预测框的id\n        src_boxes = outputs[&amp;#39;pred_boxes&amp;#39;][idx]\n        target_boxes = torch.cat([t[&amp;#39;boxes&amp;#39;][i] for t, (_, i) in zip(targets, indices)], dim=0)\n        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction=&amp;#39;none&amp;#39;)\n        losses = {}\n        losses[&amp;#39;loss_bbox&amp;#39;] = loss_bbox.sum() / num_boxes\n        loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(box_ops.box_cxcywh_to_xyxy(src_boxes),box_ops.box_cxcywh_to_xyxy(target_boxes)))\n        losses[&amp;#39;loss_giou&amp;#39;] = loss_giou.sum() / num_boxes\n        # calculate the x,y and h,w loss\n        with torch.no_grad():\n            losses[&amp;#39;loss_xy&amp;#39;] = loss_bbox[..., :2].sum() / num_boxes\n            losses[&amp;#39;loss_hw&amp;#39;] = loss_bbox[..., 2:].sum() / num_boxes\n        return losses\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/dino/dino.py</p><span class=\'hidden-code\' data-code=\'class SetCriterion(nn.Module):\n    @torch.no_grad()\n    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n        &amp;#39;&amp;#39;&amp;#39; Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n        This is not really a loss, it is intended for logging purposes only. It doesn&amp;#39;t propagate gradients\n        &amp;#39;&amp;#39;&amp;#39;\n        pred_logits = outputs[&amp;#39;pred_logits&amp;#39;]         # torch.Size([2, 198, 18])\n        device = pred_logits.device\n        tgt_lengths = torch.as_tensor([len(v[&amp;#39;labels&amp;#39;]) for v in targets], device=device)    # tensor([11,  6], device=&amp;#39;cuda:0&amp;#39;)\n        # Count the number of predictions that are NOT &amp;#39;no-object&amp;#39; (which is the last class)\n        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n        losses = {&amp;#39;cardinality_error&amp;#39;: card_err}\n        return losses\n\'> </span>'}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
