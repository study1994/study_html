<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>VIT代码流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/modeling.py</p><span class=\'hidden-code\' data-code=\'class VisionTransformer(nn.Module):\n    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n        super(VisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.zero_head = zero_head\n        self.classifier = config.classifier\n        self.transformer = Transformer(config, img_size, vis)\n        self.head = Linear(config.hidden_size, num_classes)\n    def forward(self, x, labels=None):\n        x, attn_weights = self.`transformer`(x) (16,197,768)\n        logits = self.`head`(x[:, 0])           (16,768)->(16,10)用于计算分类计算\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n            return loss\n        else:\n            return logits, attn_weights\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/modeling.py</p><span class=\'hidden-code\' data-code=\'class Transformer(nn.Module):\n    def __init__(self, config, img_size, vis):\n        super(Transformer, self).__init__()\n        self.embeddings = Embeddings(config, img_size=img_size)\n        self.encoder = Encoder(config, vis)\n    def forward(self, input_ids):\n        embedding_output = self.`embeddings`(input_ids)        # (16,197,768)\n        encoded, attn_weights = self.`encoder`(embedding_output)\n        return encoded, attn_weights\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/modeling.py</p><span class=\'hidden-code\' data-code=\'class Embeddings(nn.Module):\n    &amp;#39;&amp;#39;&amp;#39;Construct the embeddings from patch, position embeddings.\n    &amp;#39;&amp;#39;&amp;#39;\n    def __init__(self, config, img_size, in_channels=3):\n        super(Embeddings, self).__init__()\n        self.hybrid = None\n        img_size = _pair(img_size)\n        if config.patches.get(&amp;#39;grid&amp;#39;) is not None:\n            grid_size = config.patches[&amp;#39;grid&amp;#39;]\n            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n            self.hybrid = True\n        else:\n            patch_size = _pair(config.patches[&amp;#39;size&amp;#39;])\n            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n            self.hybrid = False\n        if self.hybrid:\n            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,width_factor=config.resnet.width_factor)\n            in_channels = self.hybrid_model.width * 16\n        self.patch_embeddings = Conv2d(in_channels=in_channels,out_channels=config.hidden_size,kernel_size=patch_size,stride=patch_size)\n        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))     # (1,196+1,768)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))     # config.hidden_size=768\n        self.dropout = Dropout(config.transformer[&amp;#39;dropout_rate&amp;#39;])\n    def forward(self, x):    # (16,3,224,224)\n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # (16,1,768)\n        if self.hybrid:\n            x = self.hybrid_model(x)\n        x = self.patch_embeddings(x)      # 卷积操作->(16,768,14,14)\n        x = x.flatten(2)                  # (16,768,196)\n        x = x.transpose(-1, -2)           # (16,196,768)\n        x = torch.cat((cls_tokens, x), dim=1)      # (16,197,768)\n        embeddings = x + self.position_embeddings  # (16,197,768)\n        embeddings = self.dropout(embeddings)      # (16,197,768)\n        return embeddings\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/modeling.py</p><span class=\'hidden-code\' data-code=\'class Encoder(nn.Module):\n    def __init__(self, config, vis):\n        super(Encoder, self).__init__()\n        self.vis = vis\n        self.layer = nn.ModuleList()\n        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        for _ in range(config.transformer[&amp;#39;num_layers&amp;#39;]):\n            layer = Block(config, vis)\n            self.layer.append(copy.deepcopy(layer))\n    def forward(self, hidden_states):         # (16,197,768)\n        attn_weights = []\n        for layer_block in self.layer:\n            hidden_states, weights = `layer_block`(hidden_states)\n            if self.vis:\n                attn_weights.append(weights)\n        encoded = self.`encoder_norm`(hidden_states)\n        return encoded, attn_weights\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/modeling.py</p><span class=\'hidden-code\' data-code=\'class Block(nn.Module):\n    def __init__(self, config, vis):\n        super(Block, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n        self.ffn = Mlp(config)\n        self.attn = Attention(config, vis)\n    def forward(self, x):\n        h = x\n        x = self.`attention_norm`(x)  # (16,197,768)\n        x, weights = self.`attn`(x)   # (16,197,768) (16,12，197，64)\n        x = x + h\n        h = x\n        x = self.ffn_norm(x)          # (16,197,768)\n        x = self.ffn(x)\n        x = x + h\n        return x, weights             # (16,197,768)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">attention_norm</p>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">Attention</p><span class=\'hidden-code\' data-code=\'class Attention(nn.Module):\n    def __init__(self, config, vis):\n        super(Attention, self).__init__()\n        self.vis = vis\n        self.num_attention_heads = config.transformer[&amp;#39;num_heads&amp;#39;]\n        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.query = Linear(config.hidden_size, self.all_head_size)\n        self.key = Linear(config.hidden_size, self.all_head_size)\n        self.value = Linear(config.hidden_size, self.all_head_size)\n        self.out = Linear(config.hidden_size, config.hidden_size)\n        self.attn_dropout = Dropout(config.transformer[&amp;#39;attention_dropout_rate&amp;#39;])\n        self.proj_dropout = Dropout(config.transformer[&amp;#39;attention_dropout_rate&amp;#39;])\n        self.softmax = Softmax(dim=-1)\n    def transpose_for_scores(self, x): # (16,197,768)\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)       # (16,197,12，64)   分成12个头\n        return x.permute(0, 2, 1, 3)   # (16,12，197，64)\n    def forward(self, hidden_states):      # (16,197,768)\n        mixed_query_layer = self.query(hidden_states)  # 全连接得到(16,197,768)\n        mixed_key_layer = self.key(hidden_states)      # (16,197,768)\n        mixed_value_layer = self.value(hidden_states)  # (16,197,768)\n        query_layer = self.transpose_for_scores(mixed_query_layer)  # (16,12，197，64)\n        key_layer = self.transpose_for_scores(mixed_key_layer)      # (16,12，197，64)\n        value_layer = self.transpose_for_scores(mixed_value_layer)  # (16,12，197，64)\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))  # # (16,12，197，197)  197之间的关系\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)  # 排除64长度的影响\n        attention_probs = self.softmax(attention_scores)                           # 分值->权重值\n        weights = attention_probs if self.vis else None\n        attention_probs = self.attn_dropout(attention_probs)                       # (16,12，197，64)\n        context_layer = torch.matmul(attention_probs, value_layer)                 # 将得分值作用到value上面--》重构特征\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()             # (16,12，197，64)\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)               # (16, 197，768)\n        attention_output = self.out(context_layer)                                 # 全连接\n        attention_output = self.proj_dropout(attention_output)\n        return attention_output, weights\n\'> </span>'}]}]}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
