<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>BEV_zhoushi</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">推理代码</p>指标评测<br>\n单帧可视化<br>\n<span class=\'hidden-code\' data-code=\'if __name__ == ’__main__’:\n    device = torch.device(’cuda’ if torch.cuda.is_available() else ’cpu’)\n    device = ’cpu’\n    oriimg = False\n    use_refer8m2ego = False  是否使用特定摄像头（如 front_8m）作为 ego 坐标系参考来校准其他相机。设为 `False` 表示使用默认的 lidar-to-ego 外参矩阵\n    cam_num = 6\n    imgsuffix = ’.jpg’ if oriimg else ’.png’\n    前左 → 前中（8m）→ 前右 + 后左 → 后中 → 后右\n    cams = [’camera_left_front’,’camera_front_8m’,’camera_right_front’,’camera_left_back’,’camera_back’,’camera_right_back’]\n    filtergt = ’ZhouShi_8M_GT_22_50_RepairAssocMiss’\n    cam_str2num = {\n        ’camera_back’:’camera2’,\n        ’camera_front_8m’:’camera4’,      定义 6 个摄像头的名称顺序，后续读图、校准、特征提取都按此顺序处理\n        ’camera_right_back’:’camera3’,\n        ’camera_left_back’:’camera5’,\n        ’camera_left_front’:’camera6’,\n        ’camera_right_front’:’camera7’,\n    }\n    front_x_range = 32   前方 32 米\n    back_x_range = 48    后方 48 米\n    left_y_range = 16    左侧 16 米\n    right_y_range = 16   右侧 16 米\n    modelpath = ’/root/projects/model_zoo_j6e/network_release/jetour/adas/bev_j6e_d4q/model_20251031’\n    modelname = ’bev_merge_backbone_stage1’  divide_256 = False\n    headname = ’bev_head_6output_stage2’\n    divide_256 = False  把除256放到第一层卷积里面\n    merge_backbone = `backbone_init`(backbone=modelname)\n    head_net = `head_init`(backbone=headname)\n    每次更改的路径\n    quant_files = [’/root/projects/model_zoo_j6e/network_release/jetour/adas/bev_j6e_d4q/d4q_zhoushi_quant_images_20250605/d4q_quant_500_20250708_750.txt’,]\n    quant_data_root = ’/root/projects/model_zoo_j6e/network_release/jetour/adas/bev_j6e_d4q/d4q_zhoushi_quant_images_20250605’   d4q_zhoushi_quant_images_1025\n    `main`(quant_files,quant_data_root,modelpath)\n    imgname = ’D4Q_51_20250222_seq_19_003000’\n    test_data_root = ’./d4q_zhoushi_quant_images_consistency/d4q_zhoushi_ori_images’\n    test_frames_txt = f’{test_data_root}/test_frames.txt’\n    test_frames = `get_lines`([test_frames_txt])\n    calib_dict,table_dict = `get_all_calib_from_local`(test_frames,test_data_root,use_refer8m2ego=use_refer8m2ego)  用于查找对应相机的内参/外参\n    `test_visual_single`(merge_backbone, head_net, imgname, calib_dict, table_dict)\n    `test_visual`(merge_backbone, head_net, test_frames, calib_dict, table_dict, saveout=’./onnx_visuals’)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">backbone_init</p><span class=\'hidden-code\' data-code=\'def backbone_init(backbone=’merge_backbone’):\n    onnxpath = f’{modelpath}/{backbone}.onnx’\n    onnx_model = onnx.load(onnxpath)\n    try:\n        onnx.checker.check_model(onnx_model)\n    except Exception:\n        print(f’{backbone} onnx incorrect’)\n    else:\n        print(f’{backbone} onnx correct’)\n    providers = []\n    if torch.cuda.is_available():\n        #providers.insert(0, ’CUDAExecutionProvider’)\n        providers.insert(0, (’CUDAExecutionProvider’, {’device_id’: 0,})) # insert 0 将CUDA执行放在providers的最前面\n    else:\n        providers = [’CPUExecutionProvider’]\n    model = onnxruntime.InferenceSession(onnxpath, providers=providers) \n    return model\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">head_init</p><span class=\'hidden-code\' data-code=\'def head_init(backbone=’bev_grouphead_tda4’):\n    onnxpath = f’{modelpath}/{backbone}.onnx’\n    onnx_model = onnx.load(onnxpath)\n    try:\n        onnx.checker.check_model(onnx_model)\n    except Exception:\n        print(f’{backbone} onnx incorrect’)\n    else:\n        print(f’{backbone} onnx correct’)\n    providers = []\n    if torch.cuda.is_available():\n        #providers.insert(0, ’CUDAExecutionProvider’)\n        providers.insert(0, (’CUDAExecutionProvider’, {’device_id’: 0,})) # insert 0 将CUDA执行放在providers的最前面\n    else:\n        providers = [’CPUExecutionProvider’]\n    model = onnxruntime.InferenceSession(onnxpath, providers=providers) \n    return model\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">main</p>对 quant_files 中列出的所有帧进行批量推理 + mAP 评估<br>\n<span class=\'hidden-code\' data-code=\'def main(quant_files,quant_data_root,modelpath):\n    quant_lines = `get_lines`(quant_files)\n    calib_dict,table_dict = `get_all_calib_from_local`(quant_lines,quant_data_root,use_refer8m2ego=use_refer8m2ego)  # 为所有 frame_ids 加载相机内参（K）和外参（T_cam2ego）\n    # 测试指标\n    `test_map`(merge_backbone,head_net,quant_data_root,quant_lines, calib_dict, table_dict)\n    print(f’quant_data_root:{quant_data_root}’)\n    print(f’model_path:{modelpath}/{modelname}’)\n    print(f’model_path:{modelpath}/{headname}’)\n    print(f’filtergt:{filtergt}’)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_lines</p><span class=\'hidden-code\' data-code=\'def get_lines(trainfile):\n    lines = []\n    for txt_file in trainfile:\n        with open(f’{txt_file}’,’r’) as fr:\n            temp_lines = fr.readlines()\n        temp_lines.sort()\n        lines.extend(temp_lines)\n    lines.sort()\n    return lines\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_all_calib_from_local</p>为所有frame_ids加载相机内参(K)和外参(T_cam2ego)<br>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">test_map</p>对一批样本进行前向推理（inference）<br>\n将预测结果（res）和真值（gt）统一转换到 ego 坐标系<br>\n在指定 ROI（如前方 50m × 左右 20m）内筛选有效目标<br>\n按类别计算不同距离阈值下的 AP（Average Precision）<br>\n最终输出 mAP 和误差统计<br>\n<span class=\'hidden-code\' data-code=\'def test_map(merge_backbone,head_net, quant_data_root, quant_lines, calib_dict, table_dict):\n    # 如果是test_map时那么 lidar2ego就必须给出来,不能是zeros(4,4),否则真值无法转坐标系\n    # np.set_printoptions(suppress=True) # threshold=np.nan\n    # torch.set_printoptions(precision=9, sci_mode=False)\n    print(f’测试共label_file_list:{len(quant_lines)}’)\n    res_list,gt_list= [],[]\n    for sampleidx, line in tqdm(enumerate(quant_lines)):\n        # if sampleidx>=100:\n        #     break\n        local, car, date, seq, framenum, nas = line.strip().split(’ ’)\n        lidar2ego = calib_dict[f’{car} {date}’][’lidar2ego’]\n        bin_file = table_dict[f’{car} {date}’]\n        label_file = os.path.join(quant_data_root, filtergt, f’{car}_{date}_{seq}_{framenum}.txt’)\n        # imgpaths = [label_file.replace(filtergt,cam).replace(’.txt’,imgsuffix).replace(f’{framenum}’,f’{car}_{date}_{seq}_{framenum}’) for cam in cams]\n        imgpaths = [label_file.replace(filtergt,cam).replace(’.txt’,imgsuffix) for cam in cams]\n        #  (N, 9)，格式为 [x, y, z, l, w, h, yaw, score, cls_id]\n        res,proj_table = `get_caffe_output`(merge_backbone, head_net, imgpaths, bin_file) # nx9,最后两维度是得分与类别  #np.array\n        res = `vehicle_nms`(res)\n        # save2binfile(proj_table,savepath=’static_files’,savename=f’proj_table’)\n        # npyfile = labelfile.replace(’.txt’,’.npy’).replace(f’{data_root}/{filtergt}’,f’./visual_zhengwei/visual’)\n        # res = np.load(npyfile)\n        assert res.ndim==2\n        sample_res = np.array([sampleidx]*len(res))\n        gt = `read_gt_from_txt`(label_file) # nx9,置信度置为1,最后一维是类别 #np.array \n        gt = `gtbox2ego`(gt,lidar2ego)      # 读取真值并转换到 ego 坐标系\n        assert gt.ndim==2\n        sample_gt = np.array([sampleidx]*len(gt))\n        # 将sampleid与结果拼接     最终格式：[x, y, z, l, w, h, yaw, score, cls_id, sample_id]\n        res = np.concatenate([res,sample_res[:,None]],axis=1)\n        gt = np.concatenate([gt,sample_gt[:,None]],axis=1)\n        res_list.append(res) # nx10,前7维度为 中心点长宽高 yaw,score,cls,sampleid\n        gt_list.append(gt)\n    print(f’res_list: {len(res_list)}’)\n    res_array = np.concatenate(res_list,axis=0)   # 仅评估 车辆前方感兴趣区域 内的目标（避免远处低质量检测干扰 mAP）\n    condition = np.logical_and(\n            (res_array[:, 0] `<`= front_x_range) & (res_array[:, 0] `>`= -back_x_range),\n            (res_array[:, 1] `<`= left_y_range) & (res_array[:, 1] `>`= -right_y_range))\n    res_array = res_array[condition]\n    print(’res_array shape’,res_array.shape)\n    gt_array = np.concatenate(gt_list,axis=0)\n    condition = np.logical_and(\n            (gt_array[:, 0] `<`= front_x_range) & (gt_array[:, 0] `>`= -back_x_range),\n            (gt_array[:, 1] `<`= left_y_range) & (gt_array[:, 1] `>`= -right_y_range))\n    gt_array = gt_array[condition]\n    print(’gt_array shape’,gt_array.shape)\n    cls_thresh_ap = {}\n    map_dict = {}\n    cls_err_dict = {}\n    gt_num_dict = {}\n    for cls in CLASSES:\n        clsid = CLASSES.index(cls)\n        ap_list = []\n        res_cls_array = res_array[res_array[:,-2]==clsid]\n        gt_cls_array = gt_array[gt_array[:,-2]==clsid]\n        # 第一次需要统计每个类别的GT数量。\n        gt_cls_num = len(gt_cls_array)\n        gt_num_dict[cls] = gt_cls_num\n        for dist_ths in [0.5, 1.0, 2.0, 4.0]:\n            ap,err_tmp = `compute_ap`(res_cls_array.copy(),gt_cls_array.copy(),dist_ths)\n            ap_list.append(ap)\n            if dist_ths==2.0:\n                err_dict = err_tmp\n        #print(f’{cls}:{ap_list}’)\n        cls_thresh_ap[cls] = ap_list\n        map = np.mean(np.array(ap_list))\n        map_dict[cls] = map\n        cls_err_dict[cls] = err_dict # type: ignore\n    print_format(gt_num_dict,cls_thresh_ap,map_dict,cls_err_dict)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_caffe_output</p>res: (N, 9)，格式为 [x, y, z, l, w, h, yaw, score, cls_id]<br>\nproj_table: 可能是用于 debug 或可视化（未使用）<br>\n<span class=\'hidden-code\' data-code=\'def get_caffe_output(merge_backbone,head_net,imgpaths,bin_file):\n    #np.set_printoptions(suppress=True) # threshold=np.nan\n    #torch.set_printoptions(precision=9, sci_mode=False)\n    if oriimg:\n        #imgs_tensor = preprocess_yuvimg(imgpaths)\n        #imgs_tensor = preprocess_nv12(imgpaths)\n        imgs_tensor = `preprocess_oriimg_d01`(imgpaths)\n    else:\n        imgs_tensor = `preprocess_cvdeploy`(imgpaths)\n        #imgs_tensor = preprocess_oriimg(imgpaths)\n        #imgs_tensor = preprocess_nv12(imgpaths)\n    imgs_tensor = imgs_tensor.to(device)\n    bevfea,proj_table = `get_bevfea_single`(merge_backbone,imgs_tensor,bin_file)  # 多相机图像 → BEV 特征的主干网络（如 LSS、BEVFusion 的 lift-splat-shoot 模块）\n    # BEV 特征张量，shape 通常为 (1, C_bev, H_bev, W_bev)，例如 (1, 80, 200, 200)\n    input = head_net.get_inputs()[0].name\n    # =============================单输出=============================\n    # output_name = [head_net.get_outputs()[0].name]\n    # bevout = head_net.run(output_name, {input:bevfea.cpu().numpy()}) # output_name必须是list\n    # bevout = torch.from_numpy(bevout[0])\n    # 顺序依次为 [’reg’,’height’,’dim’,’rot’,’heatmap’]\n    # every_section = len(CLASSES)*3 # 21\n    # bevout = torch.split(bevout,split_size_or_sections=every_section,dim=1) # 要分成5份,5种属性\n    # res = bevout_decode(bevout) # nx9\n    # =============================单输出=============================\n    # =============================6输出==============================\n    output_name = [head_net.get_outputs()[i].name for i in range(6)] # 多输出版本\n    reg, height,dim,rot,anglecls,heatmap= head_net.run(output_name, {input:bevfea.cpu().numpy()}) # output_name必须是list\n    bevout = [reg, height,dim,rot,anglecls,heatmap]\n    bevout = [torch.from_numpy(bo) for bo in bevout]\n    res = `bevout_decode`(bevout) # nx9\n    # =============================6输出==============================\n    return res,proj_table\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">preprocess_oriimg_d01</p><span class=\'hidden-code\' data-code=\'def preprocess_oriimg_d01(imgpaths):\n    ’’’ Params:\n            imgpaths 存放顺序: [’camera_left_front’,’camera_front_8m’,’camera_right_front’,\\\n                            ’camera_left_back’,’camera_back’,’camera_right_back’,’camera_back_roi’]\n        Return:\n            img_input_tensor: [7,3,320,480]\n    ’’’\n    input_tensors = []\n    for camid,imgpath in enumerate(imgpaths): # cvimg -> H,W,3\n        #print(imgpath)\n        img = cv2.imread(imgpath)\n        if cams[camid]==’camera_front_8m’:\n            pyramid_img = cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_LINEAR) # 1920,1080\n            pyramid_img = cv2.resize(pyramid_img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_LINEAR) # 960,540\n            pyramid_roi = dict(pt1=(0,0),pt2=(960, 450))\n            pt1 = pyramid_roi[’pt1’]  # 左上角顶点坐标\n            pt2 = pyramid_roi[’pt2’]  # 右下角顶点坐标\n            img = pyramid_img[pt1[1]:pt2[1],pt1[0]:pt2[0]] # 960,450\n            img = cv2.resize(img,resize_dim,interpolation=cv2.INTER_LINEAR) # 480,320\n        else:\n            pyramid_img = cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_LINEAR) # 960,640\n            img = cv2.resize(pyramid_img,resize_dim,interpolation=cv2.INTER_LINEAR)         # 480,320\n        img = rgb2yuv444_bt601_full_range(img,resize_dim[0],resize_dim[1],to_rgb=True)\n        img = img.astype(np.float32) # bgr mode\n        if divide_256:\n            img = img / 256.0\n        img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n        input_tensors.append(img_tensor) # 3,H,W\n    return torch.stack(input_tensors) # 7,3,H,W\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">preprocess_cvdeploy</p><span class=\'hidden-code\' data-code=\'def preprocess_cvdeploy(imgpaths):\n    ’’’ Params:\n        imgpaths 存放顺序: [’camera_left_front’,’camera_front_8m’,’camera_right_front’,\\\n                            ’camera_left_back’,’camera_back’,’camera_right_back’,’camera_back_roi’]\n        Return:\n            img_input_tensor: [7,3,320,480]\n    ’’’\n    input_tensors = []\n    for camid,imgpath in enumerate(imgpaths): # cvimg -> H,W,3\n        img = cv2.imread(imgpath)\n        img = rgb2yuv444_bt601_full_range(img,resize_dim[0],resize_dim[1],to_rgb=True)\n        img = img.astype(np.float32) # bgr mode\n        if divide_256:\n            img = img / 256.0\n        img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n        input_tensors.append(img_tensor) # 3,H,W\n    return torch.stack(input_tensors) # 7,3,H,W\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_bevfea_single</p><span class=\'hidden-code\' data-code=\'def get_bevfea_single(merge_backbone,imgs_tensor,bin_file):\n    ’’’params:\n            merge_backbone     group融合的网络\n            imgs_tensor:       网络的输入 7,3,320,480\n            bin_file:          离线生成好的表文件\n    ’’’\n    x1 = imgs_tensor[0:1,...].cpu().numpy()\n    x2 = imgs_tensor[1:2,...].cpu().numpy()\n    x3 = imgs_tensor[2:3,...].cpu().numpy()\n    x4 = imgs_tensor[3:4,...].cpu().numpy()\n    x5 = imgs_tensor[4:5,...].cpu().numpy()\n    x6 = imgs_tensor[5:6,...].cpu().numpy()\n    # x7 = imgs_tensor[6:7,...].cpu().numpy()\n    input_1 = merge_backbone.get_inputs()[0].name\n    input_2 = merge_backbone.get_inputs()[1].name\n    input_3 = merge_backbone.get_inputs()[2].name\n    input_4 = merge_backbone.get_inputs()[3].name\n    input_5 = merge_backbone.get_inputs()[4].name\n    input_6 = merge_backbone.get_inputs()[5].name\n    # input_7 = merge_backbone.get_inputs()[6].name\n    # merge_backbone_group 输出版本\n    output_name = [merge_backbone.get_outputs()[i].name for i in range(2)] # 2输出版本\n    imgfea,depth = merge_backbone.run(output_name, {input_1:x1,input_2:x2,input_3:x3,\n                                                    input_4:x4,input_5:x5,input_6:x6})  # input_7:x7   \n    imgfea = imgfea.reshape(1,cam_num,64,20,30).squeeze(0)\n    depth = depth.reshape(1,cam_num,depth_num,20,30).squeeze(0)\n    # merge_backbone_group 输出版本\n    # merge_backbone_parallel输出版本\n    # output_name=[merge_backbone.get_outputs()[i].name for i in range(14)] # 多输出版本\n    # backfe,backdepth,frontfe,frontdepth,frfe,frdepth,lfefe,lfedepth,rfefe,rfedepth,bfefe,bfedepth,roife,roidepth=merge_backbone.run(\n    #     output_name, {input_1:x1,input_2:x2,input_3:x3,input_4:x4,input_5:x5,input_6:x6,input_7:x7})\n    # imgfea = np.concatenate([backfe,frontfe,frfe,lfefe,rfefe,bfefe,roife],axis=0) # -> 7,64,20,30\n    # depth = np.concatenate([backdepth,frontdepth,frdepth,lfedepth,rfedepth,bfedepth,roidepth],axis=0) # -> 7,160,20,30 \n    # merge_backbone_parallel输出版本\n    imgfea = torch.from_numpy(imgfea).to(device)\n    depth = torch.from_numpy(depth).to(device)\n    bevfea,proj_table = get_fecam_bev(imgfea[:,...],depth[:,...],bin_file,)\n    return bevfea.contiguous(),proj_table\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_fecam_bev</p>从多视角图像特征 + 深度估计 → 3D 空间点云式特征 → BEV 特征图的完整流程。这是典型的 LSS（Lift-Splat-Shoot）架构 的“Lift + Splat”部分<br>\n输入：<br>\n每个相机视角的图像特征（通道=64，H=20, W=30）;<br>\n每个像素位置的深度分布 logits（96 个深度 bin）;<br>\n包含预计算的 normcor（归一化射线方向）、femask（有效区域掩码）、cam2ego_mats（外参）<br>\n<span class=\'hidden-code\' data-code=\'def get_fecam_bev(imgfea,depth,bin_file):  \n    normcor_array = bin_file[’normcor’].reshape(cam_num,20,30,2)   # bin文件维持7V版本，但是需要删减一个维度用来测试\n    # normcor_array = np.fromfile(savepath,dtype=np.float32).reshape(cam_num+1,20,30,2)   # bin文件维持7V版本，但是需要删减一个维度用来测试\n    # normcor_array = np.delete(normcor_array,6,axis=0)\n    normcor = normcor_array.float().to(device) # 6,H,W,2     每个特征图像素对应的 归一化相机射线方向（在相机坐标系中），即 (x/z, y/z)\n    femask_array = np.array(bin_file[’femask’]).reshape(cam_num,20,30).astype(bool) # 标记哪些像素是有效的（避免无效区域如遮挡、畸变边缘参与投影）\n    # femask_array = bin_file[’femask’].reshape(cam_num+1,20,30).astype(bool)\n    # femask_array = np.delete(femask_array,6,axis=0)\n    femask = torch.from_numpy(femask_array).to(device)  # 6,H,W\n    # 对应上tanh和softmax:\n    # depth = F.tanh(depth) * 4\n    # depth = SplitChannelSoftmax(depth)\n    # 查表实现:\n    depth = `table_softmax_split_channel`(depth)  # 输入6*96*20*30，字面意思，对深度值做softmax，原始shape输出\n    #depth_probs,topk_depth = numpy_topk(depth,topk) # 返回两个均是 5,topk,18,32 topk=5\n    depth_probs,topk_depth = `numpy_topk_split_channel`(depth,topk) # (6, 5, 20, 30) —— top-k 深度的概率 + (6, 5, 20, 30) —— 对应的真实深度值\n    depth_interval = fed_bound[2] # 0.5\n    depth_begin = fed_bound[0] + depth_interval/2 # 2+0.25    假设深度 bin 是等间隔 [2.0, 2.5, 3.0, ..., 49.5]，共 96 个\n    topk_depth = topk_depth*depth_interval+depth_begin    # 获取真实对应的深度 # f32\n    num_cams, D, _ ,_ = topk_depth.shape\n    # 通过查表获取投影的真3D坐标BEGIN-------------------------------------------------------\n    num_cams,fh,fw,cordim = normcor.shape # 5,H,W,2    5,5,H,W\n    normx = normcor[:,:,:,0].unsqueeze(1) # 5,1,H,W\n    normy = normcor[:,:,:,1].unsqueeze(1) # 5,1,H,W\n    x = normx * topk_depth # 5,1,H,W * 5,5,H,W -> 5,5,H,W\n    y = normy * topk_depth\n    padding = torch.ones_like(topk_depth)\n    points = torch.stack([x,y,topk_depth,padding],dim=-1) # 5,D,H,W,4\n    points = points.unsqueeze(-1).float() # 5,D,H,W,4,1\n    # Lidar坐标系写法\n    # lidar2cam_mats = lidar2cam_mats.view(num_cams,1,1,1,4,4)\n    # points = lidar2cam_mats.inverse().matmul(points) # 5,D,H,W,4,1\n    # Lidar坐标系写法\n    # Ego坐标系写法\n    # lidar2ego = lidar2ego.view(1,4,4) # 4,4 -> 1,4,4\n    # fecams2lidar = lidar2cam_mats.inverse() # 6,4,4\n    # fecams2ego = lidar2ego.matmul(fecams2lidar) # 6,4,4\n    # fecams2ego = fecams2ego.view(num_cams,1,1,1,4,4)\n    # points = fecams2ego.matmul(points) # 6,D,H,W,4,1\n    # Ego坐标系写法\n    # Bin文件覆盖写法  这一步是 “Splat” 的基础：知道每个特征应该 splat 到 BEV 的哪个位置\n    tablebin = bin_file[’cam2ego_mats’]\n    proj_table = tablebin.reshape(cam_num,4,4) # cam2ego矩阵\n    fecams2ego = proj_table.to(device)\n    fecams2ego = fecams2ego.view(num_cams,1,1,1,4,4)\n    points = fecams2ego.matmul(points) # 6,D,H,W,4,1   将所有 3D 点统一转换到 ego 坐标系\n    # Bin文件覆盖写法 \n    geom_xyz = points.squeeze(-1)[..., :3].contiguous() # 5,D,H,W,3\n    # 获取投影的真3D坐标END-------------------------------------------------------\n    img_feat_with_depth = depth_probs.unsqueeze(1) * imgfea.unsqueeze(2)    # 将图像特征按深度概率加权\n    # bs*5,1,D,18,32 * b*5,64,1,18,32 = b*5,64,5,18,32\n    img_feat_with_depth = img_feat_with_depth.permute(0, 2, 3, 4, 1).contiguous().float() # -> 5,D,18,32,64  每个 3D 点不仅有位置 (x,y,z)，还有对应的特征向量（64 维）\n    #save2binfile(img_feat_with_depth,’feadepth_5_10_32_88_80’)\n    global voxel_coord,voxel_size\n    voxel_size_factor = torch.tensor(voxel_size).to(device)\n    voxel_coord = torch.tensor(voxel_coord).to(device)\n    voxel_num = torch.tensor([bev_w, bev_h, 1]).to(device)\n    geom_xyz = ((geom_xyz - (voxel_coord - voxel_size_factor / 2.0)) / voxel_size_factor).int()\n    # geom_xyz减去 边缘:[-51.2,-51.2,-5] 除以单位 [0.8,0.8,8] -> # 5,D,H,W,3\n    geom_xyz = geom_xyz.unsqueeze(0) # 5,D,H,W,3 -> 1 5,D,H,W,3\n    img_feat_with_depth = img_feat_with_depth.unsqueeze(0) # 5,D,H,W,64 -> 1 5,D,H,W,64\n    # print(geom_xyz.shape)\n    # print(img_feat_with_depth.shape)\n    femask = femask.view(1,num_cams,1,fh,fw).expand(1,num_cams,topk,fh,fw)\n    bevfea = `bev_pooling`(geom_xyz,img_feat_with_depth,voxel_num,femask)\n    return bevfea.contiguous(),proj_table\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">table_softmax_split_channel</p>输入 depth：每个像素位置的深度 logits（未归一化），shape (cam_num=6, depth_num=96, H=20, W=30)<br>\n输出：归一化后的深度“概率”，但仅在每个相机的有效深度范围内归一化<br>\n<span class=\'hidden-code\' data-code=\'def table_softmax_split_channel(depth): # 6,96,20,30\n    ’’’为什么要split_channel,相当于每v按照不同的深度距离来做’’’\n    device = depth.device\n    depth = read_table(depth)\n    depth = depth.to(device)\n    fh,fw = depth.shape[2],depth.shape[3]\n    batch_depth = depth.reshape(-1,cam_num,depth_num,fh,fw) # batch_depth.shape = (1, 6, 96, 20, 30)\n    softmax_sum_lst = []\n    for i in range(cam_num):\n        split_channel = int(max_depth_per_v[i] / 0.5)      # max_depth_per_v = [d1, d2, ..., d6]  # 每个摄像头允许的最大物理深度（单位：米）\n        softmax_sum = batch_depth[:,i:i+1,:split_channel,:,:].sum(dim=2,keepdim=True) # b,1,1,h,w 对第 i 个摄像头的前 split_channel 个深度通道求和 → 得到 (B,1,1,H,W)  这个 sum 相当于该像素在有效深度范围内的总响应\n        softmax_sum_lst.append(softmax_sum)\n    softmax_sum = torch.cat(softmax_sum_lst,dim=1).flatten(end_dim=1) # b*6,1,h,w\n    depth = depth/softmax_sum  # b*6,1,h,w  将原始 depth 除以这个局部 sum → 实现 per-camera、per-pixel 的局部 softmax 归一化\n    return depth       # 对每个摄像头独立做深度归一化，仅在其有效深度范围内进行 softmax-like 归一化，提升深度分布合理性。\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">numpy_topk_split_channel</p>这是 LSS（Lift-Splat-Shoot）类 BEV 感知模型中“Lift”阶段的核心：<br>\n将 2D 图像特征提升到 3D 空间时，只考虑最可能的 K 个深度假设，以平衡精度与计算开销<br>\n通道数 96 = 深度 bin 数（对应深度范围 [2.0, 50.0)，间隔 0.5m）<br>\ntopk_probs: (B*6, topk, 20, 30) —— top-k 深度对应的“概率”值<br>\ntopk_depth: (B*6, topk, 20, 30) —— top-k 深度对应的 bin 索引（非真实深度值）<br>\n<span class=\'hidden-code\' data-code=\'def numpy_topk_split_channel(depth_fea,topk):\n    ’’’为什么要split_channel,相当于每v按照不同的深度距离来做’’’\n    device = depth_fea.device        # depth_fea: 已经过某种归一化的深度概率图，shape (B*6, 96, 20, 30) 或类似;表示每个像素保留 top-k 个最可能的深度?\n    fh,fw = depth_fea.shape[2],depth_fea.shape[3]\n    depth_fea = depth_fea.reshape(-1,cam_num,depth_num,fh,fw)\n    depth_array = depth_fea.detach().cpu().numpy() # b,cams,c,fh,fw\n    topk_probs_lst = []\n    topk_depth_lst = []\n    for i in range(cam_num):\n        split_channel = int(max_depth_per_v[i] / 0.5)\n        perv_depth_array = depth_array[:,i,:split_channel,:,:]\n        depth_sort_index = np.argsort(-perv_depth_array,axis=1,kind=’stable’) # 稳定的降序排序\n        depth_sort_value = np.take_along_axis(perv_depth_array,depth_sort_index,axis=1)\n        topk_probs = depth_sort_value[:,:topk,:,:]\n        topk_probs = torch.from_numpy(topk_probs).to(device)\n        topk_probs_lst.append(topk_probs) # b,c,h,w\n        topk_depth = depth_sort_index[:,:topk,:,:]\n        topk_depth = torch.from_numpy(topk_depth).to(device)\n        topk_depth_lst.append(topk_depth)\n    topk_probs = torch.stack(topk_probs_lst,dim=1).flatten(end_dim=1)\n    topk_depth = torch.stack(topk_depth_lst,dim=1).flatten(end_dim=1)\n    return topk_probs,topk_depth\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bev_pooling</p><span class=\'hidden-code\' data-code=\'def bev_pooling(geom_feats,x,voxel_num,mask=None):  # voxel_num = [X_voxels, Y_voxels, Z_voxels]：BEV 网格大小，注意顺序是 [X, Y, Z] \n    # geom_feats:[Bs, 6, D, 18, 32, 3]                定义函数 bev_pooling，将图像特征根据几何映射投影到 BEV 空间并进行体素池化。\n    # x: Bs,6,D,H,W,C 这里的x输入已经是并入网络深度特征后的特征。\n    B, N, D, H, W, C = x.shape               # 解包输入特征 x 的维度：B=batch, N=相机数, D=深度离散化层数, H/W=特征图尺寸, C=通道数。\n    Nprime = B * N * D * H * W # 图像上的Z,Y,X排序  计算总点数 Nprime，即所有图像位置展开后的总数\n    x = x.reshape(Nprime, C)     # 将几何坐标也展平为 (Nprime, 3)，每行是 [x, y, z] 坐标（注意：此处 x/y/z 对应 BEV 网格索引，非物理坐标）。\n    geom_feats = geom_feats.view(Nprime, 3)  # 为每个点添加所属 batch ID，生成 (Nprime, 1) 的 batch 索引张量。\n    batch_ix = torch.cat([torch.full([Nprime // B, 1], ix,device=x.device, dtype=torch.long) for ix in range(B)])\n    # batch_ix: [Nprime,batch_idx]\n    geom_feats = torch.cat((geom_feats, batch_ix), 1)  # 将 batch ID 拼接到几何坐标后，得到 (Nprime, 4) 张量：[x, y, z, batch_id]。\n    # 增加mask计算     # 注释：支持可选掩码，用于过滤无效点（如深度无效区域）。\n    if mask is not None:\n        mask = mask.reshape(Nprime)\n        mask = mask.to(geom_feats.device)\n        geom_feats = geom_feats[mask]\n        x = x[mask]\n    # filter out points that are outside box # voxel_num # voxel_num: [128.,128.,1.]\n    kept = (geom_feats[:, 0] `>`= 0) & (geom_feats[:, 0] `<` voxel_num[0]) \\\n            & (geom_feats[:, 1] `>`= 0) & (geom_feats[:, 1] `<` voxel_num[1]) \\\n            & (geom_feats[:, 2] `>`= 0) & (geom_feats[:, 2] `<` voxel_num[2])\n    x = x[kept]\n    geom_feats = geom_feats[kept]    # 构造布尔掩码 kept，保留 x∈[0,X), y∈[0,Y), z∈[0,Z) 的点。\n    # get tensors from the same voxel next to each other\n    ranks = geom_feats[:, 0] * (voxel_num[1] * voxel_num[2] * B) \\\n            + geom_feats[:, 1] * (voxel_num[2] * B) \\\n            + geom_feats[:, 2] * B \\\n            + geom_feats[:, 3]\n    sorts = ranks.argsort()   # 为每个点计算唯一 rank 值（类似线性地址），保证相同体素+同 batch 的点 rank 相近；注意顺序：x → y → z → batch。\n    # 按 B,Z,Y,X排序 确实如此     注释：实际排序顺序由 rank 公式决定，此处为 [x, y, z, batch]，但因 rank 权重不同，等效于按 batch、z、y、x 排。\n    x, geom_feats, ranks = x[sorts], geom_feats[sorts], ranks[sorts] # [num_points,C]   [num_points,4],  [num_points,]\n    x = x.cumsum(0)\n    kept = torch.ones(x.shape[0], device=x.device, dtype=torch.bool)\n    kept[:-1] = (ranks[1:] != ranks[:-1])  # 保留落在同一个格子最后一个位置。\n    x, geom_feats = x[kept], geom_feats[kept]\n    x = torch.cat((x[:1], x[1:] - x[:-1])) # 累积的结果差就是落在格子里的特征。通过差分还原每个体素内所有点的特征和（即池化结果）\n    #x, geom_feats = QuickCumsum.apply(x, geom_feats, ranks) # np,C  np,3\n    # griddify (B x C x Z x X x Y)\n    final = torch.zeros((B, C, voxel_num[2], voxel_num[1], voxel_num[0]), device=x.device)\n    final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 1], geom_feats[:, 0]] = x\n    # collapse Z\n    final = torch.cat(final.unbind(dim=2), 1) # [B,C*Z,Y,X] [B,80,128,128]  沿 Z 维拆分并拼接到通道维，输出 shape = (B, C×Z, Y, X)；若 Z=1，则为 (B, C, Y, X)。\n    return final\n\'> </span>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevout_decode</p><span class=\'hidden-code\' data-code=\'# 这里注意,如果要用来算MAP的话,这里score_threshold=0.1\ndef bevout_decode(bevout):\n    boxes3d_list = []\n    scores_list = []\n    labels_list = []\n    for idx,cls in enumerate(CLASSES):\n        boxes3d,scores,labels = `decode_bevout_single`(bevout,idx,cls,score_threshold= score_threshold)\n        boxes3d_list.append(boxes3d)\n        scores_list.append(scores)\n        labels_list.append(labels)\n    boxes3d = torch.cat(boxes3d_list,dim=0).detach().cpu().numpy() # nx7\n    scores = torch.cat(scores_list,dim=0).detach().cpu().numpy()   # n\n    labels = torch.cat(labels_list,dim=0).detach().cpu().numpy()   # n\n    #return boxes3d,scores,labels\n    res = np.concatenate([boxes3d,scores[:,None],labels[:,None]],axis=1)\n    return res\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">decode_bevout_single</p><span class=\'hidden-code\' data-code=\'def decode_bevout_single(bevout,outid,cls=’vehicle’,score_threshold = score_threshold):\n    # [’reg’,’height’,’dim’,’rot’,’heatmap’]\n    max_num = 500\n    global voxel_size\n    voxel_size_x = voxel_size[0]\n    voxel_size_y = voxel_size[1]\n    clsindex = CLASSES.index(cls)\n    reg = bevout[0][:,outid*2:(outid+1)*2,:,:]\n    hei = bevout[1][:,outid:(outid+1),:,:]\n    dim = bevout[2][:,outid*3:(outid+1)*3,:,:]\n    anglereg = bevout[3][:,outid*4:(outid+1)*4,:,:]\n    anglecls = bevout[4][:,outid*4:(outid+1)*4,:,:]\n    heat = bevout[5][:,outid:(outid+1),:,:].sigmoid()\n    batch, cat, _, _ = heat.size()\n    scores, inds, _, ys, xs = _topk(heat, K=max_num)\n    reg = _transpose_and_gather_feat(reg, inds)\n    reg = reg.view(batch, max_num, 2)\n    xs = xs.view(batch, max_num, 1) + reg[:, :, 0:1]\n    ys = ys.view(batch, max_num, 1) + reg[:, :, 1:2]\n    # rotation value and direction label\n    anglereg = _transpose_and_gather_feat(anglereg, inds)\n    anglereg = anglereg.view(batch, max_num, 4)\n    anglecls = _transpose_and_gather_feat(anglecls, inds)\n    anglecls = anglecls.view(batch, max_num, 4)\n    anglecls = anglecls.softmax(dim=-1)\n    yawcls_scores,yawcls_labels = torch.max(anglecls,dim=-1) # B,500 | B,500\n    index_expanded = yawcls_labels.unsqueeze(-1) # B,500,1\n    rot = torch.gather(anglereg,dim=2,index=index_expanded) # B,500,4 | B,500,1 -> B,500,1\n    rot = -rot.sigmoid()\n    rot = torch.asin(rot)\n    radian_map = torch.tensor([\n        torch.pi/2,  # 0 → π/2\n        torch.pi,    # 1 → π\n        -torch.pi/2, # 2 → -π/2\n        0.0          # 3 → 0\n    ], device=yawcls_labels.device)\n    refer_rot = radian_map[yawcls_labels] # B,500\n    rot = rot + refer_rot.unsqueeze(-1) # B,500,1 + B,500,1 -> B,500,1\n    # height in the bev\n    hei = _transpose_and_gather_feat(hei, inds)\n    hei = hei.view(batch, max_num, 1)\n    # dim of the box\n    dim = _transpose_and_gather_feat(dim, inds)\n    dim = dim.view(batch, max_num, 3)\n    dim = torch.exp(dim)\n    # class label\n    #clses = clses.view(batch, max_num).float()\n    clses = torch.tensor([clsindex]*max_num).view(batch, max_num).float()\n    scores = scores.view(batch, max_num)\n    xs = xs.view(batch,max_num,1) * voxel_size_x + pc_range[0]\n    ys = ys.view(batch,max_num,1) * voxel_size_y + pc_range[1]\n    final_box_preds = torch.cat([xs, ys, hei, dim, rot], dim=2)\n    final_scores = scores\n    final_preds = clses\n    # use score threshold\n    thresh_mask = final_scores >= score_threshold\n    global post_center_range\n    center_range = torch.tensor(post_center_range, device=heat.device)\n    mask = (final_box_preds[..., :3] >=center_range[:3]).all(2)\n    mask &= (final_box_preds[..., :3] <=center_range[3:]).all(2)\n    assert batch==1\n    cmask = mask[0, :]\n    cmask &= thresh_mask[0]\n    #predictions_dicts = []\n    boxes3d = final_box_preds[0, cmask]\n    scores = final_scores[0, cmask]\n    labels = final_preds[0, cmask]\n    # 使用中心点做nms\n    centers = boxes3d[:, [0, 1]]\n    boxes = torch.cat([centers, scores.view(-1, 1)], dim=1)\n    keep = torch.tensor(circle_nms(boxes.detach().cpu().numpy(),nms_radius[clsindex],post_max_size=83),\n                        dtype=torch.long,device=boxes.device)\n    boxes3d = boxes3d[keep] # (N,7)\n    scores = scores[keep]\n    labels = labels[keep]\n    return boxes3d,scores,labels\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">vehicle_nms</p><span class=\'hidden-code\' data-code=\'total_chongdie = 0\n# 增加车辆的类间(小轿车、大巴和其他大车)的NMS处理\ndef vehicle_nms(decoderes):\n    before_len = len(decoderes)\n    vehicles = decoderes[np.isin(decoderes[:,-1],[0.0,2.0,4.0])]  # 根据目标类别（0.0, 2.0, 4.0）筛选出属于车辆类别的检测结果，并将它们与非车辆类别区分开来\n    non_vehicles = decoderes[~np.isin(decoderes[:,-1],[0.0,2.0,4.0])] # (n2,9) \n    vehicle_centers = vehicles[:,[0,1]]\n    vehicle_scores = vehicles[:,-2:-1]\n    boxes = np.concatenate([vehicle_centers, vehicle_scores], axis=1) # nx3\n    keep = `circle_nms`(boxes,nms_radius[0],post_max_size=83)\n    vehicles = vehicles[keep] # (n1,9)\n    # print(’vehicles shape’,vehicles.shape)\n    # print(’non_vehicles shape’,non_vehicles.shape)\n    finalres = np.concatenate([vehicles,non_vehicles],axis=0)\n    after_len = len(finalres)\n    try:\n        assert before_len==after_len\n    except:\n        diff_len = before_len - after_len\n        global total_chongdie\n        total_chongdie += diff_len\n        print(f’车辆类间NMS去掉了{before_len}-{after_len}={diff_len}’)\n    return finalres\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">circle_nms</p><span class=\'hidden-code\' data-code=\'def circle_nms(dets, thresh, post_max_size=83):     # 定义圆形 NMS 函数：对以 (x, y) 为中心、用中心距离代替 IoU 的检测框进行非极大值抑制。\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    scores = dets[:, 2]\n    order = scores.argsort()[::-1].astype(np.int32)  # highest->lowest\n    ndets = dets.shape[0]\n    suppressed = np.zeros((ndets), dtype=np.int32)\n    keep = []\n    for _i in range(ndets):\n        i = order[_i]  # start with highest score box\n        if suppressed[i] == 1:  # if any box have enough iou with this, remove it\n            continue\n        keep.append(i)\n        for _j in range(_i + 1, ndets):\n            j = order[_j]\n            if suppressed[j] == 1:\n                continue\n            # calculate center distance between i and j box\n            dist = (x1[i] - x1[j])**2 + (y1[i] - y1[j])**2\n            # ovr = inter / areas[j]\n            if dist <= thresh:\n                suppressed[j] = 1\n    return keep[:post_max_size]\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">read_gt_from_txt</p>读取真值并转换到 ego 坐标系<br>\n<span class=\'hidden-code\' data-code=\'# 读取过滤后的GT\ndef read_gt_from_txt(labelfile):\n    gt_bboxes = []\n    #map_dict = {’小车’:0,’行人’:1,’大巴’:2,’骑行人’:3,’其他大车’:4,’二轮车’:5,’障碍物’:6}\n    with open(labelfile,’r’,encoding=’utf8’) as fp:\n        lines = fp.readlines()\n        for line in lines:\n            line_list = line.strip().split(’,’)\n            if len(line_list)==9 and line_list[8]==’invalid’:\n                continue\n            label = line_list[7]\n            line_list = [float(num) for num in line_list[:7]] # 取前7个数字\n            x,y,z,l,w,h,yaw = line_list\n            box = [x,y,z,l,w,h,yaw,1,int(classes_dict[label])]\n            #print(f’{label_name}:{box}’)\n            gt_bboxes.append(box)\n    if len(gt_bboxes)<=0:\n        gt_bboxes = np.zeros((0,9))\n    gt_bboxes = np.array(gt_bboxes)\n    return gt_bboxes # nx9\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">gtbox2ego</p><span class=\'hidden-code\' data-code=\'def gtbox2ego(gt_boxes,lidar2ego):\n    ’’’按照自己的理解进行求解’’’\n    new_gtboxes = []\n    for gt_box in gt_boxes:\n        x,y,z,l,w,h,yaw,_,cls = gt_box\n        # 求新中心点\n        egocor = lidar2ego @ (np.array([x,y,z,1]).reshape(4,1))\n        egocor = (egocor.squeeze(-1)[:3]).tolist()\n        # GPT教的求yaw,----------------------BEGIN------------------------------\n        newyaw = yaw + np.arctan2(lidar2ego[1, 0], lidar2ego[0, 0])\n        if newyaw>(np.pi):\n            newyaw = (newyaw-2*np.pi)\n        if newyaw<=(-np.pi):\n            newyaw = newyaw + 2*np.pi # (-180,180] 180是闭区间\n        #print(’newyaw’,newyaw,f’{newyaw*180/np.pi}度’) # newyaw\n        # GPT教的求yaw,----------------------END------------------------------\n        ego_box = np.array(egocor+[l,w,h,newyaw,1,cls]) # [n,9]\n        new_gtboxes.append(ego_box)\n    if len(new_gtboxes)==0:\n        new_gtboxes = np.zeros((0,9))\n    else:\n        new_gtboxes = np.array(new_gtboxes)\n    return new_gtboxes\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">compute_ap</p><span class=\'hidden-code\' data-code=\'def compute_ap(res_cls_array,gt_cls_array,dist_ths): # 每个框格式为 [x, y, z, l, w, h, yaw, score, cls_id, sample_id]\n                                                     # 定义函数 compute_ap，用于计算某类别的平均精度（AP）及定位、尺度、方向误差，输入为预测框、真值框和中心距离阈值。\n    if len(gt_cls_array)==0:     # 若该类别无任何真值框（即没有 ground truth）\n        err_dict = {’centerdis_err’: 0,’scale_err’: 0,’orient_err’: 0}\n        return 1.0,err_dict\n    if len(res_cls_array)==0:     # 若该类别无任何预测框。\n        err_dict = {’centerdis_err’: 0,’scale_err’: 0,’orient_err’: 0}\n        return 0.0,err_dict\n    def center_distance(res_center,gt_center):   # 返回 L2 范数（即 √[(x₁−x₂)² + (y₁−y₂)²]），输入为 (2,) 数组，输出标量。\n        return np.linalg.norm(res_center-gt_center)\n    def scale_iou(res_size,gt_size):\n        min_wlh = np.minimum(res_size, gt_size)\n        volume_annotation = np.prod(res_size)\n        volume_result = np.prod(gt_size)\n        intersection = np.prod(min_wlh)  # type: float\n        union = volume_annotation + volume_result - intersection  # type: float  并集体积 = V_pred + V_gt − V_inter，标量。\n        iou = intersection / union\n        return iou\n    def yaw_diff(res_yaw,gt_yaw,period=2*np.pi):\n        diff = (res_yaw - gt_yaw + period/2) % period - period/2    # 将角度差归一化到 [−π, π) 区间。\n        if diff > np.pi:\n            diff = diff - (2*np.pi)\n        diff = abs(diff)\n        return diff\n    def cummean(x: np.array) -> np.array: # type: ignore   若所有元素都是 NaN。\n        if sum(np.isnan(x)) == len(x):\n            # Is all numbers in array are NaN’s.      返回全 1 数组（长度 = len(x)），表示最大误差。\n            return np.ones(len(x))  # If all errors are NaN set to error to 1 for all operating points.\n        else:\n            # Accumulate in a nan-aware manner.\n            sum_vals = np.nancumsum(x.astype(float))  # Cumulative sum ignoring nans.\n            count_vals = np.cumsum(~np.isnan(x))  # Number of non-nans up to each position.\n            return np.divide(sum_vals, count_vals, out=np.zeros_like(sum_vals), where=count_vals != 0)\n    tp = []\n    fp = []\n    conf = []\n    match_err = {’centerdis_err’: [],’scale_err’: [],’orient_err’: [],’conf’: []}\n    err_dict = {’centerdis_err’: {},’scale_err’: {},’orient_err’: {}}\n    taken = set()\n    res_cls_array = sorted(res_cls_array,key=lambda e:e[-3],reverse=True)\n    for res in res_cls_array: # nx10 中心点长宽高yaw,score,cls,sampleid\n        sampleid = res[-1]\n        min_dist = np.inf\n        match_gt_idx = None\n        gts_sample = gt_cls_array[gt_cls_array[:,-1]==sampleid]\n        for gtid,gt in enumerate(gts_sample):\n            if (sampleid,gtid) not in taken:\n                distance = center_distance(res[:2],gt[:2])\n                if distance < min_dist:\n                    min_dist = distance\n                    match_gt_idx = gtid\n        is_match = min_dist < dist_ths\n        if is_match:\n            taken.add((sampleid,match_gt_idx))\n            tp.append(1)\n            fp.append(0)\n            conf.append(res[-3])\n            gt_box_match = gts_sample[match_gt_idx]\n            match_err[’centerdis_err’].append(center_distance(res[:2],gt_box_match[:2]))\n            match_err[’scale_err’].append(1-scale_iou(res[3:6],gt_box_match[3:6]))\n            match_err[’orient_err’].append(yaw_diff(res[6],gt_box_match[6]))\n            match_err[’conf’].append(res[-3])\n        else:\n            tp.append(0)\n            fp.append(1)\n            conf.append(res[-3])\n    # Accumulate.\n    tp = np.cumsum(tp).astype(float)\n    fp = np.cumsum(fp).astype(float)\n    conf = np.array(conf)\n    # Calculate precision and recall.\n    prec = tp / (fp + tp)\n    assert len(gt_cls_array)!=0\n    rec = tp / float(len(gt_cls_array))\n    rec_interp = np.linspace(0, 1, 101)  # 101 steps, from 0% to 100% recall.\n    prec = np.interp(rec_interp, rec, prec, right=0)\n    conf = np.interp(rec_interp, rec, conf, right=0)\n    rec = rec_interp\n    for key in [’centerdis_err’,’scale_err’,’orient_err’]:\n        tmp = cummean(np.array(match_err[key]))\n        if len(tmp)==0:\n            match_err[’conf’] = [1.0,1.0]\n            tmp = np.array([1.0,1.0])\n        # Then interpolate based on the confidences. (Note reversing since np.interp needs increasing arrays)\n        match_err[key] = np.interp(conf[::-1], match_err[’conf’][::-1], tmp[::-1])[::-1] # type: ignore\n        non_zero = np.nonzero(conf)[0]\n        if len(non_zero) == 0:  # If there are no matches, all the confidence values will be zero.\n            max_recall_ind = 0\n        else:\n            max_recall_ind = non_zero[-1]\n        first_ind = round(100 * 0.1) + 1  # +1 to exclude the error at min recall.\n        last_ind = max_recall_ind  # First instance of confidence = 0 is index of max achieved recall.\n        if last_ind < first_ind:\n            err_tp = 1.0  # Assign 1 here. If this happens for all classes, the score for that TP metric will be 0.\n        else:\n            err_tp = float(np.mean(match_err[key][first_ind: last_ind + 1]))  # +1 to include error at max recall.\n        err_dict[key] = err_tp # type: ignore\n    prec = prec[round(100 * 0.1) + 1:]  # Clip low recalls. +1 to exclude the min recall bin.\n    prec -= 0.1  # Clip low precision\n    prec[prec < 0] = 0\n    ap = float(np.mean(prec)) / (1.0 - 0.1)\n    return ap,err_dict\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_all_calib_from_local</p>用于查找对应相机的内参/外参<br>\n该函数假设数据按 {car_info}/{date} 组织（如 D4Q_51/20250222）<br>\n<span class=\'hidden-code\' data-code=\'def get_all_calib_from_local(trainfiles,calib_path,use_refer8m2ego=False):\n    if os.path.isfile(trainfiles[0]):   # trainfiles[0] 是一个文件路径 → 读取所有 .txt 文件内容，合并成一个大列表\n        lines = []\n        for tfile in trainfiles:\n            with open(tfile,’r’) as fr:\n                per_file_lines = fr.readlines()\n            lines.extend(per_file_lines)\n    else:\n        lines = trainfiles\n    lines.sort()                        # 每行包含：`<`car_id`>` `<`date`>` `<`seq`>` `<`timestamp`>` ...\n    car_date_set = set()\n    local_car_date_set = set()\n    for line in lines:\n        _,car_info, date,_,_,_ = line.strip().split(’ ’)\n        car_date_set.add(f’{car_info} {date}’)\n        local_car_date_set.add(f’{car_info} {date}’)   # 提取所有唯一的 (car_info, date) 组合（如 ’D4Q_51 20250222’）。\n    assert len(list(local_car_date_set))==len(list(car_date_set)),’maybe get one seq on two path’\n    calib_dict = defaultdict(dict)   # 存储原始标定参数、变换矩阵等\n    table_dict = defaultdict(dict)   # 存储预计算的几何投影表（用于 BEV 特征对齐）。\n    for local_car_date in list(local_car_date_set):\n        _car_info, date = local_car_date.split(’ ’)\n        calibpath = os.path.join(calib_path,’calib’,_car_info,date,’calib/calib.json’)\n        with open(calibpath,’r’) as fr:\n            calib_params = json.load(fr)\n        calib_dict[f’{_car_info} {date}’][’calib_params’] = calib_params   # calib_dict[car_date][’calib_params’]\n        # 从 camera4.json（前视8米相机）中提取其到 ego 的外参; \n        egofile = os.path.join(calib_path,’calib’,_car_info, date,f’calib/L2_calib/camera4.json’)\n        lidar2ego,cam8m2ego = `get_lidar2ego_dir`(egofile,calib_params) # numpy   结合 calib_params 推导 LiDAR → ego 的变换矩阵\n        if use_refer8m2ego:  # 使用 camera4 作为 ego 参考（use_refer8m2ego=True）为每个相机的每个下采样特征点，计算其在相机归一化坐标系下的 2D 坐标（去畸变后）\n            normcor_array,femask_array,cam2ego_mats = `get_frustum_from_refer8m2ego`(\n                cam_ori_str,cam_ori_str2num,calib_params,egofile,lidar2ego=None,\n                ogfH=320,ogfW=480,fe_downsample_factor=16,camnum=6\n            )\n        else:\n            calibpath = os.path.join(calib_path,’calib’,_car_info, date,’calib/L2_calib’)\n            normcor_array,femask_array,cam2ego_mats = `get_frustum_from_cam6v2ego`(\n                cam_ori_str,cam_ori_str2num,calibpath,\n                ogfH=320,ogfW=480,fe_downsample_factor=16,camnum=6\n            )\n        \n        table_dict[f’{_car_info} {date}’] = {\n            ’normcor’:torch.from_numpy(normcor_array), # 6,20,30,2                   在推理时，直接用 normcor 作为 grid_sample 的采样网格；  \n            ’femask’:torch.from_numpy(femask_array),   # 6,20,30                     用于 mask 掉无效区域（如天空）；\n            ’cam2ego_mats’:torch.from_numpy(cam2ego_mats).float(), # 6,4,4 float64   可用于后处理（如 3D 框变换）。\n        }    \n        lidar2ego = torch.from_numpy(lidar2ego).float()\n        calib_dict[f’{_car_info} {date}’][’lidar2ego’] = lidar2ego\n        cam_calib_dict = defaultdict(dict)      \n        for cam in cam_ori_str:                            # 加载每个相机的详细标定\n            cam_calib_file = os.path.join(calib_path,’calib’,_car_info, date,f’calib/L2_calib/{cam_ori_str2num[cam]}.json’)\n            extrinsic, intrinsic, distorts = `get_ego_cam_calib`(cam_calib_file)\n            cam_calib_dict[f’{cam}’][’ego_extrinsic’] = extrinsic # extrinsic==cam2ego   相机到 ego 的变换\n            cam_calib_dict[f’{cam}’][’ego_intrinsic’] = intrinsic                      # 相机内参 K\n            cam_calib_dict[f’{cam}’][’ego_distorts’] = distorts                        # 畸变系数\n            cam2ego,intrin_mat,distort_params = `get_lidar_cam_calib`(cam_ori_str2num[cam],calib_params,lidar2ego)\n            cam_calib_dict[f’{cam}’][’lidar_extrinsic’] = cam2ego\n            cam_calib_dict[f’{cam}’][’lidar_intrinsic’] = intrin_mat\n            cam_calib_dict[f’{cam}’][’lidar_distorts’] = distort_params\n        calib_dict[f’{_car_info} {date}’][’cam_calibs’] = cam_calib_dict\n    return calib_dict,table_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_lidar2ego_dir</p>结合 calib_params 推导 LiDAR → ego 的变换矩阵 和 camera4 → ego 的变换矩阵<br>\n<span class=\'hidden-code\' data-code=\'def get_lidar2ego_dir(cam8m2egofile,calib_params):\n    cam8m_calib_param = calib_params[’camera4’]\n    lidar2cam8m = np.array(cam8m_calib_param[’Extrinsic’],dtype=np.float64) # 外参是雷达到相机 4x4\n    with open(cam8m2egofile,’r’) as jsfr:\n        jscon = json.load(jsfr)\n        pitch = jscon[’pitch’] # -4.7763772,\n        pos = jscon[’pos’]     # [-1.14,-0.02,0.35],\n        yaw = jscon[’yaw’]     #  179.146408,\n        roll = jscon[’roll’]   # 0.0710800514\n        cam8m2ego = get_pincam2ego(roll,pitch,yaw,pos) # np array: 4x4\n    lidar2ego = cam8m2ego @ lidar2cam8m # 4,4\n    return lidar2ego,cam8m2ego\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_pincam2ego</p><span class=\'hidden-code\' data-code=\'def get_pincam2ego(roll, pitch, yaw, pos):\n    cam2ego = np.zeros((4,4))\n    cam2ego[3,3] = 1.0\n    # 程博给的转\n    pitch = pitch * np.pi/180\n    roll = roll * np.pi/180\n    yaw = yaw * np.pi/180\n    cam2ego[:3,:3] = euler_to_rotation(roll, pitch, yaw)\n    cam2ego[:3,3] = pos\n    # 非手动变维,将bev空间的坐标系转换到ego坐标系\n    change_axis = np.array([[0,0,1,0],[-1,0,0,0],[0,-1,0,0],[0,0,0,1]]) # z变x, x变-y, y变-z. 先看每行内的数值所处位置,再看那个\n    cam2ego = cam2ego @ change_axis\n    return cam2ego\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_frustum_from_refer8m2ego</p>为每个相机的每个下采样特征点，计算其在相机归一化坐标系下的 2D 坐标（去畸变后）<br>\n并提供有效性掩码和相机到 ego 的变换矩阵。<br>\nnormcor_array\t(6, 20, 30, 2)<br>\nfemask_array\t(6, 20, 30)\t有效区域 mask（1=有效，0=无效）<br>\ncam2ego_mats\t(6, 4, 4)\t每个相机到 ego 坐标系的变换矩阵<br>\n20×30 来自 ogfH=320, ogfW=480 下采样 fe_downsample_factor=16 → 320/16=20, 480/16=30<br>\n<span class=\'hidden-code\' data-code=\'# 鱼眼相机的函数\ndef get_frustum_from_refer8m2ego(cams,cam2calibcam,calib_params,egofile,lidar2ego=None,ogfH=320,ogfW=480,fe_downsample_factor=16,camnum=6):\n    ’’’Generate pseudo_points’’’\n    # make grid in image plane\n    crop_8m_cfg = dict(pt1=(0,0),pt2=(3840,1800))          # 原始 8M 前视图分辨率（3840×2160？但这里写 1800，可能是裁剪后）\n    front8m_pyramid_roi = dict(pt1=(0,0),pt2=(960, 450))   # 前视 8M 图像在金字塔层（如 1/4 分辨率）的有效 ROI（960×450）\n    other_roi_cfg = dict(pt1=(0,0),pt2=(1920,1280))        # 其他相机（如侧视、后视）的 ROI（1920×1280）\n    camera_back_pinhole = True              # 后视相机被当作针孔模型处理（即使物理上是鱼眼，也可能因标定方式不同而用针孔+畸变矫正）\n    # 创建一个下采样后的伪点网格，用于后续的投影计算。   构建下采样网格（像素中心坐标）\n    fH, fW = ogfH // fe_downsample_factor, ogfW // fe_downsample_factor            # 20,30\n    x_coords = torch.arange(0,ogfW,fe_downsample_factor,dtype=torch.float32)+fe_downsample_factor//2\n    x_coords = x_coords.view(1,1,fW).expand(camnum,fH,fW)\n    y_coords = torch.arange(0,ogfH,fe_downsample_factor,dtype=torch.float32)+fe_downsample_factor//2  \n    y_coords = y_coords.view(1,fH,1).expand(camnum,fH,fW)   # ogfW=480, step=16 → [8, 24, 40, ..., 472] 共 30 个点\n    # 这里是有体现出下采样的坐标和增强图像的关系的\n    paddings = torch.ones([camnum,fH,fW]) # camnum,20,30\n    # 6 x H x W x 3        包含每个相机的像素坐标及其填充值（通常为 1）。\n    pseudo_points = torch.stack((x_coords, y_coords, paddings), -1)\n    if lidar2ego is None:\n        lidar2ego = `get_lidar2ego_ref`(egofile, calib_params, ref_camera=’camera_front_8m’)  # numpy 4x4\n    normcor_array = []\n    femask_array = []\n    cam2ego_mats = []\n    for camid,cam in enumerate(cams):                                         # 只要求内参和畸变不变就好\n        calib_param = calib_params[cam2calibcam[cam]]\n        intrin = torch.tensor(calib_param[’Intrinsic’], dtype=torch.float32)  # 3x3\n        distort = np.array([float(x) for x in calib_param[’Distortion’]])\n        lidar2cam = np.array(calib_param[’Extrinsic’], dtype=np.float32)      # 外参是雷达到2M相机 4x4\n        cam2lidar = np.linalg.inv(lidar2cam)                                  # 4,4\n        cams2ego = lidar2ego @ cam2lidar                                      # 4,4\n        cam2ego_mats.append(cams2ego)\n        if cam == ’camera_front_8m’:          # 根据相机类型（如前视8米相机 vs 其他相机），应用不同的裁剪和缩放操作来调整图像区域\n            pt1 = front8m_pyramid_roi[’pt1’]  # 左上角顶点坐标\n            pt2 = front8m_pyramid_roi[’pt2’]  # 右下角顶点坐标\n            pyramid_mat = torch.tensor([[0.25, 0, 0], [0, 0.25, 0], [0, 0, 1]])             # 原始高分辨率 → 金字塔层（1/4 缩放） 3,3  3840,2160 -> 960,540\n            crop_ida = torch.tensor([[1.0,0.0,-pt1[0]],[0.0,1.0,-pt1[1]],[0.0,0.0,1.0]])    # 裁剪 ROI（这里 pt1=(0,0)，所以无平移）\n            wresize = ogfW / (pt2[0]-pt1[0]) #\n            hresize = ogfH / (pt2[1]-pt1[1]) #\n            ida_mat = torch.tensor([[wresize, 0, 0], [0, hresize, 0], [0, 0, 1]])  # 3,3   resize: 将 ROI（960×450）缩放到目标特征图输入尺寸（480×320）\n            ida_mat = ida_mat.matmul(crop_ida.matmul(pyramid_mat))\n        else:\n            pt1 = other_roi_cfg[’pt1’]  # 左上角顶点坐标\n            pt2 = other_roi_cfg[’pt2’]  # 右下角顶点坐标\n            pyramid_mat = torch.tensor([[0.25, 0, 0], [0, 0.25, 0], [0, 0, 1]])  # 3,3  3840,2160 -> 960,540 # 仅做 1/4 下采样，无裁剪和额外缩放（意味着其他相机输入已是 1920×1280 → 480×320）\n            ida_mat = pyramid_mat\n        ida_mat = ida_mat.inverse()\n        # 开始计算映射坐标,对于鱼眼KB,使用内参+畸变得到新的归一化坐标,对于针孔,使用内参+畸变得到新的归一化坐标\n        ida_mat = ida_mat.view(1, 1, 3, 3)  # 3,3 -> 1,1,3,3\n        # 开始计算从像素坐标系到归一化后的真3D坐标\n        points = pseudo_points[camid].unsqueeze(-1) # H,W,3,1\n        points = ida_mat.matmul(points)             # -> H,W,3,1   每个特征点在 原始图像坐标系下的齐次坐标（未去畸变）\n        if cam == ’camera_back’ and camera_back_pinhole:\n            points = points.reshape(fH,fW,3)[...,:2]\n            intrin = intrin.numpy()\n            uvpoints = points.detach().cpu().numpy() # H,W,2\n            uvpoints = uvpoints.reshape(-1,2).reshape(-1,1,2) # H*W,1,2\n            normcor = cv2.undistortPointsIter(uvpoints, intrin, distort, None, None, (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 40 , 0.03)) # type: ignore\n            normcor = normcor.reshape(-1,2).reshape(fH,fW,2)\n            femask = np.ones((fH,fW),dtype=bool) # H,W\n        else:\n            intrin = intrin.view(1, 1, 3, 3)\n            points = intrin.inverse().matmul(points) # -> H,W,3,1\n            points = points.squeeze(-1)[..., :2]  # H,W,2\n            xp = points[:, :, 0]  # H,W\n            yp = points[:, :, 1]  # H,W\n            thetaD = torch.sqrt(xp*xp+yp*yp)  # H,W\n            thetaD = thetaD.view(-1) # H*W\n            theta, femask = solvepoly(thetaD, distort)  # H*W,H*W\n            thetaD = thetaD.view(fH, fW)\n            theta = theta.view(fH, fW)\n            femask = femask.view(fH, fW)\n            r = torch.tan(theta)  # H,W\n            normx = xp * r / thetaD  # H,W\n            normy = yp * r / thetaD  # H,W\n            norm_xy = torch.stack([normx, normy], dim=-1) # H,W,2\n            normcor = norm_xy.detach().cpu().numpy() # H,W,2\n            femask = femask.detach().cpu().numpy()  # H,W\n        normcor_array.append(normcor)\n        femask_array.append(femask)\n    normcor_array = np.stack(normcor_array)\n    femask_array = np.stack(femask_array)\n    cam2ego_mats = np.stack(cam2ego_mats)  # camnum,4,4\n    return normcor_array, femask_array, cam2ego_mats\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_lidar2ego_ref</p><span class=\'hidden-code\' data-code=\'def get_lidar2ego_ref(egofile, calib_params, ref_camera=’camera_front_8m’):\n    ref_calib_param = calib_params[cam_ori_str2num[ref_camera]]\n    lidar2cam = np.array(ref_calib_param[’Extrinsic’], dtype=np.float64)  # 外参是雷达到参考相机 4x4\n    with open(egofile, ’r’, encoding=’utf8’) as egofr:\n        jscon = json.load(egofr)\n        pitch = jscon[’pitch’]  # -4.7763772,\n        pos = jscon[’pos’]  # [-1.14,-0.02,0.35],\n        yaw = jscon[’yaw’]  #  179.146408,\n        roll = jscon[’roll’]  # 0.0710800514\n    cam2ego = np.zeros((4, 4))\n    cam2ego[3, 3] = 1.0\n    pitch = pitch * np.pi / 180\n    roll = roll * np.pi / 180\n    yaw = yaw * np.pi / 180\n    cam2ego[:3, :3] = euler_to_rotation(roll, pitch, yaw)\n    cam2ego[:3, 3] = pos\n    # 将bev空间的坐标系转换到ego坐标系\n    change_axis = np.array(\n        [[0, 0, 1, 0], [-1, 0, 0, 0], [0, -1, 0, 0], [0, 0, 0, 1]]\n    )  # z变x, x变-y, y变-z. 先看每行内的数值所处位置,再看那个\n    lidar2ego = cam2ego @ (change_axis @ lidar2cam)  # 4,4\n    return lidar2ego\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_frustum_from_cam6v2ego</p>使用默认 ego（use_refer8m2ego=False）<br>\n<span class=\'hidden-code\' data-code=\'# 鱼眼相机的函数\ndef get_frustum_from_cam6v2ego(cams,cam2calibcam,calibpath,ogfH=320,ogfW=480,fe_downsample_factor=16,camnum=6):\n    ’’’Generate pseudo_points’’’\n    # make grid in image plane\n    crop_8m_cfg = dict(pt1=(0,0),pt2=(3840,1800))\n    front8m_pyramid_roi = dict(pt1=(0,0),pt2=(960, 450))\n    other_roi_cfg = dict(pt1=(0,0),pt2=(1920,1280))\n    camera_back_pinhole = True\n    fH, fW = ogfH // fe_downsample_factor, ogfW // fe_downsample_factor # 20,30\n    x_coords = torch.arange(0,ogfW,fe_downsample_factor,dtype=torch.float32)+fe_downsample_factor//2\n    x_coords = x_coords.view(1,1,fW).expand(camnum,fH,fW)\n    y_coords = torch.arange(0,ogfH,fe_downsample_factor,dtype=torch.float32)+fe_downsample_factor//2\n    y_coords = y_coords.view(1,fH,1).expand(camnum,fH,fW)\n    # 这里是有体现出下采样的坐标和增强图像的关系的\n    paddings = torch.ones([camnum,fH,fW]) # camnum,20,30\n    # 4 x H x W x 3\n    pseudo_points = torch.stack((x_coords, y_coords, paddings), -1)\n    normcor_array = []\n    femask_array = []\n    cam2ego_mats = []\n    for camid,cam in enumerate(cams):  # 只要求内参和畸变不变就好\n        with open(f’{calibpath}/{cam2calibcam[cam]}.json’, ’r’) as jsfr:\n            jscon = json.load(jsfr)\n            pitch = jscon[’pitch’]  # -4.7763772,\n            pos = jscon[’pos’]  # [-1.14,-0.02,0.35],\n            yaw = jscon[’yaw’]  #  179.146408,\n            roll = jscon[’roll’]  # 0.0710800514\n            fu = jscon[’focal_u’]\n            fv = jscon[’focal_v’]\n            cu = jscon[’cu’]\n            cv = jscon[’cv’]\n            distort = np.array(jscon[’distort_coeffs’])\n            intrin = torch.tensor(\n                [[fu, 0, cu], [0, fv, cv], [0, 0, 1]], dtype=torch.float32\n            )\n            cams2ego = get_pincam2ego(roll, pitch, yaw, pos)  # np array: 4x4\n        cam2ego_mats.append(cams2ego)\n        if cam == ’camera_front_8m’:\n            pt1 = front8m_pyramid_roi[’pt1’]  # 左上角顶点坐标  # pt1=(0,0)\n            pt2 = front8m_pyramid_roi[’pt2’]  # 右下角顶点坐标  # pt2=(960, 450)\n            pyramid_mat = torch.tensor([[0.25, 0, 0], [0, 0.25, 0], [0, 0, 1]])  # 3,3  3840,2160 -> 960,540\n            crop_ida = torch.tensor([[1.0,0.0,-pt1[0]],[0.0,1.0,-pt1[1]],[0.0,0.0,1.0]])\n            wresize = ogfW / (pt2[0]-pt1[0]) #\n            hresize = ogfH / (pt2[1]-pt1[1]) #\n            ida_mat = torch.tensor([[wresize, 0, 0], [0, hresize, 0], [0, 0, 1]])  # 3,3\n            ida_mat = ida_mat.matmul(crop_ida.matmul(pyramid_mat)) # 原始分辨率->输入分辨率\n        else:\n            pt1 = other_roi_cfg[’pt1’]  # 左上角顶点坐标\n            pt2 = other_roi_cfg[’pt2’]  # 右下角顶点坐标\n            pyramid_mat = torch.tensor([[0.25, 0, 0], [0, 0.25, 0], [0, 0, 1]])  # 3,3  3840,2160 -> 960,540\n            ida_mat = pyramid_mat\n        ida_mat = ida_mat.inverse() # 输入分辨率->原始分辨率\n        # 开始计算映射坐标,对于鱼眼KB,使用内参+畸变得到新的归一化坐标,对于针孔,使用内参+畸变得到新的归一化坐标\n        ida_mat = ida_mat.view(1, 1, 3, 3)  # 3,3 -> 1,1,3,3\n        # 开始计算从像素坐标系到归一化后的真3D坐标\n        points = pseudo_points[camid].unsqueeze(-1) # H,W,3,1\n        points = ida_mat.matmul(points) # -> H,W,3,1\n        if cam == ’camera_back’ and camera_back_pinhole:\n            points = points.reshape(fH,fW,3)[...,:2]\n            intrin = intrin.numpy()\n            uvpoints = points.detach().cpu().numpy() # H,W,2\n            uvpoints = uvpoints.reshape(-1,2).reshape(-1,1,2) # H*W,1,2\n            normcor = cv2.undistortPointsIter(uvpoints, intrin, distort, None, None, (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 40 , 0.03)) # type: ignore\n            normcor = normcor.reshape(-1,2).reshape(fH,fW,2)\n            femask = np.ones((fH,fW),dtype=bool) # H,W\n        else:\n            intrin = intrin.view(1, 1, 3, 3)\n            points = intrin.inverse().matmul(points) # -> H,W,3,1\n            points = points.squeeze(-1)[..., :2]  # H,W,2\n            xp = points[:, :, 0]  # H,W\n            yp = points[:, :, 1]  # H,W\n            thetaD = torch.sqrt(xp*xp+yp*yp)  # H,W\n            thetaD = thetaD.view(-1) # H*W\n            theta, femask = solvepoly(thetaD, distort)  # H*W,H*W\n            thetaD = thetaD.view(fH, fW)\n            theta = theta.view(fH, fW)\n            femask = femask.view(fH, fW)\n            r = torch.tan(theta)  # H,W\n            normx = xp * r / thetaD  # H,W\n            normy = yp * r / thetaD  # H,W\n            norm_xy = torch.stack([normx, normy], dim=-1) # H,W,2\n            normcor = norm_xy.detach().cpu().numpy() # H,W,2\n            femask = femask.detach().cpu().numpy()  # H,W\n        normcor_array.append(normcor)\n        femask_array.append(femask)\n    normcor_array = np.stack(normcor_array)\n    femask_array = np.stack(femask_array)\n    cam2ego_mats = np.stack(cam2ego_mats)  # camnum,4,4\n    return normcor_array, femask_array, cam2ego_mats\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_ego_cam_calib</p>畸变系数、内参矩阵以及外参（即旋转和平移）<br>\n将外参从ego坐标系到相机坐标系的变换<br>\n<span class=\'hidden-code\' data-code=\'def get_ego_cam_calib(calib_path):\n    with open(calib_path, ’r’) as fr:\n        calib_dict = json.load(fr)\n    distorts = np.array(calib_dict[’distort_coeffs’])\n    intrin = np.array([\n        [calib_dict[’focal_u’], 0, calib_dict[’cu’]],\n        [0, calib_dict[’focal_v’], calib_dict[’cv’]],\n        [0, 0, 1]\n    ])\n    intrinsic = np.zeros((4, 4))\n    intrinsic[3, 3] = 1.0\n    intrinsic[:3,:3] = intrin\n    extrinsic_rot, extrinsic_tran = `get_rot_mat_and_trans`(calib_dict)\n    # rot_ego2cam, trans_ego2cam\n    extrinsic = np.zeros((4,4), dtype=np.float32)\n    extrinsic[:3,:3] = extrinsic_rot\n    extrinsic[:3,3:] = extrinsic_tran.reshape(3,1)\n    extrinsic[3,3] = 1 # extrinsic == ego2cam\n    extrinsic = torch.from_numpy(extrinsic).inverse() # -> cam2ego\n    intrinsic = torch.from_numpy(intrinsic)\n    distorts = torch.from_numpy(distorts)\n    return extrinsic, intrinsic, distorts  # extrinsic==cam2ego\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_rot_mat_and_trans</p><span class=\'hidden-code\' data-code=\'def get_rot_mat_and_trans(calib_dict):\n    su = np.sin(calib_dict[’roll’]*np.pi/180)\n    cu = np.cos(calib_dict[’roll’]*np.pi/180)\n    sv = np.sin(calib_dict[’pitch’]*np.pi/180)\n    cv = np.cos(calib_dict[’pitch’]*np.pi/180)\n    sw = np.sin(calib_dict[’yaw’]*np.pi/180)\n    cw = np.cos(calib_dict[’yaw’]*np.pi/180)\n    rot = np.array([\n        [cv*cw, su*sv*cw-cu*sw, su*sw+cu*sv*cw],\n        [cv*sw, cu*cw+su*sv*sw, cu*sv*sw-su*cw],\n        [-sv, su*cv, cu*cv]\n    ])\n    rot = np.transpose(rot)\n    rot_yxz = np.array([\n        [0,-1,0],\n        [0,0,-1],\n        [1,0,0]\n    ])\n    rot_ego2cam = np.matmul(rot_yxz, rot)\n    rot_cam2ego = np.transpose(rot_ego2cam) # equals np.linalg.inv\n    pos = np.array(calib_dict[’pos’])\n    trans_ego2cam = -1 * np.matmul(rot_ego2cam, pos)\n    return rot_ego2cam, trans_ego2cam\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_lidar_cam_calib</p><span class=\'hidden-code\' data-code=\'def get_lidar_cam_calib(camera,calib_params,lidar2ego):\n    calib_param = calib_params[camera]\n    lidar_to_cam = np.array(calib_param[’Extrinsic’],dtype=np.float64) # 外参是雷达到相机 4x4\n    intrin_array = np.array(calib_param[’Intrinsic’],dtype=np.float64) # 3x3\n    distort_params = torch.tensor([float(x) for x in calib_param[’Distortion’]])\n    \n    intrin_mat = torch.zeros((4, 4))\n    intrin_mat[3, 3] = 1\n    intrin_mat[:3, :3] = torch.Tensor(intrin_array)\n    # lidar to camera6v\n    lidar2cam = torch.from_numpy(lidar_to_cam).float() # 4x4\n    cam2lidar = lidar2cam.inverse() # 4,4\n    cam2ego = lidar2ego.matmul(cam2lidar) # 4,4\n    return cam2ego,intrin_mat,distort_params\n\'> </span>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">test_visual_single</p><span class=\'hidden-code\' data-code=\'def test_visual_single(merge_backbone, head_net, imgname, calib_dict, table_dict):\n    refimg = f’{test_data_root}/camera_back/{imgname}{imgsuffix}’ # type: ignore\n    imgpaths = [refimg.replace(’camera_back’, cam) for cam in cams]\n    car, date = ’D4Q_51’,’20250222’\n    bin_file = table_dict[f’{car} {date}’]\n    lidar2ego = calib_dict[f’{car} {date}’][’lidar2ego’]\n    res,proj_table = get_caffe_output(merge_backbone, head_net, imgpaths, bin_file)  # nx9,最后两维度是得分与类别  #np.array\n    res = res[res[:, 7] > 0.3]  # >0.3\n    #np.save(’./bins_002971/002971_CVYUV-PCResize.npy’,res)\n    boxes3d = res[:, :7]\n    scores = res[:, 7]\n    labels = res[:, 8]\n    bevimg = `visual_pred_on_bev`(boxes3d, scores, labels, ’’, lidar2ego)\n    #bevimg = visual_multipred_on_bev(boxes3d, scores, labels, lidar_path, lidar2ego)\n    bevimg.save(f’./bev_det_{imgname}_opyaw.jpg’)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">visual_pred_on_bev</p><span class=\'hidden-code\' data-code=\'def visual_pred_on_bev(bboxes,scores,labels,lidar_path,lidar2ego):\n    # inds = labels < 3 # cls2id = {’小车’:0,’行人’:1,’大巴’:2,’骑行人’:3,’其他大车’:4,’二轮车’:5,’障碍物’:6}\n    # bboxes = bboxes[inds]\n    # scores = scores[inds]\n    # labels = labels[inds]\n    bboxes_corner = box_center_to_corner(bboxes) # Nx8x3\n    fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n    # plt.subplots_adjust(left=0.07, bottom=0.03, right=0.95, top=0.97)\n    eval_range = max([front_x_range,back_x_range,left_y_range,right_y_range])\n    # pts3d =  o3d.io.read_point_cloud(lidar_path)\n    # pts3d = np.asarray(pts3d.points) # nx3\n    # # 转ego坐标系\n    # pad = np.ones([pts3d.shape[0],1]) # nx1\n    # points_pad = np.concatenate([pts3d,pad],axis=-1) # nx4\n    # egopoints = lidar2ego @ (points_pad.T) # 4xn\n    # points = egopoints[:3,:] # 3xn\n    # lidar坐标系    \n    # points = pts3d[:3, :]\n    # dists = np.sqrt(np.sum(points[:2, :] ** 2, axis=0))\n    # colors = np.minimum(1, dists / eval_range)\n    # ax.scatter(points[0, :], points[1, :], c=colors, s=0.2)\n    canvas = fig.canvas\n    # Show ego vehicle.\n    ax.plot(0, 0, ’x’, color=’black’)\n    # Show Test boxes.\n    def render_bev(corners,ax,colors=(’r’, ’r’, ’b’), linewidth=1):\n        def draw_rect(selected_corners, color):\n            prev = selected_corners[-1]\n            for corner in selected_corners:\n                ax.plot([prev[0], corner[0]], [prev[1], corner[1]], color=color, linewidth=linewidth)\n                prev = corner\n        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n        draw_rect(corners[:4], colors[2])\n        # Draw line indicating the front\n        center_bottom_forward = np.mean(corners[0:2], axis=0)\n        center_bottom = np.mean(corners[[0,1,2,3]], axis=0)\n        ax.plot([center_bottom[0], center_bottom_forward[0]],\n                [center_bottom[1], center_bottom_forward[1]],\n                color=colors[0], linewidth=linewidth)\n    for corners in bboxes_corner:\n        corners = corners[:,:2] # 8x2\n        render_bev(corners,ax)\n    # Limit visible range.\n    ax.set_xlim(-back_x_range, front_x_range)\n    ax.set_ylim(-left_y_range, right_y_range)\n    # title\n    # plt.title(’bev points’)\n    buffer = io.BytesIO()\n    canvas.print_png(buffer) # type: ignore\n    data = buffer.getvalue()\n    buffer.write(data)\n    img = Image.open(buffer)\n    img = img.convert(’RGB’)\n    plt.close()\n    img = img.rotate(90,expand=True)\n    return img\n\'> </span>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">test_visual</p><span class=\'hidden-code\' data-code=\'def test_visual(merge_backbone,head_net,test_frames, calib_dict, table_dict, saveout=’./visualout’):\n    os.makedirs(saveout,exist_ok=True)\n    for sampleidx,test_frame in tqdm(enumerate(test_frames)):\n        local, car, date, seq, framenum, nas = test_frame.strip().split(’ ’)\n        lidar2ego = calib_dict[f’{car} {date}’][’lidar2ego’]\n        bin_file = table_dict[f’{car} {date}’]\n        imgname = f’{car}_{date}_{seq}_{framenum}’\n        refimg = f’{test_data_root}/camera_back/{imgname}{imgsuffix}’ # type: ignore\n        imgpaths = [refimg.replace(’camera_back’, cam) for cam in cams]\n        res,proj_table = get_caffe_output(merge_backbone,head_net,imgpaths,bin_file) # nx9,最后两维度是得分与类别  #np.array\n        res = vehicle_nms(res)\n        res = res[res[:,7]>0.3]\n        #np.save(f’./visual_caffe/{date}_{seq}_{framenum}.npy’,res)\n        boxes3d = res[:,:7]\n        scores = res[:,7]\n        labels = res[:,8]\n        bevimg = visual_pred_on_bev(boxes3d,scores,labels,lidar_path=’’,lidar2ego=None)\n        savepath = os.path.join(saveout,f’{imgname}_det.jpg’)\n        bevimg.save(savepath)\n\'> </span>'}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
