<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>BEV_occ_det</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">推理代码</p><span class=\'hidden-code\' data-code=\'fed_bound = backbone_conf[’fed_bound’]  鱼眼深度（fisheye depth）的范围参数 [min_depth,max_depth,depth_interval]例如 [1.0,50.0,0.3]\nmin_radius = test_cfg[’min_radius’]\npc_range = head_conf[’bbox_coder’][’pc_range’]   BEV空间的物理边界;[x_min,y_min,z_min,x_max,y_max,z_max]，例如[-51.2,-51.2,-5.0,51.2,51.2,3.0]\npost_center_range = head_conf[’bbox_coder’][’post_center_range’]  后处理时允许保留的检测框中心坐标的范围（比 pc_range 更严格，常用于裁剪边缘噪声）\nBEV 网格在 x、y、z 三个维度上的离散化参数; [start, end, step]，例如 x_bound = [-51.2, 51.2, 0.8]\nx_bound = backbone_conf[’x_bound’]\ny_bound = backbone_conf[’y_bound’]\nz_bound = backbone_conf[’z_bound’]\nvoxel_num = torch.tensor([bev_w, bev_h, 1])\nvoxel_size= torch.Tensor([row[2] for row in [x_bound, y_bound, z_bound]])  每个维度的voxel尺寸, torch.Size([3])，例如 [0.8,0.8,8.0]\nvoxel_coord=torch.Tensor([row[0] + row[2] / 2.0 for row in [x_bound, y_bound, z_bound]])  计算每个维度上第一个 voxel 的中心坐标（而非边界）\nvoxel_num= torch.LongTensor([round((row[1] - row[0]) / row[2]) for row in [x_bound, y_bound, z_bound]])  (x_max - x_min) / Δx → x 方向 voxel 数\ninput_h,input_w = final_dim                                                输入图像的最终尺寸（经过 resize/crop 后）\nfedepth_channels = round((fed_bound[1] - fed_bound[0]) / fed_bound[2])     fed_bound=[1.0,49.0,0.3]→(48.0)/0.3=160 计算鱼眼深度方向的离散通道数（即深度bins数量）\nscore_threshold = 0.1\ndevice = torch.device(’cuda’ if torch.cuda.is_available() else ’cpu’)\nif __name__ == ’__main__’:\n    初始化值\n    root_dir = ’./data’\n    初始化模型\n    单目深度估计 + BEV 特征生成（可能含 mask 输入，用于遮挡处理）\n    first_stage_onnx_model_path = ’./occ_det/model_20251023/BEVOccDetFirstStageDeployMonoDepth_D4Q_20251022_ema18_with_mask_input_float32.onnx’\n    BEV 空间下的目标 proposal 生成（如 heatmap + offset 预测）；\n    second_stage_onnx_model_path = ’./occ_det/model_20251023/BEVOccDetSecondStageDeployMonoDepth_D4Q_20251022_ema18.onnx’\n    精细化 box 回归与类别预测（refinement）。\n    third_stage_onnx_model_path = ’./occ_det/model_20251023/BEVOccDetThirdStageDeployMonoDepth_D4Q_20251022_ema18.onnx’\n    first_stage_onnx_model = init_onnx_model(first_stage_onnx_model_path)\n    second_stage_onnx_model = init_onnx_model(second_stage_onnx_model_path)\n    third_stage_onnx_model = init_onnx_model(third_stage_onnx_model_path)\n    是否保存结果以及可视化\n    save_flag,vis_flag = True,True\n    `deploy_eval`(root_dir,first_stage_onnx_model,second_stage_onnx_model,third_stage_onnx_model,save_flag,vis_flag)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">deploy_eval</p><span class=\'hidden-code\' data-code=\'def deploy_eval(root_dir,first_stage_onnx_model,second_stage_onnx_model,third_stage_onnx_model,save_flag=False,vis_flag=False):\n    # 读取data_list\n    data_list_txt = os.path.join(root_dir,’data_list.txt’)\n    data_list = `get_data_list`(data_list_txt)[:]\n    print(f’data_list len: {len(data_list)}’)\n    # 初始化统计结果\n    res_list,gt_list= [],[]          # 第 i 个样本的预测结果，shape (N_pred, 10)\n    # 加载测试数据\n    for sampleid,(ref_frame,src_frame) in enumerate(data_list):   # src_frameid 被解析但未使用，说明当前 pipeline 是单帧（mono）\n        car_name,day,seq,ref_frameid = ref_frame.split(’/’)\n        _,_,_,src_frameid = src_frame.split(’/’)\n        day_dir = os.path.join(root_dir,car_name,day)\n        seq_dir = os.path.join(root_dir,car_name,day,seq)\n        # print(f’loading data : {ref_frame}’)\n        metadata = `load_metadata`(day_dir)                      # 加载标定参数（鱼眼相机模型、内外参、BEV 参数等）\n        frame_data = `load_frame`(seq_dir,int(ref_frameid),int(src_frameid),metadata)\n        # 推理以及nms\n        det_ret = `run_inference`(first_stage_onnx_model,second_stage_onnx_model,third_stage_onnx_model,frame_data,metadata)\n        #####################################################################################\n        # 不用onnx的推理结果,用板端/pc的结果\n        # infer_root_dir = ’世权填上你的结果的文件夹路径’\n        # det_ret_path = os.path.join(infer_root_dir,car_name,day,seq,ref_frameid,’det.bin’)\n        # det_ret = np.fromfile(det_ret_path,dtype=np.float32).reshape((1,60,160,192))\n        # 解码成需要的格式\n        # det_ret = bevout_decode(det_ret) # n*9 x,y,z,l,w,h,yaw,score,label\n        #####################################################################################\n        det_nms_ret = `vehicle_nms`(det_ret)\n        assert det_nms_ret.ndim==2\n        # 画图以及保存结果\n        if vis_flag:\n            pred_vis_out_dir = osp.join(root_dir,’onnx_pred_vis’)\n            # pred_vis_out_dir = osp.join(’/media/mini/T7/ParkingBev/D4Q/deploy/20251010_deploy/onnx_pred_vis’)\n            os.makedirs(pred_vis_out_dir, exist_ok=True)\n            vis_save_path = os.path.join(pred_vis_out_dir,f’{car_name}_{day}_{seq}_{frame_data[’ref_id’]:06d}.jpg’)\n            # visual_det_and_occ(metadata,frame_data,det_ret,pred_occ,vis_save_path)\n            `visual_det_and_occ`(metadata,frame_data,det_ret,None,vis_save_path)\n        if save_flag:\n            # 保存真值和预测结果\n            ret_out_dir = osp.join(root_dir, ’onnx_infre_results’)\n            # ret_out_dir = ’/media/mini/T7/ParkingBev/D4Q/deploy/20251010_deploy/onnx_infre_results’\n            os.makedirs(ret_out_dir, exist_ok=True)\n            ret_save_path = os.path.join(ret_out_dir,f’{car_name}_{day}_{seq}_{frame_data[’ref_id’]:06d}.json’)\n            `save_infer_ret`(det_ret,frame_data,ret_save_path)\n        # if sampleid>20:\n        #     break\n        # 得到gt\n        gt_bboxes = []\n        for bboxid in range(len(frame_data[’gt_boxes’])):\n            bbox = frame_data[’gt_boxes’][bboxid].tolist() + [1] + [int(frame_data[’gt_labels’][bboxid])]\n            gt_bboxes.append(bbox)\n        if len(gt_bboxes)<=0:\n            gt_bboxes = np.zeros((0,9))\n        gt_bboxes = np.array(gt_bboxes)\n        # 将sampleid与结果拼接\n        pred_sampleid = np.array([sampleid]*len(det_nms_ret))\n        gt_sampleid = np.array([sampleid]*len(gt_bboxes))\n        res = np.concatenate([det_nms_ret,pred_sampleid[:,None]],axis=1)\n        gt = np.concatenate([gt_bboxes,gt_sampleid[:,None]],axis=1)\n        res_list.append(res) # nx10,前7维度为 中心点长宽高 yaw,score,cls,sampleid\n        gt_list.append(gt)\n    # 统计全部的结果: map \n    `eval_ret`(res_list,gt_list)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_data_list</p><span class=\'hidden-code\' data-code=\'def get_data_list(data_list_txt):\n    data_list = []\n    with open(data_list_txt,’r’) as fin:\n        lines = fin.readlines()\n        for line in lines:\n            ref_frame,src_frame = line.strip().split(’\\t’)\n            data_list.append((ref_frame,src_frame))\n    return data_list\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">load_metadata</p>加载某一天(day_dir)采集的所有鱼眼相机相关的标定参数和预计算数据<br>\n<span class=\'hidden-code\' data-code=\'def load_metadata(day_dir):\n    luts = `collect_luts`(day_dir)                # N,H,W,3 加载预计算的 坐标映射表(LUT)，用于将鱼眼图像像素快速映射到BEV网格或世界坐标\n    car_masks_dict = `collect_car_masks`(day_dir) # N,H,W  加载车辆自身结构造成的静态遮挡区域掩码\n    extrinsics,inv_poly_coeffs,affine_params,cu_cvs,input_affine_params,input_cu_cvs = `collect_cam_infos`(day_dir)  # N,4,4   N,4   N,4\n    return {\n        ’lut’:luts.to(torch.float32),\n        ’car_mask’:car_masks_dict[’d16’].unsqueeze(0),         # 如 (N, H/16, W/16) —— 对应下采样16倍\n        ’input_car_mask’: car_masks_dict[’input’],             # 如 (N, H, W)\n        ’cam2ego’:extrinsics.unsqueeze(0).to(torch.float32),   # (N, 4, 4)\t相机到自车坐标系（ego）的外参矩阵（cam → ego）\n        ’inv_poly_coeffs’:inv_poly_coeffs.to(torch.float32),   # (N, 4)\t鱼眼镜头的反向多项式系数（用于从归一化平面反推入射角）\n        ’affine_params’:affine_params.to(torch.float32),       # (N, 4)\t仿射校正参数（可能用于将理想鱼眼转为实际成像）\n        ’cu_cvs’:cu_cvs.to(torch.float32),                     # (N, 4)\t径向校正参数（cu, cv 是主点偏移？或特定模型参数）\n        ’input_affine_params’:input_affine_params.to(torch.float32),  # (N, 4)\t输入分辨率下的仿射参数\n        ’input_cu_cvs’:input_cu_cvs.to(torch.float32),         # (N, 4)\t输入分辨率下的径向参数\n    }\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">collect_luts</p>鱼眼图像像素快速映射到BEV网格或世界坐标<br>\n<span class=\'hidden-code\' data-code=\'def collect_luts(day_dir):\n    luts = []   # N,H,W,3     N为相机数目\n    for cam in cams:\n        lut_file = os.path.join(day_dir,’lut’,cam, f’d16_lut_s_model_rank10_ss.bin’)\n        norm = np.fromfile(lut_file,dtype=np.float32).reshape((-1,3))  # H,W,3  0为normx，1为normy，2为mask（0为无映射，1为有映射）\n        norm = torch.from_numpy(norm)\n        luts.append(norm)\n    luts = torch.stack(luts,dim=0)  # N,H,W,3     N为相机数目\n    return luts\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">collect_car_masks</p>所有鱼眼相机加载车辆自遮挡掩码（car mask）<br>\n<span class=\'hidden-code\' data-code=\'def collect_car_masks(day_dir):\n    scale_names = [’input’,’d16’]\n    ret = {}\n    for scale_name in scale_names:\n        masks = []   # N,H,W     N为相机数目\n        for cam in cams:\n            # 读取car_mask\n            car_mask_file = os.path.join(day_dir,’car_masks’,cam,f’{scale_name}_car_masked.jpg’)\n            car_mask = cv2.imread(car_mask_file)[:, :, 0]\n            if scale_name == ’input’:\n                roi_mask = deepcopy(car_mask)\n                roi_mask[car_mask >= 127] = 0\n                roi_mask[car_mask < 127] = 1\n                roi_mask = torch.from_numpy(roi_mask.astype(np.float32))\n                masks.append(roi_mask)\n            else:\n                temp = deepcopy(car_mask)\n                car_mask[temp >= 127] = 1\n                car_mask[temp < 127] = 0\n                car_mask = torch.from_numpy(car_mask.astype(np.float32))\n                masks.append(car_mask)\n        masks = torch.stack(masks,dim=0)  # N,H,W     N为相机数目\n        ret[scale_name] = masks\n    return ret\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">collect_cam_infos</p><span class=\'hidden-code\' data-code=\'def collect_cam_infos(day_dir):\n    extrinsics = []   # N,4,4     N为相机数目\n    inv_poly_coeffs = []\n    affine_params = []\n    input_affine_params = []\n    cu_cvs = []\n    input_cu_cvs = []\n    cam_info_file = os.path.join(day_dir,’cam_info_s_model_rank10_ss.json’)\n    with open(cam_info_file) as f:\n        cam_infos = json.load(f)\n    for cam in cams:\n        cam_info = cam_infos[cam]\n        # T = torch.linalg.inv(torch.Tensor(cam_info[’lidar2cam’]))   # 转成cam2lidar\n        lidar2cam = np.array(cam_info[’lidar2cam’])\n        lidar2car = np.array(cam_infos[’lidar2car’])\n        T = torch.Tensor(lidar2car @ np.linalg.inv(lidar2cam)) \n        inv_poly_coeff = np.zeros(10)\n        inv_poly_coeff[:len(cam_info[’inv_poly_coeffs’])] = cam_info[’inv_poly_coeffs’]\n        inv_poly_coeff = torch.Tensor(inv_poly_coeff)\n        affine_param = torch.Tensor(cam_info[f’d16_affine_params’])\n        input_affine_param = torch.Tensor(cam_info[f’input_affine_params’])\n        cu_cv = torch.Tensor(cam_info[f’d16_cu_cv’])\n        input_cu_cv = torch.Tensor(cam_info[f’input_cu_cv’])\n        extrinsics.append(T)\n        inv_poly_coeffs.append(inv_poly_coeff)\n        affine_params.append(affine_param)\n        cu_cvs.append(cu_cv)\n        input_affine_params.append(input_affine_param)\n        input_cu_cvs.append(input_cu_cv)\n    extrinsics = torch.stack(extrinsics,dim=0)\n    inv_poly_coeffs = torch.stack(inv_poly_coeffs,dim=0)\n    affine_params = torch.stack(affine_params,dim=0)\n    cu_cvs = torch.stack(cu_cvs,dim=0)\n    input_affine_params = torch.stack(input_affine_params,dim=0)\n    input_cu_cvs = torch.stack(input_cu_cvs,dim=0)\n    return extrinsics,inv_poly_coeffs,affine_params,cu_cvs,input_affine_params,input_cu_cvs\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">load_frame</p>加载一个特定序列seq_dir中两个帧(参考帧ref_id和源帧src_id)的完整数据<br>\n<span class=\'hidden-code\' data-code=\'def load_frame(seq_dir,ref_id,src_id,metadata):\n    view_ids = (ref_id,src_id)       # 将两个帧 ID 打包成元组，传递给后续加载函数\n    # 读取图片\n    fisheye_imgs,ref_img_paths = `collect_fisheye_images`(seq_dir,view_ids,metadata)   # N,2,3,H,W  两个时间帧(ref_id, src_id)\n    # 读取位姿\n    poses = `collect_poses`(seq_dir,view_ids)   # N,4,4\n    # 读取det真值\n    gt_boxes, gt_labels = `get_gt_fromtxt`(seq_dir,view_ids) # numpy \n    # 过滤非ROI的bbox\n    gt_boxes,gt_labels = `filter_box3d`(gt_boxes,gt_labels)\n    gt_boxes = torch.from_numpy(gt_boxes).float()\n    gt_labels = torch.from_numpy(gt_labels)\n    # 返回字典\n    return {\n            ’ref_id’:ref_id,\n            ’imgs’: fisheye_imgs,\n            ’pose_matrices’: poses,\n            ’gt_boxes’:gt_boxes,\n            ’gt_labels’:gt_labels,\n            ’seq_dir’:seq_dir,\n            ’ref_img_paths’:ref_img_paths\n        }\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">collect_fisheye_images</p><span class=\'hidden-code\' data-code=\'def collect_fisheye_images(seq_dir,view_ids,metadata):\n    imgs = []   # N,V,H,W,3     N为相机数目，V为nviews-应该为2？\n    ref_img_paths = []\n    for camidx,cam in enumerate(cams):\n        for vid in view_ids:\n            img_filename = os.path.join(seq_dir, f’{cam}/{vid:06d}.png’)\n            if not os.path.exists(img_filename):\n                img_filename = os.path.join(seq_dir, f’{cam}/{vid:06d}.jpg’)\n            if vid == view_ids[0]:\n                ref_img_paths.append(img_filename)\n            img = cv2.imread(img_filename)\n            H,W,_ = img.shape\n            img = rgb2yuv444_bt709_full_range(img,W,H,to_rgb=True)\n            # img = img.astype(np.float32) / 255   \n            img = img.astype(np.float32)    # j6e上不需要/255\n            # car_mask = metadata[’input_car_mask’][camidx].numpy()\n            # img[car_mask] = 0\n            img = torch.from_numpy(img).permute(2, 0, 1)\n            imgs.append(img)\n    imgs = torch.stack(imgs,dim=0)\n    imgs = imgs.view(len(cams),len(view_ids),3,H,W)    # N,V,3,H,W     N为相机数目，V为nviews\n    return imgs,ref_img_paths\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">collect_poses</p>使用 自车位姿（ego pose） 来计算帧间相对运动<br>\n当前模型是单帧（mono），不使用时序信息；因此poses很可能未被使用<br>\n<span class=\'hidden-code\' data-code=\'def collect_poses(seq_dir,view_ids):\n    poses = []   # N,4,4     N为相机数目\n    for cam in cams:\n        ref_vid,src_vid = view_ids\n        ref_pose_filename = os.path.join(seq_dir, f’cam_poses/{cam}/{ref_vid:06d}.txt’)\n        src_pose_filename = os.path.join(seq_dir, f’cam_poses/{cam}/{src_vid:06d}.txt’)\n        ref_pose = np.loadtxt(ref_pose_filename)\n        src_pose = np.loadtxt(src_pose_filename)\n        pose = inverse_pose(src_pose) @ ref_pose   # 转为两帧的相对位姿\n        pose = torch.from_numpy(pose)\n        poses.append(pose)\n    poses = torch.stack(poses,dim=0)    # N,4,4     N为相机数目\n    return poses\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_gt_fromtxt</p><span class=\'hidden-code\' data-code=\'def get_gt_fromtxt(seq_dir,view_ids):\n    ref_vid = view_ids[0]\n    labelfile = os.path.join(seq_dir,’gt_boxes’,f’{ref_vid:06d}.txt’)\n    gt_bboxes = []\n    #map_dict = {’小车’:0,’行人’:1,’大巴’:2,’骑行人’:3,’其他大车’:4,’二轮车’:5,’障碍物’:6}\n    with open(labelfile,’r’,encoding=’utf8’) as fp:\n        lines = fp.readlines()\n        for line in lines:\n            line_list = line.strip().split(’\\t’)\n            if line.strip()==’’:\n                continue\n            if len(line_list)==9 and line_list[8]==’invalid’:\n                continue\n            label = line_list[7]\n            line_list = [float(num) for num in line_list[:7]] # 取前7个数字\n            x,y,z,l,w,h,yaw = line_list\n            box = [x,y,z,l,w,h,yaw,int(classes_dict[label])]\n            #print(f’{label_name}:{box}’)\n            gt_bboxes.append(box)\n    if len(gt_bboxes)<=0:\n        gt_bboxes = np.zeros((0,8))\n    gt_bboxes = np.array(gt_bboxes)\n    return gt_bboxes[:,:7], gt_bboxes[:,7]\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">filter_box3d</p><span class=\'hidden-code\' data-code=\'def filter_box3d(gt_boxes,gt_labels):\n    condition = np.logical_and(\n        (gt_boxes[:, 0] `<`= front_x_range) & (gt_boxes[:, 0] `>`= -back_x_range),\n        (gt_boxes[:, 1] `<`= left_y_range) & (gt_boxes[:, 1] `>`= -right_y_range))\n    gt_boxes = gt_boxes[condition]\n    gt_labels = gt_labels[condition]\n    return gt_boxes,gt_labels\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">run_inference</p>Ref Frame Images (4) ──┐<br>\n├─→ First Stage → [feat, depth_feat, mono_depth] ×4<br>\nSrc Frame Images (4) ──┘<br>\nFor each camera:<br>\n（1）Use mono_depth to build depth hypotheses<br>\n（2）Warp ref depth_feat to src view using rel_pose + fisheye model<br>\n（3）Compute cost_volume = similarity(src_feat, warped_feat)<br>\n（4）Second Stage: refine cost_volume → depth probability<br>\nLift: img_feature + depth_prob → BEV features (via LUT or projection)<br>\nSplat: fuse 4-camera BEV features<br>\nShoot: BEV detection head → 3D boxes<br>\n<span class=\'hidden-code\' data-code=\'def run_inference(first_stage_onnx_model,second_stage_onnx_model,third_stage_onnx_model,sample_data,metadata,output_dir=None):\n    fe_imgs = sample_data[’imgs’].unsqueeze(0)   # (4, 2, 3, H, W) --> (1, 4, 2, 3, H, W)\n    ref_fe_imgs = fe_imgs[:,:,0,:]               # 分离参考帧（ref）和源帧（src）                                            # (1, 4, 3, H, W)\n    src_fe_imgs = fe_imgs[:,:,1,:]               # 虽然是 mono 模型，但这里用了两帧做 时序匹配（temporal stereo） 来估计深度！ # (1, 4, 3, H, W)\n    input_car_masks_tuple = torch.unbind(metadata[’input_car_mask’],0)  # metadata[’input_car_mask’] shape: (4, H, W)（来自 collect_car_masks，1=有效区域）； unbind(0) → 拆成 4 个 (H, W) 张量，用于第一阶段输入\n    # 第一阶段推理，包括了前一帧的推理（实际部署可以直接读上一帧推理出来的结果）\n    ref_input_tuple = torch.unbind(ref_fe_imgs,1)    #  拆出4个相机: [(1,3,H,W), ...]\n    ref_input_tuple = list(ref_input_tuple) + list(input_car_masks_tuple)\n    print(f’run first stage for ref’,sep=’\\t’)\n    first_stage_out = run_onnx_model(first_stage_onnx_model,ref_input_tuple)  # 12个张量(每个相机3个)[feat,depth_feat,mono_depth]×4 cameras\n    # fisheye_front_feat,fisheye_front_depth_feat,fe_front_mono_depth,          feat: 主干特征（如 ResNet 输出）\n    # fisheye_right_feat,fisheye_right_depth_feat,fe_right_mono_depth,          depth_feat: 用于深度匹配的特征（低维）\n    # fisheye_back_feat,fisheye_back_depth_feat,fe_back_mono_depth,             mono_depth: 单目深度估计（初始深度）\n    # fisheye_left_feat,fisheye_left_depth_feat,fe_left_mono_depth = first_stage_out\n    # 上一帧的推理结果   这是 “pseudo-LiDAR” 或 “stereo matching” 思想：用时序帧代替双目，构建代价体（cost volume）\n    src_input_tuple = torch.unbind(src_fe_imgs,1)\n    src_input_tuple =  list(src_input_tuple) + list(input_car_masks_tuple)\n    print(f’run first stage for src’,sep=’\\t’)\n    src_first_stage_out = run_onnx_model(first_stage_onnx_model,src_input_tuple)\n    # src_fisheye_front_feat,src_fisheye_front_depth_feat,src_fe_front_mono_depth,\\\n    # src_fisheye_right_feat,src_fisheye_right_depth_feat,src_fe_right_mono_depth,\\\n    # src_fisheye_back_feat,src_fisheye_back_depth_feat,src_fe_back_mono_depth,\\\n    # src_fisheye_left_feat,src_fisheye_left_depth_feat,src_fe_left_mono_depth = src_first_stage_out\n    #----------------------------------------------\n    # DSP操作\n    # 1. 做一个warp的操作，2D-->3D-->2D，得到一个5维的 warped_volume\n    # 2. 特征维度求和构建 代价体积 cost volumn \n    #----------------------------------------------\n    input_tuple2 = []   # 二阶段的输入\n    # 对4个鱼眼分别跑inverse_warp以及求和\n    for i in range(4):\n        fe_depth_feat = torch.Tensor(first_stage_out[3*i+1])               # ref 帧 depth feature\n        fe_mono_depth = torch.Tensor(first_stage_out[3*i+2])               # ref 帧 mono depth\n        src_fe_depth_feat = torch.Tensor(src_first_stage_out[3*i+1])       # src 帧 depth feature\n        rel_pose = sample_data[’pose_matrices’][i:i+1].to(torch.float32)   # 取第 i 个相机的相对位姿\n        inv_poly_coeffs = metadata[’inv_poly_coeffs’][i:i+1].to(torch.float32) \n        affine_params = metadata[’affine_params’][i:i+1].to(torch.float32) \n        cu_cv = metadata[’cu_cvs’][i:i+1].to(torch.float32) \n        norm_lut = metadata[’lut’][i:i+1].to(torch.float32) \n        B,fh_fw,_ = norm_lut.shape\n        # step 2. differentiable homograph, build cost volume warpped features\n        init_grad = torch.arange(-1,1,0.1).reshape((1,-1,1)).repeat(B,1,fh_fw).to(fe_mono_depth.device)  # 以单目深度为中心，±1米范围内采样20个深度(步长 0.1m)\n        depth_values = fe_mono_depth.reshape((B,1,-1)) + init_grad                                       # [B,20,H/16*W/16]  所有像素的深度候选值\n        # 执行可微分 warp（2D → 3D → 2D）  对 ref 帧的 depth_feat，在多个深度平面上进行 鱼眼逆投影 + 相机运动 warp，得到 src 帧视角下的特征体积\n        # 这是整个 pipeline 的核心：将 2D 特征 lift 到 3D，并 warp 到另一视角\n        warped_volume = `inverse_warp_s_model`(fe_depth_feat,                              # 输出 warped_volume: shape (B, C, D, H’, W’)  D=20（深度平面数）\n                                            depth_values,rel_pose,\n                                            inv_poly_coeffs,affine_params,cu_cv,norm_lut)\n        C = warped_volume.shape[1]\n        # 这是标准的 feature-metric stereo matching 方法（类似 PSMNet）\n        cost_volume = torch.einsum(’nchw, ncdhw->ndhw’, src_fe_depth_feat, warped_volume) # [B,D,H/4,W/4]  计算 src 帧特征 与 warp 后特征 在每个深度平面的 点积相似度\n        cost_volume /= C    # 除以特征channel，以免数值过大      结果：cost_volume shape (B, D, H’, W’)，值越大表示该深度越可能正确\n        input_tuple2.append(src_fe_depth_feat)\n        input_tuple2.append(cost_volume)\n        input_tuple2.append(depth_values)    # 每个相机贡献 3 个输入，共 12 个张量传给第二阶段\n        # exit()\n    #-----------------------------------------------------------------\n    # 第二阶段 （Cost Volume Refinement） 对匹配的特征做多几次卷积\n    #-----------------------------------------------------------------\n    print(f’run second stage’,sep=’\\t’)\n    mv_depth_list = run_onnx_model(second_stage_onnx_model,input_tuple2)  # 输入 12 个张量（4 相机 × 3）\n    # (front_cost_filtered,                   输出：4 个 refined cost volumes（或直接是深度概率）每个是 (B, D, H’, W’)\n    # right_cost_filtered,\n    # back_cost_filtered, \n    # left_cost_filtered) = depth_feat_list   第二阶段可能是一个 3D CNN 或 2D CNN + softargmin，用于滤波和回归深度\n    #-----------------------------------------------------------------\n    # DSP操作\n    # 1. 取topk的深度\n    # 2. 投影生成BEV特征\n    #-----------------------------------------------------------------\n    ref_feature_list = [torch.Tensor(first_stage_out[i]) for i in range(0,12,3)]  # feat only\n    img_feature = torch.stack(ref_feature_list,dim=1)                             # (B, 4, C, H, W)\n    mv_depth = torch.stack([torch.Tensor(i) for i in mv_depth_list],dim=1)        # (B, 4, D, H’, W’)\n    mv_depth = mv_depth.unsqueeze(2)                                              # (B, 4, 1, D, H’, W’)\n    imgbevfea = `gen_bev_feat`(img_feature,mv_depth,metadata)          # 将多相机图像特征 + 深度分布 → 投影到 BEV 空间\n    #----------------------------------------------------------------\n    # 三阶段\n    #----------------------------------------------------------------\n    input_tuple3 = (imgbevfea,)\n    det_ret = run_onnx_model(third_stage_onnx_model,input_tuple3)[0]\n    # 解码成需要的格式\n    det_decode_ret = `bevout_decode`(det_ret) # n*9 x,y,z,l,w,h,yaw,score,label\n    return det_decode_ret\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">inverse_warp_s_model</p>可微分的、基于LUT的鱼眼逆投影与重映射(warping)用于将源帧特征“扭曲”到目标帧在多个深度平面上的视角<br>\n给定一个源图像(source_img)、一组深度假设(norm)、相机外参(extrinsic_mat)和鱼眼标定参数(invpol,affine_param,cu_cv,norm_lut)<br>\n将源图像warp到目标图像平面在N个深度上的投影结果，输出一个5D特征体(volume) 构建 cost volume 所需的 warped feature volume<br>\n你的 inverse_warp_s_model 是 BEVDepth 思想在鱼眼场景的落地实现。<br>\n(1)LUT 加速鱼眼投影<br>\nnorm_lut 预计算了每个像素的单位方向向量，避免运行时 atan2 / sqrt<br>\n极大提升推理速度（适合 J6E 等嵌入式芯片）<br>\n(2)逆多项式模型（invpol）<br>\n传统鱼眼模型：θ → r(θ)（角度到半径）<br>\n但投影需要：r → θ（半径到角度）→ 不易解析求解<br>\n解决方案：离线拟合 逆函数多项式 ρ = f(r)，运行时直接查多项式<br>\n(3)保存中间 tensor 到 bin（调试神器）<br>\nsave_tensor_to_bin(..., \'inverse_warp_s_model/xxx\', 0)<br>\n用于 ONNX 转换对齐、芯片端 debug、精度比对<br>\n工业部署必备！<br>\n<span class=\'hidden-code\' data-code=\'def inverse_warp_s_model(source_img, norm, extrinsic_mat, invpol, affine_param, cu_cv, norm_lut) -> tuple:\n        ’’’Inverse warp a source image to the target image plane for fisheye images\n        :param source_img: source image (to sample pixels from) -- [B x 3 x H x W]  源帧特征图（如 ref 帧的 depth_feat）\n        :param norm: Distance map of the target image -- [B x N x HW]    深度假设（depth values），N=20 个深度，HW=H×W 像素\n        :param extrinsic_mat: DoF pose vector from target to source -- [B x 4 x 4]  从目标帧到源帧的相对位姿（即 T_src^tgt）\n        :param K: Camera intrinsic matrix fx,fy,cx,cy -- [B x 4]\n        :param D: Camera distortion co-efficients k1, k2, k3, k4 -- [B x 4]\n        :param theta_lut: Look up table containing coords for angle in the image plane -- [B x 1 x H * W]\n        :param angle_lut: Look up table containing coords for angle of incidence -- [B x 1 x H * W]\n        :return: Projected source image -- [B x 3 x N x H x W]\n        ’’’\n        # 这里没有使用传统针孔相机的K和D，而是用LUT+多项式模型描述鱼眼几何——这是工业部署的常见做法（避免实时三角函数计算）\n        batch_size, _, height, width = source_img.size()\n        _, ndepth,_ = norm.shape\n        world_coords = `img2world_s_model`(norm, norm_lut, extrinsic_mat)  # [B,4,N,H*W] 2D+深度→3D世界坐标 源帧坐标系下的 3D 点（齐次坐标，shape [B, 4, N, HW]）\n        image_coords = `world2img_s_model`(world_coords, invpol, affine_param, cu_cv, height, width)  # [B,2,N,HW] [B,N,H,W] 3D → 源帧 2D 像素坐标\n        # 保存输入变量和输出结果到bin文件\n        save_tensor_to_bin(source_img, ’inverse_warp_s_model/source_img’, 0)\n        save_tensor_to_bin(norm, ’inverse_warp_s_model/norm’, 0)\n        save_tensor_to_bin(extrinsic_mat, ’inverse_warp_s_model/extrinsic_mat’, 0)\n        save_tensor_to_bin(invpol, ’inverse_warp_s_model/invpol’, 0)\n        save_tensor_to_bin(affine_param, ’inverse_warp_s_model/affine_param’, 0)\n        save_tensor_to_bin(cu_cv, ’inverse_warp_s_model/cu_cv’, 0)\n        save_tensor_to_bin(norm_lut, ’inverse_warp_s_model/norm_lut’, 0)\n        save_tensor_to_bin(image_coords, ’inverse_warp_s_model/image_coords’, 0)\n        padding_mode = ’zeros’\n        projected_img = `bilinear_sampler`(source_img, image_coords, mode=’bilinear’, padding_mode=padding_mode)\n        save_tensor_to_bin(projected_img, ’inverse_warp_s_model/projected_img’, 0)\n        projected_img = projected_img.view(batch_size, -1, ndepth, height, width)  # 这个体积表示：在 N 个深度假设下，源帧特征被 warp 到目标帧视角的结果\n        save_tensor_to_bin(projected_img, ’inverse_warp_s_model/projected_img_view’, 0)\n        return projected_img\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">img2world_s_model</p>将目标帧（target view）中每个像素在多个深度假设下的 2D 坐标，提升（lift）到 3D 空间，<br>\n并通过相对位姿变换到源帧（source view）的相机坐标系下<br>\n<span class=\'hidden-code\' data-code=\'def img2world_s_model(z_depth, norm_lut, essential_mat): \n    # z_depth [1,20,1280]  在鱼眼模型中，z_depth 并不是传统针孔相机中的 Z（光轴深度），而是 沿视线方向的欧氏距离（radial distance）。\n    # norm_lut [1,1280,3]\n    # essential_mat [1,4,4]\n    # return [1, 4 , 20 , 1280]\n    # angle in the image plane\n    batch_size, ndepth = z_depth.shape[:2]  # [B,N,H*W]\n    norm_lut = norm_lut.unsqueeze(1).repeat(1,ndepth,1,1)   # [B , N , H * W, 3]\n    # 计算3D点\n    x = norm_lut[:,:,:,0] * z_depth # [B , N , H * W]\n    y = norm_lut[:,:,:,1] * z_depth # [B , N , H * W]\n    x = x.view(batch_size,1,ndepth,-1)\n    y = y.view(batch_size,1,ndepth,-1)\n    z = z_depth.view(batch_size, 1, ndepth, -1)  # B x 1 x N x H * W\n    cam_coords = torch.cat((x, y, z), 1)  # B x 3 x N x H * W\n    # 最后一维加上1,构建齐次矩阵,为后面乘以essential_mat做准备\n    cam_coords = torch.cat(\n        [\n            cam_coords,\n            torch.ones(cam_coords.size(0), 1, ndepth, cam_coords.shape[3]).to(\n                device=z_depth.device\n            ),\n        ],\n        1,\n    )  # B x 4 x N x H * W\n    essential_mat = essential_mat.view(batch_size, 1, 4, 4).repeat(\n        1, ndepth, 1, 1\n    )  # B x 4 x 4 -> B x N x 4 x 4\n    # world_coords = essential_mat @ cam_coords  # B x 4 x N x H * W\n    cam_coords = cam_coords.permute(0, 2, 1, 3)  # B x N x 4 x H * W\n    world_coords = torch.matmul(essential_mat, cam_coords)  # B x N x 4 x H * W\n    world_coords = world_coords.permute(0, 2, 1, 3)  # B x 4 x N x H * W\n    # 保存输入变量和输出结果到bin文件\n    save_tensor_to_bin(z_depth, ’img2world_s_model/z_depth’, 0)\n    save_tensor_to_bin(norm_lut, ’img2world_s_model/norm_lut’, 0)\n    save_tensor_to_bin(essential_mat, ’img2world_s_model/essential_mat’, 0)\n    save_tensor_to_bin(world_coords, ’img2world_s_model/world_coords’, 0)\n    return world_coords\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">world2img_s_model</p>实现了鱼眼相机的前向投影模型，且可微分(因为invpol是参数，LUT是预计算的)<br>\n<span class=\'hidden-code\' data-code=\'def world2img_s_model(world_coords,invpol, affine_param, cu_cv, height, width):\n    # world_coords [1,4,20,1280]\n    # invpol [1,10]\n    # cu_cv [1,2]\n    # return [1,2,20,1280]\n    device = world_coords.device\n    batch_size, _, ndepth, num_pts = world_coords.shape\n    x_cam, y_cam, z = [world_coords[:, i, :, :] for i in range(3)]\n    # 计算在xy平面的模\n    xy_norm = torch.sqrt(x_cam * x_cam + y_cam * y_cam)\n    # 计算点与Z轴的夹角θ（使用反正切函数）\n    # theta = torch.arctan(-z / xy_norm)\n    theta = torch.zeros_like(xy_norm)   # [B,D,HW]\n    xy_norm_valid_mask = (xy_norm != 0)\n    theta[xy_norm_valid_mask] = torch.arctan(-z[xy_norm_valid_mask] / xy_norm[xy_norm_valid_mask])\n    # 多项式计算（替代 np.polyval(invpol[::-1], theta)）\n    # power_terms = torch.arange(len(invpol)-1, -1, -1, device=theta.device)\n    # theta_powers = theta.unsqueeze(0) ** power_terms.unsqueeze(1)\n    # rho = (torch.flip(invpol.view(-1, 1)) * theta_powers).sum(dim=0)\n    # 通过多项式计算径向距离ρ（单位：像素）\n    rho = torch.zeros_like(theta)\n    power_terms = torch.arange(invpol.shape[1], device=device)  # 高阶到低阶排列\n    for i in range(invpol.shape[1]):\n        coeff = invpol[:,i].unsqueeze(-1).unsqueeze(-1).repeat((1,ndepth,num_pts))\n        rho += coeff * (theta ** power_terms[i])\n    # 计算归一化平面坐标\n    # xy = (world_coords[:,:2,:,:] / xy_norm) * rho\n    x = (x_cam / xy_norm) * rho\n    y = (y_cam / xy_norm) * rho\n    # 应用仿射变换\n    affine_param = affine_param.unsqueeze(1).repeat(1, ndepth, 1)\n    cu_cv = cu_cv.unsqueeze(1).repeat(1, ndepth, 1)\n    x_pixel_coor = x * affine_param[:,:,0:1] + y *affine_param[:,:,1:2]  + cu_cv[:,:,0:1]\n    y_pixel_coor = x * affine_param[:,:,2:3] + y *affine_param[:,:,3:4]  + cu_cv[:,:,1:2]\n    # 归一化到-1-1之间\n    x_norm = (2 * x_pixel_coor / (width - 1) - 1).unsqueeze(1)  # B x 1 x N x H * W\n    y_norm = (2 * y_pixel_coor / (height - 1) - 1).unsqueeze(1)\n    pcoords_norm = torch.cat([x_norm, y_norm], 1)  # B x 2 x N x H*W\n    # 保存输入变量和输出结果到bin文件\n    save_tensor_to_bin(world_coords, ’world2img_s_model/world_coords’, 0)\n    save_tensor_to_bin(invpol, ’world2img_s_model/invpol’, 0)\n    save_tensor_to_bin(affine_param, ’world2img_s_model/affine_param’, 0)\n    save_tensor_to_bin(cu_cv, ’world2img_s_model/cu_cv’, 0)\n    # save_tensor_to_bin(height, ’height’, 0)\n    # save_tensor_to_bin(width, ’width’, 0)\n    save_tensor_to_bin(pcoords_norm, ’world2img_s_model/pcoords_norm’, 0)\n    return pcoords_norm\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bilinear_sampler</p>PyTorch 风格的可微分采样(类似 F.grid_sample)<br>\n给定一张源图像im和一组归一化的采样坐标flow_field(范围 [-1, 1])，<br>\n在每个坐标位置使用双线性插值从im中采样像素值，返回采样结果。<br>\n<span class=\'hidden-code\' data-code=\'def bilinear_sampler(im: torch.Tensor, flow_field: torch.Tensor, mode=’bilinear’, padding_mode=’border’) -> torch.Tensor:\n    ’’’Perform bilinear sampling on im given list of x, y coordinates.\n    Implements the differentiable sampling mechanism with bilinear kernel in https://arxiv.org/abs/1506.02025.这篇论文首次系统性地提出了 可微分图像采样（differentiable image sampling） 的思想\n    flow_field is the tensor specifying normalized coordinates [-1, 1] to be sampled on im.\n    For example, (-1, -1) in (x, y) corresponds to pixel location (0, 0) in im,\n    and (1, 1) in (x, y) corresponds to the bottom right pixel in im.\n    :param im: Batch of images with shape -- [B x 3 x H x W]\n    :param flow_field: Tensor of normalized x, y coordinates in [-1, 1], with shape -- [B x 2 x N x H*W]  第 0 维：x 坐标 ∈ [-1, 1]+第 1 维：y 坐标 ∈ [-1, 1]+N：深度平面数（如 20）+ H×W：目标图像的像素数\n    :param mode: interpolation mode to calculate output values ’bilinear’ | ’nearest’.\n    :param padding_mode: ’zeros’ use ’0’ for out-of-bound grid locations,\n           padding_mode=’border: use border values for out-of-bound grid locations,\n           padding_mode=’reflection’: use values at locations reflected by the border\n                        for out-of-bound grid locations.\n    :return: Sampled image with shape -- [B x 3 x N x H*W]\n    ’’’\n    batch_size, channels, height, width = im.shape\n    # _, _, ndepth, _ = flow_field.shape\n    # img = img.unsqueeze(2).repeat(1, 1, ndepth, 1, 1)  # B x 3 x D x H x W\n    flow_field = flow_field.permute(0, 2, 3, 1)          # → (B, N, H*W, 2)\n    output = F.grid_sample(im, flow_field, mode=mode, padding_mode=padding_mode, align_corners=False)\n    return output\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">gen_bev_feat</p>从多视角鱼眼图像特征生成 BEV（鸟瞰图）特征<br>\n几何变换、特征融合以及 BEV pooling<br>\n<span class=\'hidden-code\' data-code=\'def gen_bev_feat(img_feature,mv_depth,metadata):\n    B,N,C,fh,fw = img_feature.shape\n    img_feature = img_feature.reshape(B*N,C,fh,fw)\n    geom_xyz = `get_fisheye_geometry`(metadata,mv_depth) # -> B,N,K,H,W,3 ego坐标系下\n    # voxel_size:[ 0.5, 0.5, 10]  voxel_coord: [-30.75,-15.75,2] voxel_num: [128,64,1] 和BEV坐标系一致的x,y,z的配置\n    # 如果需要区分不同高度的物体，建议调整 voxel_size 或者增加体素层数\n    geom_xyz = ((geom_xyz - (torch.Tensor(voxel_coord) - torch.Tensor(voxel_size) / 2.0)) /\n                torch.Tensor(voxel_size)).int() # 减去边缘 voxel_coord:[-31,-16,-5] 除以单位 [0.5,0.5,10] -> B,N,D,H//16,W//16,3     ??? voxel size 最后一维为什么那么大 -->全部落在一个格子\n    # 图像特征和depth特征做个融合\n    # img_feat_with_depth = topk_depth_feat.unsqueeze(1) * img_feature.unsqueeze(2) # -> [B*N,C,K,H/16,W/16]\n    img_feat_with_depth =  img_feature.unsqueeze(2) # -> [B*N,C,K,H/16,W/16]    将图像特征扩展到深度维度，并重新排列维度以便后续操作。\n    img_feat_with_depth = img_feat_with_depth.reshape(B,N,C,1,fh,fw,) # -> b,6,64,D,18,32 float16\n    img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2).contiguous().float() # -> B,N,D,H//16,W//16,C 强转成32\n    # 得到mask，是~car_mask & lut_mask\n    normcor = metadata[’lut’].to(mv_depth.device)  # [B,N,H/16*W/16,3]\n    normcor = normcor.reshape(B,N,fh,fw,3)\n    lut_mask = normcor[...,2].view(B,N,1,fh,fw).bool()\n    car_masks = metadata[’car_mask’].to(mv_depth.device)  # [B,N,H/16*W/16]\n    car_masks = car_masks.view(B,N,1,fh,fw).bool()    \n    femask = (~car_masks) & lut_mask    # [B,N,K,H/16,W/16]\n    # femask = car_masks & lut_mask    # [B,N,K,H/16,W/16]\n    imgbevfea = `bev_pooling`(geom_xyz,img_feat_with_depth,femask) # [B,C,Y,X]\n    return imgbevfea.contiguous()       # 返回连续的 BEV 特征图，便于后续网络层处理。\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_fisheye_geometry</p>将鱼眼图像中每个像素在多个深度假设下的3D点，从相机坐标系变换到自车(ego)坐标系<br>\n根据相机参数和深度假设计算每个像素在ego坐标系下的3D坐标(X,Y,Z)。<br>\n输出形状为(B,N,K,H,W,3)，其中K是深度假设的数量。<br>\n<span class=\'hidden-code\' data-code=\'def get_fisheye_geometry(fe_mats,topk_depth):\n    ’’’Transfer points from camera coord to ego coord. topk_depth: b*6,D,18,32’’’\n    B, N,K,H,W = topk_depth.shape \n    normcor = fe_mats[’lut’].to(topk_depth.device)  # [B,N,H/16*W/16,3] 每个像素对应的归一化方向向量（单位射线）\n    normcor = normcor.reshape(B,N,H,W,3)         # 多视角深度图，shape (B, N, D, H, W)\n    #normcor = self.normcor.view(1,num_cams,fh,fw,cordim).expand(batch_size,num_cams,fh,fw,cordim) # b,6,H,W,2\n    normx = normcor[:,:,:,:,0].unsqueeze(2) # b,6,H,W -> b,6,1,H,W\n    normy = normcor[:,:,:,:,1].unsqueeze(2) # b,6,H,W -> b,6,1,H,W\n    x = normx * topk_depth # b,6,D,H,W\n    y = normy * topk_depth # b,6,D,H,W\n    padding = torch.ones_like(topk_depth)\n    points = torch.stack([x,y,topk_depth,padding],dim=-1) # b,6,D,H,W,4\n    points = points.unsqueeze(-1).float() # b,6,D,H,W,4,1\n    #fecams2ego[0].detach().cpu().numpy().tofile(’/home/jjjiang/dev/BEVOcc/bevdepth/static_files/vhcams2ego.bin’)\n    fecams2ego = fe_mats[’cam2ego’].to(topk_depth.device)\n    fecams2ego = fecams2ego.view(B,N,1,1,1,4,4) # b,6,1,1,1,4,4\n    points = fecams2ego.matmul(points) # b,6,1,1,1,4,4 * b,6,D,H,W,4,1 -> b,6,D,H,W,4,1\n    points = points.squeeze(-1)\n    return points[..., :3] # b,6,D,H,W,3 # 转到ego坐标系下\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bev_pooling</p>将来自多个相机、多个深度平面的图像特征，根据其在 BEV 空间中的体素坐标，<br>\n聚合（pooling）到统一的 BEV 特征图中<br>\n<span class=\'hidden-code\' data-code=\'def bev_pooling(geom_feats,x,mask=None):\n    # geom_feats:[Bs, 6, D, 18, 32, 3]  \n    # x: Bs,6,D,H,W,C 这里的x输入已经是并入网络深度特征后的特征。==>将所有视角、深度、像素展平为点云形式，便于后续处理\n    B, N, D, H, W, C = x.shape\n    Nprime = B * N * D * H * W #图像上的Z,Y,X排序\n    x = x.reshape(Nprime, C)\n    geom_feats = geom_feats.view(Nprime, 3)\n    batch_ix = torch.cat([torch.full([Nprime // B, 1], ix,device=x.device, dtype=torch.long) for ix in range(B)])\n    # batch_ix: [Nprime,batch_idx]                     # 为每个点打上 batch_id，确保不同样本的点不会混在一起。\n    geom_feats = torch.cat((geom_feats, batch_ix), 1)  # 最终 geom_feats 的列含义：[voxel_x, voxel_y, voxel_z, batch_idx]\n    # 增加mask计算\n    if mask is not None:\n        mask = mask.reshape(Nprime)\n        mask = mask.to(geom_feats.device)\n        geom_feats = geom_feats[mask]\n        x = x[mask]\n    # filter out points that are outside box # voxel_num # voxel_num: [128.,128.,1.]\n    kept = (geom_feats[:, 0] `>`= 0) & (geom_feats[:, 0] `<` voxel_num[0]) \\\n            & (geom_feats[:, 1] `>`= 0) & (geom_feats[:, 1] `<` voxel_num[1]) \\\n            & (geom_feats[:, 2] `>`= 0) & (geom_feats[:, 2] `<` voxel_num[2])\n    x = x[kept]\n    geom_feats = geom_feats[kept]\n    # get tensors from the same voxel next to each other\n    ranks = geom_feats[:, 0] * (voxel_num[1] * voxel_num[2] * B) \\\n            + geom_feats[:, 1] * (voxel_num[2] * B) \\\n            + geom_feats[:, 2] * B \\\n            + geom_feats[:, 3]\n    sorts = ranks.argsort()\n    # 按 B,Z,Y,X排序 确实如此\n    x, geom_feats, ranks = x[sorts], geom_feats[sorts], ranks[sorts] # [num_points,C]   [num_points,4],  [num_points,]\n    x = x.cumsum(0)\n    kept = torch.ones(x.shape[0], device=x.device, dtype=torch.bool)\n    kept[:-1] = (ranks[1:] != ranks[:-1])  # 保留落在同一个格子最后一个位置。\n    x, geom_feats = x[kept], geom_feats[kept]\n    x = torch.cat((x[:1], x[1:] - x[:-1])) # 累积的结果差就是落在格子里的特征。\n    # x, geom_feats = QuickCumsum.apply(x, geom_feats, ranks) # np,C  np,3\n    # griddify (B x C x Z x X x Y)\n    final = torch.zeros((B, C, voxel_num[2], voxel_num[1], voxel_num[0]), device=x.device)\n    final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 1], geom_feats[:, 0]] = x\n    # collapse Z\n    final = torch.cat(final.unbind(dim=2), 1) # [B,C*Z,Y,X] [B,80,128,128]\n    return final\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevout_decode</p><span class=\'hidden-code\' data-code=\'# 这里注意,如果要用来算MAP的话,这里score_threshold=0.1\ndef bevout_decode(bevout):\n    every_section = len(CLASSES)*4 # 27\n    bevout = torch.Tensor(bevout)\n    bevout = torch.split(bevout,split_size_or_sections=every_section,dim=1) # 要分成5份,5种属性\n    boxes3d_list = []\n    scores_list = []\n    labels_list = []\n    for idx,cls in enumerate(CLASSES):\n        boxes3d,scores,labels = `decode_bevout_single`(bevout,idx,cls,score_threshold= score_threshold)\n        boxes3d_list.append(boxes3d)\n        scores_list.append(scores)\n        labels_list.append(labels)\n    boxes3d = torch.cat(boxes3d_list,dim=0).detach().cpu().numpy() # nx7\n    scores = torch.cat(scores_list,dim=0).detach().cpu().numpy()   # n\n    labels = torch.cat(labels_list,dim=0).detach().cpu().numpy()   # n\n    #return boxes3d,scores,labels\n    res = np.concatenate([boxes3d,scores[:,None],labels[:,None]],axis=1)\n    return res\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">decode_bevout_single</p><span class=\'hidden-code\' data-code=\'def decode_bevout_single(bevout,outid,cls=’vehicle’,score_threshold = score_threshold):\n    # [’reg’,’height’,’dim’,’rot’,’heatmap’]\n    max_num = 500\n    global voxel_size\n    voxel_size_x = voxel_size[0]\n    voxel_size_y = voxel_size[1]\n    clsindex = CLASSES.index(cls)\n    reg = bevout[0][:,outid*2:(outid+1)*2,:,:]\n    hei = bevout[1][:,outid:(outid+1),:,:]\n    dim = bevout[2][:,outid*3:(outid+1)*3,:,:]\n    rot = bevout[3][:,outid*4:(outid+1)*4,:,:]\n    anglecls = bevout[4][:,outid*4:(outid+1)*4,:,:]  # 方向采用 4-bin 分类 + 残差回归（类似 PointPillars）+ 分类确定主方向（0: π/2, 1: π, 2: -π/2, 3: 0）+ 回归在该 bin 内微调（residual）\n    heat = bevout[5][:,outid:(outid+1),:,:].sigmoid()\n    batch, cat, _, _ = heat.size()\n    scores, inds, _, ys, xs = _topk(heat, K=max_num)\n    reg = _transpose_and_gather_feat(reg, inds)\n    reg = reg.view(batch, max_num, 2)\n    xs = xs.view(batch, max_num, 1) + reg[:, :, 0:1]\n    ys = ys.view(batch, max_num, 1) + reg[:, :, 1:2]\n    # rotation value and direction label\n    yawclses = _transpose_and_gather_feat(anglecls, inds) # B,H*W,4\n    yawclses = yawclses.view(batch, max_num, 4) # B,500,4\n    yawclses = torch.softmax(yawclses,dim=-1) # B,500,4\n    yawcls_scores,yawcls_labels = torch.max(yawclses,dim=-1) # B,500 | B,500\n    index_expanded = yawcls_labels.unsqueeze(-1) # B,500,1\n    yawregs = _transpose_and_gather_feat(rot, inds) # B,H*W,4\n    yawregs = yawregs.view(batch, max_num, 4)   # B,500,4\n    rot = torch.gather(yawregs,dim=2,index=index_expanded) # B,500,4 | B,500,1 -> B,500,1\n    # rot = torch.clamp(rot,-1.0,0)\n    rot = -rot.sigmoid()\n    sine = copy.deepcopy(rot)\n    rot = torch.asin(rot) # B,500,1\n    angle = copy.deepcopy(rot)\n    degree = rot * 180 / torch.pi\n    yawcls = copy.deepcopy(yawcls_labels.unsqueeze(-1)) # B,500,1\n    pred_yawinfos = torch.concat([yawcls,angle,degree,sine],dim=-1) # B,500,4\n    radian_map = torch.tensor([\n            torch.pi/2,  # 0 → π/2\n            torch.pi,    # 1 → π\n            -torch.pi/2, # 2 → -π/2\n            0.0          # 3 → 0\n    ], device=yawcls_labels.device)\n    refer_rot = radian_map[yawcls_labels] # B,500\n    rot = rot + refer_rot.unsqueeze(-1) # B,500,1 + B,500,1 -> B,500,1\n    # height in the bev\n    hei = _transpose_and_gather_feat(hei, inds)\n    hei = hei.view(batch, max_num, 1)\n    # dim of the box\n    dim = _transpose_and_gather_feat(dim, inds)\n    dim = dim.view(batch, max_num, 3)\n    dim = torch.exp(dim)\n    # class label\n    # clses = clses.view(batch, max_num).float()\n    clses = torch.tensor([clsindex]*max_num).view(batch, max_num).float()\n    scores = scores.view(batch, max_num)\n    xs = xs.view(batch,max_num,1) * voxel_size_x + pc_range[0]\n    ys = ys.view(batch,max_num,1) * voxel_size_y + pc_range[1]\n    final_box_preds = torch.cat([xs, ys, hei, dim, rot], dim=2)\n    final_scores = scores\n    final_preds = clses\n    # use score threshold\n    thresh_mask = final_scores >= score_threshold\n    global post_center_range\n    center_range = torch.tensor(post_center_range, device=heat.device)\n    mask = (final_box_preds[..., :3] >=center_range[:3]).all(2)\n    mask &= (final_box_preds[..., :3] <=center_range[3:]).all(2)\n    assert batch==1\n    cmask = mask[0, :]\n    cmask &= thresh_mask[0]\n    # predictions_dicts = []\n    boxes3d = final_box_preds[0, cmask]\n    scores = final_scores[0, cmask]\n    labels = final_preds[0, cmask]\n    # 使用中心点做nms\n    centers = boxes3d[:, [0, 1]]\n    boxes = torch.cat([centers, scores.view(-1, 1)], dim=1)\n    keep = torch.tensor(circle_nms(boxes.detach().cpu().numpy(),min_radius[clsindex],post_max_size=83),\n                        dtype=torch.long,device=boxes.device)\n    boxes3d = boxes3d[keep] # (N,7)\n    scores = scores[keep]\n    labels = labels[keep]\n    return boxes3d,scores,labels\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">vehicle_nms</p><span class=\'hidden-code\' data-code=\'total_chongdie = 0\n# 增加车辆的类间(小轿车、大巴和其他大车)的NMS处理\ndef vehicle_nms(decoderes):\n    before_len = len(decoderes)\n    vehicles = decoderes[np.isin(decoderes[:,-1],[0.0,2.0,4.0])]\n    non_vehicles = decoderes[~np.isin(decoderes[:,-1],[0.0,2.0,4.0])] # (n2,9)\n    vehicle_centers = vehicles[:,[0,1]]\n    vehicle_scores = vehicles[:,7:8]\n    boxes = np.concatenate([vehicle_centers, vehicle_scores], axis=1) # nx3\n    keep = circle_nms(boxes,min_radius[0],post_max_size=83)\n    vehicles = vehicles[keep] # (n1,9)\n    # print(’vehicles shape’,vehicles.shape)\n    # print(’non_vehicles shape’,non_vehicles.shape)\n    finalres = np.concatenate([vehicles,non_vehicles],axis=0)\n    after_len = len(finalres)\n    try:\n        assert before_len==after_len\n    except:\n        diff_len = before_len - after_len\n        global total_chongdie\n        total_chongdie += diff_len\n        print(f’车辆类间NMS去掉了{before_len}-{after_len}={diff_len}’)\n    return finalres\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">visual_det_and_occ</p><span class=\'hidden-code\' data-code=\'def visual_det_and_occ(metadata,frame_data,det_ret,pred_occ=None,vis_img_path=None):\n    # +-------------------+-----------+-------------------+\n    # |       左图        |   前图    |       右图        |\n    # |                   |           |                   |\n    # +---------+---------+-----------+---------+---------+\n    # |         |                               |         |\n    # |  后图   |            BEV                |  (空)   |\n    # | (翻转)  |                               |         |\n    # +---------+-------------------------------+---------+\n    # |                 [Occupancy Pred]                  |\n    # |                 [Occupancy GT]                    |\n    # +---------------------------------------------------+\n    # 得到det结果\n    boxes3d, scores, labels = (det_ret[:, :7], det_ret[:, 7], det_ret[:, 8])\n    # 过滤低于阈值的box\n    inds = scores > VISUAL_THRESHOLD\n    bboxes = boxes3d[inds]\n    scores = scores[inds]\n    labels = labels[inds]\n    # 初始化其他参数\n    # 初始化一些参数\n    cam2egos = metadata[’cam2ego’].numpy()  # 4,4,4\n    inv_poly_coeffs = metadata[’inv_poly_coeffs’].numpy()\n    affine_params = metadata[’input_affine_params’].numpy()\n    cu_cv = metadata[’input_cu_cvs’].numpy()\n    # 初始化图片以及真值\n    ref_img_paths = frame_data[’ref_img_paths’]\n    gt_boxes = frame_data[’gt_boxes’]\n    # gt_boxes = None\n    # 初始化拼图的array\n    if pred_occ is None:\n        finalArray = np.zeros((FishH*5//2,FishW*3+BEVH,3),dtype=np.uint8)\n    else:\n        finalArray = np.zeros((FishH*5//2,FishW*3+BEVH+Occ_W*OCC_SCALE,3),dtype=np.uint8)\n    # 画3D box 在图像的投影\n    imgs_with_bboxes = `visual_pred_on_camera_single_s_model`(ref_img_paths,bboxes,gt_boxes,cam2egos,inv_poly_coeffs,affine_params,cu_cv)\n    # 加到最后的图里面去\n    # finalArray[0: FishH, FishW:2*FishW,:] = imgs_with_bboxes[0]\n    # finalArray[ FishH//2: FishH*3//2,FishW*2:FishW*3,:] = imgs_with_bboxes[1]\n    # finalArray[FishH:FishH*2,FishW:FishW*2,:] = np.fliplr(imgs_with_bboxes[2])\n    # finalArray[FishH//2:FishH*3//2,0:FishW,:] = imgs_with_bboxes[3]\n    finalArray[0: FishH, FishW:2*FishW,:] = imgs_with_bboxes[0]\n    finalArray[FishH*5//4-FishH//2: FishH*5//4+FishH//2,FishW*2:FishW*3,:] = imgs_with_bboxes[1]\n    finalArray[FishH*3//2:FishH*5//2,FishW:FishW*2,:] = np.fliplr(imgs_with_bboxes[2])\n    finalArray[FishH*5//4-FishH//2: FishH*5//4+FishH//2,0:FishW,:] = imgs_with_bboxes[3]\n    # 画BEV\n    bev_img = `visual_pred_on_bev`(bboxes, gt_boxes)\n    # finalArray[FishH-BEVW//2:FishH+BEVW//2,FishW*3:FishW*3+BEVH,:] = bev_img\n    finalArray[FishH*5//4-BEVW//2:FishH*5//4+BEVW//2,FishW*3:FishW*3+BEVH,:] = bev_img\n    # 如果有occ的话,把occ也画上去\n    if pred_occ is not None:\n        vis_pred_occ = `visual_occ`(pred_occ,scale_factor=OCC_SCALE,label=’Pred’)\n        finalArray[:vis_pred_occ.shape[0],-OCC_SCALE*Occ_W:,:] = vis_pred_occ\n        if ’gt_occ’ in frame_data:\n            gt_occ = frame_data[’gt_occ’]\n            vis_gt_occ = visual_occ(gt_occ,scale_factor=OCC_SCALE,label=’GT’)\n            finalArray[-vis_gt_occ.shape[0]:,-OCC_SCALE*Occ_W:,:] = vis_gt_occ\n    if vis_img_path is not None:\n        print(f’save vis img to {vis_img_path}’)\n        cv2.imwrite(vis_img_path,finalArray)\n    return finalArray\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">visual_pred_on_camera_single_s_model</p><span class=\'hidden-code\' data-code=\'def visual_pred_on_camera_single_s_model(ref_img_paths,pred_bboxes,gt_boxes,cam2egos,inv_poly_coeffs,affine_params,cu_cv):\n    # 对每个相机都画上pred 和 gt\n    ret = []\n    for i,ref_img_path in enumerate(ref_img_paths):\n        cam2ego = cam2egos[0,i]\n        ego2cam = np.linalg.inv(cam2ego)\n        _inv_poly_coeffs = inv_poly_coeffs[i]\n        _affine_params = affine_params[i]\n        _cu_cv = cu_cv[i]\n        img = cv2.imread(ref_img_path)\n        `draw_3d_boxes_on_image_s_model`(img,gt_boxes,_affine_params,_inv_poly_coeffs,_cu_cv,ego2cam,head_color=(0,255,0),body_color=(255,0,0))\n        draw_3d_boxes_on_image_s_model(img,pred_bboxes,_affine_params,_inv_poly_coeffs,_cu_cv,ego2cam)\n        ret.append(img)\n    return ret\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">draw_3d_boxes_on_image_s_model</p><span class=\'hidden-code\' data-code=\'def draw_3d_boxes_on_image_s_model(image, boxes, affine_params,invpol,cu_cv, extrinsics=None,head_color=(0, 0, 255),body_color=(255,0,0)):\n    # get corner points\n    bboxes_corners = bboxes_to_corners(boxes)  # n*8*3\n    bboxes_corners = bboxes_corners.reshape(-1, 3)\n    # 如果有外参，将3D点转换到相机坐标系\n    if extrinsics is not None:\n        bboxes_corners_homo = np.hstack((bboxes_corners, np.ones((bboxes_corners.shape[0], 1))))\n        bboxes_corners = (extrinsics @ bboxes_corners_homo.transpose()).T[:, :3]\n        bboxes_corners = bboxes_corners.reshape((-1,8,3))\n    vertex_dict = {’head’: [[0, 1], [1, 2], [2, 3], [3, 0]],’nonhead’: [[0, 4], [1, 5], [2, 6], [3, 7], [4, 5], [5, 6], [6, 7], [7, 4]],}\n    for bbox_corners in bboxes_corners:\n        # 计算朝向面的点 和 非朝向面的点\n        headpoints, nonheadpoints = `get_points`(bbox_corners, vertex_dict)  # -> nx3,nx3\n        `draw_points_s_model`(image,headpoints,affine_params,invpol,cu_cv, head_color)  # pred朝向点红色,其他点类别颜色\n        draw_points_s_model(image,nonheadpoints,affine_params,invpol,cu_cv, body_color)\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_points</p><span class=\'hidden-code\' data-code=\'def get_points(box_corners, vertex_dict):\n    ’’’\n    Args:\n    box_corners: 8x3\n    vertex_dict = {’head’:[[0,1],[1,5],[5,4],[4,0]],’nonhead’:[[1,2],[5,6],[0,3],[4,7],[2,6],[3,7],[2,3],[6,7]]}\n    ’’’\n    head_corners = vertex_dict[’head’]\n    nonhead_corners = vertex_dict[’nonhead’]\n    headpoints = []\n    for line in head_corners:\n        points = `sample_points`(box_corners[line[0]], box_corners[line[1]])  # nx3\n        headpoints.extend(points)  # nx3\n    nonheadpoints = []\n    for line in nonhead_corners:\n        points = sample_points(box_corners[line[0]], box_corners[line[1]])  # nx3\n        nonheadpoints.extend(points)  # nx3\n    return np.array(headpoints), np.array(nonheadpoints)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">sample_points</p><span class=\'hidden-code\' data-code=\'def sample_points(p1, p2, interval=0.02):\n    # 计算两点之间的距离\n    distance = np.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2 + (p2[2] - p1[2]) ** 2)\n    # 计算需要采样的点数\n    num_points = int(distance / interval)\n    # 对于太短的线段，我们需要至少采集2个点\n    if num_points < 1:\n        num_points = 2\n    # 计算每个采样点的坐标\n    dx = (p2[0] - p1[0]) / num_points\n    dy = (p2[1] - p1[1]) / num_points\n    dz = (p2[2] - p1[2]) / num_points\n    points = []\n    # 添加采样点\n    for i in range(num_points):\n        x = round(p1[0] + i * dx, 5)\n        y = round(p1[1] + i * dy, 5)\n        z = round(p1[2] + i * dz, 5)\n        points.append([x, y, z])\n    # 添加最后一个点\n    points.append([p2[0], p2[1], p2[2]])\n    return points\n\'> </span>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">draw_points_s_model</p><span class=\'hidden-code\' data-code=\'def draw_points_s_model(image,points,affine_params,invpol,cu_cv,color):\n    c,d,e,f = affine_params\n    points = points[points[:, 2] > 0]\n    M = points.transpose()\n    npoints = M.shape[1]  # 获取点的数量\n    # 计算XY平面投影的模长（防止除零）\n    NORM = np.sqrt(M[0, :] ** 2 + M[1, :] ** 2)\n    # 处理沿Z轴的点（XY平面投影为0的情况）\n    ind0 = np.where(NORM == 0)[0]  # 找到模长为0的索引\n    if ind0.size > 0:\n        print(ind0.size)\n        NORM[ind0] = np.finfo(float).eps  # 替换为极小值避免除零\n    # 计算点与Z轴的夹角θ（使用反正切函数）\n    theta = np.arctan(-M[2, :] / NORM)\n    # theta = np.arctan2(-M[2, :] , NORM)\n    # 通过多项式计算径向距离ρ（单位：像素）\n    rho = np.polyval(invpol[::-1], theta)\n    # rho = 0\n    # t_i = 1\n    # for i in range(len(invpol)):\n    #     rho += t_i * invpol[i]\n    #     t_i *= theta\n    # 转换为归一化平面上的坐标\n    x = (M[0, :] / NORM) * rho  # X分量投影\n    y = (M[1, :] / NORM) * rho  # Y分量投影\n    # 应用仿射变换并平移至图像中心\n    m = np.zeros((2, npoints))  # 初始化输出坐标\n    m[0, :] = x * c + y * d + cu_cv[0]  # 计算最终x坐标\n    m[1, :] = x * e + y* f + cu_cv[1]  # 计算最终y坐标\n    imgpoints = [ \n        (x, y)\n        for x, y in zip(m[0,:], m[1,:])\n        # if (x `>` 0 and y `>` 0 and x `<` 640 and y `<` 512)\n        if (x `>` 0 and y `>` 0 and x `<` image.shape[1] and y `<` image.shape[0])\n    ]\n    r = 1\n    for imgpoint in imgpoints:\n        x, y = imgpoint\n        cv2.circle(image, (int(x),int(y)), 1, color, 1)\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">visual_pred_on_bev</p>这个函数首先将3D框转换为BEV视角下的2D坐标，<br>\n然后在图像上绘制这些框以及自车位置，并添加网格线以辅助观察。<br>\n<span class=\'hidden-code\' data-code=\'def visual_pred_on_bev(pred_bboxes, gt_bboxes):\n    # 转换坐标系的辅助函数\n    def convert_point(x, y, width, height):\n        img_x = ((x + XRANGE) / (2*XRANGE)) * (width - 1)\n        img_x = int(np.clip(img_x, 0, width-1))\n        img_y = ((YRANGE - y) / (2*YRANGE)) * (height - 1)\n        img_y = int(np.clip(img_y, 0, height-1))\n        return (img_x, img_y)\n    # 框坐标转换（假设已实现）\n    pred_bboxes_corner = bboxes_to_corners(pred_bboxes)  # Nx8x3\n    gt_bboxes_corner = bboxes_to_corners(gt_bboxes)      # Nx8x3\n    # 创建画布\n    width, height = BEVW, BEVH\n    img = np.full((height, width, 3), 255, dtype=np.uint8)  # 白色背景\n    # 绘制自车位置（坐标系中心）\n    ego_x, ego_y = convert_point(0, 0, width, height)\n    ego_length,ego_width = 4,1\n    ego_length = int(ego_length / (2*XRANGE) * (width - 1))\n    ego_width = int(ego_width / (2*YRANGE) * (height - 1))\n    cv2.arrowedLine(img, (ego_x, ego_y), (ego_x+ego_length, ego_y), color=(255, 0, 0), thickness=2)\n    cv2.arrowedLine(img, (ego_x, ego_y), (ego_x, ego_y-ego_width), color=(0, 0, 255), thickness=2)\n    # 颜色定义\n    BASE_COLOR = (0, 0, 255)   # 红色 (BGR)PRED_FRONT\n    GT_FRONT = (0, 255, 0)     # 绿色\n    PRED_FRONT = (255, 0, 0)   # 蓝色\n    # 绘制预测框\n    for corners in pred_bboxes_corner:\n        corners_2d = corners[:, :2]\n        # 转换坐标\n        pts = [convert_point(x, y, width, height) for x, y in corners_2d[[3,2,6,7]]]\n        # 绘制底面框\n        cv2.polylines(img, [np.array(pts, np.int32)], True, BASE_COLOR, 1,lineType=cv2.LINE_AA)\n        # 绘制前向指示线\n        center = np.mean(corners_2d[[3,2,6,7]], 0)\n        front_center = np.mean(corners_2d[[3,2]], 0)\n        pt1 = convert_point(*center, width, height)\n        pt2 = convert_point(*front_center, width, height)\n        cv2.line(img, pt1, pt2, PRED_FRONT, 2,lineType=cv2.LINE_AA)\n    # 绘制真实框\n    for corners in gt_bboxes_corner:\n        corners_2d = corners[:, :2]\n        pts = [convert_point(x, y, width, height) for x, y in corners_2d[[3,2,6,7]]]\n        cv2.polylines(img, [np.array(pts, np.int32)], True, BASE_COLOR, 1,lineType=cv2.LINE_AA)\n        center = np.mean(corners_2d[[3,2,6,7]], 0)\n        front_center = np.mean(corners_2d[[3,2]], 0)\n        pt1 = convert_point(*center, width, height)\n        pt2 = convert_point(*front_center, width, height)\n        cv2.line(img, pt1, pt2, GT_FRONT, 2,lineType=cv2.LINE_AA)\n    # 绘制网格\n    grid_color = (230, 230, 230)  # 浅灰色\n    grid_thickness = 1\n    # X轴网格线 (每0.5米一条线)\n    for x in np.arange(-9.6, 9.7, 0.5):\n        x1, y1 = convert_point(x, -8, width, height)\n        x2, y2 = convert_point(x, 8, width, height)\n        cv2.line(img, (x1, y1), (x2, y2), grid_color, grid_thickness)\n    # Y轴网格线 (每0.5米一条线)\n    for y in np.arange(-8, 8.1, 0.5):\n        x1, y1 = convert_point(-9.6, y, width, height)\n        x2, y2 = convert_point(9.6, y, width, height)\n        cv2.line(img, (x1, y1), (x2, y2), grid_color, grid_thickness)\n    # 旋转并转换颜色空间\n    # rotated = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n    rotated = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    rgb_img = cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB)\n    return rgb_img\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">visual_occ</p>接收一个二维数组occ_ret，其中每个元素代表一个特定位置的状态(例如空闲、占用或未知)<br>\n并根据这些状态为每个单元格分配不同的颜色<br>\n<span class=\'hidden-code\' data-code=\'def visual_occ(occ_ret,scale_factor=1,label=’GT’):\n    X,Y = occ_ret.shape\n    vis_pred_occ = np.zeros((X,Y,3))\n    vis_pred_occ[occ_ret==0] = (255,255,255)\n    vis_pred_occ[occ_ret==1] = (0,0,0)\n    vis_pred_occ[occ_ret==2] = (127,127,127)\n    vis_pred_occ = cv2.resize(vis_pred_occ, None, fx=scale_factor, fy=scale_factor)\n    ego_x ,ego_y = vis_pred_occ.shape[1]//2,vis_pred_occ.shape[0]//2\n    cv2.arrowedLine(vis_pred_occ, (ego_x, ego_y), (ego_x+50, ego_y), color=(0, 0, 255), thickness=2)\n    cv2.arrowedLine(vis_pred_occ, (ego_x, ego_y), (ego_x, ego_y-50), color=(255, 0, 0), thickness=2)\n    # 添加文字标签\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    if label == ’GT’:\n        cv2.putText(vis_pred_occ, label, (10, 20), font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n    elif label == ’Pred’:\n        cv2.putText(vis_pred_occ, label, (10, 20), font, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n    else:\n        cv2.putText(vis_pred_occ, label, (10, 20), font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n    vis_pred_occ = cv2.rotate(vis_pred_occ, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    return vis_pred_occ\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">eval_ret</p><span class=\'hidden-code\' data-code=\'def eval_ret(res_list,gt_list):\n    # 过滤非roi范围的目标\n    # front_x_range,back_x_range,left_y_range,right_y_range\n    res_array = np.concatenate(res_list,axis=0)\n    condition = np.logical_and(\n            (res_array[:, 0] `<`= front_x_range) & (res_array[:, 0] `>`= -back_x_range),\n            (res_array[:, 1] `<`= left_y_range) & (res_array[:, 1] `>`= -right_y_range))\n    res_array = res_array[condition]\n    gt_array = np.concatenate(gt_list,axis=0)\n    condition = np.logical_and(\n            (gt_array[:, 0] `<`= front_x_range) & (gt_array[:, 0] `>`= -back_x_range),\n            (gt_array[:, 1] `<`= left_y_range) & (gt_array[:, 1] `>`= -right_y_range))\n    gt_array = gt_array[condition]\n    cls_thresh_ap = {}\n    map_dict = {}\n    cls_err_dict = {}\n    gt_num_dict = {}\n    for cls in CLASSES:\n        clsid = CLASSES.index(cls)\n        ap_list = []\n        res_cls_array = res_array[res_array[:,-2]==clsid]\n        gt_cls_array = gt_array[gt_array[:,-2]==clsid]\n        # 第一次需要统计每个类别的GT数量。\n        gt_cls_num = len(gt_cls_array)\n        gt_num_dict[cls] = gt_cls_num\n        for dist_ths in [0.5, 1.0, 2.0, 4.0]:\n            ap,err_tmp = `compute_ap`(res_cls_array.copy(),gt_cls_array.copy(),dist_ths)\n            ap_list.append(ap)\n            if dist_ths==2.0:\n                err_dict = err_tmp\n        #print(f’{cls}:{ap_list}’)\n        cls_thresh_ap[cls] = ap_list\n        map = np.mean(np.array(ap_list))\n        map_dict[cls] = map\n        cls_err_dict[cls] = err_dict\n    print_format(gt_num_dict,cls_thresh_ap,map_dict,cls_err_dict)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">compute_ap</p><span class=\'hidden-code\' data-code=\'def compute_ap(res_cls_array,gt_cls_array,dist_ths):\n    if len(gt_cls_array)==0:\n        err_dict = {’centerdis_err’: 0,’scale_err’: 0,’orient_err’: 0}\n        return 1.0,err_dict\n    if len(res_cls_array)==0:\n        err_dict = {’centerdis_err’: 0,’scale_err’: 0,’orient_err’: 0}\n        return 0.0,err_dict\n    def center_distance(res_center,gt_center):\n        return np.linalg.norm(res_center-gt_center)\n    def scale_iou(res_size,gt_size):\n        min_wlh = np.minimum(res_size, gt_size)\n        volume_annotation = np.prod(res_size)\n        volume_result = np.prod(gt_size)\n        intersection = np.prod(min_wlh)  # type: float\n        union = volume_annotation + volume_result - intersection  # type: float\n        iou = intersection / union\n        return iou\n    def yaw_diff(res_yaw,gt_yaw,period=2*np.pi):\n        diff = (res_yaw - gt_yaw + period/2) % period - period/2\n        if diff > np.pi:\n            diff = diff - (2*np.pi)\n        diff = abs(diff)\n        return diff\n    def cummean(x: np.array) -> np.array:\n        if sum(np.isnan(x)) == len(x):\n            # Is all numbers in array are NaN’s.\n            return np.ones(len(x))  # If all errors are NaN set to error to 1 for all operating points.\n        else:\n            # Accumulate in a nan-aware manner.\n            sum_vals = np.nancumsum(x.astype(float))  # Cumulative sum ignoring nans.\n            count_vals = np.cumsum(~np.isnan(x))  # Number of non-nans up to each position.\n            return np.divide(sum_vals, count_vals, out=np.zeros_like(sum_vals), where=count_vals != 0)\n    tp = []\n    fp = []\n    conf = []\n    match_err = {’centerdis_err’: [],’scale_err’: [],’orient_err’: [],’conf’: []}\n    err_dict = {’centerdis_err’: {},’scale_err’: {},’orient_err’: {}}\n    taken = set()\n    res_cls_array = sorted(res_cls_array,key=lambda e:e[-3],reverse=True)\n    for res in res_cls_array: # nx10 中心点长宽高yaw,score,cls,sampleid\n        sampleid = res[-1]\n        min_dist = np.inf\n        match_gt_idx = None\n        gts_sample = gt_cls_array[gt_cls_array[:,-1]==sampleid]\n        for gtid,gt in enumerate(gts_sample):\n            if (sampleid,gtid) not in taken:\n                distance = center_distance(res[:2],gt[:2])\n                if distance < min_dist:\n                    min_dist = distance\n                    match_gt_idx = gtid\n        is_match = min_dist < dist_ths\n        if is_match:\n            taken.add((sampleid,match_gt_idx))\n            tp.append(1)\n            fp.append(0)\n            conf.append(res[-3])\n            gt_box_match = gts_sample[match_gt_idx]\n            match_err[’centerdis_err’].append(center_distance(res[:2],gt_box_match[:2]))\n            match_err[’scale_err’].append(1-scale_iou(res[3:6],gt_box_match[3:6]))\n            match_err[’orient_err’].append(yaw_diff(res[6],gt_box_match[6]))\n            match_err[’conf’].append(res[-3])\n        else:\n            tp.append(0)\n            fp.append(1)\n            conf.append(res[-3])\n    # Accumulate.\n    tp = np.cumsum(tp).astype(float)\n    fp = np.cumsum(fp).astype(float)\n    conf = np.array(conf)\n    # Calculate precision and recall.\n    prec = tp / (fp + tp)\n    assert len(gt_cls_array)!=0\n    rec = tp / float(len(gt_cls_array))\n    rec_interp = np.linspace(0, 1, 101)  # 101 steps, from 0% to 100% recall.\n    prec = np.interp(rec_interp, rec, prec, right=0)\n    conf = np.interp(rec_interp, rec, conf, right=0)\n    rec = rec_interp\n    for key in [’centerdis_err’,’scale_err’,’orient_err’]:\n        tmp = cummean(np.array(match_err[key]))\n        if len(tmp)==0:\n            match_err[’conf’] = [1.0,1.0]\n            tmp = np.array([1.0,1.0])\n        # Then interpolate based on the confidences. (Note reversing since np.interp needs increasing arrays)\n        match_err[key] = np.interp(conf[::-1], match_err[’conf’][::-1], tmp[::-1])[::-1]\n        non_zero = np.nonzero(conf)[0]\n        if len(non_zero) == 0:  # If there are no matches, all the confidence values will be zero.\n            max_recall_ind = 0\n        else:\n            max_recall_ind = non_zero[-1]\n        first_ind = round(100 * 0.1) + 1  # +1 to exclude the error at min recall.\n        last_ind = max_recall_ind  # First instance of confidence = 0 is index of max achieved recall.\n        if last_ind < first_ind:\n            err_tp = 1.0  # Assign 1 here. If this happens for all classes, the score for that TP metric will be 0.\n        else:\n            err_tp = float(np.mean(match_err[key][first_ind: last_ind + 1]))  # +1 to include error at max recall.\n        err_dict[key] = err_tp\n    prec = prec[round(100 * 0.1) + 1:]  # Clip low recalls. +1 to exclude the min recall bin.\n    prec -= 0.1  # Clip low precision\n    prec[prec < 0] = 0\n    ap = float(np.mean(prec)) / (1.0 - 0.1)\n    return ap,err_dict\n\'> </span>'}]}]}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
