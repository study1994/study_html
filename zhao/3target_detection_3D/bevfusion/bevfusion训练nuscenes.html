<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>bevfusion训练nuscenes</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">初始化</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseTransform(nn.Module):\n    def __init__(self,in_channels,out_channels,image_size,feature_size,xbound,ybound,zbound,dbound):\n        super().__init__()\n        self.in_channels = in_channels\n        self.image_size = image_size\n        self.feature_size = feature_size\n        self.xbound = xbound            <span style=\'color: red\'># ybound=[-54.0, 54.0, 0.3], 点云x,y,z,范围=>360x360=108/0.3</span>\n        self.ybound = ybound            <span style=\'color: red\'># ybound=[-54.0, 54.0, 0.3],</span>\n        self.zbound = zbound            <span style=\'color: red\'># zbound=[-10.0, 10.0, 20.0],  1=20/20</span>\n        self.dbound = dbound            <span style=\'color: red\'># self.dbound=[1.0, 60.0, 0.5] 1-60的深度信息，总共118个值；   </span>\n        dx, bx, nx = <span style=\'color: green;font-weight: bold;\'>gen_dx_bx</span>(self.xbound, self.ybound, self.zbound)\n        self.dx = nn.Parameter(dx, requires_grad=False)\n        self.bx = nn.Parameter(bx, requires_grad=False)\n        self.nx = nn.Parameter(nx, requires_grad=False)\n        self.C = out_channels           <span style=\'color: red\'># 80最后的结果</span>\n        self.frustum = self.<span style=\'color: green;font-weight: bold;\'>create_frustum</span>()\n        self.D = self.frustum.shape[0]    <span style=\'color: red\'># 118</span>\n        self.fp16_enabled = False\n    @force_fp32()\n    def create_frustum(self):\n        iH, iW = self.image_size         <span style=\'color: red\'># [256, 704]原始图片大小900x1600 </span>\n        fH, fW = self.feature_size       <span style=\'color: red\'># [32, 88]下采样8倍后的大小  </span>\n        ds = (torch.arange(*self.dbound, dtype=torch.float).view(-1, 1, 1).expand(-1, fH, fW))  <span style=\'color: red\'># torch.Size([118, 32, 88])可以认为对于32x88里面有从1到60间隔0.5的tensor </span>\n        D, _, _ = ds.shape\n        xs = (torch.linspace(0, iW - 1, fW, dtype=torch.float).view(1, 1, fW).expand(D, fH, fW))# torch.Size([118, 32, 88])可以认为对于32x88里面有从1到704间隔8.0805的tensor \n        ys = (torch.linspace(0, iH - 1, fH, dtype=torch.float).view(1, fH, 1).expand(D, fH, fW))# torch.Size([118, 32, 88])可以认为对于32x88里面有从1到256间隔8.2258的tensor\n        frustum = torch.stack((xs, ys, ds), -1)\n        return nn.Parameter(frustum, requires_grad=False)     <span style=\'color: red\'># torch.Size([118, 32, 88, 3])最终由118个深度信息从1-60的 ,后面3可以认为图像上的点，x,y,ds【三维度坐标】</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><font size="0"><pre class="language-python"><code class="language-python">def gen_dx_bx(xbound, ybound, zbound):\n    dx = torch.Tensor([row[2] for row in [xbound, ybound, zbound]])                         <span style=\'color: red\'># (0.4,0.4,20)</span>\n    bx = torch.Tensor([row[0] + row[2] / 2.0 for row in [xbound, ybound, zbound]])          <span style=\'color: red\'># (-51,-51,0)</span>\n    nx = torch.LongTensor([(row[1] - row[0]) / row[2] for row in [xbound, ybound, zbound]]) <span style=\'color: red\'># (51,51,0)</span>\n    return dx, bx, nx\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练过程</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/fusion_models/bevfusion.py</p><font size="0"><pre class="language-python"><code class="language-python">class BEVFusion(Base3DFusionModel):\n    def __init__(self,encoders,fuser,decoder,heads,**kwargs):\n        super().__init__()          <span style=\'color: red\'># [sensor for sensor in self.encoders]=[\'camera\', \'lidar\'] </span>\n    @auto_fp16(apply_to=("img", "points"))\n    def forward(\n        self,\n        img=None,\n        points=None,\n        camera2ego=None,\n        lidar2ego=None,\n        lidar2camera=None,\n        lidar2image=None,\n        camera_intrinsics=None,\n        camera2lidar=None,\n        img_aug_matrix=None,\n        lidar_aug_matrix=None,\n        metas=None,\n        gt_masks_bev=None,\n        gt_bboxes_3d=None,\n        gt_labels_3d=None,\n        **kwargs,\n    ):\n        if isinstance(img, list):\n            raise NotImplementedError\n        else:\n            outputs = self.forward_single(....)\n            return outputs\n    @auto_fp16(apply_to=("img", "points"))\n    def forward_single():\n        features = []\n        for sensor in (self.encoders if self.training else list(self.encoders.keys())[::-1]):\n            if sensor == "camera":\n                feature = self.<span style=\'color: green;font-weight: bold;\'>extract_camera_features</span>(...)\n            elif sensor == "lidar":\n                feature = self.<span style=\'color: green;font-weight: bold;\'>extract_lidar_features</span>(points)  <span style=\'color: red\'># 点云(-54.0,-54.0,-5.0,54.0,54.0,3.0)-([0.075,0.075,0.2)=(1440,1440,40)->(180,180,256)</span>\n            else:\n                raise ValueError(f"unsupported sensor: {sensor}")\n            features.append(feature)\n        if not self.training:\n            <span style=\'color: red\'># avoid OOM</span>\n            features = features[::-1]\n        if self.fuser is not None:\n            x = self.<span style=\'color: green;font-weight: bold;\'>fuser</span>(features)   <span style=\'color: red\'># [i.shape for i in features]=[(2, 80, 180, 180), (2, 256, 180, 180)]->(2, 256, 180, 180)  </span>\n        else:\n            assert len(features) == 1, features\n            x = features[0]\n        ......\n</code></pre></font>\nbackbone:(2, 128, 180, 180)+(2, 256, 90, 90)<br>\nneck:[(2, 512, 180, 180)]<br>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/fusion_models/bevfusion.py</p><font size="0"><pre class="language-python"><code class="language-python">class BEVFusion(Base3DFusionModel):\n    def extract_camera_features():\n        B, N, C, H, W = x.size()        <span style=\'color: red\'># B, N, C, H, W=(2, 6, 3, 256, 704)-->1600x900(0.48):768x432 </span>\n        x = x.view(B * N, C, H, W)\n        x = self.<span style=\'color: green;font-weight: bold;\'>encoders</span>["camera"]["backbone"](x) <span style=\'color: red\'># [i.shape for i in x]=[(12,192,32,88),(12,384,16,44),(12,768,8,22)]分别下采样8,16,32倍  SwinTransformer</span>\n        x = self.<span style=\'color: green;font-weight: bold;\'>encoders</span>["camera"]["neck"](x)     <span style=\'color: red\'># [i.shape for i in x]=[(12,256,32,88),(12,256,16,44)]                                  GeneralizedLSSFPN</span>\n        if not isinstance(x, torch.Tensor):\n            x = x[0]                                 <span style=\'color: red\'># (12,256,32,88)</span>\n        BN, C, H, W = x.size()\n        x = x.view(B, int(BN / B), C, H, W)           <span style=\'color: red\'># (2,6,256,32,88)  这里用到的是下采样8倍的值</span>\n        x = self.<span style=\'color: green;font-weight: bold;\'>encoders</span>["camera"]["vtransform"]() <span style=\'color: red\'># class DepthLSSTransform:def forward【这里面的参数包括原始点云数据】  </span>\n        return x\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_image/swin.py</p><font size="0"><pre class="language-python"><code class="language-python">class SwinTransformer(nn.Module):\n    def forward(self, batch_dict):\n        x = batch_dict[\'camera_imgs\']                 <span style=\'color: red\'># torch.Size([1, 6, 3, 360, 640]) 输入到图片大小，nuscenes是256, 704</span>\n        B, N, C, H, W = x.size()\n        x = x.view(B * N, C, H, W)                    <span style=\'color: red\'># torch.Size([6, 3, 360, 640])</span>\n        x, hw_shape = self.patch_embed(x)             <span style=\'color: red\'># torch.Size([6, 14400, 96]); (90, 160)</span>\n        if self.use_abs_pos_embed:                    <span style=\'color: red\'># False</span>\n            x = x + self.absolute_pos_embed\n        x = self.drop_after_pos(x)                    <span style=\'color: red\'># torch.Size([6, 14400, 96])</span>\n        outs = []\n        for i, stage in enumerate(self.stages):\n            x, hw_shape, out, out_hw_shape = stage(x, hw_shape)    <span style=\'color: red\'># torch.Size([6, 3600, 192]); (45, 80); torch.Size([6, 14400, 96]); (90, 160)</span>\n            if i in self.out_indices:                              <span style=\'color: red\'># [1, 2, 3] Fasle,0不在里面</span>\n                norm_layer = getattr(self, f\'norm{i}\')\n                out = norm_layer(out)\n                out = out.view(-1, *out_hw_shape,\n                               self.num_features[i]).permute(0, 3, 1,\n                                                             2).contiguous()\n                outs.append(out)\n        batch_dict[\'image_features\'] = outs       <span style=\'color: red\'># torch.Size([6, 192, 45, 80])x8 torch.Size([6, 384, 23, 40])x16 , torch.Size([6, 768, 12, 20])x32</span>\n        return batch_dict\n</code></pre></font>\nopenpcdet自己的图片<br>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">i=1,2,3</p><font size="0"><pre class="language-python"><code class="language-python">for i, stage in enumerate(self.stages):\n    x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n    if i in self.out_indices:\n        norm_layer = getattr(self, f\'norm{i}\')\n        out = norm_layer(out)\n        out = out.view(-1, *out_hw_shape,self.num_features[i]).permute(0, 3, 1,2).contiguous()\n        outs.append(out)\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_image/img_neck/generalized_lss.py</p><font size="0"><pre class="language-python"><code class="language-python">class GeneralizedLSSFPN(nn.Module):\n        def forward(self, batch_dict):\n        <span style=\'color: red\'># upsample -> cat -> conv1x1 -> conv3x3</span>\n        inputs = batch_dict[\'image_features\']       <span style=\'color: red\'># torch.Size([6, 192, 45, 80])x8 torch.Size([6, 384, 23, 40])x16 , torch.Size([6, 768, 12, 20])x32</span>\n        assert len(inputs) == len(self.in_channels) <span style=\'color: red\'># [192, 384, 768]</span>\n        <span style=\'color: red\'># build laterals                           </span>\n        laterals = [inputs[i + self.start_level] for i in range(len(inputs))] \n        <span style=\'color: red\'># build top-down path</span>\n        used_backbone_levels = len(laterals) - 1   <span style=\'color: red\'># 2</span>\n        for i in range(used_backbone_levels - 1, -1, -1):     <span style=\'color: red\'># 1,0</span>\n            x = F.interpolate(\n                laterals[i + 1],                              <span style=\'color: red\'># torch.Size([6, 768, 12, 20])</span>\n                size=laterals[i].shape[2:],                   <span style=\'color: red\'># (23, 40)</span>\n                mode=\'bilinear\', align_corners=False,\n            )\n            laterals[i] = torch.cat([laterals[i], x], dim=1)  <span style=\'color: red\'># torch.Size([6, 768, 23, 40])->torch.Size([6, 1152, 23, 40])</span>\n            laterals[i] = self.lateral_convs[i](laterals[i])  <span style=\'color: red\'># torch.Size([6, 256, 23, 40])</span>\n            laterals[i] = self.fpn_convs[i](laterals[i])\n        <span style=\'color: red\'># build outputs</span>\n        outs = [laterals[i] for i in range(used_backbone_levels)]\n        batch_dict[\'image_fpn\'] = tuple(outs)      <span style=\'color: red\'># torch.Size([6, 256, 45, 80])  torch.Size([6, 256, 23, 40])</span>\n        return batch_dict\n</code></pre></font>\nopenpcdet自己的图片<br>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/depth_lss.py</p><font size="0"><pre class="language-python"><code class="language-python">@VTRANSFORMS.register_module()\nclass DepthLSSTransform(BaseDepthTransform):\n    def forward(self, *args, **kwargs):\n        x = super().<span style=\'color: green;font-weight: bold;\'>forward</span>(*args, **kwargs)      <span style=\'color: red\'># 返回的(2, 80, 360, 360) 输入图像特征进去</span>\n        x = self.downsample(x)                      <span style=\'color: red\'># self.downsample(x)=>(2, 80, 180, 180)  </span>\n        return x\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseDepthTransform(BaseTransform):\n    @force_fp32()\n    def forward(\n        self,\n        img,\n        points,            <span style=\'color: red\'># [(243766, 5), (255017, 5)]</span>\n        sensor2ego,        <span style=\'color: red\'># 没用到，camera2ego=(2, 6, 4, 4)</span>\n        lidar2ego,         <span style=\'color: red\'># 没用到，lidar2ego=(2, 4, 4)；</span>\n        lidar2camera,      <span style=\'color: red\'># 雷达到相机，没用到   (2, 6, 4, 4)；</span>\n        lidar2image,       <span style=\'color: red\'># lidar2image=(2, 6, 4, 4)；     雷达到图像坐标系(包括外参+内参+矫正系数了) data[\'img_aug_matrix\'].data[0][0]</span>\n        cam_intrinsic,     <span style=\'color: red\'># 没用到 camera_intrinsics=(2, 6, 4, 4)；</span>\n        camera2lidar,\n        img_aug_matrix,    <span style=\'color: red\'># img_aug_matrix=(2, 6, 4, 4)；  图片增强矩阵  data[\'img_aug_matrix\'].data[0][0]</span>\n        lidar_aug_matrix,  <span style=\'color: red\'># lidar_aug_matrix=(2, 4, 4)     雷达增强矩阵  data[\'lidar_aug_matrix\'].data[0][0]</span>\n        metas,\n        **kwargs,\n    ):\n        rots = sensor2ego[..., :3, :3]\n        trans = sensor2ego[..., :3, 3]\n        intrins = cam_intrinsic[..., :3, :3]\n        post_rots = img_aug_matrix[..., :3, :3]\n        post_trans = img_aug_matrix[..., :3, 3]\n        lidar2ego_rots = lidar2ego[..., :3, :3]\n        lidar2ego_trans = lidar2ego[..., :3, 3]\n        camera2lidar_rots = camera2lidar[..., :3, :3]\n        camera2lidar_trans = camera2lidar[..., :3, 3]\n        <span style=\'color: red\'># print(img.shape, self.image_size, self.feature_size)</span>\n        batch_size = len(points)      <span style=\'color: red\'># 在原图上的点云投到(256,704)上，因为图片数据增强那里应该涉及到了下采样的倍数【输入到网络图片的大小是256x704,图片增强这里处理了点云】</span>\n        depth = torch.zeros(batch_size, img.shape[1], 1, *self.image_size).to(points[0].device)  <span style=\'color: red\'># depth.shape=(2, 6, 1, 256, 704) 获取每张图片的深度</span>\n        for b in range(batch_size):\n            cur_coords = points[b][:, :3]                    <span style=\'color: red\'># torch.Size([287993, 3])</span>\n            cur_img_aug_matrix = img_aug_matrix[b]           <span style=\'color: red\'># torch.Size([6, 4, 4])</span>\n            cur_lidar_aug_matrix = lidar_aug_matrix[b]       <span style=\'color: red\'># torch.Size([4, 4])</span>\n            cur_lidar2image = lidar2image[b]                 <span style=\'color: red\'># torch.Size([6, 4, 4])</span>\n            <span style=\'color: red\'># inverse aug         cur_coords=(6, 3, 243766)雷达坐标系-》图像坐标系到图像增强的坐标---》深度</span>\n            cur_coords -= cur_lidar_aug_matrix[:3, 3]        <span style=\'color: red\'># torch.Size([287993, 3])-torch.Size([3])</span>\n            cur_coords = torch.inverse(cur_lidar_aug_matrix[:3, :3]).matmul(cur_coords.transpose(1, 0))  #(3,3)*(3*287993)=(3,287993)\n            <span style=\'color: red\'># lidar2image</span>\n            cur_coords = cur_lidar2image[:, :3, :3].matmul(cur_coords)     <span style=\'color: red\'># (6,3,3)*(3,287993)=>torch.Size([6, 3, 287993])</span>\n            cur_coords += cur_lidar2image[:, :3, 3].reshape(-1, 3, 1)      <span style=\'color: red\'># torch.Size([6, 3, 287993])+torch.Size([6, 3, 1])=torch.Size([6, 3, 287993])</span>\n            <span style=\'color: red\'># get 2d coords</span>\n            dist = cur_coords[:, 2, :]                                     <span style=\'color: red\'># torch.Size([6, 287993])</span>\n            cur_coords[:, 2, :] = torch.clamp(cur_coords[:, 2, :], 1e-5, 1e5)\n            cur_coords[:, :2, :] /= cur_coords[:, 2:3, :]                  <span style=\'color: red\'># torch.Size([6, 2, 287993])/torch.Size([6, 1, 287993])=torch.Size([6, 2, 287993])</span>\n            <span style=\'color: red\'># imgaug</span>\n            cur_coords = cur_img_aug_matrix[:, :3, :3].matmul(cur_coords)  <span style=\'color: red\'># (6,3,3)*(6,3,287993)=torch.Size([6, 3, 287993])</span>\n            cur_coords += cur_img_aug_matrix[:, :3, 3].reshape(-1, 3, 1)   <span style=\'color: red\'># torch.Size([6, 3, 287993])+torch.Size([6, 1, 287993])</span>\n            cur_coords = cur_coords[:, :2, :].transpose(1, 2)              <span style=\'color: red\'># torch.Size([6, 287993, 2])</span>\n            <span style=\'color: red\'># normalize coords for grid sample</span>\n            cur_coords = cur_coords[..., [1, 0]] <span style=\'color: red\'># image_size为(H,W)</span>\n            on_img = ((cur_coords[..., 0] < self.image_size[0]) & (cur_coords[..., 0] >= 0) & (cur_coords[..., 1] < self.image_size[1]) & (cur_coords[..., 1] >= 0))\n            for c in range(on_img.shape[0]):\n                masked_coords = cur_coords[c, on_img[c]].long()\n                masked_dist = dist[c, on_img[c]]\n                depth[b, c, 0, masked_coords[:, 0], masked_coords[:, 1]] = masked_dist\n        extra_rots = lidar_aug_matrix[..., :3, :3]\n        extra_trans = lidar_aug_matrix[..., :3, 3]\n        geom = self.<span style=\'color: green;font-weight: bold;\'>get_geometry</span>(camera2lidar_rots,camera2lidar_trans,intrins,post_rots,post_trans,extra_rots=extra_rots,extra_trans=extra_trans) \n        x = self.<span style=\'color: green;font-weight: bold;\'>get_cam_feats</span>(img, depth)    <span style=\'color: red\'># 图像特征torch.Size([2, 6, 256, 32, 88])与图像坐标系里面的弄进来的深度特征torch.Size([2, 6, 1, 256, 704]) </span>\n                                                <span style=\'color: red\'># bevdet直接对图像特征做卷积，bevDepth做了些操作，都是通过卷积生成深度</span>\n        x = self.<span style=\'color: green;font-weight: bold;\'>bev_pool</span>(geom, x)\n        return x\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseTransform(nn.Module):\n    @force_fp32()\n    def get_geometry(\n        self,\n        camera2lidar_rots,  <span style=\'color: red\'># torch.Size([1, 6, 3, 3])<span style=\'color: green;font-weight: bold;\'>相机到雷达</span></span>\n        camera2lidar_trans, <span style=\'color: red\'># torch.Size([1, 6, 3])</span>\n        intrins,            <span style=\'color: red\'># torch.Size([1, 6, 3, 3])</span>\n        post_rots,          <span style=\'color: red\'># torch.Size([1, 6, 3, 3])</span>\n        post_trans,         <span style=\'color: red\'># torch.Size([1, 6, 3])</span>\n        **kwargs,\n    ):\n        B, N, _ = camera2lidar_trans.shape\n        <span style=\'color: red\'># undo post-transformation B x N x D x H x W x 3</span>\n        <span style=\'color: red\'># 可以认为self.frustum是图像上的点，到增强前图像上的点，到相机坐标系，到ego，到雷达坐标系【后面3为3维坐标的值】</span>\n        points = self.frustum - post_trans.view(B, N, 1, 1, 1, 3)  <span style=\'color: red\'># 从增强后的点到增强前图片的点:[1, 6, 196, 40, 80, 3]->[1, 6, 196, 40, 80, 3] </span>\n        points = (torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3).matmul(points.unsqueeze(-1)))     <span style=\'color: red\'># points.unsqueeze(-1)=torch.Size([1, 6, 196, 40, 80, 3, 1])</span>\n        <span style=\'color: red\'># cam_to_lidar  像素的2D像素坐标以及深度值，再加上相机的内参以及外参，即可计算得出像素对应的在车身坐标系中的3D坐标。</span>\n        <span style=\'color: red\'># torch.Size([1, 6, 196, 40, 80, 2, 1])*torch.Size([1, 6, 196, 40, 80, 1, 1]),torch.Size([1, 6, 196, 40, 80, 1, 1])</span>\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],points[:, :, :, :, :, 2:3],),5,)  <span style=\'color: red\'># 有深度值的点云</span>\n        combine = camera2lidar_rots.matmul(torch.inverse(intrins))      <span style=\'color: red\'># torch.Size([1, 6, 3, 3])@torch.Size([1, 6, 3, 3])=torch.Size([1, 6, 3, 3])</span>\n        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)   <span style=\'color: red\'># 得到torch.Size([1, 6, 196, 40, 80, 3])</span>\n        points += camera2lidar_trans.view(B, N, 1, 1, 1, 3)      <span style=\'color: red\'># torch.Size([1, 6, 196, 40, 80, 3])</span>\n        if "extra_rots" in kwargs:                               <span style=\'color: red\'># 点云增强</span>\n            extra_rots = kwargs["extra_rots"]     <span style=\'color: red\'># torch.Size([1, 3, 3])</span>\n            points = (extra_rots.view(B, 1, 1, 1, 1, 3, 3).repeat(1, N, 1, 1, 1, 1, 1).matmul(points.unsqueeze(-1)).squeeze(-1))\n        if "extra_trans" in kwargs:\n            extra_trans = kwargs["extra_trans"]   <span style=\'color: red\'># torch.Size([1, 3])</span>\n            points += extra_trans.view(B, 1, 1, 1, 1, 3).repeat(1, N, 1, 1, 1, 1)\n        return points                             <span style=\'color: red\'># torch.Size([1, 6, 196, 40, 80, 3])</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/depth_lss.py</p><font size="0"><pre class="language-python"><code class="language-python">@VTRANSFORMS.register_module()\nclass DepthLSSTransform(BaseDepthTransform):\n    @force_fp32()\n    def get_cam_feats(self, x, d):\n        B, N, C, fH, fW = x.shape             <span style=\'color: red\'># x-img:(2,6,256,32,88)->(12, 256, 32, 88) 32x88是输入网络分辨率256x704下采样8倍后的值</span>\n        d = d.view(B * N, *d.shape[2:])       <span style=\'color: red\'># 这里的深度是神经网络输入的深度</span>\n        x = x.view(B * N, C, fH, fW)\n        d = self.<span style=\'color: green;font-weight: bold;\'>dtransform</span>(d)          <span style=\'color: red\'># 一系列卷积BN池化=》d-depth:(2, 6, 1, 256, 704)->(12, 1, 256, 704)->{self.dtransform}->(12,64,32,88)；64可以认为每个点的深度的特征维度</span>\n        x = torch.cat([d, x], dim=1)    \n        x = self.<span style=\'color: green;font-weight: bold;\'>depthnet</span>(x)            <span style=\'color: red\'># 一系列卷积BN池化=》(12, 320=256+64, 32, 88)->(12, 198, 32, 88)  198=80+118</span>\n        depth = x[:, : self.D].softmax(dim=1)\n        <span style=\'color: red\'># (12, 118, 32, 88)-{depth.unsqueeze(1)}-(12, 1, 118, 32, 88)+x[:, self.D : (self.D + self.C)].unsqueeze(2).shape=(12, 80, 1, 32, 88)  ；0-118是深度0-60并且间隔为0.5的取值，每个值有80个特征？</span>\n        x = depth.unsqueeze(1) * x[:, self.D : (self.D + self.C)].unsqueeze(2)  <span style=\'color: red\'># 深度值*特征 = 2D特征转变为3D空间(俯视图)内的特征-(12, 80, 118, 32, 88)</span>\n        x = x.view(B, N, self.C, self.D, fH, fW)    <span style=\'color: red\'># (2,6,80,118,32,88)</span>\n        x = x.permute(0, 1, 3, 4, 5, 2)             <span style=\'color: red\'># x-(2,6,118,32,88,80)-(B, N, self.D, fH, fW, self.C) 对应bevdet里面的self.D=59/118；self.C=16/80</span>\n        return x\n</code></pre></font>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseTransform(nn.Module):\n    @force_fp32()\n    def bev_pool(self, geom_feats, x):\n        B, N, D, H, W, C = x.shape\n        Nprime = B * N * D * H * W       <span style=\'color: red\'># Nprime=3987456=2,6,118,88,80   </span>\n        <span style=\'color: red\'># flatten x</span>\n        x = x.reshape(Nprime, C)         <span style=\'color: red\'># torch.Size([3987456, 80])【self.C=80】  </span>\n        <span style=\'color: red\'># flatten indices                  self.bx=tensor([-53.8500, -53.8500,   0.0000])；self.dx=tensor([0.3000,  0.3000, 20.0000])；self.dx=tensor([0.3000,  0.3000, 20.0000]) </span>\n        geom_feats = ((geom_feats - (self.bx - self.dx / 2.0)) / self.dx).long()\n        geom_feats = geom_feats.view(Nprime, 3)     <span style=\'color: red\'># geom_feats.shape=torch.Size([2, 6, 118, 32, 88, 3])-> torch.Size([3987456, 3])->(3987456, 4)</span>\n        batch_ix = torch.cat([torch.full([Nprime <span style=\'color: red\'>// B, 1], ix, device=x.device, dtype=torch.long) for ix in range(B)])</span>\n        geom_feats = torch.cat((geom_feats, batch_ix), 1)\n        <span style=\'color: red\'># filter out points that are outside box</span>\n        kept = ((geom_feats[:, 0] >= 0) & (geom_feats[:, 0] < self.nx[0]) & (geom_feats[:, 1] >= 0)\n            & (geom_feats[:, 1] < self.nx[1]) & (geom_feats[:, 2] >= 0) & (geom_feats[:, 2] < self.nx[2]))\n        x = x[kept]\n        geom_feats = geom_feats[kept]          <span style=\'color: red\'># geom_feats->(3667059, 4)  </span>\n        x = <span style=\'color: green;font-weight: bold;\'>bev_pool</span>(x, geom_feats, B, self.nx[2], self.nx[0], self.nx[1])     <span style=\'color: red\'># self.nx=tensor([360, 360,   1], device=\'cuda:0\')</span>\n        <span style=\'color: red\'># collapse Z</span>\n        final = torch.cat(x.unbind(dim=2), 1)\n        return final        <span style=\'color: red\'># final->(2, 80, 360, 360) </span>\n</code></pre><p></font>\n<code>mmdet3d/ops/bev_pool/bev_pool.py</code>:<code>def bev_pool</code><br>\nranks.shape=torch.Size([3667059]);<br>\nx-&gt;(2, 80, 1, 360, 360)<br></p>'}]}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/fusers/conv.py</p><font size="0"><pre class="language-python"><code class="language-python">class ConvFuser(nn.Sequential):\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        super().__init__(\n            nn.Conv2d(sum(in_channels), out_channels, 3, padding=1, bias=False),   <span style=\'color: red\'># 80+256的通道到256</span>\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n        return super().forward(torch.cat(inputs, dim=1))\n</code></pre></font>'}]}]}]})</script></body>
</html>
