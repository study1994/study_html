<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>bevfusion训练nuscenes</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">初始化</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><span class=\'hidden-code\' data-code=\'class BaseTransform(nn.Module):\n    def __init__(self,in_channels,out_channels,image_size,feature_size,xbound,ybound,zbound,dbound):\n        super().__init__()\n        self.in_channels = in_channels\n        self.image_size = image_size\n        self.feature_size = feature_size\n        self.xbound = xbound            # ybound=[-54.0, 54.0, 0.3], 点云x,y,z,范围=>360x360=108/0.3\n        self.ybound = ybound            # ybound=[-54.0, 54.0, 0.3],\n        self.zbound = zbound            # zbound=[-10.0, 10.0, 20.0],  1=20/20\n        self.dbound = dbound            # self.dbound=[1.0, 60.0, 0.5] 1-60的深度信息，总共118个值；   \n        dx, bx, nx = `gen_dx_bx`(self.xbound, self.ybound, self.zbound)\n        self.dx = nn.Parameter(dx, requires_grad=False)\n        self.bx = nn.Parameter(bx, requires_grad=False)\n        self.nx = nn.Parameter(nx, requires_grad=False)\n        self.C = out_channels           # 80最后的结果\n        self.frustum = self.`create_frustum`()\n        self.D = self.frustum.shape[0]    # 118\n        self.fp16_enabled = False\n    @force_fp32()\n    def create_frustum(self):\n        iH, iW = self.image_size         # [256, 704]原始图片大小900x1600 \n        fH, fW = self.feature_size       # [32, 88]下采样8倍后的大小  \n        ds = (torch.arange(*self.dbound, dtype=torch.float).view(-1, 1, 1).expand(-1, fH, fW))  # torch.Size([118, 32, 88])可以认为对于32x88里面有从1到60间隔0.5的tensor \n        D, _, _ = ds.shape\n        xs = (torch.linspace(0, iW - 1, fW, dtype=torch.float).view(1, 1, fW).expand(D, fH, fW))# torch.Size([118, 32, 88])可以认为对于32x88里面有从1到704间隔8.0805的tensor \n        ys = (torch.linspace(0, iH - 1, fH, dtype=torch.float).view(1, fH, 1).expand(D, fH, fW))# torch.Size([118, 32, 88])可以认为对于32x88里面有从1到256间隔8.2258的tensor\n        frustum = torch.stack((xs, ys, ds), -1)\n        return nn.Parameter(frustum, requires_grad=False)     # torch.Size([118, 32, 88, 3])最终由118个深度信息从1-60的 ,后面3可以认为图像上的点，x,y,ds【三维度坐标】\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><span class=\'hidden-code\' data-code=\'def gen_dx_bx(xbound, ybound, zbound):\n    dx = torch.Tensor([row[2] for row in [xbound, ybound, zbound]])                         # (0.4,0.4,20)\n    bx = torch.Tensor([row[0] + row[2] / 2.0 for row in [xbound, ybound, zbound]])          # (-51,-51,0)\n    nx = torch.LongTensor([(row[1] - row[0]) / row[2] for row in [xbound, ybound, zbound]]) # (51,51,0)\n    return dx, bx, nx\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练过程</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/fusion_models/bevfusion.py</p><span class=\'hidden-code\' data-code=\'class BEVFusion(Base3DFusionModel):\n    def __init__(self,encoders,fuser,decoder,heads,**kwargs):\n        super().__init__()          # [sensor for sensor in self.encoders]=[&amp;#39;camera&amp;#39;, &amp;#39;lidar&amp;#39;] \n    @auto_fp16(apply_to=(&amp;#39;img&amp;#39;, &amp;#39;points&amp;#39;))\n    def forward(\n        self,\n        img=None,\n        points=None,\n        camera2ego=None,\n        lidar2ego=None,\n        lidar2camera=None,\n        lidar2image=None,\n        camera_intrinsics=None,\n        camera2lidar=None,\n        img_aug_matrix=None,\n        lidar_aug_matrix=None,\n        metas=None,\n        gt_masks_bev=None,\n        gt_bboxes_3d=None,\n        gt_labels_3d=None,\n        **kwargs,\n    ):\n        if isinstance(img, list):\n            raise NotImplementedError\n        else:\n            outputs = self.forward_single(....)\n            return outputs\n    @auto_fp16(apply_to=(&amp;#39;img&amp;#39;, &amp;#39;points&amp;#39;))\n    def forward_single():\n        features = []\n        for sensor in (self.encoders if self.training else list(self.encoders.keys())[::-1]):\n            if sensor == &amp;#39;camera&amp;#39;:\n                feature = self.`extract_camera_features`(...)\n            elif sensor == &amp;#39;lidar&amp;#39;:\n                feature = self.`extract_lidar_features`(points)  # 点云(-54.0,-54.0,-5.0,54.0,54.0,3.0)-([0.075,0.075,0.2)=(1440,1440,40)->(180,180,256)\n            else:\n                raise ValueError(f&amp;#39;unsupported sensor: {sensor}&amp;#39;)\n            features.append(feature)\n        if not self.training:\n            # avoid OOM\n            features = features[::-1]\n        if self.fuser is not None:\n            x = self.`fuser`(features)   # [i.shape for i in features]=[(2, 80, 180, 180), (2, 256, 180, 180)]->(2, 256, 180, 180)  \n        else:\n            assert len(features) == 1, features\n            x = features[0]\n        ......\n\'> </span>\nbackbone:(2, 128, 180, 180)+(2, 256, 90, 90)<br>\nneck:[(2, 512, 180, 180)]<br>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/fusion_models/bevfusion.py</p><span class=\'hidden-code\' data-code=\'class BEVFusion(Base3DFusionModel):\n    def extract_camera_features():\n        B, N, C, H, W = x.size()        # B, N, C, H, W=(2, 6, 3, 256, 704)-->1600x900(0.48):768x432 \n        x = x.view(B * N, C, H, W)\n        x = self.`encoders`[&amp;#39;camera&amp;#39;][&amp;#39;backbone&amp;#39;](x) # [i.shape for i in x]=[(12,192,32,88),(12,384,16,44),(12,768,8,22)]分别下采样8,16,32倍  SwinTransformer\n        x = self.`encoders`[&amp;#39;camera&amp;#39;][&amp;#39;neck&amp;#39;](x)     # [i.shape for i in x]=[(12,256,32,88),(12,256,16,44)]                                  GeneralizedLSSFPN\n        if not isinstance(x, torch.Tensor):\n            x = x[0]                                 # (12,256,32,88)\n        BN, C, H, W = x.size()\n        x = x.view(B, int(BN / B), C, H, W)           # (2,6,256,32,88)  这里用到的是下采样8倍的值\n        x = self.`encoders`[&amp;#39;camera&amp;#39;][&amp;#39;vtransform&amp;#39;]() # class DepthLSSTransform:def forward【这里面的参数包括原始点云数据】  \n        return x\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_image/swin.py</p><span class=\'hidden-code\' data-code=\'class SwinTransformer(nn.Module):\n    def forward(self, batch_dict):\n        x = batch_dict[&amp;#39;camera_imgs&amp;#39;]                 # torch.Size([1, 6, 3, 360, 640]) 输入到图片大小，nuscenes是256, 704\n        B, N, C, H, W = x.size()\n        x = x.view(B * N, C, H, W)                    # torch.Size([6, 3, 360, 640])\n        x, hw_shape = self.patch_embed(x)             # torch.Size([6, 14400, 96]); (90, 160)\n        if self.use_abs_pos_embed:                    # False\n            x = x + self.absolute_pos_embed\n        x = self.drop_after_pos(x)                    # torch.Size([6, 14400, 96])\n        outs = []\n        for i, stage in enumerate(self.stages):\n            x, hw_shape, out, out_hw_shape = stage(x, hw_shape)    # torch.Size([6, 3600, 192]); (45, 80); torch.Size([6, 14400, 96]); (90, 160)\n            if i in self.out_indices:                              # [1, 2, 3] Fasle,0不在里面\n                norm_layer = getattr(self, f&amp;#39;norm{i}&amp;#39;)\n                out = norm_layer(out)\n                out = out.view(-1, *out_hw_shape,\n                               self.num_features[i]).permute(0, 3, 1,\n                                                             2).contiguous()\n                outs.append(out)\n        batch_dict[&amp;#39;image_features&amp;#39;] = outs       # torch.Size([6, 192, 45, 80])x8 torch.Size([6, 384, 23, 40])x16 , torch.Size([6, 768, 12, 20])x32\n        return batch_dict\n\'> </span>\nopenpcdet自己的图片<br>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">i=1,2,3</p><span class=\'hidden-code\' data-code=\'for i, stage in enumerate(self.stages):\n    x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n    if i in self.out_indices:\n        norm_layer = getattr(self, f&amp;#39;norm{i}&amp;#39;)\n        out = norm_layer(out)\n        out = out.view(-1, *out_hw_shape,self.num_features[i]).permute(0, 3, 1,2).contiguous()\n        outs.append(out)\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_image/img_neck/generalized_lss.py</p><span class=\'hidden-code\' data-code=\'class GeneralizedLSSFPN(nn.Module):\n        def forward(self, batch_dict):\n        # upsample -> cat -> conv1x1 -> conv3x3\n        inputs = batch_dict[&amp;#39;image_features&amp;#39;]       # torch.Size([6, 192, 45, 80])x8 torch.Size([6, 384, 23, 40])x16 , torch.Size([6, 768, 12, 20])x32\n        assert len(inputs) == len(self.in_channels) # [192, 384, 768]\n        # build laterals                            # torch.Size([6, 192, 45, 80])x8 torch.Size([6, 384, 23, 40])x16 , torch.Size([6, 768, 12, 20])x32\n        laterals = [inputs[i + self.start_level] for i in range(len(inputs))] \n        # build top-down path\n        used_backbone_levels = len(laterals) - 1   # 2\n        for i in range(used_backbone_levels - 1, -1, -1):     # 1,0\n            x = F.interpolate(\n                laterals[i + 1],                              # torch.Size([6, 768, 12, 20])\n                size=laterals[i].shape[2:],                   # (23, 40)\n                mode=&amp;#39;bilinear&amp;#39;, align_corners=False,\n            )\n            laterals[i] = torch.cat([laterals[i], x], dim=1)  # torch.Size([6, 768, 23, 40])->torch.Size([6, 1152, 23, 40])\n            laterals[i] = self.lateral_convs[i](laterals[i])  # torch.Size([6, 256, 23, 40])\n            laterals[i] = self.fpn_convs[i](laterals[i])\n        # build outputs\n        outs = [laterals[i] for i in range(used_backbone_levels)]\n        batch_dict[&amp;#39;image_fpn&amp;#39;] = tuple(outs)      # torch.Size([6, 256, 45, 80])  torch.Size([6, 256, 23, 40])\n        return batch_dict\n\'> </span>\nopenpcdet自己的图片<br>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/depth_lss.py</p><span class=\'hidden-code\' data-code=\'@VTRANSFORMS.register_module()\nclass DepthLSSTransform(BaseDepthTransform):\n    def forward(self, *args, **kwargs):\n        x = super().`forward`(*args, **kwargs)      # 返回的(2, 80, 360, 360) 输入图像特征进去\n        x = self.downsample(x)                      # self.downsample(x)=>(2, 80, 180, 180)  \n        return x\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><span class=\'hidden-code\' data-code=\'class BaseDepthTransform(BaseTransform):\n    @force_fp32()\n    def forward(\n        self,\n        img,\n        points,            # [(243766, 5), (255017, 5)]\n        sensor2ego,        # 没用到，camera2ego=(2, 6, 4, 4)\n        lidar2ego,         # 没用到，lidar2ego=(2, 4, 4)；\n        lidar2camera,      # 雷达到相机，没用到   (2, 6, 4, 4)；\n        lidar2image,       # lidar2image=(2, 6, 4, 4)；     雷达到图像坐标系(包括外参+内参+矫正系数了) data[&amp;#39;img_aug_matrix&amp;#39;].data[0][0]\n        cam_intrinsic,     # 没用到 camera_intrinsics=(2, 6, 4, 4)；\n        camera2lidar,\n        img_aug_matrix,    # img_aug_matrix=(2, 6, 4, 4)；  图片增强矩阵  data[&amp;#39;img_aug_matrix&amp;#39;].data[0][0]\n        lidar_aug_matrix,  # lidar_aug_matrix=(2, 4, 4)     雷达增强矩阵  data[&amp;#39;lidar_aug_matrix&amp;#39;].data[0][0]\n        metas,\n        **kwargs,\n    ):\n        rots = sensor2ego[..., :3, :3]\n        trans = sensor2ego[..., :3, 3]\n        intrins = cam_intrinsic[..., :3, :3]\n        post_rots = img_aug_matrix[..., :3, :3]\n        post_trans = img_aug_matrix[..., :3, 3]\n        lidar2ego_rots = lidar2ego[..., :3, :3]\n        lidar2ego_trans = lidar2ego[..., :3, 3]\n        camera2lidar_rots = camera2lidar[..., :3, :3]\n        camera2lidar_trans = camera2lidar[..., :3, 3]\n        # print(img.shape, self.image_size, self.feature_size)\n        batch_size = len(points)      # 在原图上的点云投到(256,704)上，因为图片数据增强那里应该涉及到了下采样的倍数【输入到网络图片的大小是256x704,图片增强这里处理了点云】\n        depth = torch.zeros(batch_size, img.shape[1], 1, *self.image_size).to(points[0].device)  # depth.shape=(2, 6, 1, 256, 704) 获取每张图片的深度\n        for b in range(batch_size):\n            cur_coords = points[b][:, :3]                    # torch.Size([287993, 3])\n            cur_img_aug_matrix = img_aug_matrix[b]           # torch.Size([6, 4, 4])\n            cur_lidar_aug_matrix = lidar_aug_matrix[b]       # torch.Size([4, 4])\n            cur_lidar2image = lidar2image[b]                 # torch.Size([6, 4, 4])\n            # inverse aug         cur_coords=(6, 3, 243766)雷达坐标系-》图像坐标系到图像增强的坐标---》深度\n            cur_coords -= cur_lidar_aug_matrix[:3, 3]        # torch.Size([287993, 3])-torch.Size([3])\n            cur_coords = torch.inverse(cur_lidar_aug_matrix[:3, :3]).matmul(cur_coords.transpose(1, 0))  #(3,3)*(3*287993)=(3,287993)\n            # lidar2image\n            cur_coords = cur_lidar2image[:, :3, :3].matmul(cur_coords)     # (6,3,3)*(3,287993)=>torch.Size([6, 3, 287993])\n            cur_coords += cur_lidar2image[:, :3, 3].reshape(-1, 3, 1)      # torch.Size([6, 3, 287993])+torch.Size([6, 3, 1])=torch.Size([6, 3, 287993])\n            # get 2d coords\n            dist = cur_coords[:, 2, :]                                     # torch.Size([6, 287993])\n            cur_coords[:, 2, :] = torch.clamp(cur_coords[:, 2, :], 1e-5, 1e5)\n            cur_coords[:, :2, :] /= cur_coords[:, 2:3, :]                  # torch.Size([6, 2, 287993])/torch.Size([6, 1, 287993])=torch.Size([6, 2, 287993])\n            # imgaug\n            cur_coords = cur_img_aug_matrix[:, :3, :3].matmul(cur_coords)  # (6,3,3)*(6,3,287993)=torch.Size([6, 3, 287993])\n            cur_coords += cur_img_aug_matrix[:, :3, 3].reshape(-1, 3, 1)   # torch.Size([6, 3, 287993])+torch.Size([6, 1, 287993])\n            cur_coords = cur_coords[:, :2, :].transpose(1, 2)              # torch.Size([6, 287993, 2])\n            # normalize coords for grid sample\n            cur_coords = cur_coords[..., [1, 0]] # image_size为(H,W)\n            on_img = ((cur_coords[..., 0] `<` self.image_size[0]) &amp; (cur_coords[..., 0] `>`= 0) &amp; (cur_coords[..., 1] `<` self.image_size[1]) &amp; (cur_coords[..., 1] `>`= 0))\n            for c in range(on_img.shape[0]):\n                masked_coords = cur_coords[c, on_img[c]].long()\n                masked_dist = dist[c, on_img[c]]\n                depth[b, c, 0, masked_coords[:, 0], masked_coords[:, 1]] = masked_dist\n        extra_rots = lidar_aug_matrix[..., :3, :3]\n        extra_trans = lidar_aug_matrix[..., :3, 3]\n        geom = self.`get_geometry`(camera2lidar_rots,camera2lidar_trans,intrins,post_rots,post_trans,extra_rots=extra_rots,extra_trans=extra_trans) \n        x = self.`get_cam_feats`(img, depth)    # 图像特征torch.Size([2, 6, 256, 32, 88])与图像坐标系里面的弄进来的深度特征torch.Size([2, 6, 1, 256, 704]) \n                                                # bevdet直接对图像特征做卷积，bevDepth做了些操作，都是通过卷积生成深度\n        x = self.`bev_pool`(geom, x)\n        return x\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><span class=\'hidden-code\' data-code=\'class BaseTransform(nn.Module):\n    @force_fp32()\n    def get_geometry(\n        self,\n        camera2lidar_rots,  # torch.Size([1, 6, 3, 3])`相机到雷达`\n        camera2lidar_trans, # torch.Size([1, 6, 3])\n        intrins,            # torch.Size([1, 6, 3, 3])\n        post_rots,          # torch.Size([1, 6, 3, 3])\n        post_trans,         # torch.Size([1, 6, 3])\n        **kwargs,\n    ):\n        B, N, _ = camera2lidar_trans.shape\n        # undo post-transformation B x N x D x H x W x 3\n        # 可以认为self.frustum是图像上的点，到增强前图像上的点，到相机坐标系，到ego，到雷达坐标系【后面3为3维坐标的值】\n        points = self.frustum - post_trans.view(B, N, 1, 1, 1, 3)  # 从增强后的点到增强前图片的点:[1, 6, 196, 40, 80, 3]->[1, 6, 196, 40, 80, 3] \n        points = (torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3).matmul(points.unsqueeze(-1)))     # points.unsqueeze(-1)=torch.Size([1, 6, 196, 40, 80, 3, 1])\n        # cam_to_lidar  像素的2D像素坐标以及深度值，再加上相机的内参以及外参，即可计算得出像素对应的在车身坐标系中的3D坐标。\n        # torch.Size([1, 6, 196, 40, 80, 2, 1])*torch.Size([1, 6, 196, 40, 80, 1, 1]),torch.Size([1, 6, 196, 40, 80, 1, 1])\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],points[:, :, :, :, :, 2:3],),5,)  # 有深度值的点云\n        combine = camera2lidar_rots.matmul(torch.inverse(intrins))      # torch.Size([1, 6, 3, 3])@torch.Size([1, 6, 3, 3])=torch.Size([1, 6, 3, 3])\n        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)   # 得到torch.Size([1, 6, 196, 40, 80, 3])\n        points += camera2lidar_trans.view(B, N, 1, 1, 1, 3)      # torch.Size([1, 6, 196, 40, 80, 3])\n        if &amp;#39;extra_rots&amp;#39; in kwargs:                               # 点云增强\n            extra_rots = kwargs[&amp;#39;extra_rots&amp;#39;]     # torch.Size([1, 3, 3])\n            points = (extra_rots.view(B, 1, 1, 1, 1, 3, 3).repeat(1, N, 1, 1, 1, 1, 1).matmul(points.unsqueeze(-1)).squeeze(-1))\n        if &amp;#39;extra_trans&amp;#39; in kwargs:\n            extra_trans = kwargs[&amp;#39;extra_trans&amp;#39;]   # torch.Size([1, 3])\n            points += extra_trans.view(B, 1, 1, 1, 1, 3).repeat(1, N, 1, 1, 1, 1)\n        return points                             # torch.Size([1, 6, 196, 40, 80, 3])\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/depth_lss.py</p><span class=\'hidden-code\' data-code=\'@VTRANSFORMS.register_module()\nclass DepthLSSTransform(BaseDepthTransform):\n    @force_fp32()\n    def get_cam_feats(self, x, d):\n        B, N, C, fH, fW = x.shape             # x-img:(2,6,256,32,88)->(12, 256, 32, 88) 32x88是输入网络分辨率256x704下采样8倍后的值\n        d = d.view(B * N, *d.shape[2:])       # 这里的深度是神经网络输入的深度\n        x = x.view(B * N, C, fH, fW)\n        d = self.`dtransform`(d)          # 一系列卷积BN池化=》d-depth:(2, 6, 1, 256, 704)->(12, 1, 256, 704)->{self.dtransform}->(12,64,32,88)；64可以认为每个点的深度的特征维度\n        x = torch.cat([d, x], dim=1)    \n        x = self.`depthnet`(x)            # 一系列卷积BN池化=》(12, 320=256+64, 32, 88)->(12, 198, 32, 88)  198=80+118\n        depth = x[:, : self.D].softmax(dim=1)\n        # (12, 118, 32, 88)-{depth.unsqueeze(1)}-(12, 1, 118, 32, 88)+x[:, self.D : (self.D + self.C)].unsqueeze(2).shape=(12, 80, 1, 32, 88)  ；0-118是深度0-60并且间隔为0.5的取值，每个值有80个特征？\n        x = depth.unsqueeze(1) * x[:, self.D : (self.D + self.C)].unsqueeze(2)  # 深度值*特征 = 2D特征转变为3D空间(俯视图)内的特征-(12, 80, 118, 32, 88)\n        x = x.view(B, N, self.C, self.D, fH, fW)    # (2,6,80,118,32,88)\n        x = x.permute(0, 1, 3, 4, 5, 2)             # x-(2,6,118,32,88,80)-(B, N, self.D, fH, fW, self.C) 对应bevdet里面的self.D=59/118；self.C=16/80\n        return x\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/vtransforms/base.py</p><span class=\'hidden-code\' data-code=\'class BaseTransform(nn.Module):\n    @force_fp32()\n    def bev_pool(self, geom_feats, x):\n        B, N, D, H, W, C = x.shape\n        Nprime = B * N * D * H * W       # Nprime=3987456=2,6,118,88,80   \n        # flatten x\n        x = x.reshape(Nprime, C)         # torch.Size([3987456, 80])【self.C=80】  \n        # flatten indices                  self.bx=tensor([-53.8500, -53.8500,   0.0000])；self.dx=tensor([0.3000,  0.3000, 20.0000])；self.dx=tensor([0.3000,  0.3000, 20.0000]) \n        geom_feats = ((geom_feats - (self.bx - self.dx / 2.0)) / self.dx).long()\n        geom_feats = geom_feats.view(Nprime, 3)     # geom_feats.shape=torch.Size([2, 6, 118, 32, 88, 3])-> torch.Size([3987456, 3])->(3987456, 4)\n        batch_ix = torch.cat([torch.full([Nprime // B, 1], ix, device=x.device, dtype=torch.long) for ix in range(B)])\n        geom_feats = torch.cat((geom_feats, batch_ix), 1)\n        # filter out points that are outside box\n        kept = ((geom_feats[:, 0] `>`= 0) &amp; (geom_feats[:, 0] `<` self.nx[0]) &amp; (geom_feats[:, 1] `>`= 0)\n            &amp; (geom_feats[:, 1] `<` self.nx[1]) &amp; (geom_feats[:, 2] `>`= 0) &amp; (geom_feats[:, 2] `<` self.nx[2]))\n        x = x[kept]\n        geom_feats = geom_feats[kept]          # geom_feats->(3667059, 4)  \n        x = `bev_pool`(x, geom_feats, B, self.nx[2], self.nx[0], self.nx[1])     # self.nx=tensor([360, 360,   1], device=&amp;#39;cuda:0&amp;#39;)\n        # collapse Z\n        final = torch.cat(x.unbind(dim=2), 1)\n        return final        # final->(2, 80, 360, 360) \n\'> </span><p><code>mmdet3d/ops/bev_pool/bev_pool.py</code>:<code>def bev_pool</code><br>\nranks.shape=torch.Size([3667059]);<br>\nx-&gt;(2, 80, 1, 360, 360)<br></p>'}]}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/fusers/conv.py</p><span class=\'hidden-code\' data-code=\'class ConvFuser(nn.Sequential):\n    def __init__(self, in_channels: int, out_channels: int) -> None:\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        super().__init__(\n            nn.Conv2d(sum(in_channels), out_channels, 3, padding=1, bias=False),   # 80+256的通道到256\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n        return super().forward(torch.cat(inputs, dim=1))\n\'> </span>'}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
