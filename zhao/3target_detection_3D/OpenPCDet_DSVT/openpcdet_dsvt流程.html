<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>openpcdet_dsvt流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">CenterPoint-pillar</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/vfe/dynamic_voxel_vfe.py</p><font size="0"><pre class="language-python"><code class="language-python">class DynamicVoxelVFE(VFETemplate):\n    def forward(self, batch_dict, **kwargs):\n        points = batch_dict[\'points\'] <span style=\'color: red\'># (batch_idx, x, y, z, i, e)  torch.Size([186187, 6])</span>\n        points_coords = torch.floor((points[:, [1,2,3]] - self.point_cloud_range[[0,1,2]]) / self.voxel_size[[0,1,2]]).int()  <span style=\'color: red\'># 范围调整到(0-W,0-H)内</span>\n        mask = ((points_coords >= 0) & (points_coords < self.grid_size[[0,1,2]])).all(dim=1)          <span style=\'color: red\'># self.grid_size = tensor([468, 468,   1], device=\'cuda:0\')</span>\n        points = points[mask]\n        points_coords = points_coords[mask]\n        points_xyz = points[:, [1, 2, 3]].contiguous()\n        <span style=\'color: red\'># 468x468,468,1  把点弄成个索引  因为是int成整数，所以有些点会是同一个索引【相同voxel是同一个索引】  torch.Size([185485])</span>\n        merge_coords = points[:, 0].int() * self.scale_xyz + points_coords[:, 0] * self.scale_yz + points_coords[:, 1] * self.scale_z + points_coords[:, 2]\n        <span style=\'color: red\'># torch.Size([9295])唯一索引值，torch.Size([185485])属于哪个唯一索引值，torch.Size([9295])唯一索引值有多少个点是这个数</span>\n        unq_coords, unq_inv, unq_cnt = torch.unique(merge_coords, return_inverse=True, return_counts=True, dim=0)\n        <span style=\'color: red\'># torch.Size([185485, 3])+torch.Size([185485])= torch.Size([9295, 3])；</span>\n        points_mean = torch_scatter.scatter_mean(points_xyz, unq_inv, dim=0)\n        f_cluster = points_xyz - points_mean[unq_inv, :]           <span style=\'color: red\'># torch.Size([185485, 3])</span>\n        f_center = torch.zeros_like(points_xyz)      <span style=\'color: red\'># 距离每个voxel的距离</span>\n        f_center[:, 0] = points_xyz[:, 0] - (points_coords[:, 0].to(points_xyz.dtype) * self.voxel_x + self.x_offset)\n        f_center[:, 1] = points_xyz[:, 1] - (points_coords[:, 1].to(points_xyz.dtype) * self.voxel_y + self.y_offset)\n        <span style=\'color: red\'># f_center[:, 2] = points_xyz[:, 2] - self.z_offset</span>\n        f_center[:, 2] = points_xyz[:, 2] - (points_coords[:, 2].to(points_xyz.dtype) * self.voxel_z + self.z_offset)\n        if self.use_absolute_xyz:\n            features = [points[:, 1:], f_cluster, f_center]\n        else:\n            features = [points[:, 4:], f_cluster, f_center]\n        if self.with_distance:        <span style=\'color: red\'># False</span>\n            points_dist = torch.norm(points[:, 1:4], 2, dim=1, keepdim=True)\n            features.append(points_dist)\n        features = torch.cat(features, dim=-1)       <span style=\'color: red\'># torch.Size([185485, 11])</span>\n        for pfn in self.pfn_layers:\n            features = pfn(features, unq_inv)       <span style=\'color: red\'># torch.Size([9295, 192])</span>\n        <span style=\'color: red\'># generate voxel coordinates</span>\n        unq_coords = unq_coords.int()\n        voxel_coords = torch.stack((unq_coords <span style=\'color: red\'>// self.scale_xyz, (unq_coords % self.scale_xyz)</span>\n        voxel_coords = voxel_coords[:, [0, 3, 2, 1]]  <span style=\'color: red\'># B,z,y,x</span>\n        batch_dict[\'pillar_features\'] = batch_dict[\'voxel_features\'] = features\n        batch_dict[\'voxel_coords\'] = voxel_coords        <span style=\'color: red\'># torch.Size([9295, 4])</span>\n        return batch_dict\n</code></pre></font>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVT(nn.Module):\n    def forward(self, batch_dict):\n        voxel_info = self.<span style=\'color: green;font-weight: bold;\'>input_layer</span>(batch_dict)\n        voxel_feat = voxel_info[\'voxel_feats_stage0\']            <span style=\'color: red\'># torch.Size([9295, 192])</span>\n        <span style=\'color: red\'># [[torch.Size([2, 766, 36]), torch.Size([2, 543, 36])]]  766个set，每个set里面36个非空voxel</span>\n        set_voxel_inds_list = [[voxel_info[f\'set_voxel_inds_stage{s}_shift{i}\'] for i in range(self.num_shifts[s])] for s in range(self.stage_num)]\n        <span style=\'color: red\'># [[torch.Size([2, 766, 36]), torch.Size([2, 543, 36])]]  针对上面的voxel有重复</span>\n        set_voxel_masks_list = [[voxel_info[f\'set_voxel_mask_stage{s}_shift{i}\'] for i in range(self.num_shifts[s])] for s in range(self.stage_num)]\n        <span style=\'color: red\'># [[[torch.Size([766, 36]), torch.Size([766, 36])], [torch.Size([543, 36]), torch.Size([543, 36])]]]</span>\n        pos_embed_list = [[[voxel_info[f\'pos_embed_stage{s}_block{b}_shift{i}\'] for i in range(self.num_shifts[s])] for b in range(self.set_info[s][1])] for s in range(self.stage_num)]\n        pooling_mapping_index = [voxel_info[f\'pooling_mapping_index_stage{s+1}\'] for s in range(self.stage_num-1)]     <span style=\'color: red\'># []</span>\n        pooling_index_in_pool = [voxel_info[f\'pooling_index_in_pool_stage{s+1}\'] for s in range(self.stage_num-1)]     <span style=\'color: red\'># []</span>\n        pooling_preholder_feats = [voxel_info[f\'pooling_preholder_feats_stage{s+1}\'] for s in range(self.stage_num-1)] <span style=\'color: red\'># []</span>\n        output = voxel_feat\n        block_id = 0\n        for stage_id in range(self.stage_num):\n            block_layers = self.__getattr__(f\'stage_{stage_id}\')                           <span style=\'color: red\'># 三个DSVTBlock</span>\n            residual_norm_layers = self.__getattr__(f\'residual_norm_stage_{stage_id}\')     <span style=\'color: red\'># 四个LayerNorm</span>\n            for i in range(len(block_layers)):\n                block = block_layers[i]\n                residual = output.clone()\n                if self.use_torch_ckpt==False:\n                    output = block(output, set_voxel_inds_list[stage_id], set_voxel_masks_list[stage_id], pos_embed_list[stage_id][i],block_id=block_id)\n                else:\n                    output = checkpoint(<span style=\'color: green;font-weight: bold;\'>block</span>, output, set_voxel_inds_list[stage_id], set_voxel_masks_list[stage_id], pos_embed_list[stage_id][i], block_id)\n                output = residual_norm_layers[i](output + residual)\n                block_id += 1\n            if stage_id < self.stage_num - 1:\n                <span style=\'color: red\'># pooling</span>\n                prepool_features = pooling_preholder_feats[stage_id].type_as(output)\n                pooled_voxel_num = prepool_features.shape[0]\n                pool_volume = prepool_features.shape[1]\n                prepool_features[pooling_mapping_index[stage_id], pooling_index_in_pool[stage_id]] = output\n                prepool_features = prepool_features.view(prepool_features.shape[0], -1)\n                \n                if self.reduction_type == \'linear\':\n                    output = self.__getattr__(f\'stage_{stage_id}_reduction\')(prepool_features)\n                elif self.reduction_type == \'maxpool\':\n                    prepool_features = prepool_features.view(pooled_voxel_num, pool_volume, -1).permute(0, 2, 1)\n                    output = self.__getattr__(f\'stage_{stage_id}_reduction\')(prepool_features).squeeze(-1)\n                elif self.reduction_type == \'attention\':\n                    prepool_features = prepool_features.view(pooled_voxel_num, pool_volume, -1).permute(0, 2, 1)\n                    key_padding_mask = torch.zeros((pooled_voxel_num, pool_volume)).to(prepool_features.device).int()\n                    output = self.__getattr__(f\'stage_{stage_id}_reduction\')(prepool_features, key_padding_mask)\n                else:\n                    raise NotImplementedError\n        batch_dict[\'pillar_features\'] = batch_dict[\'voxel_features\'] = output              <span style=\'color: red\'># torch.Size([15912, 192])</span>\n        batch_dict[\'voxel_coords\'] = voxel_info[f\'voxel_coors_stage{self.stage_num - 1}\']  <span style=\'color: red\'># torch.Size([15912, 4])</span>\n        return batch_dict\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVTInputLayer(nn.Module):\n    def forward(self, batch_dict):\n        voxel_feats = batch_dict[\'voxel_features\']           <span style=\'color: red\'># torch.Size([9295, 192])</span>\n        voxel_coors = batch_dict[\'voxel_coords\'].long()      <span style=\'color: red\'># torch.Size([9295, 4]) </span>\n        voxel_info = {}\n        voxel_info[\'voxel_feats_stage0\'] = voxel_feats.clone()\n        voxel_info[\'voxel_coors_stage0\'] = voxel_coors.clone()\n        for stage_id in range(self.stage_num):                <span style=\'color: red\'># 1</span>\n            <span style=\'color: red\'># window partition of corrsponding stage-map</span>\n            voxel_info = self.<span style=\'color: green;font-weight: bold;\'>window_partition</span>(voxel_info, stage_id)\n            <span style=\'color: red\'># generate set id of corrsponding stage-map</span>\n            voxel_info = self.<span style=\'color: green;font-weight: bold;\'>get_set</span>(voxel_info, stage_id)\n            for block_id in range(self.set_info[stage_id][1]):         <span style=\'color: red\'># [[36, 4]]->4</span>\n                for shift_id in range(self.num_shifts[stage_id]):      <span style=\'color: red\'># [2]->2</span>\n                    voxel_info[f\'pos_embed_stage{stage_id}_block{block_id}_shift{shift_id}\'] = self.<span style=\'color: green;font-weight: bold;\'>get_pos_embed</span>(voxel_info[f\'coors_in_win_stage{stage_id}_shift{shift_id}\'], stage_id, block_id, shift_id)\n            <span style=\'color: red\'># compute pooling information</span>\n            if stage_id < self.stage_num - 1:\n                voxel_info = self.<span style=\'color: green;font-weight: bold;\'>subm_pooling</span>(voxel_info, stage_id)\n        return voxel_info\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVTInputLayer(nn.Module):\n    def window_partition(self, voxel_info, stage_id):\n        for i in range(2):\n            batch_win_inds, coors_in_win = <span style=\'color: green;font-weight: bold;\'>get_window_coors</span>(voxel_info[f\'voxel_coors_stage{stage_id}\'], self.sparse_shape_list[stage_id], self.window_shape[stage_id][i], i == 1, self.shift_list[stage_id][i])                                         \n            voxel_info[f\'batch_win_inds_stage{stage_id}_shift{i}\'] = batch_win_inds\n            voxel_info[f\'coors_in_win_stage{stage_id}_shift{i}\'] = coors_in_win    \n        return voxel_info\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/model_utils/dsvt_utils.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_window_coors(coors, sparse_shape, window_shape, do_shift, shift_list=None, return_win_coors=False):\n    <span style=\'color: red\'># torch.Size([9295, 4]);[468, 468, 1]; [12, 12, 1]; Fasle; [0, 0, 0]; False  把468x468化成了40x40，每个格子有12x12个voxel</span>\n    if len(window_shape) == 2:\n        win_shape_x, win_shape_y = window_shape\n        win_shape_z = sparse_shape[-1]\n    else:\n        win_shape_x, win_shape_y, win_shape_z = window_shape  <span style=\'color: red\'># [12, 12, 1]</span>\n    sparse_shape_x, sparse_shape_y, sparse_shape_z = sparse_shape        <span style=\'color: red\'># [468, 468, 1]</span>\n    assert sparse_shape_z < sparse_shape_x, \'Usually holds... in case of wrong order\'\n    max_num_win_x = int(np.ceil((sparse_shape_x / win_shape_x)) + 1) <span style=\'color: red\'># plus one here to meet the needs of shift.  40</span>\n    max_num_win_y = int(np.ceil((sparse_shape_y / win_shape_y)) + 1) <span style=\'color: red\'># plus one here to meet the needs of shift.  40</span>\n    max_num_win_z = int(np.ceil((sparse_shape_z / win_shape_z)) + 1) <span style=\'color: red\'># plus one here to meet the needs of shift.  2</span>\n    max_num_win_per_sample = max_num_win_x * max_num_win_y * max_num_win_z      <span style=\'color: red\'># 3200</span>\n    if do_shift:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]\n        else:\n            shift_x, shift_y, shift_z = win_shape_x <span style=\'color: red\'>// 2, win_shape_y</span>\n    else:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]    <span style=\'color: red\'># [0,0,0]</span>\n        else:\n            shift_x, shift_y, shift_z = win_shape_x, win_shape_y, win_shape_z\n    \n    <span style=\'color: red\'># compatibility between 2D window and 3D window</span>\n    if sparse_shape_z == win_shape_z:  <span style=\'color: red\'># 1==1</span>\n        shift_z = 0\n    shifted_coors_x = coors[:, 3] + shift_x  <span style=\'color: red\'># torch.Size([9295, 4])  B,Z,Y,X</span>\n    shifted_coors_y = coors[:, 2] + shift_y\n    shifted_coors_z = coors[:, 1] + shift_z\n    win_coors_x = shifted_coors_x // win_shape_x  <span style=\'color: red\'># 值范围从0-468到0-40</span>\n    win_coors_y = shifted_coors_y <span style=\'color: red\'>// win_shape_y</span>\n    win_coors_z = shifted_coors_z <span style=\'color: red\'>// win_shape_z</span>\n    if len(window_shape) == 2:\n        assert (win_coors_z == 0).all()\n    <span style=\'color: red\'># 转成唯一索引  torch.Size([9295])  这个值是非空voxel的数量</span>\n    batch_win_inds = coors[:, 0] * max_num_win_per_sample + win_coors_x * max_num_win_y * max_num_win_z + win_coors_y * max_num_win_z +  win_coors_z\n    coors_in_win_x = shifted_coors_x % win_shape_x   <span style=\'color: red\'># 余数，在窗口里面偏移多少个voxel</span>\n    coors_in_win_y = shifted_coors_y % win_shape_y\n    coors_in_win_z = shifted_coors_z % win_shape_z\n    coors_in_win = torch.stack([coors_in_win_z, coors_in_win_y, coors_in_win_x], dim=-1)  <span style=\'color: red\'># torch.Size([9295, 3])</span>\n    <span style=\'color: red\'># coors_in_win = torch.stack([coors_in_win_x, coors_in_win_y], dim=-1)</span>\n    if return_win_coors:   <span style=\'color: red\'># False</span>\n        batch_win_coords = torch.stack([win_coors_z, win_coors_y, win_coors_x], dim=-1)\n        return batch_win_inds, coors_in_win, batch_win_coords\n    return batch_win_inds, coors_in_win    <span style=\'color: red\'># 唯一索引 + 窗口偏移数 = torch.Size([9295]) + torch.Size([9295，3])</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/model_utils/dsvt_utils.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_window_coors(coors, sparse_shape, window_shape, do_shift, shift_list=None, return_win_coors=False):\n    <span style=\'color: red\'># torch.Size([9295, 4]);[468, 468, 1]; [24, 24, 1]; True; [6, 6, 0]; False  把468x468化成了40x40，每个格子有12x12个voxel</span>\n    if len(window_shape) == 2:\n        win_shape_x, win_shape_y = window_shape\n        win_shape_z = sparse_shape[-1]\n    else:\n        win_shape_x, win_shape_y, win_shape_z = window_shape  <span style=\'color: red\'># [24, 24, 1]</span>\n    sparse_shape_x, sparse_shape_y, sparse_shape_z = sparse_shape        <span style=\'color: red\'># [468, 468, 1]</span>\n    assert sparse_shape_z < sparse_shape_x, \'Usually holds... in case of wrong order\'\n    max_num_win_x = int(np.ceil((sparse_shape_x / win_shape_x)) + 1) <span style=\'color: red\'># plus one here to meet the needs of shift.  21</span>\n    max_num_win_y = int(np.ceil((sparse_shape_y / win_shape_y)) + 1) <span style=\'color: red\'># plus one here to meet the needs of shift.  21</span>\n    max_num_win_z = int(np.ceil((sparse_shape_z / win_shape_z)) + 1) <span style=\'color: red\'># plus one here to meet the needs of shift.  2</span>\n    max_num_win_per_sample = max_num_win_x * max_num_win_y * max_num_win_z      <span style=\'color: red\'># 882</span>\n    if do_shift:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]    <span style=\'color: red\'># [6, 6, 0]</span>\n        else:\n            shift_x, shift_y, shift_z = win_shape_x <span style=\'color: red\'>// 2, win_shape_y</span>\n    else:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]   \n        else:\n            shift_x, shift_y, shift_z = win_shape_x, win_shape_y, win_shape_z\n    \n    <span style=\'color: red\'># compatibility between 2D window and 3D window</span>\n    if sparse_shape_z == win_shape_z:  <span style=\'color: red\'># 1==1</span>\n        shift_z = 0\n    shifted_coors_x = coors[:, 3] + shift_x  <span style=\'color: red\'># torch.Size([9295, 4])  B,Z,Y,X</span>\n    shifted_coors_y = coors[:, 2] + shift_y  <span style=\'color: red\'># 移动6，6，0</span>\n    shifted_coors_z = coors[:, 1] + shift_z\n    win_coors_x = shifted_coors_x // win_shape_x  <span style=\'color: red\'># 值范围从6-474到0-19(474//24=19)</span>\n    win_coors_y = shifted_coors_y <span style=\'color: red\'>// win_shape_y</span>\n    win_coors_z = shifted_coors_z <span style=\'color: red\'>// win_shape_z</span>\n    if len(window_shape) == 2:\n        assert (win_coors_z == 0).all()\n    <span style=\'color: red\'># 转成唯一索引  torch.Size([9295])  这个值是非空voxel的数量</span>\n    batch_win_inds = coors[:, 0] * max_num_win_per_sample + win_coors_x * max_num_win_y * max_num_win_z + win_coors_y * max_num_win_z +  win_coors_z\n    coors_in_win_x = shifted_coors_x % win_shape_x   <span style=\'color: red\'># 余数，在窗口里面偏移多少个voxel</span>\n    coors_in_win_y = shifted_coors_y % win_shape_y\n    coors_in_win_z = shifted_coors_z % win_shape_z\n    coors_in_win = torch.stack([coors_in_win_z, coors_in_win_y, coors_in_win_x], dim=-1)  <span style=\'color: red\'># torch.Size([9295, 3])</span>\n    <span style=\'color: red\'># coors_in_win = torch.stack([coors_in_win_x, coors_in_win_y], dim=-1)</span>\n    if return_win_coors:   <span style=\'color: red\'># False</span>\n        batch_win_coords = torch.stack([win_coors_z, win_coors_y, win_coors_x], dim=-1)\n        return batch_win_inds, coors_in_win, batch_win_coords\n    return batch_win_inds, coors_in_win    <span style=\'color: red\'># 唯一索引 + 窗口偏移数 = torch.Size([9295]) + torch.Size([9295，3])</span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVTInputLayer(nn.Module):\n    def get_set(self, voxel_info, stage_id):\n        batch_win_inds_shift0 = voxel_info[f\'batch_win_inds_stage{stage_id}_shift0\']       <span style=\'color: red\'># torch.Size([9295])    划分成多个窗口的索引; 也就是每个voxel对应的窗口</span>\n        coors_in_win_shift0 = voxel_info[f\'coors_in_win_stage{stage_id}_shift0\']           <span style=\'color: red\'># torch.Size([9295, 3]) 多个窗口索引的偏移的voxel数量； 是整数</span>\n        set_voxel_inds_shift0 = self.<span style=\'color: green;font-weight: bold;\'>get_set_single_shift</span>(batch_win_inds_shift0, stage_id, shift_id=0, coors_in_win=coors_in_win_shift0)\n        voxel_info[f\'set_voxel_inds_stage{stage_id}_shift0\'] = set_voxel_inds_shift0       <span style=\'color: red\'># torch.Size([2, 357, 36]) 这里面的索引是《第几个窗口*窗口的最大voxel数+窗口内voxel按一定方向的索引》</span>\n        <span style=\'color: red\'># compute key masks, voxel duplication must happen continuously                    [1,1,2,3,3]->[1,2,3,3,1]=>[T,F,F,T,F]  True表示是重复voxel索引</span>\n        prefix_set_voxel_inds_s0 = torch.roll(set_voxel_inds_shift0.clone(), shifts=1, dims=-1)\n        prefix_set_voxel_inds_s0[ :, :, 0] = -1\n        set_voxel_mask_s0 = (set_voxel_inds_shift0 == prefix_set_voxel_inds_s0)            <span style=\'color: red\'># torch.Size([2, 357, 36])</span>\n        voxel_info[f\'set_voxel_mask_stage{stage_id}_shift0\'] = set_voxel_mask_s0\n        batch_win_inds_shift1 = voxel_info[f\'batch_win_inds_stage{stage_id}_shift1\']       <span style=\'color: red\'># torch.Size([9295])</span>\n        coors_in_win_shift1 = voxel_info[f\'coors_in_win_stage{stage_id}_shift1\']           <span style=\'color: red\'># torch.Size([9295, 3])</span>\n        set_voxel_inds_shift1 = self.<span style=\'color: green;font-weight: bold;\'>get_set_single_shift</span>(batch_win_inds_shift1, stage_id, shift_id=1, coors_in_win=coors_in_win_shift1)\n        voxel_info[f\'set_voxel_inds_stage{stage_id}_shift1\'] = set_voxel_inds_shift1       <span style=\'color: red\'># torch.Size([2, 295, 36])</span>\n        <span style=\'color: red\'># compute key masks, voxel duplication must happen continuously</span>\n        prefix_set_voxel_inds_s1 = torch.roll(set_voxel_inds_shift1.clone(), shifts=1, dims=-1)\n        prefix_set_voxel_inds_s1[ :, :, 0] = -1\n        set_voxel_mask_s1 = (set_voxel_inds_shift1 == prefix_set_voxel_inds_s1)\n        voxel_info[f\'set_voxel_mask_stage{stage_id}_shift1\'] = set_voxel_mask_s1           <span style=\'color: red\'># torch.Size([2, 295, 36])</span>\n        return voxel_info\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVTInputLayer(nn.Module):\n    def get_set_single_shift(self, batch_win_inds, stage_id, shift_id=None, coors_in_win=None):  <span style=\'color: red\'># 0,0</span>\n        device = batch_win_inds.device                <span style=\'color: red\'># device(type=\'cuda\', index=0)</span>\n        <span style=\'color: red\'># the number of voxels assigned to a set</span>\n        voxel_num_set = self.set_info[stage_id][0]    <span style=\'color: red\'># [[36, 4]]->36</span>\n        <span style=\'color: red\'># max number of voxels in a window           </span>\n        max_voxel = self.window_shape[stage_id][shift_id][0] * self.window_shape[stage_id][shift_id][1] * self.window_shape[stage_id][shift_id][2]\n        <span style=\'color: red\'># get unique set indexs</span>\n        contiguous_win_inds = torch.unique(batch_win_inds, return_inverse=True)[1]   <span style=\'color: red\'># torch.Size([9295])->torch.Size([9295])个voxels在40x40个windows 这里面重复值，表示属于哪个窗口</span>\n        voxelnum_per_win = torch.bincount(contiguous_win_inds)                       <span style=\'color: red\'># torch.Size([168])-168个windows里面有voxel，这里面的值是每个windows里有多少个voxel</span>\n        win_num = voxelnum_per_win.shape[0]                                          <span style=\'color: red\'># 168个窗口，每个窗口里面36个voxel就是一个set</span>\n        setnum_per_win_float = voxelnum_per_win / voxel_num_set                      <span style=\'color: red\'># 如果超过36就是1了  torch.Size([168])=torch.Size([168])/36</span>\n        setnum_per_win = torch.ceil(setnum_per_win_float).long()                     <span style=\'color: red\'># 往上四舍五入</span>\n        set_win_inds, set_inds_in_win = <span style=\'color: green;font-weight: bold;\'>get_continous_inds</span>(setnum_per_win)         <span style=\'color: red\'># torch.Size([168])->torch.Size([357])(值范围0-167)+torch.Size([357])(值范围0-3,一个set最多4个voxel)</span>\n                                                                                     <span style=\'color: red\'># 168个窗口，357个set，</span>\n        <span style=\'color: red\'># compution of Eq.3 in \'DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets\' - https://arxiv.org/abs/2301.06051, </span>\n        <span style=\'color: red\'># for each window, we can get voxel indexs belong to different sets.          </span>\n        offset_idx = set_inds_in_win[:,None].repeat(1, voxel_num_set) * voxel_num_set  <span style=\'color: red\'># torch.Size([357, 36])</span>\n        base_idx = torch.arange(0, voxel_num_set, 1, device=device)                    <span style=\'color: red\'># torch.Size([36])  公式里面的k从0-35</span>\n        base_select_idx = offset_idx + base_idx                                        <span style=\'color: red\'># torch.Size([357, 36])  原先的值是0，则为0-35，是1，则为36-71</span>\n        base_select_idx = base_select_idx * voxelnum_per_win[set_win_inds][:,None]\n        base_select_idx = base_select_idx.double() / (setnum_per_win[set_win_inds] * voxel_num_set)[:,None].double()  <span style=\'color: red\'># torch.Size([357, 36])  357个set，每个set最多36个voxel</span>\n        base_select_idx = torch.floor(base_select_idx)                        <span style=\'color: red\'># 第0个set的voxel的索引依次是 [0., 0., 0., 0., 0., 1., 1., 1.,......]</span>\n        <span style=\'color: red\'># obtain unique indexs in whole space</span>\n        select_idx = base_select_idx\n        select_idx = select_idx + set_win_inds.view(-1, 1) * max_voxel        <span style=\'color: red\'># torch.Size([357, 36]) 共357个set，每个set有36个voxel，里面的voxel索引有重复，值的范围在0->9295</span>\n           \n        <span style=\'color: red\'># this function will return unordered inner window indexs of each voxel </span>\n                                                                                 <span style=\'color: red\'># 依次表示属于第0个窗口，第1个窗口，第0个窗口 -> 第0个窗口的第0个，第1个窗口的第0个，第0个窗口的第1个      </span>\n        inner_voxel_inds = get_inner_win_inds_cuda(contiguous_win_inds)         <span style=\'color: red\'># torch.Size([9295]) 最大值167(168个窗口)->最大值142(窗口最大的voxel的数量为142+1)</span>\n        global_voxel_inds = contiguous_win_inds * max_voxel + inner_voxel_inds  <span style=\'color: red\'># torch.Size([9295]) 前10个[0*144, 1*144, 0*144, 0*144, 0*144, 0*144, 1*144, 0*144, 0*144, 1*144]+[0, 0, 1, 2, 3, 4, 1, 5, 6, 2]</span>\n        _, order1 = torch.sort(global_voxel_inds)                               <span style=\'color: red\'># 从小到大排序的索引 inner_voxel_inds[order1]->[(0,1,2,3,4,5,6),(0,1,2)]  顺序是第0个窗口的0个voxel，1个voxel，。。。</span>\n        <span style=\'color: red\'># get y-axis partition results</span>\n        global_voxel_inds_sorty = contiguous_win_inds * max_voxel + \\\n                coors_in_win[:,1] * self.window_shape[stage_id][shift_id][0] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,2] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,0]                                               <span style=\'color: red\'># coors_in_win->torch.Size([9295, 3]) + [12, 12, 1]  </span>\n        _, order2 = torch.sort(global_voxel_inds_sorty)                         <span style=\'color: red\'># torch.Size([9295])  这里的顺序是第0个窗口的，然后按照 y*12*1+x*1+z</span>\n        inner_voxel_inds_sorty = -torch.ones_like(inner_voxel_inds)                      \n        inner_voxel_inds_sorty.scatter_(dim=0, index=order2, src=inner_voxel_inds[order1])     <span style=\'color: red\'># get y-axis ordered inner window indexs of each voxel  torch.Size([9295])前10个 </span>\n                                                                                               <span style=\'color: red\'># inner_voxel_inds_sorty[:10] = [(2, 3, 0, 1, 4), (0, 1, 2, 3), 2]</span>\n        voxel_inds_in_batch_sorty = inner_voxel_inds_sorty + max_voxel * contiguous_win_inds   <span style=\'color: red\'># voxel_inds_in_batch_sorty[:10] = [(2, 3, 0, 1, 4),(576, 577, 578, 579), 434]</span>\n        voxel_inds_padding_sorty = -1 * torch.ones((win_num * max_voxel), dtype=torch.long, device=device)   <span style=\'color: red\'># 168*144 = torch.Size([36864])=</span>\n        voxel_inds_padding_sorty[voxel_inds_in_batch_sorty] = torch.arange(0, voxel_inds_in_batch_sorty.shape[0], dtype=torch.long, device=device)\n        set_voxel_inds_sorty = voxel_inds_padding_sorty[select_idx.long()]     <span style=\'color: red\'># torch.Size([357, 36])  范围0->9295</span>\n \n        <span style=\'color: red\'># get x-axis partition results</span>\n        global_voxel_inds_sortx = contiguous_win_inds * max_voxel + \\\n                coors_in_win[:,2] * self.window_shape[stage_id][shift_id][1] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,1] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,0]\n        _, order2 = torch.sort(global_voxel_inds_sortx)                         <span style=\'color: red\'># torch.Size([9295])  这里的顺序是第0个窗口的，然后按照 x*12*1+y*1+z</span>\n        inner_voxel_inds_sortx = -torch.ones_like(inner_voxel_inds)\n        inner_voxel_inds_sortx.scatter_(dim=0,index=order2, src=inner_voxel_inds[order1])      <span style=\'color: red\'># get x-axis ordered inner window indexs of each voxel</span>\n        voxel_inds_in_batch_sortx = inner_voxel_inds_sortx + max_voxel * contiguous_win_inds\n        voxel_inds_padding_sortx = -1 * torch.ones((win_num * max_voxel), dtype=torch.long, device=device)\n        voxel_inds_padding_sortx[voxel_inds_in_batch_sortx] = torch.arange(0, voxel_inds_in_batch_sortx.shape[0], dtype=torch.long, device=device)\n        set_voxel_inds_sortx = voxel_inds_padding_sortx[select_idx.long()]                     <span style=\'color: red\'># torch.Size([357, 36])  范围0->9295</span>\n        all_set_voxel_inds = torch.stack((set_voxel_inds_sorty, set_voxel_inds_sortx), dim=0)  <span style=\'color: red\'># torch.Size([2, 357, 36])</span>\n        return all_set_voxel_inds                                                \n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/model_utils/dsvt_utils.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_continous_inds(setnum_per_win):\n    \'\'\'\n    Args:\n        setnum_per_win (Tensor[int]): Number of sets assigned to each window with shape (win_num).\n    Returns:\n        set_win_inds (Tensor[int]): Window indexs of each set with shape (set_num).\n        set_inds_in_win (Tensor[int]): Set indexs inner window with shape (set_num).\n    Examples:\n        setnum_per_win = torch.tensor([1, 2, 1, 3]) \n        set_inds_in_win = get_continous_inds(setnum_per_win)\n        <span style=\'color: red\'># we can get: set_inds_in_win = tensor([0, 0, 1, 0, 0, 1, 2])</span>\n    \'\'\'\n    set_num = setnum_per_win.sum().item()                            <span style=\'color: red\'># set_num = 7</span>\n    setnum_per_win_cumsum = torch.cumsum(setnum_per_win, dim=0)[:-1] <span style=\'color: red\'># [1, 3, 4]</span>\n    set_win_inds = torch.full((set_num,), 0, device=setnum_per_win.device)\n    set_win_inds[setnum_per_win_cumsum] = 1                          <span style=\'color: red\'># [0, 1, 0, 1, 1, 0, 0]</span>\n    set_win_inds = torch.cumsum(set_win_inds, dim=0)                 <span style=\'color: red\'># [0, 1, 1, 2, 3, 3, 3]</span>\n    \n    roll_set_win_inds_left = torch.roll(set_win_inds, -1)  <span style=\'color: red\'># [1,  1,  2,  3, 3, 3, 0]</span>\n    diff = set_win_inds - roll_set_win_inds_left           <span style=\'color: red\'># [-1, 0, -1, -1, 0, 0, 3]</span>\n    end_pos_mask = diff != 0\n    template = torch.ones_like(set_win_inds)\n    template[end_pos_mask] = (setnum_per_win - 1) * -1     <span style=\'color: red\'># [0, 1, -1, 0, 1, 1, -2]</span>\n    set_inds_in_win = torch.cumsum(template,dim=0)         <span style=\'color: red\'># [0, 1, 0, 0, 1, 2, 0]</span>\n    set_inds_in_win[end_pos_mask] = setnum_per_win         <span style=\'color: red\'># [1, 1, 2, 1, 1, 2, 3]</span>\n    set_inds_in_win = set_inds_in_win - 1                  <span style=\'color: red\'># [0, 0, 1, 0, 0, 1, 2]</span>\n    <span style=\'color: red\'># 有7个set分到了4个window里面，第一个window有1个set，最后一个window有3个set【按前面，每个set至少36个voxel】</span>\n    <span style=\'color: red\'># 在每个window里面的索引，比如[0, 0, 1, 0, 0, 1, 2]->[0,(0,1),(0),(0,1,2)]</span>\n    return set_win_inds, set_inds_in_win   <span style=\'color: red\'># [1, 2, 1, 3]->[0, 1, 1, 2, 3, 3, 3]+[0, 0, 1, 0, 0, 1, 2]</span>\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVTInputLayer(nn.Module):\n    def get_pos_embed(self, coors_in_win, stage_id, block_id, shift_id):\n        <span style=\'color: red\'># torch.Size([15912, 3]),0,0,0</span>\n        window_shape = self.window_shape[stage_id][shift_id]              <span style=\'color: red\'># [[12, 12, 1], [24, 24, 1]]-> [12, 12, 1]</span>\n        embed_layer = self.posembed_layers[stage_id][block_id][shift_id]  <span style=\'color: red\'># Linear->BN->ReLU->Linear</span>\n        if len(window_shape) == 2:\n            ndim = 2\n            win_x, win_y = window_shape \n            win_z = 0\n        elif  window_shape[-1] == 1:            <span style=\'color: red\'># True</span>\n            ndim = 2\n            win_x, win_y = window_shape[:2]     <span style=\'color: red\'># [12, 12, 1]</span>\n            win_z = 0\n        else:\n            win_x, win_y, win_z = window_shape\n            ndim = 3\n        assert coors_in_win.size(1) == 3\n        z, y, x = coors_in_win[:, 0] - win_z/2, coors_in_win[:, 1] - win_y/2, coors_in_win[:, 2] - win_x/2  <span style=\'color: red\'># 以windows的中心左右上下偏移 ,y的范围(-6->5),x的范围(-6->5)</span>\n        if self.normalize_pos:         <span style=\'color: red\'># False</span>\n            x = x / win_x * 2 * 3.1415 <span style=\'color: red\'># [-pi, pi]</span>\n            y = y / win_y * 2 * 3.1415 <span style=\'color: red\'># [-pi, pi]</span>\n            z = z / win_z * 2 * 3.1415 <span style=\'color: red\'># [-pi, pi] </span>\n        if ndim==2:\n            location = torch.stack((x, y), dim=-1)   <span style=\'color: red\'># torch.Size([9295, 2])</span>\n        else:\n            location = torch.stack((x, y, z), dim=-1)\n        pos_embed = embed_layer(location)            <span style=\'color: red\'># torch.Size([9295, 192])</span>\n        return pos_embed\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVTInputLayer(nn.Module):\n    def get_pos_embed(self, coors_in_win, stage_id, block_id, shift_id):\n        <span style=\'color: red\'># torch.Size([15912, 3]),0,0,0</span>\n        window_shape = self.window_shape[stage_id][shift_id]              <span style=\'color: red\'># [[12, 12, 1], [24, 24, 1]]-> [24, 24, 1]</span>\n        embed_layer = self.posembed_layers[stage_id][block_id][shift_id]  <span style=\'color: red\'># Linear->BN->ReLU->Linear</span>\n        if len(window_shape) == 2:\n            ndim = 2\n            win_x, win_y = window_shape \n            win_z = 0\n        elif  window_shape[-1] == 1:            <span style=\'color: red\'># True</span>\n            ndim = 2\n            win_x, win_y = window_shape[:2]     <span style=\'color: red\'># [24, 24, 1]</span>\n            win_z = 0\n        else:\n            win_x, win_y, win_z = window_shape\n            ndim = 3\n        assert coors_in_win.size(1) == 3\n        z, y, x = coors_in_win[:, 0] - win_z/2, coors_in_win[:, 1] - win_y/2, coors_in_win[:, 2] - win_x/2  <span style=\'color: red\'># 以windows的中心左右上下偏移 ,y的范围(-12->11),x的范围(-12->11)</span>\n        if self.normalize_pos:         <span style=\'color: red\'># False</span>\n            x = x / win_x * 2 * 3.1415 <span style=\'color: red\'># [-pi, pi]</span>\n            y = y / win_y * 2 * 3.1415 <span style=\'color: red\'># [-pi, pi]</span>\n            z = z / win_z * 2 * 3.1415 <span style=\'color: red\'># [-pi, pi] </span>\n        if ndim==2:\n            location = torch.stack((x, y), dim=-1)   <span style=\'color: red\'># torch.Size([9295, 2])</span>\n        else:\n            location = torch.stack((x, y, z), dim=-1)\n        pos_embed = embed_layer(location)            <span style=\'color: red\'># torch.Size([9295, 192])</span>\n        return pos_embed\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVTBlock(nn.Module):\n    def forward(self, src, set_voxel_inds_list, set_voxel_masks_list, pos_embed_list, block_id):\n        <span style=\'color: red\'># torch.Size([15912, 192]);  [torch.Size([2, 766, 36]), torch.Size([2, 543, 36])];  [torch.Size([2, 766, 36]), torch.Size([2, 543, 36])];  </span>\n        <span style=\'color: red\'># pos_embed_list = [torch.Size([15912, 192]), torch.Size([15912, 192])]</span>\n        num_shifts = 2\n        output = src     <span style=\'color: red\'># torch.Size([9295, 192])</span>\n        <span style=\'color: red\'># TODO: bug to be fixed, mismatch of pos_embed</span>\n        for i in range(num_shifts):       <span style=\'color: red\'># 0</span>\n            set_id = i\n            shift_id = block_id % 2       <span style=\'color: red\'># 0</span>\n            pos_embed_id = i              <span style=\'color: red\'># 0</span>\n            set_voxel_inds = set_voxel_inds_list[shift_id][set_id]        <span style=\'color: red\'># torch.Size([357, 36])</span>\n            set_voxel_masks = set_voxel_masks_list[shift_id][set_id]      <span style=\'color: red\'># torch.Size([357, 36])</span>\n            pos_embed = pos_embed_list[pos_embed_id]                      <span style=\'color: red\'># torch.Size([9295, 192])</span>\n            layer = self.encoder_list[i]                                  <span style=\'color: red\'># DSVT_EncoderLayer</span>\n            output = <span style=\'color: green;font-weight: bold;\'>layer</span>(output, set_voxel_inds, set_voxel_masks, pos_embed)\n        return output\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class DSVT_EncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,activation="relu", batch_first=True, mlp_dropout=0):\n        super().__init__()\n        self.win_attn = SetAttention(d_model, nhead, dropout, dim_feedforward, activation, batch_first, mlp_dropout)\n        self.norm = nn.LayerNorm(d_model)\n        self.d_model = d_model\n    def forward(self,src,set_voxel_inds,set_voxel_masks,pos=None):  <span style=\'color: red\'># torch.Size([9295, 192]) ; torch.Size([357, 36]) ; torch.Size([357, 36]) ; torch.Size([9295, 192])</span>\n        identity = src                     \n        src = self.<span style=\'color: green;font-weight: bold;\'>win_attn</span>(src, pos, set_voxel_masks, set_voxel_inds)\n        src = src + identity\n        src = self.norm(src)\n        return src\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><font size="0"><pre class="language-python"><code class="language-python">class SetAttention(nn.Module):\n    def forward(self, src, pos=None, key_padding_mask=None, voxel_inds=None):\n        \'\'\'\n        Args:\n            src (Tensor[float]): Voxel features with shape (N, C), where N is the number of voxels.  torch.Size([9295, 192])\n            pos (Tensor[float]): Position embedding vectors with shape (N, C).                       torch.Size([9295, 192])\n            key_padding_mask (Tensor[bool]): Mask for redundant voxels within set. Shape of (set_num, set_size).  torch.Size([357, 36])\n            voxel_inds (Tensor[int]): Voxel indexs for each set. Shape of (set_num, set_size).                    torch.Size([357, 36])\n        Returns:\n            src (Tensor[float]): Voxel features.\n        \'\'\'\n        set_features = src[voxel_inds]       <span style=\'color: red\'># torch.Size([357, 36, 192])</span>\n        if pos is not None:\n            set_pos = pos[voxel_inds]        <span style=\'color: red\'># torch.Size([357, 36, 192])</span>\n        else:\n            set_pos = None\n        if pos is not None:\n            query = set_features + set_pos   <span style=\'color: red\'># torch.Size([357, 36, 192])</span>\n            key = set_features + set_pos     <span style=\'color: red\'># torch.Size([357, 36, 192])</span>\n            value = set_features             <span style=\'color: red\'># torch.Size([357, 36, 192])</span>\n        if key_padding_mask is not None:     <span style=\'color: red\'># torch.Size([357, 36])</span>\n            src2 = self.self_attn(query, key, value, key_padding_mask)[0]   <span style=\'color: red\'># torch.Size([357, 36, 192])</span>\n        else:\n            src2 = self.self_attn(query, key, value)[0]\n        <span style=\'color: red\'># map voxel featurs from set space to voxel space: (set_num, set_size, C) --> (N, C)</span>\n        flatten_inds = voxel_inds.reshape(-1)                                            <span style=\'color: red\'># torch.Size([357, 36])--》torch.Size([12852])</span>\n        unique_flatten_inds, inverse = torch.unique(flatten_inds, return_inverse=True)   <span style=\'color: red\'># torch.Size([9295])，torch.Size([12852])</span>\n        perm = torch.arange(inverse.size(0), dtype=inverse.dtype, device=inverse.device)\n        inverse, perm = inverse.flip([0]), perm.flip([0])\n        perm = inverse.new_empty(unique_flatten_inds.size(0)).scatter_(0, inverse, perm)  <span style=\'color: red\'># inverse大小12852的范围是0-9294；</span>\n        src2 = src2.reshape(-1, self.d_model)[perm]           <span style=\'color: red\'># torch.Size([9295, 192])</span>\n        <span style=\'color: red\'># FFN layer</span>\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src                                            <span style=\'color: red\'># torch.Size([9295, 192])</span>\n</code></pre></font>'}]}]}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_2d/map_to_bev/pointpillar3d_scatter.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointPillarScatter3d(nn.Module):\n    def forward(self, batch_dict, **kwargs):\n        pillar_features, coords = batch_dict[\'pillar_features\'], batch_dict[\'voxel_coords\']  <span style=\'color: red\'># torch.Size([15912, 192]) + torch.Size([15912, 4])</span>\n        batch_spatial_features = []\n        batch_size = coords[:, 0].max().int().item() + 1\n        for batch_idx in range(batch_size):\n            spatial_feature = torch.zeros(\n                self.num_bev_features_before_compression,\n                self.nz * self.nx * self.ny,\n                dtype=pillar_features.dtype,\n                device=pillar_features.device)\n            batch_mask = coords[:, 0] == batch_idx\n            this_coords = coords[batch_mask, :]\n            indices = this_coords[:, 1] * self.ny * self.nx + this_coords[:, 2] * self.nx + this_coords[:, 3]\n            indices = indices.type(torch.long)\n            pillars = pillar_features[batch_mask, :]\n            pillars = pillars.t()\n            spatial_feature[:, indices] = pillars\n            batch_spatial_features.append(spatial_feature)\n        batch_spatial_features = torch.stack(batch_spatial_features, 0)\n        batch_spatial_features = batch_spatial_features.view(batch_size, self.num_bev_features_before_compression * self.nz, self.ny, self.nx)\n        batch_dict[\'spatial_features\'] = batch_spatial_features   <span style=\'color: red\'># torch.Size([1, 192, 468, 468])</span>\n        return batch_dict\n</code></pre></font>'}]}]})</script></body>
</html>
