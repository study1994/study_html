<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>openpcdet_dsvt_pillar流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">CenterPoint-pillar(openpcdet_dsvt_pillar.yaml)</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/vfe/dynamic_voxel_vfe.py</p><span class=\'hidden-code\' data-code=\'class DynamicVoxelVFE(VFETemplate):\n    def forward(self, batch_dict, **kwargs):\n        points = batch_dict[&amp;#39;points&amp;#39;] # (batch_idx, x, y, z, i, e)  torch.Size([186187, 6])\n        points_coords = torch.floor((points[:, [1,2,3]] - self.point_cloud_range[[0,1,2]]) / self.voxel_size[[0,1,2]]).int()  # 范围调整到(0-W,0-H)内\n        mask = ((points_coords `>`= 0) &amp; (points_coords `<` self.grid_size[[0,1,2]])).all(dim=1)          # self.grid_size = tensor([468, 468,   1], device=&amp;#39;cuda:0&amp;#39;)\n        points = points[mask]\n        points_coords = points_coords[mask]\n        points_xyz = points[:, [1, 2, 3]].contiguous()\n        # 468x468,468,1  把点弄成个索引  因为是int成整数，所以有些点会是同一个索引【相同voxel是同一个索引】  torch.Size([185485])\n        merge_coords = points[:, 0].int() * self.scale_xyz + points_coords[:, 0] * self.scale_yz + points_coords[:, 1] * self.scale_z + points_coords[:, 2]\n        # torch.Size([9295])唯一索引值，torch.Size([185485])属于哪个唯一索引值，torch.Size([9295])唯一索引值有多少个点是这个数\n        unq_coords, unq_inv, unq_cnt = torch.unique(merge_coords, return_inverse=True, return_counts=True, dim=0)\n        # torch.Size([185485, 3])+torch.Size([185485])= torch.Size([9295, 3])；\n        points_mean = torch_scatter.scatter_mean(points_xyz, unq_inv, dim=0)\n        f_cluster = points_xyz - points_mean[unq_inv, :]           # torch.Size([185485, 3])\n        f_center = torch.zeros_like(points_xyz)      # 距离每个voxel的距离\n        f_center[:, 0] = points_xyz[:, 0] - (points_coords[:, 0].to(points_xyz.dtype) * self.voxel_x + self.x_offset)\n        f_center[:, 1] = points_xyz[:, 1] - (points_coords[:, 1].to(points_xyz.dtype) * self.voxel_y + self.y_offset)\n        # f_center[:, 2] = points_xyz[:, 2] - self.z_offset\n        f_center[:, 2] = points_xyz[:, 2] - (points_coords[:, 2].to(points_xyz.dtype) * self.voxel_z + self.z_offset)\n        if self.use_absolute_xyz:\n            features = [points[:, 1:], f_cluster, f_center]\n        else:\n            features = [points[:, 4:], f_cluster, f_center]\n        if self.with_distance:        # False\n            points_dist = torch.norm(points[:, 1:4], 2, dim=1, keepdim=True)\n            features.append(points_dist)\n        features = torch.cat(features, dim=-1)       # torch.Size([185485, 11])\n        for pfn in self.pfn_layers:\n            features = pfn(features, unq_inv)       # torch.Size([9295, 192])\n        # generate voxel coordinates\n        unq_coords = unq_coords.int()\n        voxel_coords = torch.stack((unq_coords // self.scale_xyz, (unq_coords % self.scale_xyz) // self.scale_yz, (unq_coords % self.scale_yz) // self.scale_z, unq_coords % self.scale_z), dim=1)\n        voxel_coords = voxel_coords[:, [0, 3, 2, 1]]  # B,z,y,x\n        batch_dict[&amp;#39;pillar_features&amp;#39;] = batch_dict[&amp;#39;voxel_features&amp;#39;] = features\n        batch_dict[&amp;#39;voxel_coords&amp;#39;] = voxel_coords        # torch.Size([9295, 4])\n        return batch_dict\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVT(nn.Module):\n    def forward(self, batch_dict):\n        voxel_info = self.`input_layer`(batch_dict)\n        voxel_feat = voxel_info[&amp;#39;voxel_feats_stage0&amp;#39;]            # torch.Size([9295, 192])\n        # [[torch.Size([2, 766, 36]), torch.Size([2, 543, 36])]]  766个set，每个set里面36个非空voxel\n        set_voxel_inds_list = [[voxel_info[f&amp;#39;set_voxel_inds_stage{s}_shift{i}&amp;#39;] for i in range(self.num_shifts[s])] for s in range(self.stage_num)]\n        # [[torch.Size([2, 766, 36]), torch.Size([2, 543, 36])]]  针对上面的voxel有重复\n        set_voxel_masks_list = [[voxel_info[f&amp;#39;set_voxel_mask_stage{s}_shift{i}&amp;#39;] for i in range(self.num_shifts[s])] for s in range(self.stage_num)]\n        # [[[torch.Size([766, 36]), torch.Size([766, 36])], [torch.Size([543, 36]), torch.Size([543, 36])]]]\n        pos_embed_list = [[[voxel_info[f&amp;#39;pos_embed_stage{s}_block{b}_shift{i}&amp;#39;] for i in range(self.num_shifts[s])] for b in range(self.set_info[s][1])] for s in range(self.stage_num)]\n        pooling_mapping_index = [voxel_info[f&amp;#39;pooling_mapping_index_stage{s+1}&amp;#39;] for s in range(self.stage_num-1)]     # []\n        pooling_index_in_pool = [voxel_info[f&amp;#39;pooling_index_in_pool_stage{s+1}&amp;#39;] for s in range(self.stage_num-1)]     # []\n        pooling_preholder_feats = [voxel_info[f&amp;#39;pooling_preholder_feats_stage{s+1}&amp;#39;] for s in range(self.stage_num-1)] # []\n        output = voxel_feat\n        block_id = 0\n        for stage_id in range(self.stage_num):\n            block_layers = self.__getattr__(f&amp;#39;stage_{stage_id}&amp;#39;)                           # 三个DSVTBlock\n            residual_norm_layers = self.__getattr__(f&amp;#39;residual_norm_stage_{stage_id}&amp;#39;)     # 四个LayerNorm\n            for i in range(len(block_layers)):\n                block = block_layers[i]\n                residual = output.clone()\n                if self.use_torch_ckpt==False:\n                    output = block(output, set_voxel_inds_list[stage_id], set_voxel_masks_list[stage_id], pos_embed_list[stage_id][i],block_id=block_id)\n                else:\n                    output = checkpoint(`block`, output, set_voxel_inds_list[stage_id], set_voxel_masks_list[stage_id], pos_embed_list[stage_id][i], block_id)\n                output = residual_norm_layers[i](output + residual)\n                block_id += 1\n            if stage_id < self.stage_num - 1:\n                # pooling\n                prepool_features = pooling_preholder_feats[stage_id].type_as(output)\n                pooled_voxel_num = prepool_features.shape[0]\n                pool_volume = prepool_features.shape[1]\n                prepool_features[pooling_mapping_index[stage_id], pooling_index_in_pool[stage_id]] = output\n                prepool_features = prepool_features.view(prepool_features.shape[0], -1)\n                \n                if self.reduction_type == &amp;#39;linear&amp;#39;:\n                    output = self.__getattr__(f&amp;#39;stage_{stage_id}_reduction&amp;#39;)(prepool_features)\n                elif self.reduction_type == &amp;#39;maxpool&amp;#39;:\n                    prepool_features = prepool_features.view(pooled_voxel_num, pool_volume, -1).permute(0, 2, 1)\n                    output = self.__getattr__(f&amp;#39;stage_{stage_id}_reduction&amp;#39;)(prepool_features).squeeze(-1)\n                elif self.reduction_type == &amp;#39;attention&amp;#39;:\n                    prepool_features = prepool_features.view(pooled_voxel_num, pool_volume, -1).permute(0, 2, 1)\n                    key_padding_mask = torch.zeros((pooled_voxel_num, pool_volume)).to(prepool_features.device).int()\n                    output = self.__getattr__(f&amp;#39;stage_{stage_id}_reduction&amp;#39;)(prepool_features, key_padding_mask)\n                else:\n                    raise NotImplementedError\n        batch_dict[&amp;#39;pillar_features&amp;#39;] = batch_dict[&amp;#39;voxel_features&amp;#39;] = output              # torch.Size([15912, 192])\n        batch_dict[&amp;#39;voxel_coords&amp;#39;] = voxel_info[f&amp;#39;voxel_coors_stage{self.stage_num - 1}&amp;#39;]  # torch.Size([15912, 4])\n        return batch_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVTInputLayer(nn.Module):\n    def forward(self, batch_dict):\n        voxel_feats = batch_dict[&amp;#39;voxel_features&amp;#39;]           # torch.Size([9295, 192])\n        voxel_coors = batch_dict[&amp;#39;voxel_coords&amp;#39;].long()      # torch.Size([9295, 4])  # B,z,y,x\n        voxel_info = {}\n        voxel_info[&amp;#39;voxel_feats_stage0&amp;#39;] = voxel_feats.clone()\n        voxel_info[&amp;#39;voxel_coors_stage0&amp;#39;] = voxel_coors.clone()\n        for stage_id in range(self.stage_num):                # 1\n            # window partition of corrsponding stage-map\n            voxel_info = self.`window_partition`(voxel_info, stage_id)\n            # generate set id of corrsponding stage-map\n            voxel_info = self.`get_set`(voxel_info, stage_id)\n            for block_id in range(self.set_info[stage_id][1]):         # [[36, 4]]->4\n                for shift_id in range(self.num_shifts[stage_id]):      # [2]->2\n                    voxel_info[f&amp;#39;pos_embed_stage{stage_id}_block{block_id}_shift{shift_id}&amp;#39;] = self.`get_pos_embed`(voxel_info[f&amp;#39;coors_in_win_stage{stage_id}_shift{shift_id}&amp;#39;], stage_id, block_id, shift_id)\n            # compute pooling information\n            if stage_id < self.stage_num - 1:\n                voxel_info = self.`subm_pooling`(voxel_info, stage_id)\n        return voxel_info\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVTInputLayer(nn.Module):\n    def window_partition(self, voxel_info, stage_id):\n        for i in range(2):\n            batch_win_inds, coors_in_win = `get_window_coors`(voxel_info[f&amp;#39;voxel_coors_stage{stage_id}&amp;#39;], self.sparse_shape_list[stage_id], self.window_shape[stage_id][i], i == 1, self.shift_list[stage_id][i])                                         \n            voxel_info[f&amp;#39;batch_win_inds_stage{stage_id}_shift{i}&amp;#39;] = batch_win_inds\n            voxel_info[f&amp;#39;coors_in_win_stage{stage_id}_shift{i}&amp;#39;] = coors_in_win    \n        return voxel_info\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/model_utils/dsvt_utils.py</p><span class=\'hidden-code\' data-code=\'def get_window_coors(coors, sparse_shape, window_shape, do_shift, shift_list=None, return_win_coors=False):\n    # torch.Size([9295, 4]);[468, 468, 1]; [12, 12, 1]; Fasle; [0, 0, 0]; False  把468x468化成了40x40个win，每个win有12x12个voxel\n    if len(window_shape) == 2:\n        win_shape_x, win_shape_y = window_shape\n        win_shape_z = sparse_shape[-1]\n    else:\n        win_shape_x, win_shape_y, win_shape_z = window_shape             # [12, 12, 1]\n    sparse_shape_x, sparse_shape_y, sparse_shape_z = sparse_shape        # [468, 468, 1]\n    assert sparse_shape_z < sparse_shape_x, &amp;#39;Usually holds... in case of wrong order&amp;#39;\n    max_num_win_x = int(np.ceil((sparse_shape_x / win_shape_x)) + 1) # plus one here to meet the needs of shift.  40\n    max_num_win_y = int(np.ceil((sparse_shape_y / win_shape_y)) + 1) # plus one here to meet the needs of shift.  40\n    max_num_win_z = int(np.ceil((sparse_shape_z / win_shape_z)) + 1) # plus one here to meet the needs of shift.  2\n    max_num_win_per_sample = max_num_win_x * max_num_win_y * max_num_win_z      # 3200\n    if do_shift:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]\n        else:\n            shift_x, shift_y, shift_z = win_shape_x//2, win_shape_y//2, win_shape_z//2\n    else:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]    # [0,0,0]\n        else:\n            shift_x, shift_y, shift_z = win_shape_x, win_shape_y, win_shape_z\n    \n    # compatibility between 2D window and 3D window\n    if sparse_shape_z == win_shape_z:  # 1==1\n        shift_z = 0\n    shifted_coors_x = coors[:, 3] + shift_x                                               # torch.Size([9295, 4])  B,Z,Y,X\n    shifted_coors_y = coors[:, 2] + shift_y\n    shifted_coors_z = coors[:, 1] + shift_z\n    win_coors_x = shifted_coors_x // win_shape_x                                          # 值范围从0-468到0-40\n    win_coors_y = shifted_coors_y // win_shape_y\n    win_coors_z = shifted_coors_z // win_shape_z\n    if len(window_shape) == 2:\n        assert (win_coors_z == 0).all()\n    # 转成唯一索引  torch.Size([9295])  这个值是非空voxel的数量\n    batch_win_inds = coors[:, 0] * max_num_win_per_sample + win_coors_x * max_num_win_y * max_num_win_z + win_coors_y * max_num_win_z +  win_coors_z\n    coors_in_win_x = shifted_coors_x % win_shape_x                                        # 余数，在窗口里面偏移多少个voxel\n    coors_in_win_y = shifted_coors_y % win_shape_y\n    coors_in_win_z = shifted_coors_z % win_shape_z\n    coors_in_win = torch.stack([coors_in_win_z, coors_in_win_y, coors_in_win_x], dim=-1)  # torch.Size([9295, 3])\n    # coors_in_win = torch.stack([coors_in_win_x, coors_in_win_y], dim=-1)\n    if return_win_coors:   # False\n        batch_win_coords = torch.stack([win_coors_z, win_coors_y, win_coors_x], dim=-1)\n        return batch_win_inds, coors_in_win, batch_win_coords\n    return batch_win_inds, coors_in_win    # 唯一索引(属于哪个win) + 窗口偏移数 = torch.Size([9295]) + torch.Size([9295，3])  这里可能属于同一个win\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/model_utils/dsvt_utils.py</p><span class=\'hidden-code\' data-code=\'def get_window_coors(coors, sparse_shape, window_shape, do_shift, shift_list=None, return_win_coors=False):\n    # torch.Size([9295, 4]);[468, 468, 1]; [24, 24, 1]; True; [6, 6, 0]相当于第(0,0,0)voxel变成了(6,6,0)voxel; False  把468x468化成了21x21个win，每个win有24x24个voxel\n    if len(window_shape) == 2:\n        win_shape_x, win_shape_y = window_shape\n        win_shape_z = sparse_shape[-1]\n    else:\n        win_shape_x, win_shape_y, win_shape_z = window_shape              # [24, 24, 1]\n    sparse_shape_x, sparse_shape_y, sparse_shape_z = sparse_shape         # [468, 468, 1]\n    assert sparse_shape_z < sparse_shape_x, &amp;#39;Usually holds... in case of wrong order&amp;#39;\n    max_num_win_x = int(np.ceil((sparse_shape_x / win_shape_x)) + 1) # plus one here to meet the needs of shift.  21\n    max_num_win_y = int(np.ceil((sparse_shape_y / win_shape_y)) + 1) # plus one here to meet the needs of shift.  21\n    max_num_win_z = int(np.ceil((sparse_shape_z / win_shape_z)) + 1) # plus one here to meet the needs of shift.  2\n    max_num_win_per_sample = max_num_win_x * max_num_win_y * max_num_win_z             # 882\n    if do_shift:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]    # [6, 6, 0]\n        else:\n            shift_x, shift_y, shift_z = win_shape_x // 2, win_shape_y // 2, win_shape_z // 2\n    else:\n        if shift_list is not None:\n            shift_x, shift_y, shift_z = shift_list[0], shift_list[1], shift_list[2]   \n        else:\n            shift_x, shift_y, shift_z = win_shape_x, win_shape_y, win_shape_z\n    \n    # compatibility between 2D window and 3D window\n    if sparse_shape_z == win_shape_z:  # 1==1\n        shift_z = 0\n    shifted_coors_x = coors[:, 3] + shift_x       # torch.Size([9295, 4])  B,Z,Y,X\n    shifted_coors_y = coors[:, 2] + shift_y       # 移动6，6，0\n    shifted_coors_z = coors[:, 1] + shift_z\n    win_coors_x = shifted_coors_x // win_shape_x  # 值范围从6-474到0-19(474//24=19)\n    win_coors_y = shifted_coors_y // win_shape_y\n    win_coors_z = shifted_coors_z // win_shape_z\n    if len(window_shape) == 2:\n        assert (win_coors_z == 0).all()\n    # 转成唯一索引  torch.Size([9295])  这个值是非空voxel的数量\n    batch_win_inds = coors[:, 0] * max_num_win_per_sample + win_coors_x * max_num_win_y * max_num_win_z + win_coors_y * max_num_win_z +  win_coors_z\n    coors_in_win_x = shifted_coors_x % win_shape_x                                        # 余数，在窗口里面偏移多少个voxel\n    coors_in_win_y = shifted_coors_y % win_shape_y\n    coors_in_win_z = shifted_coors_z % win_shape_z\n    coors_in_win = torch.stack([coors_in_win_z, coors_in_win_y, coors_in_win_x], dim=-1)  # torch.Size([9295, 3])\n    # coors_in_win = torch.stack([coors_in_win_x, coors_in_win_y], dim=-1)\n    if return_win_coors:   # False\n        batch_win_coords = torch.stack([win_coors_z, win_coors_y, win_coors_x], dim=-1)\n        return batch_win_inds, coors_in_win, batch_win_coords\n    return batch_win_inds, coors_in_win    # 唯一索引(属于哪个win) + 窗口偏移数 = torch.Size([9295]) + torch.Size([9295，3])这里可能属于同一个win\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVTInputLayer(nn.Module):\n    def get_set(self, voxel_info, stage_id):\n        batch_win_inds_shift0 = voxel_info[f&amp;#39;batch_win_inds_stage{stage_id}_shift0&amp;#39;]       # torch.Size([9295])    划分成多个窗口的索引; 40x40个win\n        coors_in_win_shift0 = voxel_info[f&amp;#39;coors_in_win_stage{stage_id}_shift0&amp;#39;]           # torch.Size([9295, 3]) 多个窗口索引的偏移的voxel数量； 是整数\n        set_voxel_inds_shift0 = self.`get_set_single_shift`(batch_win_inds_shift0, stage_id, shift_id=0, coors_in_win=coors_in_win_shift0)\n        voxel_info[f&amp;#39;set_voxel_inds_stage{stage_id}_shift0&amp;#39;] = set_voxel_inds_shift0       # torch.Size([2, 357, 36]) 这里面的索引是《第几个窗口*窗口的最大voxel数+窗口内voxel按一定方向的索引》\n        # compute key masks, voxel duplication must happen continuously                    [1,1,2,3,3]->[1,2,3,3,1]=>[T,F,F,T,F]  True表示是重复voxel索引\n        prefix_set_voxel_inds_s0 = torch.roll(set_voxel_inds_shift0.clone(), shifts=1, dims=-1)\n        prefix_set_voxel_inds_s0[ :, :, 0] = -1\n        set_voxel_mask_s0 = (set_voxel_inds_shift0 == prefix_set_voxel_inds_s0)            # torch.Size([2, 357, 36])\n        voxel_info[f&amp;#39;set_voxel_mask_stage{stage_id}_shift0&amp;#39;] = set_voxel_mask_s0\n        batch_win_inds_shift1 = voxel_info[f&amp;#39;batch_win_inds_stage{stage_id}_shift1&amp;#39;]       # torch.Size([9295])        # 12X12个win\n        coors_in_win_shift1 = voxel_info[f&amp;#39;coors_in_win_stage{stage_id}_shift1&amp;#39;]           # torch.Size([9295, 3])\n        set_voxel_inds_shift1 = self.`get_set_single_shift`(batch_win_inds_shift1, stage_id, shift_id=1, coors_in_win=coors_in_win_shift1)\n        voxel_info[f&amp;#39;set_voxel_inds_stage{stage_id}_shift1&amp;#39;] = set_voxel_inds_shift1       # torch.Size([2, 295, 36])\n        # compute key masks, voxel duplication must happen continuously\n        prefix_set_voxel_inds_s1 = torch.roll(set_voxel_inds_shift1.clone(), shifts=1, dims=-1)\n        prefix_set_voxel_inds_s1[ :, :, 0] = -1\n        set_voxel_mask_s1 = (set_voxel_inds_shift1 == prefix_set_voxel_inds_s1)\n        voxel_info[f&amp;#39;set_voxel_mask_stage{stage_id}_shift1&amp;#39;] = set_voxel_mask_s1           # torch.Size([2, 295, 36])\n        return voxel_info\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVTInputLayer(nn.Module):\n    def get_set_single_shift(self, batch_win_inds, stage_id, shift_id=None, coors_in_win=None):  # 0,0\n        device = batch_win_inds.device                # device(type=&amp;#39;cuda&amp;#39;, index=0)\n        # the number of voxels assigned to a set\n        voxel_num_set = self.set_info[stage_id][0]    # [[36, 4]]->36\n        # max number of voxels in a window            # [[12, 12, 1], [24, 24, 1]]  ->12*12*1=144  也就是一个窗口最多有144个voxels\n        max_voxel = self.window_shape[stage_id][shift_id][0] * self.window_shape[stage_id][shift_id][1] * self.window_shape[stage_id][shift_id][2]\n        # get unique set indexs\n        contiguous_win_inds = torch.unique(batch_win_inds, return_inverse=True)[1]   # torch.Size([9295])->torch.Size([9295])个voxels在40x40个windows 这里面重复值，表示属于哪个窗口\n        voxelnum_per_win = torch.bincount(contiguous_win_inds)                       # torch.Size([168])-168个windows里面有voxel，这里面的值是每个windows里有多少个voxel\n        win_num = voxelnum_per_win.shape[0]                                          # 168个窗口，每个窗口里面36个voxel就是一个set\n        setnum_per_win_float = voxelnum_per_win / voxel_num_set                      # 如果超过36就是1了  torch.Size([168])=torch.Size([168])/36\n        setnum_per_win = torch.ceil(setnum_per_win_float).long()                     # 往上四舍五入\n        set_win_inds, set_inds_in_win = `get_continous_inds`(setnum_per_win)         # torch.Size([168])->torch.Size([357])(值范围0-167)+torch.Size([357])(值范围0-3,一个set最多4个voxel)\n                                                                                     # 168个窗口，357个set，\n        # compution of Eq.3 in &amp;#39;DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets&amp;#39; - https://arxiv.org/abs/2301.06051, \n        # for each window, we can get voxel indexs belong to different sets.           # 假设[0, 1, 1, 2, 3, 3, 3]，[0, 0, 1, 0, 0, 1, 2]\n        offset_idx = set_inds_in_win[:,None].repeat(1, voxel_num_set) * voxel_num_set  # torch.Size([357, 36])\n        base_idx = torch.arange(0, voxel_num_set, 1, device=device)                    # torch.Size([36])  公式里面的k从0-35\n        base_select_idx = offset_idx + base_idx                                        # torch.Size([357, 36])  原先的值是0，则为0-35，是1，则为36-71\n        base_select_idx = base_select_idx * voxelnum_per_win[set_win_inds][:,None]\n        base_select_idx = base_select_idx.double() / (setnum_per_win[set_win_inds] * voxel_num_set)[:,None].double()  # torch.Size([357, 36])  357个set，每个set最多36个voxel\n        base_select_idx = torch.floor(base_select_idx)                        # 第0个set的voxel的索引依次是 [0., 0., 0., 0., 0., 1., 1., 1.,......]\n        # obtain unique indexs in whole space\n        select_idx = base_select_idx\n        select_idx = select_idx + set_win_inds.view(-1, 1) * max_voxel        # torch.Size([357, 36]) 共357个set，每个set有36个voxel，里面的voxel索引有重复，值的范围在0->9295\n           \n        # this function will return unordered inner window indexs of each voxel # contiguous_win_inds[:10]=[0, 1, 0, 0, 0, 0, 1, 0, 0, 1]->[0, 0, 1, 2, 3, 4, 1, 5, 6, 2]\n                                                                                # 依次表示属于第0个窗口，第1个窗口，第0个窗口 -> 第0个窗口的第0个，第1个窗口的第0个，第0个窗口的第1个      \n        inner_voxel_inds = get_inner_win_inds_cuda(contiguous_win_inds)         # torch.Size([9295]) 最大值167(168个窗口)->最大值142(窗口最大的voxel的数量为142+1)\n        global_voxel_inds = contiguous_win_inds * max_voxel + inner_voxel_inds  # torch.Size([9295]) 前10个[0*144, 1*144, 0*144, 0*144, 0*144, 0*144, 1*144, 0*144, 0*144, 1*144]+[0, 0, 1, 2, 3, 4, 1, 5, 6, 2]\n        _, order1 = torch.sort(global_voxel_inds)                               # 从小到大排序的索引 inner_voxel_inds[order1]->[(0,1,2,3,4,5,6),(0,1,2)]  顺序是第0个窗口的0个voxel，1个voxel，。。。\n        # get y-axis partition results\n        global_voxel_inds_sorty = contiguous_win_inds * max_voxel + \\\n                coors_in_win[:,1] * self.window_shape[stage_id][shift_id][0] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,2] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,0]                                               # coors_in_win->torch.Size([9295, 3]) + [12, 12, 1]  \n        _, order2 = torch.sort(global_voxel_inds_sorty)                         # torch.Size([9295])  这里的顺序是第0个窗口的，然后按照 y*12*1+x*1+z\n        inner_voxel_inds_sorty = -torch.ones_like(inner_voxel_inds)                      \n        inner_voxel_inds_sorty.scatter_(dim=0, index=order2, src=inner_voxel_inds[order1])     # get y-axis ordered inner window indexs of each voxel  torch.Size([9295])前10个 \n                                                                                               # inner_voxel_inds_sorty[:10] = [(2, 3, 0, 1, 4), (0, 1, 2, 3), 2]\n        voxel_inds_in_batch_sorty = inner_voxel_inds_sorty + max_voxel * contiguous_win_inds   # voxel_inds_in_batch_sorty[:10] = [(2, 3, 0, 1, 4),(576, 577, 578, 579), 434]\n        voxel_inds_padding_sorty = -1 * torch.ones((win_num * max_voxel), dtype=torch.long, device=device)   # 168*144 = torch.Size([36864])=\n        voxel_inds_padding_sorty[voxel_inds_in_batch_sorty] = torch.arange(0, voxel_inds_in_batch_sorty.shape[0], dtype=torch.long, device=device)\n        set_voxel_inds_sorty = voxel_inds_padding_sorty[select_idx.long()]     # torch.Size([357, 36])  范围0->9295\n \n        # get x-axis partition results\n        global_voxel_inds_sortx = contiguous_win_inds * max_voxel + \\\n                coors_in_win[:,2] * self.window_shape[stage_id][shift_id][1] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,1] * self.window_shape[stage_id][shift_id][2] + \\\n                coors_in_win[:,0]\n        _, order2 = torch.sort(global_voxel_inds_sortx)                         # torch.Size([9295])  这里的顺序是第0个窗口的，然后按照 x*12*1+y*1+z\n        inner_voxel_inds_sortx = -torch.ones_like(inner_voxel_inds)\n        inner_voxel_inds_sortx.scatter_(dim=0,index=order2, src=inner_voxel_inds[order1])      # get x-axis ordered inner window indexs of each voxel\n        voxel_inds_in_batch_sortx = inner_voxel_inds_sortx + max_voxel * contiguous_win_inds\n        voxel_inds_padding_sortx = -1 * torch.ones((win_num * max_voxel), dtype=torch.long, device=device)\n        voxel_inds_padding_sortx[voxel_inds_in_batch_sortx] = torch.arange(0, voxel_inds_in_batch_sortx.shape[0], dtype=torch.long, device=device)\n        set_voxel_inds_sortx = voxel_inds_padding_sortx[select_idx.long()]                     # torch.Size([357, 36])  范围0->9295\n        all_set_voxel_inds = torch.stack((set_voxel_inds_sorty, set_voxel_inds_sortx), dim=0)  # torch.Size([2, 357, 36])\n        return all_set_voxel_inds                                                \n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/model_utils/dsvt_utils.py</p><span class=\'hidden-code\' data-code=\'def get_continous_inds(setnum_per_win):\n    &amp;#39;&amp;#39;&amp;#39;\n    Args:\n        setnum_per_win (Tensor[int]): Number of sets assigned to each window with shape (win_num).\n    Returns:\n        set_win_inds (Tensor[int]): Window indexs of each set with shape (set_num).\n        set_inds_in_win (Tensor[int]): Set indexs inner window with shape (set_num).\n    Examples:\n        setnum_per_win = torch.tensor([1, 2, 1, 3]) \n        set_inds_in_win = get_continous_inds(setnum_per_win)\n        # we can get: set_inds_in_win = tensor([0, 0, 1, 0, 0, 1, 2])\n    &amp;#39;&amp;#39;&amp;#39;\n    set_num = setnum_per_win.sum().item()                            # set_num = 7\n    setnum_per_win_cumsum = torch.cumsum(setnum_per_win, dim=0)[:-1] # [1, 3, 4]\n    set_win_inds = torch.full((set_num,), 0, device=setnum_per_win.device)\n    set_win_inds[setnum_per_win_cumsum] = 1                          # [0, 1, 0, 1, 1, 0, 0]\n    set_win_inds = torch.cumsum(set_win_inds, dim=0)                 # [0, 1, 1, 2, 3, 3, 3]\n    \n    roll_set_win_inds_left = torch.roll(set_win_inds, -1)  # [1,  1,  2,  3, 3, 3, 0]\n    diff = set_win_inds - roll_set_win_inds_left           # [-1, 0, -1, -1, 0, 0, 3]\n    end_pos_mask = diff != 0\n    template = torch.ones_like(set_win_inds)\n    template[end_pos_mask] = (setnum_per_win - 1) * -1     # [0, 1, -1, 0, 1, 1, -2]\n    set_inds_in_win = torch.cumsum(template,dim=0)         # [0, 1, 0, 0, 1, 2, 0]\n    set_inds_in_win[end_pos_mask] = setnum_per_win         # [1, 1, 2, 1, 1, 2, 3]\n    set_inds_in_win = set_inds_in_win - 1                  # [0, 0, 1, 0, 0, 1, 2]\n    # 有7个set分到了4个window里面，第一个window有1个set，最后一个window有3个set【按前面，每个set至少36个voxel】\n    # 在每个window里面的索引，比如[0, 0, 1, 0, 0, 1, 2]->[0,(0,1),(0),(0,1,2)]\n    return set_win_inds, set_inds_in_win   # [1, 2, 1, 3]->[0, 1, 1, 2, 3, 3, 3]+[0, 0, 1, 0, 0, 1, 2]\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVTInputLayer(nn.Module):\n    def get_pos_embed(self, coors_in_win, stage_id, block_id, shift_id):\n        # torch.Size([15912, 3]),0,0,0\n        window_shape = self.window_shape[stage_id][shift_id]              # [[12, 12, 1], [24, 24, 1]]-> [12, 12, 1]   (0,0)+(0,1)\n        embed_layer = self.posembed_layers[stage_id][block_id][shift_id]  # Linear->BN->ReLU->Linear   # 这里有四个block_id\n        if len(window_shape) == 2:\n            ndim = 2\n            win_x, win_y = window_shape \n            win_z = 0\n        elif  window_shape[-1] == 1:            # True\n            ndim = 2\n            win_x, win_y = window_shape[:2]     # [12, 12, 1]\n            win_z = 0\n        else:\n            win_x, win_y, win_z = window_shape\n            ndim = 3\n        assert coors_in_win.size(1) == 3\n        z, y, x = coors_in_win[:, 0] - win_z/2, coors_in_win[:, 1] - win_y/2, coors_in_win[:, 2] - win_x/2  # 以windows的中心左右上下偏移 ,y的范围(-6->5),x的范围(-6->5)\n        if self.normalize_pos:         # False\n            x = x / win_x * 2 * 3.1415 # [-pi, pi]\n            y = y / win_y * 2 * 3.1415 # [-pi, pi]\n            z = z / win_z * 2 * 3.1415 # [-pi, pi] \n        if ndim==2:\n            location = torch.stack((x, y), dim=-1)   # torch.Size([9295, 2])\n        else:\n            location = torch.stack((x, y, z), dim=-1)\n        pos_embed = embed_layer(location)            # torch.Size([9295, 192])\n        return pos_embed\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVTInputLayer(nn.Module):\n    def get_pos_embed(self, coors_in_win, stage_id, block_id, shift_id):\n        # torch.Size([15912, 3]),0,0,1\n        window_shape = self.window_shape[stage_id][shift_id]              # [[12, 12, 1], [24, 24, 1]]-> [24, 24, 1]\n        embed_layer = self.posembed_layers[stage_id][block_id][shift_id]  # Linear->BN->ReLU->Linear\n        if len(window_shape) == 2:\n            ndim = 2\n            win_x, win_y = window_shape \n            win_z = 0\n        elif  window_shape[-1] == 1:            # True\n            ndim = 2\n            win_x, win_y = window_shape[:2]     # [24, 24, 1]\n            win_z = 0\n        else:\n            win_x, win_y, win_z = window_shape\n            ndim = 3\n        assert coors_in_win.size(1) == 3\n        z, y, x = coors_in_win[:, 0] - win_z/2, coors_in_win[:, 1] - win_y/2, coors_in_win[:, 2] - win_x/2  # 以windows的中心左右上下偏移 ,y的范围(-12->11),x的范围(-12->11)\n        if self.normalize_pos:         # False\n            x = x / win_x * 2 * 3.1415 # [-pi, pi]\n            y = y / win_y * 2 * 3.1415 # [-pi, pi]\n            z = z / win_z * 2 * 3.1415 # [-pi, pi] \n        if ndim==2:\n            location = torch.stack((x, y), dim=-1)   # torch.Size([9295, 2])\n        else:\n            location = torch.stack((x, y, z), dim=-1)\n        pos_embed = embed_layer(location)            # torch.Size([9295, 192])\n        return pos_embed\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVTBlock(nn.Module):\n    def forward(self, src, set_voxel_inds_list, set_voxel_masks_list, pos_embed_list, block_id):\n        # torch.Size([15912, 192]);  [torch.Size([2, 766, 36]), torch.Size([2, 543, 36])];  [torch.Size([2, 766, 36]), torch.Size([2, 543, 36])];  \n        # pos_embed_list = [torch.Size([15912, 192]), torch.Size([15912, 192])]\n        num_shifts = 2\n        output = src     # torch.Size([9295, 192])\n        # TODO: bug to be fixed, mismatch of pos_embed\n        for i in range(num_shifts):       # 0\n            set_id = i\n            shift_id = block_id % 2       # 0\n            pos_embed_id = i              # 0\n            set_voxel_inds = set_voxel_inds_list[shift_id][set_id]        # torch.Size([357, 36])\n            set_voxel_masks = set_voxel_masks_list[shift_id][set_id]      # torch.Size([357, 36])\n            pos_embed = pos_embed_list[pos_embed_id]                      # torch.Size([9295, 192])\n            layer = self.encoder_list[i]                                  # DSVT_EncoderLayer\n            output = `layer`(output, set_voxel_inds, set_voxel_masks, pos_embed)\n        return output\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class DSVT_EncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,activation=&amp;#39;relu&amp;#39;, batch_first=True, mlp_dropout=0):\n        super().__init__()\n        self.win_attn = SetAttention(d_model, nhead, dropout, dim_feedforward, activation, batch_first, mlp_dropout)\n        self.norm = nn.LayerNorm(d_model)\n        self.d_model = d_model\n    def forward(self,src,set_voxel_inds,set_voxel_masks,pos=None):  # torch.Size([9295, 192]) ; torch.Size([357, 36]) ; torch.Size([357, 36]) ; torch.Size([9295, 192])\n        identity = src                     \n        src = self.`win_attn`(src, pos, set_voxel_masks, set_voxel_inds)\n        src = src + identity\n        src = self.norm(src)\n        return src\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/dsvt.py</p><span class=\'hidden-code\' data-code=\'class SetAttention(nn.Module):\n    def forward(self, src, pos=None, key_padding_mask=None, voxel_inds=None):\n        &amp;#39;&amp;#39;&amp;#39;\n        Args:\n            src (Tensor[float]): Voxel features with shape (N, C), where N is the number of voxels.  torch.Size([9295, 192])\n            pos (Tensor[float]): Position embedding vectors with shape (N, C).                       torch.Size([9295, 192])\n            key_padding_mask (Tensor[bool]): Mask for redundant voxels within set. Shape of (set_num, set_size).  torch.Size([357, 36])\n            voxel_inds (Tensor[int]): Voxel indexs for each set. Shape of (set_num, set_size).                    torch.Size([357, 36])\n        Returns:\n            src (Tensor[float]): Voxel features.\n        &amp;#39;&amp;#39;&amp;#39;\n        set_features = src[voxel_inds]       # torch.Size([357, 36, 192])\n        if pos is not None:\n            set_pos = pos[voxel_inds]        # torch.Size([357, 36, 192])\n        else:\n            set_pos = None\n        if pos is not None:\n            query = set_features + set_pos   # torch.Size([357, 36, 192])\n            key = set_features + set_pos     # torch.Size([357, 36, 192])\n            value = set_features             # torch.Size([357, 36, 192])\n        if key_padding_mask is not None:     # torch.Size([357, 36])\n            src2 = self.self_attn(query, key, value, key_padding_mask)[0]   # torch.Size([357, 36, 192])\n        else:\n            src2 = self.self_attn(query, key, value)[0]\n        # map voxel featurs from set space to voxel space: (set_num, set_size, C) --> (N, C)\n        flatten_inds = voxel_inds.reshape(-1)                                            # torch.Size([357, 36])--》torch.Size([12852])\n        unique_flatten_inds, inverse = torch.unique(flatten_inds, return_inverse=True)   # torch.Size([9295])，torch.Size([12852])\n        perm = torch.arange(inverse.size(0), dtype=inverse.dtype, device=inverse.device)\n        inverse, perm = inverse.flip([0]), perm.flip([0])\n        perm = inverse.new_empty(unique_flatten_inds.size(0)).scatter_(0, inverse, perm)  # inverse大小12852的范围是0-9294；\n        src2 = src2.reshape(-1, self.d_model)[perm]           # torch.Size([9295, 192])\n        # FFN layer\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src                                            # torch.Size([9295, 192])\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_2d/map_to_bev/pointpillar3d_scatter.py</p><span class=\'hidden-code\' data-code=\'class PointPillarScatter3d(nn.Module):\n    def forward(self, batch_dict, **kwargs):\n        pillar_features, coords = batch_dict[&amp;#39;pillar_features&amp;#39;], batch_dict[&amp;#39;voxel_coords&amp;#39;]  # torch.Size([15912, 192]) + torch.Size([15912, 4])\n        batch_spatial_features = []\n        batch_size = coords[:, 0].max().int().item() + 1\n        for batch_idx in range(batch_size):\n            spatial_feature = torch.zeros(\n                self.num_bev_features_before_compression,\n                self.nz * self.nx * self.ny,\n                dtype=pillar_features.dtype,\n                device=pillar_features.device)\n            batch_mask = coords[:, 0] == batch_idx\n            this_coords = coords[batch_mask, :]\n            indices = this_coords[:, 1] * self.ny * self.nx + this_coords[:, 2] * self.nx + this_coords[:, 3]\n            indices = indices.type(torch.long)\n            pillars = pillar_features[batch_mask, :]\n            pillars = pillars.t()\n            spatial_feature[:, indices] = pillars\n            batch_spatial_features.append(spatial_feature)\n        batch_spatial_features = torch.stack(batch_spatial_features, 0)\n        batch_spatial_features = batch_spatial_features.view(batch_size, self.num_bev_features_before_compression * self.nz, self.ny, self.nx)\n        batch_dict[&amp;#39;spatial_features&amp;#39;] = batch_spatial_features   # torch.Size([1, 192, 468, 468])\n        return batch_dict\n\'> </span>'}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
