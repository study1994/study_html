<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>IASSD训练过程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">数据处理</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/datasets/kitti/kitti_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class KittiDataset(DatasetTemplate):\n    def __getitem__(self, index):            <span style=\'color: red\'># index = 4</span>\n        if self._merge_all_iters_to_one_epoch:\n            index = index % len(self.kitti_infos)\n        info = copy.deepcopy(self.kitti_infos[index])       <span style=\'color: red\'># info.keys()=dict_keys([\'point_cloud\', \'image\', \'calib\', \'annos\'])</span>\n        sample_idx = info[\'point_cloud\'][\'lidar_idx\']       <span style=\'color: red\'># sample_idx=\'006736\'</span>\n        img_shape = info[\'image\'][\'image_shape\']            <span style=\'color: red\'># img_shape=array([ 375, 1242]) </span>\n        calib = self.get_calib(sample_idx)                  <span style=\'color: red\'># calib=<pcdet.utils.calibration_kitti.Calibration object at 0x7fcd5f42ddd8>    </span>\n        get_item_list = self.dataset_cfg.get(\'GET_ITEM_LIST\', [\'points\'])      <span style=\'color: red\'># get_item_list=[\'points\']</span>\n        input_dict = {\'frame_id\': sample_idx, \'calib\': calib}\n        if \'annos\' in info:                                 <span style=\'color: red\'># 相机坐标系转到雷达坐标系：8个目标  </span>\n            annos = info[\'annos\']\n            annos = common_utils.drop_info_with_name(annos, name=\'DontCare\')\n            loc, dims, rots = annos[\'location\'], annos[\'dimensions\'], annos[\'rotation_y\']\n            gt_names = annos[\'name\']\n            gt_boxes_camera = np.concatenate([loc, dims, rots[..., np.newaxis]], axis=1).astype(np.float32)   <span style=\'color: red\'># gt_boxes_camera.shape=(8,7)</span>\n            gt_boxes_lidar = box_utils.boxes3d_kitti_camera_to_lidar(gt_boxes_camera, calib)\n            input_dict.update({\'gt_names\': gt_names, \'gt_boxes\': gt_boxes_lidar})                             <span style=\'color: red\'># gt_names.shape=(8,)</span>\n            if "gt_boxes2d" in get_item_list:\n                input_dict[\'gt_boxes2d\'] = annos["bbox"]\n            road_plane = self.get_road_plane(sample_idx)      <span style=\'color: red\'># 下载road planes，这对于训练中的数据增强是可选的？</span>\n            if road_plane is not None:                        <span style=\'color: red\'># road_plane=array([-0.0154115,-0.99985959,0.00657865,1.63900299]) </span>\n                input_dict[\'road_plane\'] = road_plane\n        if "points" in get_item_list:  \n            points = self.get_lidar(sample_idx)               <span style=\'color: red\'># points=(115922,4); FOV_POINTS_ONLY=True;</span>\n            if self.dataset_cfg.FOV_POINTS_ONLY:\n                pts_rect = calib.<span style=\'color: green;font-weight: bold;\'>lidar_to_rect</span>(points[:, 0:3])\n                fov_flag = self.get_fov_flag(pts_rect, img_shape, calib) <span style=\'color: red\'># 这里相当于映射到图片上过滤掉一些点</span>\n                points = points[fov_flag]\n            input_dict[\'points\'] = points\n        if "images" in get_item_list:\n            input_dict[\'images\'] = self.get_image(sample_idx)\n        if "depth_maps" in get_item_list:\n            input_dict[\'depth_maps\'] = self.get_depth_map(sample_idx)\n        if "calib_matricies" in get_item_list:\n            input_dict["trans_lidar_to_cam"], input_dict["trans_cam_to_img"] = kitti_utils.calib_to_matricies(calib)\n        data_dict = self.<span style=\'color: green;font-weight: bold;\'>prepare_data</span>(data_dict=input_dict)\n        data_dict[\'image_shape\'] = img_shape\n        return data_dict\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/utils/calibration_kitti.py</p><font size="0"><pre class="language-python"><code class="language-python">class Calibration(object):\n    def lidar_to_rect(self, pts_lidar):\n        pts_lidar_hom = self.cart_to_hom(pts_lidar)                      <span style=\'color: red\'># pts_lidar=(115922,3)->(115922,4);</span>\n        pts_rect = np.dot(pts_lidar_hom, np.dot(self.V2C.T, self.R0.T))  <span style=\'color: red\'># pts_rect=(115922,3);</span>\n        <span style=\'color: red\'># pts_rect = reduce(np.dot, (pts_lidar_hom, self.V2C.T, self.R0.T))</span>\n        return pts_rect\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/utils/calibration_kitti.py</p><font size="0"><pre class="language-python"><code class="language-python">class Calibration(object):\n    def lidar_to_rect(self, pts_lidar):\n        pts_lidar_hom = self.cart_to_hom(pts_lidar)  <span style=\'color: red\'># pts_hom = np.hstack((pts,np.ones((pts.shape[0],1),dtype=np.float32)))          </span>\n                                                     <span style=\'color: red\'># pts_lidar=(115922,3)->(115922,4);</span>\n        pts_rect = np.dot(pts_lidar_hom, np.dot(self.V2C.T, self.R0.T))  <span style=\'color: red\'># pts_rect=(115922,3);这里相当于映射到图片上过滤掉一些点</span>\n        <span style=\'color: red\'># pts_rect = reduce(np.dot, (pts_lidar_hom, self.V2C.T, self.R0.T))</span>\n        return pts_rect\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/datasets/dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class DatasetTemplate(torch_data.Dataset):\n    def prepare_data(self, data_dict):\n        if self.training:\n            assert \'gt_boxes\' in data_dict, \'gt_boxes should be provided for training\'\n            gt_boxes_mask = np.array([n in self.class_names for n in data_dict[\'gt_names\']], dtype=np.bool_)\n            data_dict = self.data_augmentor.forward(data_dict={**data_dict,\'gt_boxes_mask\': gt_boxes_mask})\n        if data_dict.get(\'gt_boxes\', None) is not None:          <span style=\'color: red\'># True   </span>\n            selected = common_utils.keep_arrays_by_name(data_dict[\'gt_names\'], self.class_names)\n            data_dict[\'gt_boxes\'] = data_dict[\'gt_boxes\'][selected]\n            data_dict[\'gt_names\'] = data_dict[\'gt_names\'][selected]\n            gt_classes = np.array([self.class_names.index(n) + 1 for n in data_dict[\'gt_names\']], dtype=np.int32)\n            <span style=\'color: red\'># 把类别合到gt_boxes里面:data_dict[\'gt_boxes\'].shape=(39,8)</span>\n            gt_boxes = np.concatenate((data_dict[\'gt_boxes\'], gt_classes.reshape(-1, 1).astype(np.float32)), axis=1)  \n            data_dict[\'gt_boxes\'] = gt_boxes\n            if data_dict.get(\'gt_boxes2d\', None) is not None:\n                data_dict[\'gt_boxes2d\'] = data_dict[\'gt_boxes2d\'][selected]\n        if data_dict.get(\'points\', None) is not None:  <span style=\'color: red\'># data_dict[\'points\'][0]=array([28.520061,16.495598,-0.36022797,0.],dtype=float32)</span>\n            data_dict = self.point_feature_encoder.<span style=\'color: green;font-weight: bold;\'>forward</span>(data_dict)\n        data_dict = self.data_processor.<span style=\'color: green;font-weight: bold;\'>forward</span>(data_dict=data_dict)    <span style=\'color: red\'># 数据增强处理</span>\n        if self.training and len(data_dict[\'gt_boxes\']) == 0:\n            new_index = np.random.randint(self.__len__())\n            return self.__getitem__(new_index)\n        data_dict.pop(\'gt_names\', None)\n        return data_dict\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/datasets/processor/point_feature_encoder.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointFeatureEncoder(object):\n    def forward(self, data_dict):\n        <span style=\'color: red\'># data_dict[\'points\'].shape=(22307, 4); use_lead_xyz=True;后面是false </span>\n        data_dict[\'points\'], use_lead_xyz = getattr(self, self.point_encoding_config.encoding_type)(data_dict[\'points\'])\n        data_dict[\'use_lead_xyz\'] = use_lead_xyz\n        if self.point_encoding_config.get(\'filter_sweeps\', False) and \'timestamp\' in self.src_feature_list:\n            max_sweeps = self.point_encoding_config.max_sweeps\n            idx = self.src_feature_list.index(\'timestamp\')\n            dt = np.round(data_dict[\'points\'][:, idx], 2)\n            max_dt = sorted(np.unique(dt))[min(len(np.unique(dt))-1, max_sweeps-1)]\n            data_dict[\'points\'] = data_dict[\'points\'][dt <= max_dt]\n        return data_dict\n</code></pre></font>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型训练</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/detectors/IASSD.py:</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD(Detector3DTemplate):\n    def forward(self, batch_dict):\n        for cur_module in self.module_list:\n            batch_dict = <span style=\'color: green;font-weight: bold;\'>cur_module</span>(batch_dict)\n        if self.training:\n            loss, tb_dict, disp_dict = self.get_training_loss()\n            ret_dict = {\'loss\': loss}\n            return ret_dict, tb_dict, disp_dict\n        else:\n            pred_dicts, recall_dicts = self.post_processing(batch_dict)\n            return pred_dicts, recall_dicts\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      <span style=\'color: red\'># batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]</span>\n        batch_size = batch_dict[\'batch_size\']\n        points = batch_dict[\'points\']\n        batch_idx, xyz, features = self.break_up_pc(points)     <span style=\'color: red\'># batch_idx.shape=(32768); xyz.shape=(32768,3); features.shape=(32768,1)</span>\n        xyz_batch_cnt = xyz.new_zeros(batch_size).int()\n        for bs_idx in range(batch_size):\n            xyz_batch_cnt[bs_idx] = (batch_idx == bs_idx).sum()\n        assert xyz_batch_cnt.min() == xyz_batch_cnt.max()\n        xyz = xyz.view(batch_size, -1, 3)                       <span style=\'color: red\'># (2,16384,3);</span>\n        features = features.view(batch_size, -1, features.shape[-1]).permute(0, 2, 1).contiguous() if features is not None else None  <span style=\'color: red\'># (2,1,16384);</span>\n        encoder_xyz, encoder_features, sa_ins_preds = [xyz], [features], []\n        encoder_coords = [torch.cat([batch_idx.view(batch_size, -1, 1), xyz], dim=-1)]  <span style=\'color: red\'># (2,16384,4);</span>\n        <span style=\'color: red\'># self.layer_types=[\'SA_Layer\', \'SA_Layer\', \'SA_Layer\', \'SA_Layer\', \'Vote_Layer\', \'SA_Layer\'];</span>\n        li_cls_pred = None \n        for <span style=\'color: green;font-weight: bold;\'>i</span> in range(len(self.SA_modules)):\n            xyz_input = encoder_xyz[self.layer_inputs[i]]                  <span style=\'color: red\'># self.layer_inputs=[0, 1, 2, 3, 4, 3];</span>\n            feature_input = encoder_features[self.layer_inputs[i]]      \n            if self.layer_types[i] == \'SA_Layer\':                          <span style=\'color: red\'># self.ctr_idx_list=[-1, -1, -1, -1, -1, 5];</span>\n                ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None\n                li_xyz, li_features, li_cls_pred = self.SA_modules[i](xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)\n            elif self.layer_types[i] == \'Vote_Layer\': <span style=\'color: red\'># i=4</span>\n                li_xyz, li_features, xyz_select, ctr_offsets = self.SA_modules[i](xyz_input, feature_input)\n                centers = li_xyz                       <span style=\'color: red\'># (2,256,3);;->(512,4)</span>\n                centers_origin = xyz_select            <span style=\'color: red\'># (2,256,3);;->(512,4)</span>\n                center_origin_batch_idx = batch_idx.view(batch_size, -1)[:, :centers_origin.shape[1]]\n                encoder_coords.append(torch.cat([center_origin_batch_idx[..., None].float(),centers_origin.view(batch_size, -1, 3)],dim =-1))\n                    \n            encoder_xyz.append(li_xyz)\n            li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n            encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1))\n            encoder_features.append(li_features)            \n            if li_cls_pred is not None:\n                li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]\n                sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n            else:\n                sa_ins_preds.append([])\n           \n        ctr_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]       <span style=\'color: red\'># (2,256)</span>\n        ctr_batch_idx = ctr_batch_idx.contiguous().view(-1)                       <span style=\'color: red\'># (512)</span>\n        <span style=\'color: red\'># batch_dict里面的key和value值</span>\n        <span style=\'color: red\'># frame_id:array([\'006209\', \'005966\'], dtype=\'<U6\')</span>\n        <span style=\'color: red\'># gt_boxes:torch.Size([2, 36, 8])</span>\n        <span style=\'color: red\'># points:torch.Size([32768, 5])</span>\n        <span style=\'color: red\'># use_lead_xyz:tensor([1., 1.], device=\'cuda:0\')</span>\n        <span style=\'color: red\'># image_shape:tensor([[ 375, 1242],[ 375, 1242]],torch.int32)</span>\n        <span style=\'color: red\'># batch_size:2</span>\n        batch_dict[\'ctr_offsets\'] = torch.cat((ctr_batch_idx[:, None].float(), ctr_offsets.contiguous().view(-1, 3)), dim=1)  <span style=\'color: red\'># torch.Size([512, 4])</span>\n        batch_dict[\'centers\'] = torch.cat((ctr_batch_idx[:, None].float(), centers.contiguous().view(-1, 3)), dim=1)          <span style=\'color: red\'># torch.Size([512, 4])</span>\n        batch_dict[\'centers_origin\'] = torch.cat((ctr_batch_idx[:, None].float(), centers_origin.contiguous().view(-1, 3)), dim=1) <span style=\'color: red\'># torch.Size([512, 4])</span>\n    \n        center_features = encoder_features[-1].permute(0, 2, 1).contiguous().view(-1, encoder_features[-1].shape[1])  <span style=\'color: red\'># torch.Size([512, 512])</span>\n        batch_dict[\'centers_features\'] = center_features\n        batch_dict[\'ctr_batch_idx\'] = ctr_batch_idx        <span style=\'color: red\'># torch.Size([512])值在0->batch_size-1之间</span>\n        batch_dict[\'encoder_xyz\'] = encoder_xyz            <span style=\'color: red\'># len()=7  [(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3),(2,256,3),(2,256,3)]  vote_ayer层的时候加了两次</span>\n        batch_dict[\'encoder_coords\'] = encoder_coords      <span style=\'color: red\'># len()=8  [(2,16384,4),(2,4096,4),(2,1024,4), (2,512,4),(2,256,4),(2,256,4),(2,256,4),(2,256,4)] </span>\n        batch_dict[\'sa_ins_preds\'] = sa_ins_preds          <span style=\'color: red\'># len()=6  [             [],        (2,1024,4),  (2,512,4),   [],        [], []] </span>\n        batch_dict[\'encoder_features\'] = encoder_features  <span style=\'color: red\'># len()=7 [(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256),[],(2,512,256)]</span>\n        return batch_dict\n</code></pre><p></font>\n<a href="https://gitee.com/zhao-study/data_code/blob/master/3target_detection_3D/project/IA_SSD/model.log">model.log</a><br></p>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      <span style=\'color: red\'># batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]</span>\n        i=0\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  <span style=\'color: red\'># 0->xyz_input.shape=(2,16384,3);</span>\n        feature_input = encoder_features[self.layer_inputs[i]]         <span style=\'color: red\'># 0->feature_input=(2,1,16384);</span>\n        if self.layer_types[i] == \'SA_Layer\':                          <span style=\'color: red\'># \'SA_Layer\'</span>\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         <span style=\'color: red\'># -1,None</span>\n            li_xyz, li_features, li_cls_pred = self.<span style=\'color: green;font-weight: bold;\'>SA_modules[i]</span>(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  <span style=\'color: red\'># li_cls_pred=None;</span>\n        elif self.layer_types[i] == \'Vote_Layer\': <span style=\'color: red\'># i=4</span>\n            pass\n                \n        encoder_xyz.append(li_xyz)                                                <span style=\'color: red\'># encoder_xyz=[(2,16384,3),(2,4096,3)] </span>\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) <span style=\'color: red\'># encoder_coords=[(2,16384,4),(2,4096,4)]->batch_size+ xyz </span>\n        encoder_features.append(li_features)                                      <span style=\'color: red\'># encoder_features=[(2,1,16384),(2,64,4096)]</span>\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              <span style=\'color: red\'># [[]] </span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        <span style=\'color: red\'># :param xyz: (B, N, 3) tensor of the xyz coordinates of the features</span>\n        <span style=\'color: red\'># :param features: (B, C, N) tensor of the descriptors of the the features</span>\n        <span style=\'color: red\'># :param cls_features: (B, N, num_class) tensor of the descriptors of the the confidence (classification) features </span>\n        <span style=\'color: red\'># :param new_xyz: (B, M, 3) tensor of the xyz coordinates of the sampled points</span>\n        <span style=\'color: red\'># "param ctr_xyz: tensor of the xyz coordinates of the centers </span>\n        <span style=\'color: red\'># :return:</span>\n        <span style=\'color: red\'>#     new_xyz: (B, npoint, 3) tensor of the new features\' xyz</span>\n        <span style=\'color: red\'>#     new_features: (B, \\sum_k(mlps[k][-1]), npoint) tensor of the new_features descriptors</span>\n        <span style=\'color: red\'>#     cls_features: (B, npoint, num_class) tensor of confidence (classification) features</span>\n        new_features_list = []\n        xyz_flipped = xyz.transpose(1, 2).contiguous() \n        sampled_idx_list = []\n        if ctr_xyz is None:               <span style=\'color: red\'># True</span>\n            last_sample_end_index = 0\n            for i in range(len(self.sample_type_list)):   <span style=\'color: red\'># self.sample_type_list=[\'D-FPS\'];</span>\n                sample_type = self.sample_type_list[i]    <span style=\'color: red\'># \'D-FPS\'</span>\n                sample_range = self.sample_range_list[i]  <span style=\'color: red\'># -1</span>\n                npoint = self.npoint_list[i]              <span style=\'color: red\'># self.npoint_list=[4096],npoint=4096 </span>\n                if npoint <= 0:\n                    continue\n                if sample_range == -1: <span style=\'color: red\'># 全部</span>\n                    xyz_tmp = xyz[:, last_sample_end_index:, :]                <span style=\'color: red\'># xyz_tmp.shape=(2,16384,3)</span>\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:, :].contiguous()    <span style=\'color: red\'># feature_tmp.shape=(2,16384,1); </span>\n                    cls_features_tmp = cls_features[:, last_sample_end_index:, :] if cls_features is not None else None   <span style=\'color: red\'># None</span>\n                else:\n                    xyz_tmp = xyz[:, last_sample_end_index:sample_range, :].contiguous()\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:sample_range, :]\n                    cls_features_tmp = cls_features[:, last_sample_end_index:sample_range, :] if cls_features is not None else None \n                    last_sample_end_index += sample_range\n                if xyz_tmp.shape[1] <= npoint:                                 <span style=\'color: red\'># No downsampling  False</span>\n                    sample_idx = torch.arange(xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32) * torch.ones(xyz_tmp.shape[0], xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32)\n                elif (\'cls\' in sample_type) or (\'ctr\' in sample_type):         <span style=\'color: red\'># False</span>\n                    pass\n                elif \'D-FPS\' in sample_type or \'DFS\' in sample_type:                                    <span style=\'color: red\'># True</span>\n                    sample_idx = pointnet2_utils.furthest_point_sample(xyz_tmp.contiguous(), npoint)    <span style=\'color: red\'># sample_idx.shape=(2,4096);</span>\n                elif \'F-FPS\' in sample_type or \'FFS\' in sample_type:\n                    pass\n                elif sample_type == \'FS\':\n                    pass\n                elif \'Rand\' in sample_type:\n                    pass\n                elif sample_type == \'ds_FPS\' or sample_type == \'ds-FPS\':\n                    pass\n                elif sample_type == \'ry_FPS\' or sample_type == \'ry-FPS\':\n                    pass\n                sampled_idx_list.append(sample_idx)\n            sampled_idx_list = torch.cat(sampled_idx_list, dim=-1) \n            new_xyz = pointnet2_utils.gather_operation(xyz_flipped, sampled_idx_list).transpose(1, 2).contiguous()  <span style=\'color: red\'># new_xyz=(2,4096,3);</span>\n        else:\n            new_xyz = ctr_xyz\n        if len(self.groupers) > 0:        <span style=\'color: red\'># self.groupers=ModuleList((0): QueryAndGroup() (1): QueryAndGroup())</span>\n            for i in range(<span style=\'color: green;font-weight: bold;\'>len(self.groupers)</span>):                        <span style=\'color: red\'># groupers->mlps->aggregation_layer</span>\n                new_features = self.groupers[i](xyz, new_xyz, features)  <span style=\'color: red\'># (B, C, npoint, nsample)</span>\n                new_features = self.mlps[i](new_features)                <span style=\'color: red\'># (B, mlp[-1], npoint, nsample)</span>\n                if self.pool_method == \'max_pool\':\n                    new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n                elif self.pool_method == \'avg_pool\':\n                    new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n                else:\n                    raise NotImplementedError\n                new_features = new_features.squeeze(-1)  <span style=\'color: red\'># (B, mlp[-1], npoint)</span>\n                new_features_list.append(new_features)\n            new_features = torch.cat(new_features_list, dim=1)  <span style=\'color: red\'># (2,96,4096)</span>\n            if self.aggregation_layer is not None:       <span style=\'color: red\'># True</span>\n                new_features = self.aggregation_layer(new_features)  <span style=\'color: red\'># CBR:new_features.shape=(2,64,4096);</span>\n        else:\n            new_features = pointnet2_utils.gather_operation(features, sampled_idx_list).contiguous()\n        if self.confidence_layers is not None:\n            cls_features = self.confidence_layers(new_features).transpose(1, 2)\n        else:\n            cls_features = None\n        return new_xyz, new_features, cls_features    <span style=\'color: red\'># new_xyz=(2,4096,3); (2,64,4096); cls_features=None</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=0\n        new_features = self.groupers[i](xyz, new_xyz, features)  <span style=\'color: red\'># xyz.shape=(2,16384,3),new_xyz.shpae=(2,4096,3),features.shape=(2,1,16384)-></span>\n                                                                 <span style=\'color: red\'># (B, C, npoint, nsample)=(2,4,4096,16)</span>\n        new_features = self.mlps[i](new_features)                <span style=\'color: red\'># (B, mlp[-1], npoint, nsample)=(2,32,4096,16) </span>\n        if self.pool_method == \'max_pool\':\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1) (2,32,4096,1)</span>\n        elif self.pool_method == \'avg_pool\':\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  <span style=\'color: red\'># (B, mlp[-1], npoint) (2,32,4096)</span>\n        new_features_list.append(new_features)\n</code></pre></font>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=1\n        new_features = self.groupers[i](xyz, new_xyz, features)  <span style=\'color: red\'># xyz.shape=(2,16384,3),new_xyz.shpae=(2,4096,3),features.shape=(2,1,16384)-></span>\n                                                                 <span style=\'color: red\'># (B, C, npoint, nsample)=(2,4,4096,32)</span>\n        new_features = self.mlps[i](new_features)                <span style=\'color: red\'># (B, mlp[-1], npoint, nsample)=(2,64,4096,32)</span>\n        if self.pool_method == \'max_pool\':\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1) (2,64,4096,1)</span>\n        elif self.pool_method == \'avg_pool\':\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  <span style=\'color: red\'># (B, mlp[-1], npoint)=(2,64,4096)</span>\n        new_features_list.append(new_features)\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      <span style=\'color: red\'># batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]</span>\n        i=1\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  <span style=\'color: red\'># 1->xyz_input.shape=(2,4096,3);</span>\n        feature_input = encoder_features[self.layer_inputs[i]]         <span style=\'color: red\'># 1->feature_input=(2,64,4096);</span>\n        if self.layer_types[i] == \'SA_Layer\':                          <span style=\'color: red\'># \'SA_Layer\'</span>\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         <span style=\'color: red\'># -1,None</span>\n            li_xyz, li_features, li_cls_pred = self.<span style=\'color: green;font-weight: bold;\'>SA_modules[i]</span>(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  <span style=\'color: red\'># li_cls_pred=None;</span>\n        elif self.layer_types[i] == \'Vote_Layer\': <span style=\'color: red\'># i=4</span>\n            pass\n                \n        encoder_xyz.append(li_xyz)                                                <span style=\'color: red\'># encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3)]</span>\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        <span style=\'color: red\'># encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4)] </span>\n        encoder_features.append(li_features)                                      <span style=\'color: red\'># encoder_features==[(2,1,16384),(2,64,4096),(2,128,1024)]</span>\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]    <span style=\'color: red\'># sa_ins_preds=[[],(2,1024,4)] </span>\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        new_features_list = []\n        xyz_flipped = xyz.transpose(1, 2).contiguous() \n        sampled_idx_list = []\n        if ctr_xyz is None:               <span style=\'color: red\'># True</span>\n            last_sample_end_index = 0\n            for i in range(len(self.sample_type_list)):   <span style=\'color: red\'># self.sample_type_list=[\'D-FPS\'];</span>\n                sample_type = self.sample_type_list[i]    <span style=\'color: red\'># \'D-FPS\'</span>\n                sample_range = self.sample_range_list[i]  <span style=\'color: red\'># -1</span>\n                npoint = self.npoint_list[i]              <span style=\'color: red\'># self.npoint_list=[1024],npoint=1024 </span>\n                if npoint <= 0:\n                    continue\n                if sample_range == -1: <span style=\'color: red\'># 全部</span>\n                    xyz_tmp = xyz[:, last_sample_end_index:, :]                <span style=\'color: red\'># xyz_tmp.shape=(2,4096,3)</span>\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:, :].contiguous()    <span style=\'color: red\'># feature_tmp.shape=(2,4096,64); </span>\n                    cls_features_tmp = cls_features[:, last_sample_end_index:, :] if cls_features is not None else None   <span style=\'color: red\'># None</span>\n                else:\n                    xyz_tmp = xyz[:, last_sample_end_index:sample_range, :].contiguous()\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:sample_range, :]\n                    cls_features_tmp = cls_features[:, last_sample_end_index:sample_range, :] if cls_features is not None else None \n                    last_sample_end_index += sample_range\n                if xyz_tmp.shape[1] <= npoint:                                 <span style=\'color: red\'># No downsampling  False</span>\n                    sample_idx = torch.arange(xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32) * torch.ones(xyz_tmp.shape[0], xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32)\n                elif (\'cls\' in sample_type) or (\'ctr\' in sample_type):         <span style=\'color: red\'># False</span>\n                    pass\n                elif \'D-FPS\' in sample_type or \'DFS\' in sample_type:                                    <span style=\'color: red\'># True</span>\n                    sample_idx = pointnet2_utils.furthest_point_sample(xyz_tmp.contiguous(), npoint)    <span style=\'color: red\'># sample_idx.shape=(2,1024);</span>\n                elif \'F-FPS\' in sample_type or \'FFS\' in sample_type:\n                    pass\n                elif sample_type == \'FS\':\n                    pass\n                elif \'Rand\' in sample_type:\n                    pass\n                elif sample_type == \'ds_FPS\' or sample_type == \'ds-FPS\':\n                    pass\n                elif sample_type == \'ry_FPS\' or sample_type == \'ry-FPS\':\n                    pass\n                sampled_idx_list.append(sample_idx)\n            sampled_idx_list = torch.cat(sampled_idx_list, dim=-1) \n            new_xyz = pointnet2_utils.gather_operation(xyz_flipped, sampled_idx_list).transpose(1, 2).contiguous()  <span style=\'color: red\'># new_xyz=(2,1024,3);</span>\n        else:\n            new_xyz = ctr_xyz\n        if len(self.groupers) > 0:        <span style=\'color: red\'># self.groupers=ModuleList((0): QueryAndGroup() (1): QueryAndGroup())</span>\n            for i in range(<span style=\'color: green;font-weight: bold;\'>len(self.groupers)</span>):                        <span style=\'color: red\'># groupers->mlps->aggregation_layer</span>\n                new_features = self.groupers[i](xyz, new_xyz, features)  <span style=\'color: red\'># (B, C, npoint, nsample)</span>\n                new_features = self.mlps[i](new_features)                <span style=\'color: red\'># (B, mlp[-1], npoint, nsample)</span>\n                if self.pool_method == \'max_pool\':\n                    new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n                elif self.pool_method == \'avg_pool\':\n                    new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n                else:\n                    raise NotImplementedError\n                new_features = new_features.squeeze(-1)  <span style=\'color: red\'># (B, mlp[-1], npoint)</span>\n                new_features_list.append(new_features)\n            new_features = torch.cat(new_features_list, dim=1)  <span style=\'color: red\'># (2,256,1024)</span>\n            if self.aggregation_layer is not None:       <span style=\'color: red\'># True</span>\n                new_features = self.aggregation_layer(new_features)  <span style=\'color: red\'># CBR:new_features.shape=(2,128,1024);</span>\n        else:\n            new_features = pointnet2_utils.gather_operation(features, sampled_idx_list).contiguous()\n        if self.confidence_layers is not None:\n            cls_features = self.confidence_layers(new_features).transpose(1, 2)  <span style=\'color: red\'># cls_features.shape=(2,1024,?) 用于ctr_aware选取前景点的</span>\n        else:\n            cls_features = None\n        return new_xyz, new_features, cls_features    <span style=\'color: red\'># new_xyz=(2,1024,3); (2,128,1024); cls_features=(2,1024,?)</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=0\n        new_features = self.groupers[i](xyz, new_xyz, features)  <span style=\'color: red\'># xyz.shape=(2,4096,3),new_xyz.shpae=(2,1024,3),features.shape=(2,64,4096)-></span>\n                                                                 <span style=\'color: red\'># (B, C, npoint, nsample)=(2,67,1024,16)</span>\n        new_features = self.mlps[i](new_features)                <span style=\'color: red\'># (B, mlp[-1], npoint, nsample)=(2,128,1024,16) </span>\n        if self.pool_method == \'max_pool\':\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1) (2,128,1024,1)</span>\n        elif self.pool_method == \'avg_pool\':\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  <span style=\'color: red\'># (B, mlp[-1], npoint) (2,128,1024)</span>\n        new_features_list.append(new_features)\n</code></pre></font>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=1\n        new_features = self.groupers[i](xyz, new_xyz, features)  <span style=\'color: red\'># xyz.shape=(2,4096,3),new_xyz.shpae=(2,1024,3),features.shape=(2,64,4096)-></span>\n                                                                 <span style=\'color: red\'># (B, C, npoint, nsample)=(2,67,1024,32)</span>\n        new_features = self.mlps[i](new_features)                <span style=\'color: red\'># (B, mlp[-1], npoint, nsample)=(2,128,1024,32)</span>\n        if self.pool_method == \'max_pool\':\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1) (2,128,1024,1)</span>\n        elif self.pool_method == \'avg_pool\':\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  <span style=\'color: red\'># (B, mlp[-1], npoint)=(2,128,1024)</span>\n        new_features_list.append(new_features)\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      <span style=\'color: red\'># batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]</span>\n        i=2\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  <span style=\'color: red\'># 1->xyz_input.shape=(2,1024,3);</span>\n        feature_input = encoder_features[self.layer_inputs[i]]         <span style=\'color: red\'># 1->feature_input=(2,128,1024);</span>\n        if self.layer_types[i] == \'SA_Layer\':                          <span style=\'color: red\'># \'SA_Layer\'</span>\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         <span style=\'color: red\'># -1,None</span>\n            li_xyz, li_features, li_cls_pred = self.<span style=\'color: green;font-weight: bold;\'>SA_modules[i]</span>(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  <span style=\'color: red\'># li_cls_pred=(2,1024,3)</span>\n        elif self.layer_types[i] == \'Vote_Layer\': <span style=\'color: red\'># i=4</span>\n            pass\n                \n        encoder_xyz.append(li_xyz)                                                <span style=\'color: red\'># encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3)]</span>\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) <span style=\'color: red\'># encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4)]  </span>\n        encoder_features.append(li_features)                                      <span style=\'color: red\'># encoder_features=[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512)] </span>\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]      <span style=\'color: red\'># sa_ins_preds=[[],(2,1024,4),(2,512，4)]</span>\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              <span style=\'color: red\'># [[]] </span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><font size="0"><pre class="language-python"><code class="language-python">class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        new_features_list = []\n        xyz_flipped = xyz.transpose(1, 2).contiguous() \n        sampled_idx_list = []\n        if ctr_xyz is None:               <span style=\'color: red\'># True</span>\n            last_sample_end_index = 0\n            for i in range(len(self.sample_type_list)):   <span style=\'color: red\'># self.sample_type_list=[\'ctr_aware\'];</span>\n                sample_type = self.sample_type_list[i]    <span style=\'color: red\'># \'ctr_aware\'</span>\n                sample_range = self.sample_range_list[i]  <span style=\'color: red\'># -1</span>\n                npoint = self.npoint_list[i]              <span style=\'color: red\'># self.npoint_list=[512],npoint=512 </span>\n                if npoint <= 0:\n                    continue\n                if sample_range == -1: <span style=\'color: red\'># 全部</span>\n                    xyz_tmp = xyz[:, last_sample_end_index:, :]                                          <span style=\'color: red\'># xyz_tmp.shape=(2,1024,3)</span>\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:, :].contiguous()    <span style=\'color: red\'># feature_tmp.shape=(2,1024,128); </span>\n                    cls_features_tmp = cls_features[:, last_sample_end_index:, :] if cls_features is not None else None   <span style=\'color: red\'># (2,1024,3)</span>\n                else:\n                    xyz_tmp = xyz[:, last_sample_end_index:sample_range, :].contiguous()\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:sample_range, :]\n                    cls_features_tmp = cls_features[:, last_sample_end_index:sample_range, :] if cls_features is not None else None \n                    last_sample_end_index += sample_range\n                if xyz_tmp.shape[1] <= npoint:                                 <span style=\'color: red\'># No downsampling  False</span>\n                    sample_idx = torch.arange(xyz_tmp.shape[1],device=xyz_tmp.device,dtype=torch.int32)*torch.ones(xyz_tmp.shape[0],xyz_tmp.shape[1],device=xyz_tmp.device,dtype=torch.int32)\n                elif (\'cls\' in sample_type) or (\'ctr\' in sample_type):         <span style=\'color: red\'># True</span>\n                    cls_features_max, class_pred = cls_features_tmp.max(dim=-1)            <span style=\'color: red\'># cls_features_max.shape=(2,1024);</span>\n                    score_pred = torch.sigmoid(cls_features_max)                           <span style=\'color: red\'># B,N   score_pred.shape=(2,1024);    </span>\n                    score_picked, sample_idx = torch.topk(score_pred, npoint, dim=-1)           \n                    sample_idx = sample_idx.int()                                          <span style=\'color: red\'># sample_idx.shape=(2,512);</span>\n                elif \'D-FPS\' in sample_type or \'DFS\' in sample_type:                                    <span style=\'color: red\'># False</span>\n                    sample_idx = pointnet2_utils.furthest_point_sample(xyz_tmp.contiguous(), npoint)\n                elif \'F-FPS\' in sample_type or \'FFS\' in sample_type:\n                    pass\n                elif sample_type == \'FS\':\n                    pass\n                elif \'Rand\' in sample_type:\n                    pass\n                elif sample_type == \'ds_FPS\' or sample_type == \'ds-FPS\':\n                    pass\n                elif sample_type == \'ry_FPS\' or sample_type == \'ry-FPS\':\n                    pass\n                sampled_idx_list.append(sample_idx)\n            sampled_idx_list = torch.cat(sampled_idx_list, dim=-1) \n            new_xyz = pointnet2_utils.gather_operation(xyz_flipped, sampled_idx_list).transpose(1, 2).contiguous()  <span style=\'color: red\'># new_xyz=(2,512,3);</span>\n        else:\n            new_xyz = ctr_xyz\n        if len(self.groupers) > 0:        <span style=\'color: red\'># self.groupers=ModuleList((0): QueryAndGroup() (1): QueryAndGroup())</span>\n            for i in range(<span style=\'color: green;font-weight: bold;\'>len(self.groupers)</span>):                        <span style=\'color: red\'># groupers->mlps->aggregation_layer</span>\n                new_features = self.groupers[i](xyz, new_xyz, features)  <span style=\'color: red\'># (B, C, npoint, nsample)</span>\n                new_features = self.mlps[i](new_features)                <span style=\'color: red\'># (B, mlp[-1], npoint, nsample)</span>\n                if self.pool_method == \'max_pool\':\n                    new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n                elif self.pool_method == \'avg_pool\':\n                    new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  <span style=\'color: red\'># (B, mlp[-1], npoint, 1)</span>\n                else:\n                    raise NotImplementedError\n                new_features = new_features.squeeze(-1)  <span style=\'color: red\'># (B, mlp[-1], npoint)</span>\n                new_features_list.append(new_features)\n            new_features = torch.cat(new_features_list, dim=1)  \n            if self.aggregation_layer is not None:     \n                new_features = self.aggregation_layer(new_features)\n        else:\n            new_features = pointnet2_utils.gather_operation(features, sampled_idx_list).contiguous()\n        if self.confidence_layers is not None:\n            cls_features = self.confidence_layers(new_features).transpose(1, 2)  \n        else:\n            cls_features = None\n        return new_xyz, new_features, cls_features    <span style=\'color: red\'># new_xyz=(2,512,3); (2,256,512);</span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      <span style=\'color: red\'># batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]</span>\n        i=3\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  <span style=\'color: red\'># 1->xyz_input.shape=(2,512,3);</span>\n        feature_input = encoder_features[self.layer_inputs[i]]         <span style=\'color: red\'># 1->feature_input=(2,256,512);</span>\n        if self.layer_types[i] == \'SA_Layer\':                          <span style=\'color: red\'># \'SA_Layer\'</span>\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         <span style=\'color: red\'># -1,None</span>\n            li_xyz, li_features, li_cls_pred = self.<span style=\'color: green;font-weight: bold;\'>SA_modules[i]</span>(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  <span style=\'color: red\'># li_cls_pred=None;</span>\n        elif self.layer_types[i] == \'Vote_Layer\': <span style=\'color: red\'># i=4</span>\n            pass\n                \n        encoder_xyz.append(li_xyz)                              <span style=\'color: red\'># encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3)]</span>\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        <span style=\'color: red\'># encoder_coords[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4),(2,256,4)]->batch_size+ xyz </span>\n        encoder_features.append(li_features)                   <span style=\'color: red\'># encoder_features=[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256)]</span>\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]   <span style=\'color: red\'># [[],(2,1024,4),(2,512,4)，[]]</span>\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              <span style=\'color: red\'># [[]] </span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      <span style=\'color: red\'># batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]</span>\n        i=4\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  <span style=\'color: red\'># 1->xyz_input.shape=(2,256,3);</span>\n        feature_input = encoder_features[self.layer_inputs[i]]         <span style=\'color: red\'># 1->feature_input=(2,256,256);</span>\n        if self.layer_types[i] == \'SA_Layer\':                          \n            pass\n        elif self.layer_types[i] == \'Vote_Layer\':                      <span style=\'color: red\'># i=4</span>\n            li_xyz, li_features, xyz_select, ctr_offsets = <span style=\'color: green;font-weight: bold;\'>self.SA_modules[i]</span>(xyz_input, feature_input)\n            centers = li_xyz\n            centers_origin = xyz_select\n            center_origin_batch_idx = batch_idx.view(batch_size, -1)[:, :centers_origin.shape[1]]\n            encoder_coords.append(torch.cat([center_origin_batch_idx[..., None].float(),centers_origin.view(batch_size, -1, 3)],dim =-1))\n                \n        encoder_xyz.append(li_xyz)                                       <span style=\'color: red\'># encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3),(2,256,3)]</span>\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        <span style=\'color: red\'># encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4),(2,256,4),(2,256,4)]</span>\n        encoder_features.append(li_features)                            <span style=\'color: red\'># encoder_features==[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256),[]]</span>\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]    <span style=\'color: red\'># sa_ins_preds=[[],(2,1024,4),(2,512,4),[],[]]</span>\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      <span style=\'color: red\'># batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]</span>\n        i=5\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  <span style=\'color: red\'># 1->xyz_input.shape=(2,256,3);</span>\n        feature_input = encoder_features[self.layer_inputs[i]]         <span style=\'color: red\'># 1->feature_input=(2,256,256);</span>\n        if self.layer_types[i] == \'SA_Layer\':                          <span style=\'color: red\'># \'SA_Layer\'</span>\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         <span style=\'color: red\'># -1,None</span>\n            li_xyz, li_features, li_cls_pred = self.<span style=\'color: green;font-weight: bold;\'>SA_modules[i]</span>(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  <span style=\'color: red\'># li_cls_pred=None;</span>\n        elif self.layer_types[i] == \'Vote_Layer\': <span style=\'color: red\'># i=4</span>\n            pass\n                \n        encoder_xyz.append(li_xyz)                     <span style=\'color: red\'># encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3),(2,256,3),(2,256,3)] </span>\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        <span style=\'color: red\'># encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4),(2,256,4),(2,256,4),(2,256,4),(2,256,4)]->batch_size+ xyz </span>\n        encoder_features.append(li_features)     <span style=\'color: red\'># encoder_features=[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256),[],(2,512,256)]</span>\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]   <span style=\'color: red\'># [[],(2,1024,4),(2,512,4),[],[],[]] </span>\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              <span style=\'color: red\'># [[]] </span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/dense_heads/IASSD_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Head(PointHeadTemplate):\n    def forward(self, batch_dict):\n        <span style=\'color: red\'># input:     batch_dict:</span>\n        <span style=\'color: red\'>#     batch_size:</span>\n        <span style=\'color: red\'>#     centers_features: (N1 + N2 + N3 + ..., C) or (B, N, C)</span>\n        <span style=\'color: red\'>#     centers: (N1 + N2 + N3 + ..., 4) [bs_idx, x, y, z]</span>\n        <span style=\'color: red\'>#     encoder_xyz: List of points_coords in SA</span>\n        <span style=\'color: red\'>#     gt_boxes (optional): (B, M, 8)</span>\n        <span style=\'color: red\'># Returns:  batch_dict:</span>\n        <span style=\'color: red\'>#         batch_cls_preds: (N1 + N2 + N3 + ..., num_class)</span>\n        <span style=\'color: red\'>#         point_box_preds: (N1 + N2 + N3 + ..., 7)</span>\n        center_features = batch_dict[\'centers_features\']        <span style=\'color: red\'># (512,512)</span>\n        center_coords = batch_dict[\'centers\']                   <span style=\'color: red\'># (512,4);</span>\n        center_cls_preds = self.cls_center_layers(center_features)  <span style=\'color: red\'># (total_centers, num_class)     (512,3);</span>\n        center_box_preds = self.box_center_layers(center_features)  <span style=\'color: red\'># (total_centers, box_code_size) (512,30);</span>\n                                                                    <span style=\'color: red\'># 30为6+12*2（12个bin以及每个bin的迁移）</span>\n        box_iou3d_preds = self.box_iou3d_layers(center_features) if self.box_iou3d_layers is not None else None <span style=\'color: red\'># None,</span>\n        ret_dict = {\'center_cls_preds\': center_cls_preds,\n                    \'center_box_preds\': center_box_preds,\n                    \'ctr_offsets\': batch_dict[\'ctr_offsets\'],\n                    \'centers\': batch_dict[\'centers\'],\n                    \'centers_origin\': batch_dict[\'centers_origin\'],\n                    \'sa_ins_preds\': batch_dict[\'sa_ins_preds\'],\n                    \'box_iou3d_preds\': box_iou3d_preds}\n        if self.training:\n            targets_dict = self.<span style=\'color: green;font-weight: bold;\'>assign_targets</span>(batch_dict)\n            ret_dict.update(targets_dict)\n        if not self.training or self.predict_boxes_when_training or \\\n                self.model_cfg.LOSS_CONFIG.CORNER_LOSS_REGULARIZATION or \\\n                self.model_cfg.LOSS_CONFIG.CENTERNESS_REGULARIZATION or \\\n                self.model_cfg.LOSS_CONFIG.IOU3D_REGULARIZATION:\n            point_cls_preds, point_box_preds = self.generate_predicted_boxes(\n                    points=center_coords[:, 1:4],point_cls_preds=center_cls_preds, point_box_preds=center_box_preds)\n            batch_dict[\'batch_cls_preds\'] = point_cls_preds\n            batch_dict[\'batch_box_preds\'] = point_box_preds\n            batch_dict[\'box_iou3d_preds\'] = box_iou3d_preds\n            batch_dict[\'batch_index\'] = center_coords[:,0]\n            batch_dict[\'cls_preds_normalized\'] = False\n            ret_dict[\'point_box_preds\'] = point_box_preds\n        self.forward_ret_dict = ret_dict\n        return batch_dict\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/dense_heads/IASSD_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Head(PointHeadTemplate):\n    def assign_targets(self, input_dict):\n        ........\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/dense_heads/IASSD_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class IASSD_Head(PointHeadTemplate):\n    def assign_stack_targets_IASSD(self, points, gt_boxes, extend_gt_boxes=None, weighted_labels=False,\n                             ret_box_labels=False, ret_offset_labels=True,\n                             set_ignore_flag=True, use_ball_constraint=False, central_radius=2.0,\n                             use_query_assign=False, central_radii=2.0, use_ex_gt_assign=False, fg_pc_ignore=False,\n                             binary_label=False):\n        <span style=\'color: red\'># Args:</span>\n        <span style=\'color: red\'>#     points: (N1 + N2 + N3 + ..., 4) [bs_idx, x, y, z]</span>\n        <span style=\'color: red\'>#     gt_boxes: (B, M, 8)</span>\n        <span style=\'color: red\'>#     extend_gt_boxes: [B, M, 8]</span>\n        <span style=\'color: red\'># Returns:</span>\n        <span style=\'color: red\'>#     point_cls_labels: (N1 + N2 + N3 + ...), long type, 0:background, -1:ignored</span>\n        <span style=\'color: red\'>#     point_box_labels: (N1 + N2 + N3 + ..., code_size)</span>\n        assert len(points.shape) == 2 and points.shape[1] == 4, \'points.shape=%s\' % str(points.shape)\n        assert len(gt_boxes.shape) == 3 and (gt_boxes.shape[2] == 8 or gt_boxes.shape[2] == 10), \'gt_boxes.shape=%s\' % str(gt_boxes.shape)\n        assert extend_gt_boxes is None or len(extend_gt_boxes.shape) == 3 and (extend_gt_boxes.shape[2] == 8 or extend_gt_boxes.shape[2] == 10), \\\n            \'extend_gt_boxes.shape=%s\' % str(extend_gt_boxes.shape)\n        batch_size = gt_boxes.shape[0]\n        box_size = gt_boxes.shape[2]\n        bs_idx = points[:, 0]                                              <span style=\'color: red\'># bs_idx=512它的值是第几个batch_id</span>\n        point_cls_labels = points.new_zeros(points.shape[0]).long()        \n        point_box_labels = gt_boxes.new_zeros((points.shape[0], box_size)) if ret_box_labels else None\n        box_idxs_labels = points.new_zeros(points.shape[0]).long()        \n        gt_boxes_of_fg_points = []                                         \n        gt_box_of_points = gt_boxes.new_zeros((points.shape[0], box_size))\n        for k in range(batch_size):            \n            bs_mask = (bs_idx == k)\n            points_single = points[bs_mask][:, 1:4]        <span style=\'color: red\'># (256,3)某个batch的那些点</span>\n            point_cls_labels_single = point_cls_labels.new_zeros(bs_mask.sum())    <span style=\'color: red\'># (256);</span>\n            box_idxs_of_pts = roiaware_pool3d_utils.points_in_boxes_gpu(           <span style=\'color: red\'># (256);256个点是不是在3Dbox里面 </span>\n                points_single.unsqueeze(dim=0), gt_boxes[k:k + 1, :, 0:7].contiguous()).long().squeeze(dim=0)\n            box_fg_flag = (box_idxs_of_pts >= 0)           <span style=\'color: red\'># .sum=42表示该batch有42个点在3D box里面</span>\n            if use_query_assign: <span style=\'color: red\'># False</span>\n                pass\n            elif use_ex_gt_assign: #\n                pass                   \n            elif set_ignore_flag:       <span style=\'color: red\'># True </span>\n                extend_box_idxs_of_pts = roiaware_pool3d_utils.points_in_boxes_gpu( <span style=\'color: red\'># .sum()=54个点在扩充box里面，不在里面但是在扩充的范围之内的有12个(54-42)</span>\n                    points_single.unsqueeze(dim=0), extend_gt_boxes[k:k+1, :, 0:7].contiguous()).long().squeeze(dim=0)\n                fg_flag = box_fg_flag\n                ignore_flag = fg_flag ^ (extend_box_idxs_of_pts >= 0)\n                point_cls_labels_single[ignore_flag] = -1        <span style=\'color: red\'># 不在里面但是在扩充的范围之内的 </span>\n            elif use_ball_constraint: \n                box_centers = gt_boxes[k][box_idxs_of_pts][:, 0:3].clone()\n                box_centers[:, 2] += gt_boxes[k][box_idxs_of_pts][:, 5] / 2\n                ball_flag = ((box_centers - points_single).norm(dim=1) < central_radius)\n                fg_flag = box_fg_flag & ball_flag\n            else:\n                raise NotImplementedError\n            gt_box_of_fg_points = gt_boxes[k][box_idxs_of_pts[fg_flag]]    <span style=\'color: red\'># (44,8)有44个点在box里面(不包括扩充部分)，这里bbox有重复；</span>\n            point_cls_labels_single[fg_flag] = 1 if self.num_class == 1 or binary_label else gt_box_of_fg_points[:, -1].long()\n            point_cls_labels[bs_mask] = point_cls_labels_single\n            bg_flag = (point_cls_labels_single == 0) <span style=\'color: red\'># except ignore_id  为背景的是True</span>\n            <span style=\'color: red\'># box_bg_flag</span>\n            fg_flag = fg_flag ^ (fg_flag & bg_flag)\n            gt_box_of_fg_points = gt_boxes[k][box_idxs_of_pts[fg_flag]]\n            gt_boxes_of_fg_points.append(gt_box_of_fg_points)\n            box_idxs_labels[bs_mask] = box_idxs_of_pts                 <span style=\'color: red\'># (512,)；里面的值表示在哪个box3D里面 </span>\n            gt_box_of_points[bs_mask] = gt_boxes[k][box_idxs_of_pts]   <span style=\'color: red\'># (512,8) 512个点含有box3D的对应值 </span>\n            if ret_box_labels and gt_box_of_fg_points.shape[0] > 0:    <span style=\'color: red\'># True</span>\n                point_box_labels_single = point_box_labels.new_zeros((bs_mask.sum(), box_size))  <span style=\'color: red\'># bs_mask.sum()=256->point_box_labels_single.shape=(256,8)</span>\n                fg_point_box_labels = self.box_coder.<span style=\'color: green;font-weight: bold;\'>encode_torch</span>(\n                    gt_boxes=gt_box_of_fg_points[:, :-1], points=points_single[fg_flag],gt_classes=gt_box_of_fg_points[:, -1].long())\n                point_box_labels_single[fg_flag] = fg_point_box_labels    <span style=\'color: red\'># (256,8)</span>\n                point_box_labels[bs_mask] = point_box_labels_single\n        gt_boxes_of_fg_points = torch.cat(gt_boxes_of_fg_points, dim=0)\n        targets_dict = {\n            \'point_cls_labels\': point_cls_labels,           <span style=\'color: red\'># torch.Size([512]);0是背景的label；-1是ignore，1-n_class是类别的值</span>\n            \'point_box_labels\': point_box_labels,           <span style=\'color: red\'># torch.Size([512, 8])；经过encode的结果；有重复的，最后两维为bin_id和bin_res</span>\n            \'gt_box_of_fg_points\': gt_boxes_of_fg_points,   <span style=\'color: red\'># torch.Size([67, 8])；原始的3D box有重复的</span>\n            \'box_idxs_labels\': box_idxs_labels,             <span style=\'color: red\'># torch.Size([512]); torch.sum(box_idxs_labels>=0)=67; 值为是哪个3D box</span>\n            \'gt_box_of_points\': gt_box_of_points,           <span style=\'color: red\'># torch.Size([512, 8]);背景和-1的box为0，其它为该点对应的GTbox</span>\n        }\n        return targets_dict\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/utils/box_coder_utils.py:</p><font size="0"><pre class="language-python"><code class="language-python">class PointResidual_BinOri_Coder(object):\n    def encode_torch(self, gt_boxes, points, gt_classes=None):\n        <span style=\'color: red\'># Args:</span>\n        <span style=\'color: red\'>#     gt_boxes: (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]</span>\n        <span style=\'color: red\'>#     points: (N, 3) [x, y, z]</span>\n        <span style=\'color: red\'>#     gt_classes: (N) [1, num_classes]</span>\n        <span style=\'color: red\'># Returns:</span>\n        <span style=\'color: red\'>#     box_coding: (N, 8 + C)</span>\n        gt_boxes[:, 3:6] = torch.clamp_min(gt_boxes[:, 3:6], min=1e-5)  <span style=\'color: red\'># gt_boxes.shape=(44,7); points.shape=(44,3);</span>\n        xg, yg, zg, dxg, dyg, dzg, rg, *cgs = torch.split(gt_boxes, 1, dim=-1)\n        xa, ya, za = torch.split(points, 1, dim=-1)\n        if self.use_mean_size:             \n            assert gt_classes.max() <= self.mean_size.shape[0]  <span style=\'color: red\'># gt_classes.shape=(44)上面44个box的真实标签 </span>\n            point_anchor_size = self.mean_size[gt_classes - 1]  <span style=\'color: red\'># tensor([[3.9,1.6,1.56],[0.8,0.6,1.73],[1.76,0.6,1.73]],device=\'cuda:0\')</span>\n            <span style=\'color: red\'># gt_classes.unique()</span>\n            dxa, dya, dza = torch.split(point_anchor_size, 1, dim=-1)\n            diagonal = torch.sqrt(dxa ** 2 + dya ** 2)\n            xt = (xg - xa) / diagonal\n            yt = (yg - ya) / diagonal\n            zt = (zg - za) / dza\n            dxt = torch.log(dxg / dxa)\n            dyt = torch.log(dyg / dya)\n            dzt = torch.log(dzg / dza)\n        else:\n            pass\n        rg = torch.clamp(rg, max=np.pi - 1e-5, min=-np.pi + 1e-5)   #################\n        bin_id = torch.floor((rg + np.pi) / self.bin_inter)   <span style=\'color: red\'># -np.pi->np.pi; 0->2*np.pi; 0->12</span>\n        <span style=\'color: red\'># if bin_id.max() >= self.bin_size:                  </span>\n        <span style=\'color: red\'>#     a = 1</span>\n        bin_res = ((rg + np.pi) - (bin_id * self.bin_inter + self.bin_inter / 2)) / (self.bin_inter / 2)  <span style=\'color: red\'># norm to [-1, 1]</span>\n        cts = [g for g in cgs]\n        return torch.cat([xt, yt, zt, dxt, dyt, dzt, bin_id, bin_res, *cts], dim=-1)\n</code></pre></font>'}]}]}]}]}]}]})</script></body>
</html>
