<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>IASSD训练过程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">数据处理</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/datasets/kitti/kitti_dataset.py</p><span class=\'hidden-code\' data-code=\'class KittiDataset(DatasetTemplate):\n    def __getitem__(self, index):            # index = 4\n        if self._merge_all_iters_to_one_epoch:\n            index = index % len(self.kitti_infos)\n        info = copy.deepcopy(self.kitti_infos[index])       # info.keys()=dict_keys([&amp;#39;point_cloud&amp;#39;, &amp;#39;image&amp;#39;, &amp;#39;calib&amp;#39;, &amp;#39;annos&amp;#39;])\n        sample_idx = info[&amp;#39;point_cloud&amp;#39;][&amp;#39;lidar_idx&amp;#39;]       # sample_idx=&amp;#39;006736&amp;#39;\n        img_shape = info[&amp;#39;image&amp;#39;][&amp;#39;image_shape&amp;#39;]            # img_shape=array([ 375, 1242]) \n        calib = self.get_calib(sample_idx)                  # calib=`<`pcdet.utils.calibration_kitti.Calibration object at 0x7fcd5f42ddd8`>`    \n        get_item_list = self.dataset_cfg.get(&amp;#39;GET_ITEM_LIST&amp;#39;, [&amp;#39;points&amp;#39;])      # get_item_list=[&amp;#39;points&amp;#39;]\n        input_dict = {&amp;#39;frame_id&amp;#39;: sample_idx, &amp;#39;calib&amp;#39;: calib}\n        if &amp;#39;annos&amp;#39; in info:                                 # 相机坐标系转到雷达坐标系：8个目标  \n            annos = info[&amp;#39;annos&amp;#39;]\n            annos = common_utils.drop_info_with_name(annos, name=&amp;#39;DontCare&amp;#39;)\n            loc, dims, rots = annos[&amp;#39;location&amp;#39;], annos[&amp;#39;dimensions&amp;#39;], annos[&amp;#39;rotation_y&amp;#39;]\n            gt_names = annos[&amp;#39;name&amp;#39;]\n            gt_boxes_camera = np.concatenate([loc, dims, rots[..., np.newaxis]], axis=1).astype(np.float32)   # gt_boxes_camera.shape=(8,7)\n            gt_boxes_lidar = box_utils.boxes3d_kitti_camera_to_lidar(gt_boxes_camera, calib)\n            input_dict.update({&amp;#39;gt_names&amp;#39;: gt_names, &amp;#39;gt_boxes&amp;#39;: gt_boxes_lidar})                             # gt_names.shape=(8,)\n            if &amp;#39;gt_boxes2d&amp;#39; in get_item_list:\n                input_dict[&amp;#39;gt_boxes2d&amp;#39;] = annos[&amp;#39;bbox&amp;#39;]\n            road_plane = self.get_road_plane(sample_idx)      # 下载road planes，这对于训练中的数据增强是可选的？\n            if road_plane is not None:                        # road_plane=array([-0.0154115,-0.99985959,0.00657865,1.63900299]) \n                input_dict[&amp;#39;road_plane&amp;#39;] = road_plane\n        if &amp;#39;points&amp;#39; in get_item_list:  \n            points = self.get_lidar(sample_idx)               # points=(115922,4); FOV_POINTS_ONLY=True;\n            if self.dataset_cfg.FOV_POINTS_ONLY:\n                pts_rect = calib.`lidar_to_rect`(points[:, 0:3])\n                fov_flag = self.get_fov_flag(pts_rect, img_shape, calib) # 这里相当于映射到图片上过滤掉一些点\n                points = points[fov_flag]\n            input_dict[&amp;#39;points&amp;#39;] = points\n        if &amp;#39;images&amp;#39; in get_item_list:\n            input_dict[&amp;#39;images&amp;#39;] = self.get_image(sample_idx)\n        if &amp;#39;depth_maps&amp;#39; in get_item_list:\n            input_dict[&amp;#39;depth_maps&amp;#39;] = self.get_depth_map(sample_idx)\n        if &amp;#39;calib_matricies&amp;#39; in get_item_list:\n            input_dict[&amp;#39;trans_lidar_to_cam&amp;#39;], input_dict[&amp;#39;trans_cam_to_img&amp;#39;] = kitti_utils.calib_to_matricies(calib)\n        data_dict = self.`prepare_data`(data_dict=input_dict)\n        data_dict[&amp;#39;image_shape&amp;#39;] = img_shape\n        return data_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/utils/calibration_kitti.py</p><span class=\'hidden-code\' data-code=\'class Calibration(object):\n    def lidar_to_rect(self, pts_lidar):\n        pts_lidar_hom = self.cart_to_hom(pts_lidar)                      # pts_lidar=(115922,3)->(115922,4);\n        pts_rect = np.dot(pts_lidar_hom, np.dot(self.V2C.T, self.R0.T))  # pts_rect=(115922,3);\n        # pts_rect = reduce(np.dot, (pts_lidar_hom, self.V2C.T, self.R0.T))\n        return pts_rect\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/utils/calibration_kitti.py</p><span class=\'hidden-code\' data-code=\'class Calibration(object):\n    def lidar_to_rect(self, pts_lidar):\n        pts_lidar_hom = self.cart_to_hom(pts_lidar)  # pts_hom = np.hstack((pts,np.ones((pts.shape[0],1),dtype=np.float32)))          \n                                                     # pts_lidar=(115922,3)->(115922,4);\n        pts_rect = np.dot(pts_lidar_hom, np.dot(self.V2C.T, self.R0.T))  # pts_rect=(115922,3);这里相当于映射到图片上过滤掉一些点\n        # pts_rect = reduce(np.dot, (pts_lidar_hom, self.V2C.T, self.R0.T))\n        return pts_rect\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/datasets/dataset.py</p><span class=\'hidden-code\' data-code=\'class DatasetTemplate(torch_data.Dataset):\n    def prepare_data(self, data_dict):\n        if self.training:\n            assert &amp;#39;gt_boxes&amp;#39; in data_dict, &amp;#39;gt_boxes should be provided for training&amp;#39;\n            gt_boxes_mask = np.array([n in self.class_names for n in data_dict[&amp;#39;gt_names&amp;#39;]], dtype=np.bool_)\n            data_dict = self.data_augmentor.forward(data_dict={**data_dict,&amp;#39;gt_boxes_mask&amp;#39;: gt_boxes_mask})\n        if data_dict.get(&amp;#39;gt_boxes&amp;#39;, None) is not None:          # True   \n            selected = common_utils.keep_arrays_by_name(data_dict[&amp;#39;gt_names&amp;#39;], self.class_names)\n            data_dict[&amp;#39;gt_boxes&amp;#39;] = data_dict[&amp;#39;gt_boxes&amp;#39;][selected]\n            data_dict[&amp;#39;gt_names&amp;#39;] = data_dict[&amp;#39;gt_names&amp;#39;][selected]\n            gt_classes = np.array([self.class_names.index(n) + 1 for n in data_dict[&amp;#39;gt_names&amp;#39;]], dtype=np.int32)\n            # 把类别合到gt_boxes里面:data_dict[&amp;#39;gt_boxes&amp;#39;].shape=(39,8)\n            gt_boxes = np.concatenate((data_dict[&amp;#39;gt_boxes&amp;#39;], gt_classes.reshape(-1, 1).astype(np.float32)), axis=1)  \n            data_dict[&amp;#39;gt_boxes&amp;#39;] = gt_boxes\n            if data_dict.get(&amp;#39;gt_boxes2d&amp;#39;, None) is not None:\n                data_dict[&amp;#39;gt_boxes2d&amp;#39;] = data_dict[&amp;#39;gt_boxes2d&amp;#39;][selected]\n        if data_dict.get(&amp;#39;points&amp;#39;, None) is not None:  # data_dict[&amp;#39;points&amp;#39;][0]=array([28.520061,16.495598,-0.36022797,0.],dtype=float32)\n            data_dict = self.point_feature_encoder.`forward`(data_dict)\n        data_dict = self.data_processor.`forward`(data_dict=data_dict)    # 数据增强处理\n        if self.training and len(data_dict[&amp;#39;gt_boxes&amp;#39;]) == 0:\n            new_index = np.random.randint(self.__len__())\n            return self.__getitem__(new_index)\n        data_dict.pop(&amp;#39;gt_names&amp;#39;, None)\n        return data_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/datasets/processor/point_feature_encoder.py</p><span class=\'hidden-code\' data-code=\'class PointFeatureEncoder(object):\n    def forward(self, data_dict):\n        # data_dict[&amp;#39;points&amp;#39;].shape=(22307, 4); use_lead_xyz=True;后面是false \n        data_dict[&amp;#39;points&amp;#39;], use_lead_xyz = getattr(self, self.point_encoding_config.encoding_type)(data_dict[&amp;#39;points&amp;#39;])\n        data_dict[&amp;#39;use_lead_xyz&amp;#39;] = use_lead_xyz\n        if self.point_encoding_config.get(&amp;#39;filter_sweeps&amp;#39;, False) and &amp;#39;timestamp&amp;#39; in self.src_feature_list:\n            max_sweeps = self.point_encoding_config.max_sweeps\n            idx = self.src_feature_list.index(&amp;#39;timestamp&amp;#39;)\n            dt = np.round(data_dict[&amp;#39;points&amp;#39;][:, idx], 2)\n            max_dt = sorted(np.unique(dt))[min(len(np.unique(dt))-1, max_sweeps-1)]\n            data_dict[&amp;#39;points&amp;#39;] = data_dict[&amp;#39;points&amp;#39;][dt <= max_dt]\n        return data_dict\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">模型训练</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/detectors/IASSD.py:</p><span class=\'hidden-code\' data-code=\'class IASSD(Detector3DTemplate):\n    def forward(self, batch_dict):\n        for cur_module in self.module_list:\n            batch_dict = `cur_module`(batch_dict)\n        if self.training:\n            loss, tb_dict, disp_dict = self.get_training_loss()\n            ret_dict = {&amp;#39;loss&amp;#39;: loss}\n            return ret_dict, tb_dict, disp_dict\n        else:\n            pred_dicts, recall_dicts = self.post_processing(batch_dict)\n            return pred_dicts, recall_dicts\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      # batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]\n        batch_size = batch_dict[&amp;#39;batch_size&amp;#39;]\n        points = batch_dict[&amp;#39;points&amp;#39;]\n        batch_idx, xyz, features = self.break_up_pc(points)     # batch_idx.shape=(32768); xyz.shape=(32768,3); features.shape=(32768,1)\n        xyz_batch_cnt = xyz.new_zeros(batch_size).int()\n        for bs_idx in range(batch_size):\n            xyz_batch_cnt[bs_idx] = (batch_idx == bs_idx).sum()\n        assert xyz_batch_cnt.min() == xyz_batch_cnt.max()\n        xyz = xyz.view(batch_size, -1, 3)                       # (2,16384,3);\n        features = features.view(batch_size, -1, features.shape[-1]).permute(0, 2, 1).contiguous() if features is not None else None  # (2,1,16384);\n        encoder_xyz, encoder_features, sa_ins_preds = [xyz], [features], []\n        encoder_coords = [torch.cat([batch_idx.view(batch_size, -1, 1), xyz], dim=-1)]  # (2,16384,4);\n        # self.layer_types=[&amp;#39;SA_Layer&amp;#39;, &amp;#39;SA_Layer&amp;#39;, &amp;#39;SA_Layer&amp;#39;, &amp;#39;SA_Layer&amp;#39;, &amp;#39;Vote_Layer&amp;#39;, &amp;#39;SA_Layer&amp;#39;];\n        li_cls_pred = None \n        for `i` in range(len(self.SA_modules)):\n            xyz_input = encoder_xyz[self.layer_inputs[i]]                  # self.layer_inputs=[0, 1, 2, 3, 4, 3];\n            feature_input = encoder_features[self.layer_inputs[i]]      \n            if self.layer_types[i] == &amp;#39;SA_Layer&amp;#39;:                          # self.ctr_idx_list=[-1, -1, -1, -1, -1, 5];\n                ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None\n                li_xyz, li_features, li_cls_pred = self.SA_modules[i](xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)\n            elif self.layer_types[i] == &amp;#39;Vote_Layer&amp;#39;: # i=4\n                li_xyz, li_features, xyz_select, ctr_offsets = self.SA_modules[i](xyz_input, feature_input)\n                centers = li_xyz                       # (2,256,3);;->(512,4)\n                centers_origin = xyz_select            # (2,256,3);;->(512,4)\n                center_origin_batch_idx = batch_idx.view(batch_size, -1)[:, :centers_origin.shape[1]]\n                encoder_coords.append(torch.cat([center_origin_batch_idx[..., None].float(),centers_origin.view(batch_size, -1, 3)],dim =-1))\n                    \n            encoder_xyz.append(li_xyz)\n            li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n            encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1))\n            encoder_features.append(li_features)            \n            if li_cls_pred is not None:\n                li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]\n                sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n            else:\n                sa_ins_preds.append([])\n           \n        ctr_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]       # (2,256)\n        ctr_batch_idx = ctr_batch_idx.contiguous().view(-1)                       # (512)\n        # batch_dict里面的key和value值\n        # frame_id:array([&amp;#39;006209&amp;#39;, &amp;#39;005966&amp;#39;], dtype=&amp;#39;<U6&amp;#39;)\n        # gt_boxes:torch.Size([2, 36, 8])\n        # points:torch.Size([32768, 5])\n        # use_lead_xyz:tensor([1., 1.], device=&amp;#39;cuda:0&amp;#39;)\n        # image_shape:tensor([[ 375, 1242],[ 375, 1242]],torch.int32)\n        # batch_size:2\n        batch_dict[&amp;#39;ctr_offsets&amp;#39;] = torch.cat((ctr_batch_idx[:, None].float(), ctr_offsets.contiguous().view(-1, 3)), dim=1)  # torch.Size([512, 4])\n        batch_dict[&amp;#39;centers&amp;#39;] = torch.cat((ctr_batch_idx[:, None].float(), centers.contiguous().view(-1, 3)), dim=1)          # torch.Size([512, 4])\n        batch_dict[&amp;#39;centers_origin&amp;#39;] = torch.cat((ctr_batch_idx[:, None].float(), centers_origin.contiguous().view(-1, 3)), dim=1) # torch.Size([512, 4])\n    \n        center_features = encoder_features[-1].permute(0, 2, 1).contiguous().view(-1, encoder_features[-1].shape[1])  # torch.Size([512, 512])\n        batch_dict[&amp;#39;centers_features&amp;#39;] = center_features\n        batch_dict[&amp;#39;ctr_batch_idx&amp;#39;] = ctr_batch_idx        # torch.Size([512])值在0->batch_size-1之间\n        batch_dict[&amp;#39;encoder_xyz&amp;#39;] = encoder_xyz            # len()=7  [(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3),(2,256,3),(2,256,3)]  vote_ayer层的时候加了两次\n        batch_dict[&amp;#39;encoder_coords&amp;#39;] = encoder_coords      # len()=8  [(2,16384,4),(2,4096,4),(2,1024,4), (2,512,4),(2,256,4),(2,256,4),(2,256,4),(2,256,4)] \n        batch_dict[&amp;#39;sa_ins_preds&amp;#39;] = sa_ins_preds          # len()=6  [             [],        (2,1024,4),  (2,512,4),   [],        [], []] \n        batch_dict[&amp;#39;encoder_features&amp;#39;] = encoder_features  # len()=7 [(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256),[],(2,512,256)]\n        return batch_dict\n\'> </span><p><a href="https://gitee.com/zhao-study/data_code/blob/master/3target_detection_3D/project/IA_SSD/model.log">model.log</a><br></p>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      # batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]\n        i=0\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  # 0->xyz_input.shape=(2,16384,3);\n        feature_input = encoder_features[self.layer_inputs[i]]         # 0->feature_input=(2,1,16384);\n        if self.layer_types[i] == &amp;#39;SA_Layer&amp;#39;:                          # &amp;#39;SA_Layer&amp;#39;\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         # -1,None\n            li_xyz, li_features, li_cls_pred = self.`SA_modules[i]`(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  # li_cls_pred=None;\n        elif self.layer_types[i] == &amp;#39;Vote_Layer&amp;#39;: # i=4\n            pass\n                \n        encoder_xyz.append(li_xyz)                                                # encoder_xyz=[(2,16384,3),(2,4096,3)] \n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) # encoder_coords=[(2,16384,4),(2,4096,4)]->batch_size+ xyz \n        encoder_features.append(li_features)                                      # encoder_features=[(2,1,16384),(2,64,4096)]\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              # [[]] \n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><span class=\'hidden-code\' data-code=\'class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        # :param xyz: (B, N, 3) tensor of the xyz coordinates of the features\n        # :param features: (B, C, N) tensor of the descriptors of the the features\n        # :param cls_features: (B, N, num_class) tensor of the descriptors of the the confidence (classification) features \n        # :param new_xyz: (B, M, 3) tensor of the xyz coordinates of the sampled points\n        # &amp;#39;param ctr_xyz: tensor of the xyz coordinates of the centers \n        # :return:\n        #     new_xyz: (B, npoint, 3) tensor of the new features&amp;#39; xyz\n        #     new_features: (B, \\sum_k(mlps[k][-1]), npoint) tensor of the new_features descriptors\n        #     cls_features: (B, npoint, num_class) tensor of confidence (classification) features\n        new_features_list = []\n        xyz_flipped = xyz.transpose(1, 2).contiguous() \n        sampled_idx_list = []\n        if ctr_xyz is None:               # True\n            last_sample_end_index = 0\n            for i in range(len(self.sample_type_list)):   # self.sample_type_list=[&amp;#39;D-FPS&amp;#39;];\n                sample_type = self.sample_type_list[i]    # &amp;#39;D-FPS&amp;#39;\n                sample_range = self.sample_range_list[i]  # -1\n                npoint = self.npoint_list[i]              # self.npoint_list=[4096],npoint=4096 \n                if npoint <= 0:\n                    continue\n                if sample_range == -1: # 全部\n                    xyz_tmp = xyz[:, last_sample_end_index:, :]                # xyz_tmp.shape=(2,16384,3)\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:, :].contiguous()    # feature_tmp.shape=(2,16384,1); \n                    cls_features_tmp = cls_features[:, last_sample_end_index:, :] if cls_features is not None else None   # None\n                else:\n                    xyz_tmp = xyz[:, last_sample_end_index:sample_range, :].contiguous()\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:sample_range, :]\n                    cls_features_tmp = cls_features[:, last_sample_end_index:sample_range, :] if cls_features is not None else None \n                    last_sample_end_index += sample_range\n                if xyz_tmp.shape[1] <= npoint:                                 # No downsampling  False\n                    sample_idx = torch.arange(xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32) * torch.ones(xyz_tmp.shape[0], xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32)\n                elif (&amp;#39;cls&amp;#39; in sample_type) or (&amp;#39;ctr&amp;#39; in sample_type):         # False\n                    pass\n                elif &amp;#39;D-FPS&amp;#39; in sample_type or &amp;#39;DFS&amp;#39; in sample_type:                                    # True\n                    sample_idx = pointnet2_utils.furthest_point_sample(xyz_tmp.contiguous(), npoint)    # sample_idx.shape=(2,4096);\n                elif &amp;#39;F-FPS&amp;#39; in sample_type or &amp;#39;FFS&amp;#39; in sample_type:\n                    pass\n                elif sample_type == &amp;#39;FS&amp;#39;:\n                    pass\n                elif &amp;#39;Rand&amp;#39; in sample_type:\n                    pass\n                elif sample_type == &amp;#39;ds_FPS&amp;#39; or sample_type == &amp;#39;ds-FPS&amp;#39;:\n                    pass\n                elif sample_type == &amp;#39;ry_FPS&amp;#39; or sample_type == &amp;#39;ry-FPS&amp;#39;:\n                    pass\n                sampled_idx_list.append(sample_idx)\n            sampled_idx_list = torch.cat(sampled_idx_list, dim=-1) \n            new_xyz = pointnet2_utils.gather_operation(xyz_flipped, sampled_idx_list).transpose(1, 2).contiguous()  # new_xyz=(2,4096,3);\n        else:\n            new_xyz = ctr_xyz\n        if len(self.groupers) > 0:        # self.groupers=ModuleList((0): QueryAndGroup() (1): QueryAndGroup())\n            for i in range(`len(self.groupers)`):                        # groupers->mlps->aggregation_layer\n                new_features = self.groupers[i](xyz, new_xyz, features)  # (B, C, npoint, nsample)\n                new_features = self.mlps[i](new_features)                # (B, mlp[-1], npoint, nsample)\n                if self.pool_method == &amp;#39;max_pool&amp;#39;:\n                    new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n                elif self.pool_method == &amp;#39;avg_pool&amp;#39;:\n                    new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n                else:\n                    raise NotImplementedError\n                new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint)\n                new_features_list.append(new_features)\n            new_features = torch.cat(new_features_list, dim=1)  # (2,96,4096)\n            if self.aggregation_layer is not None:       # True\n                new_features = self.aggregation_layer(new_features)  # CBR:new_features.shape=(2,64,4096);\n        else:\n            new_features = pointnet2_utils.gather_operation(features, sampled_idx_list).contiguous()\n        if self.confidence_layers is not None:\n            cls_features = self.confidence_layers(new_features).transpose(1, 2)\n        else:\n            cls_features = None\n        return new_xyz, new_features, cls_features    # new_xyz=(2,4096,3); (2,64,4096); cls_features=None\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><span class=\'hidden-code\' data-code=\'class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=0\n        new_features = self.groupers[i](xyz, new_xyz, features)  # xyz.shape=(2,16384,3),new_xyz.shpae=(2,4096,3),features.shape=(2,1,16384)->\n                                                                 # (B, C, npoint, nsample)=(2,4,4096,16)\n        new_features = self.mlps[i](new_features)                # (B, mlp[-1], npoint, nsample)=(2,32,4096,16) \n        if self.pool_method == &amp;#39;max_pool&amp;#39;:\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1) (2,32,4096,1)\n        elif self.pool_method == &amp;#39;avg_pool&amp;#39;:\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint) (2,32,4096)\n        new_features_list.append(new_features)\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><span class=\'hidden-code\' data-code=\'class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=1\n        new_features = self.groupers[i](xyz, new_xyz, features)  # xyz.shape=(2,16384,3),new_xyz.shpae=(2,4096,3),features.shape=(2,1,16384)->\n                                                                 # (B, C, npoint, nsample)=(2,4,4096,32)\n        new_features = self.mlps[i](new_features)                # (B, mlp[-1], npoint, nsample)=(2,64,4096,32)\n        if self.pool_method == &amp;#39;max_pool&amp;#39;:\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1) (2,64,4096,1)\n        elif self.pool_method == &amp;#39;avg_pool&amp;#39;:\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint)=(2,64,4096)\n        new_features_list.append(new_features)\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      # batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]\n        i=1\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  # 1->xyz_input.shape=(2,4096,3);\n        feature_input = encoder_features[self.layer_inputs[i]]         # 1->feature_input=(2,64,4096);\n        if self.layer_types[i] == &amp;#39;SA_Layer&amp;#39;:                          # &amp;#39;SA_Layer&amp;#39;\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         # -1,None\n            li_xyz, li_features, li_cls_pred = self.`SA_modules[i]`(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  # li_cls_pred=None;\n        elif self.layer_types[i] == &amp;#39;Vote_Layer&amp;#39;: # i=4\n            pass\n                \n        encoder_xyz.append(li_xyz)                                                # encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3)]\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        # encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4)] \n        encoder_features.append(li_features)                                      # encoder_features==[(2,1,16384),(2,64,4096),(2,128,1024)]\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]    # sa_ins_preds=[[],(2,1024,4)] \n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><span class=\'hidden-code\' data-code=\'class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        new_features_list = []\n        xyz_flipped = xyz.transpose(1, 2).contiguous() \n        sampled_idx_list = []\n        if ctr_xyz is None:               # True\n            last_sample_end_index = 0\n            for i in range(len(self.sample_type_list)):   # self.sample_type_list=[&amp;#39;D-FPS&amp;#39;];\n                sample_type = self.sample_type_list[i]    # &amp;#39;D-FPS&amp;#39;\n                sample_range = self.sample_range_list[i]  # -1\n                npoint = self.npoint_list[i]              # self.npoint_list=[1024],npoint=1024 \n                if npoint <= 0:\n                    continue\n                if sample_range == -1: # 全部\n                    xyz_tmp = xyz[:, last_sample_end_index:, :]                # xyz_tmp.shape=(2,4096,3)\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:, :].contiguous()    # feature_tmp.shape=(2,4096,64); \n                    cls_features_tmp = cls_features[:, last_sample_end_index:, :] if cls_features is not None else None   # None\n                else:\n                    xyz_tmp = xyz[:, last_sample_end_index:sample_range, :].contiguous()\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:sample_range, :]\n                    cls_features_tmp = cls_features[:, last_sample_end_index:sample_range, :] if cls_features is not None else None \n                    last_sample_end_index += sample_range\n                if xyz_tmp.shape[1] <= npoint:                                 # No downsampling  False\n                    sample_idx = torch.arange(xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32) * torch.ones(xyz_tmp.shape[0], xyz_tmp.shape[1], device=xyz_tmp.device, dtype=torch.int32)\n                elif (&amp;#39;cls&amp;#39; in sample_type) or (&amp;#39;ctr&amp;#39; in sample_type):         # False\n                    pass\n                elif &amp;#39;D-FPS&amp;#39; in sample_type or &amp;#39;DFS&amp;#39; in sample_type:                                    # True\n                    sample_idx = pointnet2_utils.furthest_point_sample(xyz_tmp.contiguous(), npoint)    # sample_idx.shape=(2,1024);\n                elif &amp;#39;F-FPS&amp;#39; in sample_type or &amp;#39;FFS&amp;#39; in sample_type:\n                    pass\n                elif sample_type == &amp;#39;FS&amp;#39;:\n                    pass\n                elif &amp;#39;Rand&amp;#39; in sample_type:\n                    pass\n                elif sample_type == &amp;#39;ds_FPS&amp;#39; or sample_type == &amp;#39;ds-FPS&amp;#39;:\n                    pass\n                elif sample_type == &amp;#39;ry_FPS&amp;#39; or sample_type == &amp;#39;ry-FPS&amp;#39;:\n                    pass\n                sampled_idx_list.append(sample_idx)\n            sampled_idx_list = torch.cat(sampled_idx_list, dim=-1) \n            new_xyz = pointnet2_utils.gather_operation(xyz_flipped, sampled_idx_list).transpose(1, 2).contiguous()  # new_xyz=(2,1024,3);\n        else:\n            new_xyz = ctr_xyz\n        if len(self.groupers) > 0:        # self.groupers=ModuleList((0): QueryAndGroup() (1): QueryAndGroup())\n            for i in range(`len(self.groupers)`):                        # groupers->mlps->aggregation_layer\n                new_features = self.groupers[i](xyz, new_xyz, features)  # (B, C, npoint, nsample)\n                new_features = self.mlps[i](new_features)                # (B, mlp[-1], npoint, nsample)\n                if self.pool_method == &amp;#39;max_pool&amp;#39;:\n                    new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n                elif self.pool_method == &amp;#39;avg_pool&amp;#39;:\n                    new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n                else:\n                    raise NotImplementedError\n                new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint)\n                new_features_list.append(new_features)\n            new_features = torch.cat(new_features_list, dim=1)  # (2,256,1024)\n            if self.aggregation_layer is not None:       # True\n                new_features = self.aggregation_layer(new_features)  # CBR:new_features.shape=(2,128,1024);\n        else:\n            new_features = pointnet2_utils.gather_operation(features, sampled_idx_list).contiguous()\n        if self.confidence_layers is not None:\n            cls_features = self.confidence_layers(new_features).transpose(1, 2)  # cls_features.shape=(2,1024,?) 用于ctr_aware选取前景点的\n        else:\n            cls_features = None\n        return new_xyz, new_features, cls_features    # new_xyz=(2,1024,3); (2,128,1024); cls_features=(2,1024,?)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><span class=\'hidden-code\' data-code=\'class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=0\n        new_features = self.groupers[i](xyz, new_xyz, features)  # xyz.shape=(2,4096,3),new_xyz.shpae=(2,1024,3),features.shape=(2,64,4096)->\n                                                                 # (B, C, npoint, nsample)=(2,67,1024,16)\n        new_features = self.mlps[i](new_features)                # (B, mlp[-1], npoint, nsample)=(2,128,1024,16) \n        if self.pool_method == &amp;#39;max_pool&amp;#39;:\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1) (2,128,1024,1)\n        elif self.pool_method == &amp;#39;avg_pool&amp;#39;:\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint) (2,128,1024)\n        new_features_list.append(new_features)\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><span class=\'hidden-code\' data-code=\'class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        i=1\n        new_features = self.groupers[i](xyz, new_xyz, features)  # xyz.shape=(2,4096,3),new_xyz.shpae=(2,1024,3),features.shape=(2,64,4096)->\n                                                                 # (B, C, npoint, nsample)=(2,67,1024,32)\n        new_features = self.mlps[i](new_features)                # (B, mlp[-1], npoint, nsample)=(2,128,1024,32)\n        if self.pool_method == &amp;#39;max_pool&amp;#39;:\n            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1) (2,128,1024,1)\n        elif self.pool_method == &amp;#39;avg_pool&amp;#39;:\n            new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n        else:\n            raise NotImplementedError\n        new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint)=(2,128,1024)\n        new_features_list.append(new_features)\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      # batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]\n        i=2\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  # 1->xyz_input.shape=(2,1024,3);\n        feature_input = encoder_features[self.layer_inputs[i]]         # 1->feature_input=(2,128,1024);\n        if self.layer_types[i] == &amp;#39;SA_Layer&amp;#39;:                          # &amp;#39;SA_Layer&amp;#39;\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         # -1,None\n            li_xyz, li_features, li_cls_pred = self.`SA_modules[i]`(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  # li_cls_pred=(2,1024,3)\n        elif self.layer_types[i] == &amp;#39;Vote_Layer&amp;#39;: # i=4\n            pass\n                \n        encoder_xyz.append(li_xyz)                                                # encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3)]\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) # encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4)]  \n        encoder_features.append(li_features)                                      # encoder_features=[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512)] \n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]      # sa_ins_preds=[[],(2,1024,4),(2,512，4)]\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              # [[]] \n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/ops/pointnet2/pointnet2_batch/pointnet2_modules.py</p><span class=\'hidden-code\' data-code=\'class PointnetSAModuleMSG_WithSampling(_PointnetSAModuleBase):\n    def forward(self, xyz: torch.Tensor, features: torch.Tensor = None, cls_features: torch.Tensor = None, new_xyz=None, ctr_xyz=None):\n        new_features_list = []\n        xyz_flipped = xyz.transpose(1, 2).contiguous() \n        sampled_idx_list = []\n        if ctr_xyz is None:               # True\n            last_sample_end_index = 0\n            for i in range(len(self.sample_type_list)):   # self.sample_type_list=[&amp;#39;ctr_aware&amp;#39;];\n                sample_type = self.sample_type_list[i]    # &amp;#39;ctr_aware&amp;#39;\n                sample_range = self.sample_range_list[i]  # -1\n                npoint = self.npoint_list[i]              # self.npoint_list=[512],npoint=512 \n                if npoint <= 0:\n                    continue\n                if sample_range == -1: # 全部\n                    xyz_tmp = xyz[:, last_sample_end_index:, :]                                          # xyz_tmp.shape=(2,1024,3)\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:, :].contiguous()    # feature_tmp.shape=(2,1024,128); \n                    cls_features_tmp = cls_features[:, last_sample_end_index:, :] if cls_features is not None else None   # (2,1024,3)\n                else:\n                    xyz_tmp = xyz[:, last_sample_end_index:sample_range, :].contiguous()\n                    feature_tmp = features.transpose(1, 2)[:, last_sample_end_index:sample_range, :]\n                    cls_features_tmp = cls_features[:, last_sample_end_index:sample_range, :] if cls_features is not None else None \n                    last_sample_end_index += sample_range\n                if xyz_tmp.shape[1] <= npoint:                                 # No downsampling  False\n                    sample_idx = torch.arange(xyz_tmp.shape[1],device=xyz_tmp.device,dtype=torch.int32)*torch.ones(xyz_tmp.shape[0],xyz_tmp.shape[1],device=xyz_tmp.device,dtype=torch.int32)\n                elif (&amp;#39;cls&amp;#39; in sample_type) or (&amp;#39;ctr&amp;#39; in sample_type):         # True\n                    cls_features_max, class_pred = cls_features_tmp.max(dim=-1)            # cls_features_max.shape=(2,1024);\n                    score_pred = torch.sigmoid(cls_features_max)                           # B,N   score_pred.shape=(2,1024);    \n                    score_picked, sample_idx = torch.topk(score_pred, npoint, dim=-1)           \n                    sample_idx = sample_idx.int()                                          # sample_idx.shape=(2,512);\n                elif &amp;#39;D-FPS&amp;#39; in sample_type or &amp;#39;DFS&amp;#39; in sample_type:                                    # False\n                    sample_idx = pointnet2_utils.furthest_point_sample(xyz_tmp.contiguous(), npoint)\n                elif &amp;#39;F-FPS&amp;#39; in sample_type or &amp;#39;FFS&amp;#39; in sample_type:\n                    pass\n                elif sample_type == &amp;#39;FS&amp;#39;:\n                    pass\n                elif &amp;#39;Rand&amp;#39; in sample_type:\n                    pass\n                elif sample_type == &amp;#39;ds_FPS&amp;#39; or sample_type == &amp;#39;ds-FPS&amp;#39;:\n                    pass\n                elif sample_type == &amp;#39;ry_FPS&amp;#39; or sample_type == &amp;#39;ry-FPS&amp;#39;:\n                    pass\n                sampled_idx_list.append(sample_idx)\n            sampled_idx_list = torch.cat(sampled_idx_list, dim=-1) \n            new_xyz = pointnet2_utils.gather_operation(xyz_flipped, sampled_idx_list).transpose(1, 2).contiguous()  # new_xyz=(2,512,3);\n        else:\n            new_xyz = ctr_xyz\n        if len(self.groupers) > 0:        # self.groupers=ModuleList((0): QueryAndGroup() (1): QueryAndGroup())\n            for i in range(`len(self.groupers)`):                        # groupers->mlps->aggregation_layer\n                new_features = self.groupers[i](xyz, new_xyz, features)  # (B, C, npoint, nsample)\n                new_features = self.mlps[i](new_features)                # (B, mlp[-1], npoint, nsample)\n                if self.pool_method == &amp;#39;max_pool&amp;#39;:\n                    new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n                elif self.pool_method == &amp;#39;avg_pool&amp;#39;:\n                    new_features = F.avg_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n                else:\n                    raise NotImplementedError\n                new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint)\n                new_features_list.append(new_features)\n            new_features = torch.cat(new_features_list, dim=1)  \n            if self.aggregation_layer is not None:     \n                new_features = self.aggregation_layer(new_features)\n        else:\n            new_features = pointnet2_utils.gather_operation(features, sampled_idx_list).contiguous()\n        if self.confidence_layers is not None:\n            cls_features = self.confidence_layers(new_features).transpose(1, 2)  \n        else:\n            cls_features = None\n        return new_xyz, new_features, cls_features    # new_xyz=(2,512,3); (2,256,512);\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      # batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]\n        i=3\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  # 1->xyz_input.shape=(2,512,3);\n        feature_input = encoder_features[self.layer_inputs[i]]         # 1->feature_input=(2,256,512);\n        if self.layer_types[i] == &amp;#39;SA_Layer&amp;#39;:                          # &amp;#39;SA_Layer&amp;#39;\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         # -1,None\n            li_xyz, li_features, li_cls_pred = self.`SA_modules[i]`(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  # li_cls_pred=None;\n        elif self.layer_types[i] == &amp;#39;Vote_Layer&amp;#39;: # i=4\n            pass\n                \n        encoder_xyz.append(li_xyz)                              # encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3)]\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        # encoder_coords[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4),(2,256,4)]->batch_size+ xyz \n        encoder_features.append(li_features)                   # encoder_features=[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256)]\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]   # [[],(2,1024,4),(2,512,4)，[]]\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              # [[]] \n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      # batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]\n        i=4\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  # 1->xyz_input.shape=(2,256,3);\n        feature_input = encoder_features[self.layer_inputs[i]]         # 1->feature_input=(2,256,256);\n        if self.layer_types[i] == &amp;#39;SA_Layer&amp;#39;:                          \n            pass\n        elif self.layer_types[i] == &amp;#39;Vote_Layer&amp;#39;:                      # i=4 # &amp;#39;Vote_Layer&amp;#39;\n            li_xyz, li_features, xyz_select, ctr_offsets = `self.SA_modules[i]`(xyz_input, feature_input)\n            centers = li_xyz\n            centers_origin = xyz_select\n            center_origin_batch_idx = batch_idx.view(batch_size, -1)[:, :centers_origin.shape[1]]\n            encoder_coords.append(torch.cat([center_origin_batch_idx[..., None].float(),centers_origin.view(batch_size, -1, 3)],dim =-1))\n                \n        encoder_xyz.append(li_xyz)                                       # encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3),(2,256,3)]\n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        # encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4),(2,256,4),(2,256,4)]\n        encoder_features.append(li_features)                            # encoder_features==[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256),[]]\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]    # sa_ins_preds=[[],(2,1024,4),(2,512,4),[],[]]\n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/backbones_3d/IASSD_backbone.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Backbone(nn.Module):\n    def forward(self, batch_dict):      # batch_size:int  vfe_features:(num_voxels,C) points:(num_points,4+C),[batch_idx,x,y,z,...]\n        i=5\n        xyz_input = encoder_xyz[self.layer_inputs[i]]                  # 1->xyz_input.shape=(2,256,3);\n        feature_input = encoder_features[self.layer_inputs[i]]         # 1->feature_input=(2,256,256);\n        if self.layer_types[i] == &amp;#39;SA_Layer&amp;#39;:                          # &amp;#39;SA_Layer&amp;#39;\n            ctr_xyz = encoder_xyz[self.ctr_idx_list[i]] if self.ctr_idx_list[i] != -1 else None         # -1,None\n            li_xyz, li_features, li_cls_pred = self.`SA_modules[i]`(xyz_input, feature_input, li_cls_pred, ctr_xyz=ctr_xyz)  # li_cls_pred=None;\n        elif self.layer_types[i] == &amp;#39;Vote_Layer&amp;#39;: # i=4\n            pass\n                \n        encoder_xyz.append(li_xyz)                     # encoder_xyz=[(2,16384,3),(2,4096,3),(2,1024,3),(2,512,3),(2,256,3),(2,256,3),(2,256,3)] \n        li_batch_idx = batch_idx.view(batch_size, -1)[:, :li_xyz.shape[1]]\n        encoder_coords.append(torch.cat([li_batch_idx[..., None].float(),li_xyz.view(batch_size, -1, 3)],dim =-1)) \n        # encoder_coords=[(2,16384,4),(2,4096,4),(2,1024,4),(2,512,4),(2,256,4),(2,256,4),(2,256,4),(2,256,4)]->batch_size+ xyz \n        encoder_features.append(li_features)     # encoder_features=[(2,1,16384),(2,64,4096),(2,128,1024),(2,256,512),(2,256,256),[],(2,512,256)]\n        if li_cls_pred is not None: \n            li_cls_batch_idx = batch_idx.view(batch_size, -1)[:, :li_cls_pred.shape[1]]   # [[],(2,1024,4),(2,512,4),[],[],[]] \n            sa_ins_preds.append(torch.cat([li_cls_batch_idx[..., None].float(),li_cls_pred.view(batch_size, -1, li_cls_pred.shape[-1])],dim =-1)) \n        else:\n            sa_ins_preds.append([])              # [[]] \n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/dense_heads/IASSD_head.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Head(PointHeadTemplate):\n    def forward(self, batch_dict):\n        # input:     batch_dict:\n        #     batch_size:\n        #     centers_features: (N1 + N2 + N3 + ..., C) or (B, N, C)\n        #     centers: (N1 + N2 + N3 + ..., 4) [bs_idx, x, y, z]\n        #     encoder_xyz: List of points_coords in SA\n        #     gt_boxes (optional): (B, M, 8)\n        # Returns:  batch_dict:\n        #         batch_cls_preds: (N1 + N2 + N3 + ..., num_class)\n        #         point_box_preds: (N1 + N2 + N3 + ..., 7)\n        center_features = batch_dict[&amp;#39;centers_features&amp;#39;]        # (512,512)\n        center_coords = batch_dict[&amp;#39;centers&amp;#39;]                   # (512,4);\n        center_cls_preds = self.cls_center_layers(center_features)  # (total_centers, num_class)     (512,3);\n        center_box_preds = self.box_center_layers(center_features)  # (total_centers, box_code_size) (512,30);\n                                                                    # 30为6+12*2（12个bin以及每个bin的迁移）\n        box_iou3d_preds = self.box_iou3d_layers(center_features) if self.box_iou3d_layers is not None else None # None,\n        ret_dict = {&amp;#39;center_cls_preds&amp;#39;: center_cls_preds,\n                    &amp;#39;center_box_preds&amp;#39;: center_box_preds,\n                    &amp;#39;ctr_offsets&amp;#39;: batch_dict[&amp;#39;ctr_offsets&amp;#39;],\n                    &amp;#39;centers&amp;#39;: batch_dict[&amp;#39;centers&amp;#39;],\n                    &amp;#39;centers_origin&amp;#39;: batch_dict[&amp;#39;centers_origin&amp;#39;],\n                    &amp;#39;sa_ins_preds&amp;#39;: batch_dict[&amp;#39;sa_ins_preds&amp;#39;],\n                    &amp;#39;box_iou3d_preds&amp;#39;: box_iou3d_preds}\n        if self.training:\n            targets_dict = self.`assign_targets`(batch_dict)\n            ret_dict.update(targets_dict)\n        if not self.training or self.predict_boxes_when_training or \\\n                self.model_cfg.LOSS_CONFIG.CORNER_LOSS_REGULARIZATION or \\\n                self.model_cfg.LOSS_CONFIG.CENTERNESS_REGULARIZATION or \\\n                self.model_cfg.LOSS_CONFIG.IOU3D_REGULARIZATION:\n            point_cls_preds, point_box_preds = self.generate_predicted_boxes(\n                    points=center_coords[:, 1:4],point_cls_preds=center_cls_preds, point_box_preds=center_box_preds)\n            batch_dict[&amp;#39;batch_cls_preds&amp;#39;] = point_cls_preds\n            batch_dict[&amp;#39;batch_box_preds&amp;#39;] = point_box_preds\n            batch_dict[&amp;#39;box_iou3d_preds&amp;#39;] = box_iou3d_preds\n            batch_dict[&amp;#39;batch_index&amp;#39;] = center_coords[:,0]\n            batch_dict[&amp;#39;cls_preds_normalized&amp;#39;] = False\n            ret_dict[&amp;#39;point_box_preds&amp;#39;] = point_box_preds\n        self.forward_ret_dict = ret_dict\n        return batch_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/dense_heads/IASSD_head.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Head(PointHeadTemplate):\n    def assign_targets(self, input_dict):\n        ........\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/models/dense_heads/IASSD_head.py</p><span class=\'hidden-code\' data-code=\'class IASSD_Head(PointHeadTemplate):\n    def assign_stack_targets_IASSD(self, points, gt_boxes, extend_gt_boxes=None, weighted_labels=False,\n                             ret_box_labels=False, ret_offset_labels=True,\n                             set_ignore_flag=True, use_ball_constraint=False, central_radius=2.0,\n                             use_query_assign=False, central_radii=2.0, use_ex_gt_assign=False, fg_pc_ignore=False,\n                             binary_label=False):\n        # Args:\n        #     points: (N1 + N2 + N3 + ..., 4) [bs_idx, x, y, z]\n        #     gt_boxes: (B, M, 8)\n        #     extend_gt_boxes: [B, M, 8]\n        # Returns:\n        #     point_cls_labels: (N1 + N2 + N3 + ...), long type, 0:background, -1:ignored\n        #     point_box_labels: (N1 + N2 + N3 + ..., code_size)\n        assert len(points.shape) == 2 and points.shape[1] == 4, &amp;#39;points.shape=%s&amp;#39; % str(points.shape)\n        assert len(gt_boxes.shape) == 3 and (gt_boxes.shape[2] == 8 or gt_boxes.shape[2] == 10), &amp;#39;gt_boxes.shape=%s&amp;#39; % str(gt_boxes.shape)\n        assert extend_gt_boxes is None or len(extend_gt_boxes.shape) == 3 and (extend_gt_boxes.shape[2] == 8 or extend_gt_boxes.shape[2] == 10), \\\n            &amp;#39;extend_gt_boxes.shape=%s&amp;#39; % str(extend_gt_boxes.shape)\n        batch_size = gt_boxes.shape[0]\n        box_size = gt_boxes.shape[2]\n        bs_idx = points[:, 0]                                              # bs_idx=512它的值是第几个batch_id\n        point_cls_labels = points.new_zeros(points.shape[0]).long()        \n        point_box_labels = gt_boxes.new_zeros((points.shape[0], box_size)) if ret_box_labels else None\n        box_idxs_labels = points.new_zeros(points.shape[0]).long()        \n        gt_boxes_of_fg_points = []                                         \n        gt_box_of_points = gt_boxes.new_zeros((points.shape[0], box_size))\n        for k in range(batch_size):            \n            bs_mask = (bs_idx == k)\n            points_single = points[bs_mask][:, 1:4]        # (256,3)某个batch的那些点\n            point_cls_labels_single = point_cls_labels.new_zeros(bs_mask.sum())    # (256);\n            box_idxs_of_pts = roiaware_pool3d_utils.points_in_boxes_gpu(           # (256);256个点是不是在3Dbox里面 \n                points_single.unsqueeze(dim=0), gt_boxes[k:k + 1, :, 0:7].contiguous()).long().squeeze(dim=0)\n            box_fg_flag = (box_idxs_of_pts >= 0)           # .sum=42表示该batch有42个点在3D box里面\n            if use_query_assign: # False\n                pass\n            elif use_ex_gt_assign: #\n                pass                   \n            elif set_ignore_flag:       # True \n                extend_box_idxs_of_pts = roiaware_pool3d_utils.points_in_boxes_gpu( # .sum()=54个点在扩充box里面，不在里面但是在扩充的范围之内的有12个(54-42)\n                    points_single.unsqueeze(dim=0), extend_gt_boxes[k:k+1, :, 0:7].contiguous()).long().squeeze(dim=0)\n                fg_flag = box_fg_flag\n                ignore_flag = fg_flag ^ (extend_box_idxs_of_pts >= 0)\n                point_cls_labels_single[ignore_flag] = -1        # 不在里面但是在扩充的范围之内的 \n            elif use_ball_constraint: \n                box_centers = gt_boxes[k][box_idxs_of_pts][:, 0:3].clone()\n                box_centers[:, 2] += gt_boxes[k][box_idxs_of_pts][:, 5] / 2\n                ball_flag = ((box_centers - points_single).norm(dim=1) < central_radius)\n                fg_flag = box_fg_flag &amp; ball_flag\n            else:\n                raise NotImplementedError\n            gt_box_of_fg_points = gt_boxes[k][box_idxs_of_pts[fg_flag]]    # (44,8)有44个点在box里面(不包括扩充部分)，这里bbox有重复；\n            point_cls_labels_single[fg_flag] = 1 if self.num_class == 1 or binary_label else gt_box_of_fg_points[:, -1].long()\n            point_cls_labels[bs_mask] = point_cls_labels_single\n            bg_flag = (point_cls_labels_single == 0) # except ignore_id  为背景的是True\n            # box_bg_flag\n            fg_flag = fg_flag ^ (fg_flag &amp; bg_flag)\n            gt_box_of_fg_points = gt_boxes[k][box_idxs_of_pts[fg_flag]]\n            gt_boxes_of_fg_points.append(gt_box_of_fg_points)\n            box_idxs_labels[bs_mask] = box_idxs_of_pts                 # (512,)；里面的值表示在哪个box3D里面 \n            gt_box_of_points[bs_mask] = gt_boxes[k][box_idxs_of_pts]   # (512,8) 512个点含有box3D的对应值 \n            if ret_box_labels and gt_box_of_fg_points.shape[0] > 0:    # True\n                point_box_labels_single = point_box_labels.new_zeros((bs_mask.sum(), box_size))  # bs_mask.sum()=256->point_box_labels_single.shape=(256,8)\n                fg_point_box_labels = self.box_coder.`encode_torch`(\n                    gt_boxes=gt_box_of_fg_points[:, :-1], points=points_single[fg_flag],gt_classes=gt_box_of_fg_points[:, -1].long())\n                point_box_labels_single[fg_flag] = fg_point_box_labels    # (256,8)\n                point_box_labels[bs_mask] = point_box_labels_single\n        gt_boxes_of_fg_points = torch.cat(gt_boxes_of_fg_points, dim=0)\n        targets_dict = {\n            &amp;#39;point_cls_labels&amp;#39;: point_cls_labels,           # torch.Size([512]);0是背景的label；-1是ignore，1-n_class是类别的值\n            &amp;#39;point_box_labels&amp;#39;: point_box_labels,           # torch.Size([512, 8])；经过encode的结果；有重复的，最后两维为bin_id和bin_res\n            &amp;#39;gt_box_of_fg_points&amp;#39;: gt_boxes_of_fg_points,   # torch.Size([67, 8])；原始的3D box有重复的\n            &amp;#39;box_idxs_labels&amp;#39;: box_idxs_labels,             # torch.Size([512]); torch.sum(box_idxs_labels>=0)=67; 值为是哪个3D box\n            &amp;#39;gt_box_of_points&amp;#39;: gt_box_of_points,           # torch.Size([512, 8]);背景和-1的box为0，其它为该点对应的GTbox\n        }\n        return targets_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">pcdet/utils/box_coder_utils.py:</p><span class=\'hidden-code\' data-code=\'class PointResidual_BinOri_Coder(object):\n    def encode_torch(self, gt_boxes, points, gt_classes=None):\n        # Args:\n        #     gt_boxes: (N, 7 + C) [x, y, z, dx, dy, dz, heading, ...]\n        #     points: (N, 3) [x, y, z]\n        #     gt_classes: (N) [1, num_classes]\n        # Returns:\n        #     box_coding: (N, 8 + C)\n        gt_boxes[:, 3:6] = torch.clamp_min(gt_boxes[:, 3:6], min=1e-5)  # gt_boxes.shape=(44,7); points.shape=(44,3);\n        xg, yg, zg, dxg, dyg, dzg, rg, *cgs = torch.split(gt_boxes, 1, dim=-1)\n        xa, ya, za = torch.split(points, 1, dim=-1)\n        if self.use_mean_size:             \n            assert gt_classes.max() <= self.mean_size.shape[0]  # gt_classes.shape=(44)上面44个box的真实标签 \n            point_anchor_size = self.mean_size[gt_classes - 1]  # tensor([[3.9,1.6,1.56],[0.8,0.6,1.73],[1.76,0.6,1.73]],device=&amp;#39;cuda:0&amp;#39;)\n            # gt_classes.unique()\n            dxa, dya, dza = torch.split(point_anchor_size, 1, dim=-1)\n            diagonal = torch.sqrt(dxa ** 2 + dya ** 2)\n            xt = (xg - xa) / diagonal\n            yt = (yg - ya) / diagonal\n            zt = (zg - za) / dza\n            dxt = torch.log(dxg / dxa)\n            dyt = torch.log(dyg / dya)\n            dzt = torch.log(dzg / dza)\n        else:\n            pass\n        rg = torch.clamp(rg, max=np.pi - 1e-5, min=-np.pi + 1e-5)   #################\n        bin_id = torch.floor((rg + np.pi) / self.bin_inter)   # -np.pi->np.pi; 0->2*np.pi; 0->12\n        # if bin_id.max() >= self.bin_size:                   # self.bin_inter=0.5235987755982988【0/self.bin_inter=0.0； 2*np.pi/self.bin_inter=12.0】\n        #     a = 1\n        bin_res = ((rg + np.pi) - (bin_id * self.bin_inter + self.bin_inter / 2)) / (self.bin_inter / 2)  # norm to [-1, 1]\n        cts = [g for g in cgs]\n        return torch.cat([xt, yt, zt, dxt, dyt, dzt, bin_id, bin_res, *cts], dim=-1)\n\'> </span>'}]}]}]}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
