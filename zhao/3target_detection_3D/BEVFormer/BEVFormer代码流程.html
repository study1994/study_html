<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>BEVFormer代码流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">数据处理</p>'}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/detectors/bevformer.py：模型训练</p><span class=\'hidden-code\' data-code=\'@DETECTORS.register_module()\nclass BEVFormer(MVXTwoStageDetector):\n    def forward(self, return_loss=True, **kwargs):\n        if return_loss:\n            return self.forward_train(**kwargs)\n        else:\n            return self.forward_test(**kwargs)\n    @auto_fp16(apply_to=(’img’, ’points’))\n    def forward_train(self,points=None,img_metas=None,gt_bboxes_3d=None,gt_labels_3d=None,gt_labels=None,\n                      gt_bboxes=None,img=None,proposals=None,gt_bboxes_ignore=None,img_depth=None,img_mask=None): \n        len_queue = img.size(1)     3  前两个历史数据\n        prev_img = img[:, :-1, ...] (1, 2, 6, 3, 480, 800)  bs, len_queue, num_cams, C, H, W\n        img = img[:, -1, ...]       (1, 6, 3, 480, 800)\n        prev_img_metas = copy.deepcopy(img_metas)\n        prev_bev = self.`obtain_history_bev`(prev_img, prev_img_metas) (1, 2500, 256)\n        img_metas = [each[len_queue-1] for each in img_metas]\n        if not img_metas[0][’prev_bev_exists’]:\n            prev_bev = None\n        img_feats = self.`extract_feat`(img=img, img_metas=img_metas)\n        losses = dict()\n        losses_pts = self.`forward_pts_train`(img_feats, gt_bboxes_3d,gt_labels_3d, img_metas,gt_bboxes_ignore, prev_bev)\n        losses.update(losses_pts)\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/detectors/bevformer.py</p><span class=\'hidden-code\' data-code=\'@DETECTORS.register_module()\nclass BEVFormer(MVXTwoStageDetector):\n    def obtain_history_bev(self, imgs_queue, img_metas_list):\n        # Obtain history BEV features iteratively. To save GPU memory, gradients are not calculated\n        self.eval()\n        with torch.no_grad():\n            prev_bev = None                                                  # 初始化前一帧的BEV特征图为None\n            bs, len_queue, num_cams, C, H, W = imgs_queue.shape              # 1, 2, 6, 3, 480, 800\n            imgs_queue = imgs_queue.reshape(bs*len_queue, num_cams, C, H, W) # (2, 6, 3, 480, 800)\n            img_feats_list = self.extract_feat(img=imgs_queue, len_queue=len_queue)       # list, (1, 2, 6, 256, 15, 25)\n            for i in range(len_queue):                                       # len_queue，帧，逐帧处理\n                img_metas = [each[i] for each in img_metas_list]             # 取出第i帧的图像信息\n                if not img_metas[0][’prev_bev_exists’]:\n                    prev_bev = None\n                # img_feats = self.extract_feat(img=img, img_metas=img_metas)   img_feature_list是多尺度特征\n                # each scale: (1, 2, 6, 256, 15, 25) -> each scale[:, i]: (1, 6, 256, 15, 25)\n                img_feats = [each_scale[:, i] for each_scale in img_feats_list]              # list, (1, 6, 256, 15, 25)\n                prev_bev = self.pts_bbox_head(img_feats, img_metas, prev_bev, only_bev=True) # BEVFormerHead, (1, 2500, 256)\n            self.train()\n            return prev_bev\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/detectors/bevformer.py</p><span class=\'hidden-code\' data-code=\'@DETECTORS.register_module()\nclass BEVFormer(MVXTwoStageDetector):\n    @auto_fp16(apply_to=(’img’))\n    def extract_feat(self, img, img_metas=None, len_queue=None):\n        # Extract features from images and points.\n        img_feats = self.`extract_img_feat`(img, img_metas, len_queue=len_queue)     # ->(1, 2, 6, 256, 15, 25)\n        return img_feats\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/detectors/bevformer.py</p><span class=\'hidden-code\' data-code=\'@DETECTORS.register_module()\nclass BEVFormer(MVXTwoStageDetector):\n    def extract_img_feat(self, img, img_metas, len_queue=None):      # Extract features of images.’’’\n        B = img.size(0)                                              # bs*len_queue, 2, img.size: (2, 6, 3, 480, 800)\n        if img is not None:\n            # input_shape = img.shape[-2:]\n            # # update real input shape of each single img\n            # for img_meta in img_metas:\n            #     img_meta.update(input_shape=input_shape)\n            if img.dim() == 5 and img.size(0) == 1:\n                img.squeeze_()\n            elif img.dim() == 5 and img.size(0) > 1:\n                B, N, C, H, W = img.size()\n                img = img.reshape(B * N, C, H, W)    # (12, 3, 480, 800)\n            if self.use_grid_mask:                   # True     --是不是实行网格屏蔽的策略？把一些区域屏蔽掉了-提高模型鲁棒性\n                img = self.grid_mask(img)            # (12, 3, 480, 800)\n            img_feats = self.img_backbone(img)       # backbone模块，包括resnet等\n            if isinstance(img_feats, dict):\n                img_feats = list(img_feats.values()) # img_feats, (12, 2048, 15, 25), 32倍下采样\n        else:\n            return None\n        if self.with_img_neck:                       # fpn模块\n            img_feats = self.img_neck(img_feats)     # (12, 256, 15, 25)\n        img_feats_reshaped = []                      # 维度重新排列一下\n        for img_feat in img_feats:                   # 多尺度\n            BN, C, H, W = img_feat.size()            # 12, 256, 15, 25\n            if len_queue is not None:\n                img_feats_reshaped.append(img_feat.view(int(B/len_queue), len_queue, int(BN / B), C, H, W)) # (1, 2, 6, 256, 15, 25)\n            else:\n                img_feats_reshaped.append(img_feat.view(B, int(BN / B), C, H, W))\n        return img_feats_reshaped\n\'> </span>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/detectors/bevformer.py</p><span class=\'hidden-code\' data-code=\'@DETECTORS.register_module()\nclass BEVFormer(MVXTwoStageDetector):\n    def forward_pts_train(self,pts_feats,gt_bboxes_3d,gt_labels_3d,img_metas,gt_bboxes_ignore=None,prev_bev=None):\n        outs = self.`pts_bbox_head`(pts_feats, img_metas, prev_bev)\n        loss_inputs = [gt_bboxes_3d, gt_labels_3d, outs]\n        losses = self.pts_bbox_head.`loss`(*loss_inputs, img_metas=img_metas)\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/dense_heads/bevformer_head.py</p><span class=\'hidden-code\' data-code=\'@HEADS.register_module()\nclass BEVFormerHead(DETRHead):\n    @auto_fp16(apply_to=(’mlvl_feats’))\n    def forward(self, mlvl_feats, img_metas, prev_bev=None,  only_bev=False):\n        bs, num_cam, _, _, _ = mlvl_feats[0].shape                   # (1, 6, 256, 15, 25)\n        dtype = mlvl_feats[0].dtype                                  # torch.float32\n        # self.query_embedding = nn.Embedding(self.num_query,self.embed_dims * 2) # (900, 512)\n        object_query_embeds = self.query_embedding.weight.to(dtype)  # 物体query，(900, 512)\n        bev_queries = self.bev_embedding.weight.to(dtype)            # BEV query, (2500=50x50, 256)\n        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w),=device=bev_queries.device).to(dtype)   # (1, 50, 50), 0值\n        bev_pos = self.`positional_encoding`(bev_mask).to(dtype)                                      # (1, 256, 50, 50)，位置编码 \n        # 来自from mmdet.models.dense_heads import DETRHead\n        if only_bev:                # only use encoder to obtain BEV features, TODO: refine the workaround【用历史信息的时候】\n            return self.transformer.`get_bev_features`(        # PerceptionTransformer\n                mlvl_feats,         # 多尺寸图像特征, (1, 6, 256, 15, 25)\n                bev_queries,        # (2500, 256)\n                self.bev_h,         # 50\n                self.bev_w,         # 50\n                grid_length=(self.real_h / self.bev_h, self.real_w / self.bev_w), # 网格尺寸，每一个bev网格的真实尺寸, (102.4/50=2.048, 2.048)\n                bev_pos=bev_pos,     # (1, 256, 50, 50)\n                img_metas=img_metas, # 图像信息\n                prev_bev=prev_bev,   # history bev\n            )                        # (1, 2500, 256)\n        else:\n            outputs = self.`transformer`(\n                mlvl_feats,          # 多尺寸图像特征, (1, 6, 256, 15, 25)\n                bev_queries,         # (2500, 256)\n                object_query_embeds, # 物体query，(900, 512)\n                self.bev_h,          # 50 \n                self.bev_w,          # 50\n                grid_length=(self.real_h / self.bev_h, \n                             self.real_w / self.bev_w),                            # 2.048\n                bev_pos=bev_pos,     # (1, 256, 50, 50)\n                reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501 6层\n                cls_branches=self.cls_branches if self.as_two_stage else None,     # 6层\n                img_metas=img_metas, # 图像信息，cur_frame\n                prev_bev=prev_bev    # 历史信息 (1, 2500, 256)\n        )\n        # bev_embed:(2500, 1, 256)       bev的拉直嵌入\n        # hs:(6, 900, 1, 256)            内部decoder layer输出的object query\n        # init_reference_out:(1, 900, 3) 随机初始化的参考点\n        # inter_references_out:(6, 1, 900, 3) 内部decoder layer输出的参考点\n        bev_embed, hs, init_reference, inter_references = outputs\n        hs = hs.permute(0, 2, 1, 3)         # (6, 900, 1, 256) -> (6, 1, 900, 256)\n        outputs_classes = []\n        outputs_coords = []\n        for lvl in range(hs.shape[0]):      # 6层都处理\n            if lvl == 0: \n                reference = init_reference            # (1, 900, 3)\n            else:\n                reference = inter_references[lvl - 1] # (1, 900, 3) sigmoid值\n            reference = inverse_sigmoid(reference)    # 逆sigmoid\n            outputs_class = self.cls_branches[lvl](hs[lvl]) # (1, 900, 256) -> (1, 900, 10)\n            tmp = self.reg_branches[lvl](hs[lvl])           # (1, 900, 256) -> (1, 900, 10)\n            assert reference.shape[-1] == 3\n            tmp[..., 0:2] += reference[..., 0:2]            # 参考点 + 偏移量\n            tmp[..., 0:2] = tmp[..., 0:2].sigmoid()\n            tmp[..., 4:5] += reference[..., 2:3]\n            tmp[..., 4:5] = tmp[..., 4:5].sigmoid()\n            tmp[..., 0:1] = (tmp[..., 0:1] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0])  # 恢复到lidar坐标\n            tmp[..., 1:2] = (tmp[..., 1:2] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1])\n            tmp[..., 4:5] = (tmp[..., 4:5] * (self.pc_range[5] - self.pc_range[2]) + self.pc_range[2])\n            # TODO: check if using sigmoid\n            outputs_coord = tmp\n            outputs_classes.append(outputs_class) # 将分类预测加入outputs list (1, 900, 10)\n            outputs_coords.append(outputs_coord)  # 将回归预测加入outputs list (1, 900, 10)\n        outputs_classes = torch.stack(outputs_classes)\n        outputs_coords = torch.stack(outputs_coords)\n        outs = {\n            ’bev_embed’: bev_embed,            # (2500, 1, 256)\n            ’all_cls_scores’: outputs_classes, # (6, 1, 900, 10)\n            ’all_bbox_preds’: outputs_coords,  # (6, 1, 900, 3)\n            ’enc_cls_scores’: None,\n            ’enc_bbox_preds’: None,\n        }\n        return outs\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/utils/positional_encoding.py</p><span class=\'hidden-code\' data-code=\'@POSITIONAL_ENCODING.register_module()\nclass LearnedPositionalEncoding(BaseModule):\n    def forward(self, mask):\n        h, w = mask.shape[-2:]\n        x = torch.arange(w, device=mask.device)\n        y = torch.arange(h, device=mask.device)\n        x_embed = self.col_embed(x)             # self.row_embed = nn.Embedding(row_num_embed, num_feats)\n        y_embed = self.row_embed(y)             # self.col_embed = nn.Embedding(col_num_embed, num_feats)\n        pos = torch.cat((x_embed.unsqueeze(0).repeat(h, 1, 1), y_embed.unsqueeze(1).repeat(1, w, 1)),\n            dim=-1).permute(2, 0,1).unsqueeze(0).repeat(mask.shape[0], 1, 1, 1)\n        return pos\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/transformer.py</p><span class=\'hidden-code\' data-code=\'@TRANSFORMER.register_module()\nclass PerceptionTransformer(BaseModule):\n    @auto_fp16(apply_to=(’mlvl_feats’, ’bev_queries’, ’prev_bev’, ’bev_pos’))\n    def get_bev_features(self, mlvl_feats, bev_queries, bev_h, bev_w, grid_length=[0.512, 0.512], bev_pos=None, prev_bev=None, **kwargs):\n        # obtain bev features\n        bs = mlvl_feats[0].size(0)                              # batch 1\n        bev_queries = bev_queries.unsqueeze(1).repeat(1, bs, 1) # (2500, 256) -> (2500, 1, 256) -> (2500, 1*bs, 256), 按batch复制等份的bev query\n        bev_pos = bev_pos.flatten(2).permute(2, 0, 1)           # (1, 256, 50, 50) -> (1, 256, 2500) -> (2500, 1, 256) 和queries一样维度\n        # obtain rotation angle and shift with ego motion\n        # kwargs[’img_metas’]: dict_keys([’filename’, ’ori_shape’, ’img_shape’, ’lidar2img’, ’pad_shape’ , ’scale_factor’, ’box_mode_3d’, \n        #       ’box_type_3d’, ’img_norm_cfg’, ’sample_idx’, ’prev_idx’, ’next_idx’, ’pts_filename’, ’scene_token’, ’can_bus’,’prev_bev_exists’])\n        # delta_x: x方向平移，delta_y: y方向平移，ego_angle: 自车行进角度\n        delta_x = np.array([each[’can_bus’][0] for each in kwargs[’img_metas’]])   # 车辆在全局坐标下的一个偏移量\n        delta_y = np.array([each[’can_bus’][1] for each in kwargs[’img_metas’]])\n        ego_angle = np.array([each[’can_bus’][-2] / np.pi * 180 for each in kwargs[’img_metas’]])\n        grid_length_y = grid_length[0] # 2.048 = 102.4 / 50  点云范围/bev空间尺寸，每个格子尺寸\n        grid_length_x = grid_length[1] # 2.048\n        translation_length = np.sqrt(delta_x ** 2 + delta_y ** 2)      # 平移距离，勾股定理\n        translation_angle = np.arctan2(delta_y, delta_x) / np.pi * 180 # 平移偏差角度，转过的偏航角\n        bev_angle = ego_angle - translation_angle                      # BEV下的偏航角  \n        shift_y = translation_length * np.cos(bev_angle / 180 * np.pi) / grid_length_y / bev_h\n        shift_x = translation_length * np.sin(bev_angle / 180 * np.pi) / grid_length_x / bev_w # BEV特征图上的偏移量\n        shift_y = shift_y * self.use_shift\n        shift_x = shift_x * self.use_shift                                # self.use_shift True\n        shift = bev_queries.new_tensor([shift_x, shift_y]).permute(1, 0)  # xy, bs -> bs, xy # (1, 2)\n        if prev_bev is not None:                   # 如果存在历史bev，将其旋转到当前bev朝向\n            if prev_bev.shape[1] == bev_h * bev_w: # (1, 2500, 256) -> (2500, 1, 256)\n                prev_bev = prev_bev.permute(1, 0, 2)\n            if self.rotate_prev_bev:\n                for i in range(bs):                 # 按batch遍历\n                    # num_prev_bev = prev_bev.size(1)\n                    rotation_angle = kwargs[’img_metas’][i][’can_bus’][-1]                        # 角度偏移量\n                    tmp_prev_bev = prev_bev[:, i].reshape(bev_h, bev_w, -1).permute(2, 0, 1)      # (2500, 256) -> (50, 50, 256) -> (256, 50, 50)\n                    tmp_prev_bev = rotate(tmp_prev_bev, rotation_angle,center=self.rotate_center) # bev角度对准, (256, 50, 50)\n                    tmp_prev_bev = tmp_prev_bev.permute(1, 2, 0).reshape(bev_h * bev_w, 1, -1)    # (256, 50, 50) -> (50, 50, 256) -> (2500, 1, 256)\n                    prev_bev[:, i] = tmp_prev_bev[:, 0] # 放到prev_bev对应batch index上\n            # prev_bev: (2500, 1, 256)\n        # add can bus signals\n        can_bus = bev_queries.new_tensor([each[’can_bus’] for each in kwargs[’img_metas’]])  # [:, :] 18维can_bus量，3维translation, 4维rotation，4维roration-rate，3维加速度，4维速度\n        can_bus = self.can_bus_mlp(can_bus)[None, :, :]                                      # (1, 1, 256), canbus encoding  self.can_bus_mlp = nn.Sequential(......)\n        bev_queries = bev_queries + can_bus * self.use_can_bus                               # (2500, 1, 256) bev query增加canbus信息\n        feat_flatten = []\n        spatial_shapes = []\n        for lvl, feat in enumerate(mlvl_feats):        # 遍历多尺度特征\n            bs, num_cam, c, h, w = feat.shape          # 1, 6, 256, 15, 25\n            spatial_shape = (h, w)                     # (15, 25)\n            feat = feat.flatten(3).permute(1, 0, 3, 2) # (1, 6, 256, 375) -> (6, 1, 375, 256)  375=15x25\n            if self.use_cams_embeds:                   # self.cams_embeds, (6, 256) 相机特征\n                feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)       # (6, 1, 375, 256) + (6, 1, 1, 256) -> (6, 1, 375, 256)\n            # self.level_embeds: (4, 256)\n            feat = feat + self.level_embeds[None,None, lvl:lvl + 1, :].to(feat.dtype) # (6, 1, 375, 256) + (1, 1, 1, 256) -> (6, 1, 375, 256)\n            spatial_shapes.append(spatial_shape)\n            feat_flatten.append(feat)\n        feat_flatten = torch.cat(feat_flatten, 2)                                                 # 特征图拼接, (6, 1, 375, 256)\n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=bev_pos.device) # [[15, 25]]\n        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1])) # [0]\n        feat_flatten = feat_flatten.permute(0, 2, 1, 3)    # (num_cam, H*W, bs, embed_dims) (6, 1, 375, 256) -> (6, 375, 1, 256)\n        bev_embed = self.`encoder`(          # BEVFormerEncoder\n            bev_queries,  # (2500, 1, 256)\n            feat_flatten, # (6, 375, 1, 256)      key图像特征\n            feat_flatten, # (6, 375, 1, 256)      value图像特征\n            bev_h=bev_h,  # 50\n            bev_w=bev_w,  # 50\n            bev_pos=bev_pos,               # (2500, 1, 256)\n            spatial_shapes=spatial_shapes, # [[15, 25]]\n            level_start_index=level_start_index, # [0]\n            prev_bev=prev_bev,                   # None or (2500, 1, 256)\n            shift=shift,                         # (1, 2)\n            **kwargs\n        ) # (1, 2500, 256)\n        return bev_embed\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/encoder.py</p><span class=\'hidden-code\' data-code=\'@TRANSFORMER_LAYER_SEQUENCE.register_module()\nclass BEVFormerEncoder(TransformerLayerSequence):\n    @auto_fp16()\n    def forward(self,bev_query,key,value,*args,bev_h=None,bev_w=None,bev_pos=None,spatial_shapes=None,level_start_index=None,valid_ratios=None,prev_bev=None,hift=0.,**kwargs):\n        output = bev_query                       # (2500, 1, 256)\n        intermediate = []\n        # 3d参考点 ref_3d: (1, 4, 2500, 3)空间位置的attation      self.pc_range[5]-self.pc_range[2] 8               self.num_points_in_pillar 4\n        ref_3d = self.`get_reference_points`(bev_h, bev_w, self.pc_range[5]-self.pc_range[2], self.num_points_in_pillar, dim=’3d’, bs=bev_query.size(1), device=bev_query.device, dtype=bev_query.dtype) # (1, 4, 2500, 3)\n        ref_2d = self.`get_reference_points`(bev_h, bev_w, dim=’2d’, bs=bev_query.size(1), device=bev_query.device, dtype=bev_query.dtype) # (1, 2500, 1, 2) \n        reference_points_cam, bev_mask = self.point_sampling(ref_3d, self.pc_range, kwargs[’img_metas’]) # ref_3d -> ref_3d_cam, (6, 1, 2500, 4, 2), (6, 1, 2500, 4)\n        # bug: this code should be ’shift_ref_2d = ref_2d.clone()’, we keep this bug for reproducing our results in paper.\n        shift_ref_2d = ref_2d                    # .clone() # 获取BEV平面上的2D参考点\n        shift_ref_2d += shift[:, None, None, :]  # 在原始参考点的基础上+偏移量 (1, 2500, 1, 2) \n        # (num_query, bs, embed_dims) -> (bs, num_query, embed_dims)\n        bev_query = bev_query.permute(1, 0, 2)        # (2500, 1, 256) -> (1, 2500, 256)\n        bev_pos = bev_pos.permute(1, 0, 2)            # (2500, 1, 256) -> (1, 2500, 256)\n        bs, len_bev, num_bev_level, _ = ref_2d.shape  # 1, 2500, 1, 2\n        if prev_bev is not None:\n            prev_bev = prev_bev.permute(1, 0, 2)      # (2500, 1, 256) -> (1, 2500, 256)\n            prev_bev = torch.stack([prev_bev, bev_query], 1).reshape(bs*2, len_bev, -1)              # (2, 2500, 256)\n            hybird_ref_2d = torch.stack([shift_ref_2d, ref_2d], 1).reshape(bs*2, len_bev, num_bev_level, 2) # (2, 2500, 1, 2)\n        else:\n            hybird_ref_2d = torch.stack([ref_2d, ref_2d], 1).reshape(bs*2, len_bev, num_bev_level, 2)       # prev_bev=None\n        for lid, layer in enumerate(self.layers):\n            output = `layer`(\n                bev_query,  # (1, 2500, 256)\n                key,        # (6, 375, 1, 256)  图像特征\n                value,      # (6, 375, 1, 256)  图像特征\n                *args,\n                bev_pos=bev_pos,      # (1, 2500, 256)  位置编码\n                ref_2d=hybird_ref_2d, # (2, 2500, 1, 2) 图片的点  \n                ref_3d=ref_3d,        # (1, 4, 2500, 3) 3D点\n                bev_h=bev_h, # 50\n                bev_w=bev_w, # 50\n                spatial_shapes=spatial_shapes,       # [[15, 25]]\n                level_start_index=level_start_index, # [0]\n                reference_points_cam=reference_points_cam,  # (6, 1, 2500, 4, 2)  3D投到图片2D的点\n                bev_mask=bev_mask,                   # (6, 1, 2500, 4)\n                prev_bev=prev_bev,                   # None or (2, 2500, 256)\n                **kwargs)\n            bev_query = output        # (1, 2500, 256)\n            if self.return_intermediate:\n                intermediate.append(output)\n        if self.return_intermediate:\n            return torch.stack(intermediate)\n        return output\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/encoder.py</p><span class=\'hidden-code\' data-code=\'@TRANSFORMER_LAYER_SEQUENCE.register_module()\nclass BEVFormerEncoder(TransformerLayerSequence):\n    @staticmethod\n    def get_reference_points(H, W, Z=8, num_points_in_pillar=4, dim=’3d’, bs=1, device=’cuda’, dtype=torch.float):\n        # H, W: spatial shape of bev.       Z: hight of pillar[-3->5].         D: sample D points uniformly from each pillar.\n        # device (obj:`device`): The device where reference_points should be.\n        # 返回---->Tensor: reference points used in decoder, has shape (bs, num_keys, num_levels, 2).\n        if dim == ’3d’:                # reference points in 3D space, used in spatial cross-attention (SCA)\n            # 0.5~7.5中间均匀采样4个点 (4,)-->(4, 1, 1)-->(4, 50, 50) 归一化\n            zs = torch.linspace(0.5, Z - 0.5, num_points_in_pillar, dtype=dtype, device=device).view(-1, 1, 1).expand(num_points_in_pillar, H, W) / Z\n            # 0.5~49.5中间均匀采样50个点 (50,)-->(1, 1, 50)-->(4, 50, 50) 归一化\n            xs = torch.linspace(0.5, W - 0.5, W, dtype=dtype, device=device).view(1, 1, W).expand(num_points_in_pillar, H, W) / W\n            # 0.5~49.5中间均匀采样50个点 (50,)-->(1, 50, 1)-->(4, 50, 50) 归一化\n            ys = torch.linspace(0.5, H - 0.5, H, dtype=dtype, device=device).view(1, H, 1).expand(num_points_in_pillar, H, W) / H\n            ref_3d = torch.stack((xs, ys, zs), -1)                          # (4, 50, 50, 3)\n            ref_3d = ref_3d.permute(0, 3, 1, 2).flatten(2).permute(0, 2, 1) # (4, 50, 50, 3) -> (4, 3, 50, 50) -> (4, 3, 2500) -> (4, 2500, 3)\n            ref_3d = ref_3d[None].repeat(bs, 1, 1, 1)                       # (4, 2500, 3) -> (1, 4, 2500, 3) -> (1, 4, 2500, 3)\n            return ref_3d                                                   # (1, 4, 2500, 3)  每个柱子采样4个点，由50x50个柱子，每个点坐标为3维度\n        elif dim == ’2d’:              # reference points on 2D bev plane, used in temporal self-attention (TSA).\n            ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H - 0.5, H, dtype=dtype, device=device),\n                torch.linspace(0.5, W - 0.5, W, dtype=dtype, device=device))           # (50, 50)\n            ref_y = ref_y.reshape(-1)[None] / H                             # (50, 50) -> (2500, ) ->  (1, 2500)\n            ref_x = ref_x.reshape(-1)[None] / W                             # (50, 50) -> (2500, ) ->  (1, 2500)\n            ref_2d = torch.stack((ref_x, ref_y), -1)                        # (1, 2500, 2)\n            ref_2d = ref_2d.repeat(bs, 1, 1).unsqueeze(2)                   # (1, 2500, 2) -> (1, 2500, 1, 2)\n            return ref_2d                                                   # (1, 2500, 1, 2)\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/encoder.py</p><span class=\'hidden-code\' data-code=\'@TRANSFORMER_LAYER_SEQUENCE.register_module()\nclass BEVFormerEncoder(TransformerLayerSequence):\n    @force_fp32(apply_to=(’reference_points’, ’img_metas’))                # This function must use fp32!!!\n    def point_sampling(self, reference_points, pc_range,  img_metas):      # reference_points:(1, 4, 2500, 3), lidar2img: lidar -> ego -> camera -> 内参 -> img\n        lidar2img = []\n        for img_meta in img_metas:\n            lidar2img.append(img_meta[’lidar2img’])\n        lidar2img = np.asarray(lidar2img) \n        lidar2img = reference_points.new_tensor(lidar2img)                  # (B, N, 4, 4), (1, 6, 4, 4)\n        reference_points = reference_points.clone()\n        # 将归一化参考点恢复到lidar系下的实际位置\n        reference_points[..., 0:1] = reference_points[..., 0:1] * (pc_range[3] - pc_range[0]) + pc_range[0]\n        reference_points[..., 1:2] = reference_points[..., 1:2] * (pc_range[4] - pc_range[1]) + pc_range[1]\n        reference_points[..., 2:3] = reference_points[..., 2:3] * (pc_range[5] - pc_range[2]) + pc_range[2]\n        # ref_3d: (1, 4, 2500, 3)\n        reference_points = torch.cat((reference_points, torch.ones_like(reference_points[..., :1])), -1) # cat: (1, 4, 2500, 1), ref_points: (1, 4, 2500, 4)\n        reference_points = reference_points.permute(1, 0, 2, 3)             # (1, 4, 2500, 4) -> (4, 1, 2500, 4)\n        D, B, num_query = reference_points.size()[:3]                       # 4, 1, 2500\n        num_cam = lidar2img.size(1)                                         # 6\n        # (4, 1, 2500, 4) -> (4, 1, 1, 2500, 4) -> (4, 1, 6, 2500, 4) -> (4, 1, 6, 2500, 4, 1)\n        reference_points = reference_points.view(D, B, 1, num_query, 4).repeat(1, 1, num_cam, 1, 1).unsqueeze(-1) \n        lidar2img = lidar2img.view(1, B, num_cam, 1, 4, 4).repeat(D, 1, 1, num_query, 1, 1) # (1, 6, 4, 4) -> (1, 1, 6, 1, 4, 4) -> (4, 1, 6, 2500, 4, 4)\n        reference_points_cam = torch.matmul(lidar2img.to(torch.float32),reference_points.to(torch.float32)).squeeze(-1) # 矩阵乘法, (4, 1, 6, 2500, 4)\n        eps = 1e-5\n        bev_mask = (reference_points_cam[..., 2:3] > eps) # (4, 1, 6, 2500, 1)\n        reference_points_cam = reference_points_cam[..., 0:2] / torch.maximum(\n            reference_points_cam[..., 2:3], torch.ones_like(reference_points_cam[..., 2:3]) * eps) # (4, 1, 6, 2500, 2)\n        reference_points_cam[..., 0] /= img_metas[0][’img_shape’][0][1]\n        reference_points_cam[..., 1] /= img_metas[0][’img_shape’][0][0]\n        # 用于评判某一 三维坐标点 是否落在了 二维坐标平面上 比例区间(0,1), (4, 1, 6, 2500, 1)\n        bev_mask = (bev_mask & (reference_points_cam[..., 1:2] > 0.0)      \n                    & (reference_points_cam[..., 1:2] < 1.0)\n                    & (reference_points_cam[..., 0:1] < 1.0)\n                    & (reference_points_cam[..., 0:1] > 0.0))\n        if digit_version(TORCH_VERSION) >= digit_version(’1.8’):\n            bev_mask = torch.nan_to_num(bev_mask)                          # nan->number, default 0\n        else:\n            bev_mask = bev_mask.new_tensor(\n                np.nan_to_num(bev_mask.cpu().numpy()))\n        reference_points_cam = reference_points_cam.permute(2, 1, 3, 0, 4) # (4, 1, 6, 2500, 2) -> (6, 1, 2500, 4, 2)\n        bev_mask = bev_mask.permute(2, 1, 3, 0, 4).squeeze(-1)             # (4, 1, 6, 2500, 1) -> (6, 1, 2500, 4, 1) -> (6, 1, 2500, 4)\n        return reference_points_cam, bev_mask\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/encoder.py</p><span class=\'hidden-code\' data-code=\'@TRANSFORMER_LAYER.register_module()\nclass BEVFormerLayer(MyCustomBaseTransformerLayer):\n    def forward(self,query,key=None,value=None,bev_pos=None,query_pos=None,key_pos=None,attn_masks=None,query_key_padding_mask=None,key_padding_mask=None,\n            ref_2d=None,ref_3d=None,bev_h=None,bev_w=None,reference_points_cam=None,mask=None,spatial_shapes=None,level_start_index=None,prev_bev=None,**kwargs):\n        norm_index = 0\n        attn_index = 0\n        ffn_index = 0\n        identity = query                            # (1, 2500, 256)\n        if attn_masks is None:                      # attn_masks: [None, None]            \n            attn_masks = [None for _ in range(self.num_attn)]\n        elif isinstance(attn_masks, torch.Tensor):\n            attn_masks = [copy.deepcopy(attn_masks) for _ in range(self.num_attn)]\n            warnings.warn(f’Use same attn_mask in all attentions in {self.__class__.__name__} ’)\n        else:\n            assert len(attn_masks) == self.num_attn, f’The length of attn_masks {len(attn_masks)} must be equal to the number of attention in ’ \\\n                f’operation_order {self.num_attn}’\n        # operation_order: (’self_attn’, ’norm’, ’cross_attn’, ’norm’, ’ffn’, ’norm’)\n        for layer in self.operation_order:\n            if layer == ’self_attn’:                     # temporal self attention\n                query = self.`attentions[attn_index]`(   # class TemporalSelfAttention(BaseModule):\n                    query,                               # (1, 2500, 256)  \n                    prev_bev,                            # None或(2, 2500, 256)\n                    prev_bev,                            # None或(2, 2500, 256)\n                    identity if self.pre_norm else None, # None\n                    query_pos=bev_pos,                   # (1, 2500, 256)\n                    key_pos=bev_pos,                     # (1, 2500, 256)\n                    attn_mask=attn_masks[attn_index],    # None\n                    key_padding_mask=query_key_padding_mask, # None\n                    reference_points=ref_2d,                 # (2, 2500, 1, 2) bev空间网格化坐标点的位置\n                    spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device), # [[50, 50]]\n                    level_start_index=torch.tensor([0], device=query.device),           # [0]\n                    **kwargs)\n                attn_index += 1\n                identity = query                         # (1, 2500, 256)\n            elif layer == ’norm’:\n                query = self.norms[norm_index](query)\n                norm_index += 1\n            elif layer == ’cross_attn’:                  # spaital cross attention\n                query = `self.attentions[attn_index]`(\n                    query,                               # (1, 2500, 256)\n                    key,                                 # (6, 375, 1, 256)\n                    value,                               # (6, 375, 1, 256)\n                    identity if self.pre_norm else None, # None\n                    query_pos=query_pos,                 # (1, 2500, 256)\n                    key_pos=key_pos,                     # (1, 2500, 256)\n                    reference_points=ref_3d,             # (1, 4, 2500, 3)\n                    reference_points_cam=reference_points_cam, # (6, 1, 2500, 4, 2)\n                    mask=mask,                           # (6, 1, 2500, 4)\n                    attn_mask=attn_masks[attn_index],    # None\n                    key_padding_mask=key_padding_mask,   # None\n                    spatial_shapes=spatial_shapes,       # [[15, 25]]  和上面bev空间50x50不一样\n                    level_start_index=level_start_index, # [0]\n                    **kwargs)\n                attn_index += 1\n                identity = query\n            elif layer == ’ffn’:\n                query = self.ffns[ffn_index](query, identity if self.pre_norm else None)\n                ffn_index += 1\n        return query\n\'> </span>', 'children': [{'type': 'heading', 'depth': 7, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py</p><span class=\'hidden-code\' data-code=\'@ATTENTION.register_module()\nclass TemporalSelfAttention(BaseModule):\n    def forward(self, query, key=None, value=None, identity=None, query_pos=None, key_padding_mask=None, reference_points=None, spatial_shapes=None,\n                level_start_index=None, flag=’decoder’, **kwargs):\n        if value is None:                         # value prev_bev, value=None, prev_bev=None\n            assert self.batch_first\n            bs, len_bev, c = query.shape\n            value = torch.stack([query, query], 1).reshape(bs*2, len_bev, c)       # (2, 2500, 256)\n            # value = torch.cat([query, query], 0)\n        if identity is None:\n            identity = query          # (1, 2500, 256)\n        if query_pos is not None:\n            query = query + query_pos # (1, 2500, 256) = (1, 2500, 256) + (1, 2500, 256)\n        if not self.batch_first: \n            query = query.permute(1, 0, 2)         # (1, 2500, 256)  # change to (bs, num_query ,embed_dims) \n            value = value.permute(1, 0, 2)         # (2, 2500, 256)\n        bs,  num_query, embed_dims = query.shape   # 1, 2500, 256\n        _, num_value, _ = value.shape              # 2500\n        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value    # 50 * 50 = 2500\n        assert self.num_bev_queue == 2             # prev_bev & bev\n        query = torch.cat([value[:bs], query], -1) # pre_query & query, (1, 2500, 512), cat_dim= -1\n        value = self.value_proj(value)             # (2, 2500, 256)   nn.Linear(embed_dims, embed_dims)\n        if key_padding_mask is not None:\n            value = value.masked_fill(key_padding_mask[..., None], 0.0)\n        value = value.reshape(bs*self.num_bev_queue,num_value, self.num_heads, -1) # (2, 2500, 256) -> (2, 2500, 8, 32)\n        # 前面都是初始化操作，下面是一些可形变卷积参量\n        sampling_offsets = self.sampling_offsets(query)                            # (1, 2500, 512) -> (1, 2500, 128) nn.Linear\n        sampling_offsets = sampling_offsets.view(                                  # self.num_points采样点个数，每个点偏移到哪个位置上，总共四个\n            bs, num_query, self.num_heads,  self.num_bev_queue, self.num_levels, self.num_points, 2) # 偏移量 (1, 2500, 128) -> (1, 2500, 8, 2, 1, 4, 2)\n        attention_weights = self.attention_weights(query).view(                    # 每个位置只用一个权重\n            bs, num_query,  self.num_heads, self.num_bev_queue, self.num_levels * self.num_points)   # 权重 (1, 2500, 512) -> (1, 2500, 8, 2, 4)\n        attention_weights = attention_weights.softmax(-1)\n        attention_weights = attention_weights.view(bs, num_query,self.num_heads,self.num_bev_queue,self.num_levels,\n                                                   self.num_points) # (1, 2500, 8, 2, 4) -> (1, 2500, 8, 2, 1, 4)\n        # (1, 2500, 8, 2, 1, 4) -> (1, 2, 2500, 8, 1, 4) -> (2, 2500, 8, 1, 4)\n        attention_weights = attention_weights.permute(0, 3, 1, 2, 4, 5)\\\n            .reshape(bs*self.num_bev_queue, num_query, self.num_heads, self.num_levels, self.num_points).contiguous()\n        # (1, 2500, 8, 2, 1, 4, 2) -> (1, 2, 2500, 8, 1, 4, 2) -> (2, 2500, 8, 1, 4, 2)\n        sampling_offsets = sampling_offsets.permute(0, 3, 1, 2, 4, 5, 6)\\\n            .reshape(bs*self.num_bev_queue, num_query, self.num_heads, self.num_levels, self.num_points, 2)\n        # \n        if reference_points.shape[-1] == 2:                                                       # ref_2d, (2, 2500, 1, 2)空间网格的位置\n            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1) # [[50, 50]]\n            # (2, 2500, 1, 1, 1, 2) + (2, 2500, 8, 1, 4, 2) / (1, 1, 1, 1, 1, 2)\n            # ref_2d是归一化坐标，所以这里需要对offsets同样做归一化 sampling_locations, (2, 2500, 8, 1, 4, 2)  每个网格需要偏移的四个位置\n            sampling_locations = reference_points[:, :, None, :, None, :]+sampling_offsets/offset_normalizer[None, None, None, :, None, :] \n        elif reference_points.shape[-1] == 4:\n            sampling_locations = reference_points[:, :, None, :, None, :2]+sampling_offsets/self.num_points*reference_points[:, :, None, :, None, 2:]*0.5\n        else:\n            raise ValueError(f’Last dim of reference_points must be 2 or 4, but get {reference_points.shape[-1]} instead.’)\n        if torch.cuda.is_available() and value.is_cuda:                                             # cuda可用\n            # using fp16 deformable attention is unstable because it performs many sum operations【可形变注意力】\n            if value.dtype == torch.float16:\n                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32\n            else:\n                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32\n            # value, (2, 2500, 8, 32)          图像特征\n            # spatial_shapes, [[50, 50]]       空间为hi\n            # level_start_index, [0]\n            # sampling_locations, (2, 2500, 8, 1, 4, 2)  采样位置\n            # attention_weights, (2, 2500, 8, 1, 4)\n            output = MultiScaleDeformableAttnFunction.apply(value, spatial_shapes, level_start_index, sampling_locations,\n                attention_weights, self.im2col_step) # (2, 2500, 256) 特征维度与value维度是一致的\n        else:\n            output = multi_scale_deformable_attn_pytorch(value, spatial_shapes, sampling_locations, attention_weights)\n        # output shape (bs*num_bev_queue, num_query, embed_dims)(bs*num_bev_queue, num_query, embed_dims)-> (num_query, embed_dims, bs*num_bev_queue)\n        output = output.permute(1, 2, 0)                                   # (2, 2500, 256) -> (2500, 256, 2)\n        # fuse history value and current value (num_query, embed_dims, bs*num_bev_queue)-> (num_query, embed_dims, bs, num_bev_queue)\n        output = output.view(num_query, embed_dims, bs, self.num_bev_queue) # (2500, 256, 2) -> (2500, 256, 1, 2)\n        output = output.mean(-1)                                            # prev_bev和cur_bev取均值, (2500, 256, 1)\n        # (num_query, embed_dims, bs)-> (bs, num_query, embed_dims)\n        output = output.permute(2, 0, 1)                                    # (2500, 256, 1) -> (1, 2500, 256)\n        output = self.output_proj(output)                                   # (1, 2500, 256) -> 线性层 -> (1, 2500, 256) nn.Linear(embed_dims, embed_dims)\n        if not self.batch_first:\n            output = output.permute(1, 0, 2)\n        return self.dropout(output) + identity # (1, 2500, 256)       随机丢弃添上初始query\n\'> </span>'}, {'type': 'heading', 'depth': 7, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py</p><span class=\'hidden-code\' data-code=\'@ATTENTION.register_module()\nclass SpatialCrossAttention(BaseModule):\n    @force_fp32(apply_to=(’query’, ’key’, ’value’, ’query_pos’, ’reference_points_cam’))\n    def forward(self,query,key,value,residual=None,query_pos=None,key_padding_mask=None,reference_points=None,spatial_shapes=None,\n                reference_points_cam=None,bev_mask=None,level_start_index=None,flag=’encoder’,**kwargs):\n        if key is None:\n            key = query\n        if value is None:\n            value = key\n        if residual is None:\n            inp_residual = query\n            slots = torch.zeros_like(query)\n        if query_pos is not None:\n            query = query + query_pos    # (1, 2500, 256) + (1, 2500, 256) -> (1, 2500, 256)\n        bs, num_query, _ = query.size()  # 1, 2500, 256\n        D = reference_points_cam.size(3) # (6, 1, 2500, 4, 2), D=4 4是每个柱子上采样点的数量\n        indexes = []\n        for i, mask_per_img in enumerate(bev_mask): # bev_mask: (6, 1, 2500, 4)    mask_per_img: (1, 2500, 4) 单相机mask\n            index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)    # (1, 2500, 4)->(2500, 4)->(2500,)->(393,1)->(393,)采样数量\n            indexes.append(index_query_per_img)\n        max_len = max([len(each) for each in indexes])                             # index最大长度\n        # each camera only interacts with its corresponding BEV queries. This step can  greatly save GPU memory.\n        queries_rebatch = query.new_zeros([bs, self.num_cams, max_len, self.embed_dims])              # (1, 6, max_len, 256)\n        reference_points_rebatch = reference_points_cam.new_zeros([bs, self.num_cams, max_len, D, 2]) # (1, 6, max_len, 4, 2)\n        for j in range(bs):                                                      # per_batch\n            for i, reference_points_per_img in enumerate(reference_points_cam):  # per_cam  某张图\n                index_query_per_img = indexes[i]                                 # (393, )\n                # queries_rebatch[j, i, :len(index_query_per_img)]: j-th batch, i-th cam, max_len\n                # query[j, index_query_per_img]: query, (1, 2500, 256), j-th batch, query_index, 从对应index采样query，放到queries_rebatch里面\n                queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]\n                # 从对应index采样reference_points，放到reference_points_rebatch里面\n                reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]\n        num_cams, l, bs, embed_dims = key.shape                                           # 6, 15*25, 1, 256 \n        key = key.permute(2, 0, 1, 3).reshape(bs * self.num_cams, l, self.embed_dims)     # (6, 375, 1, 256) -> (1, 6, 375, 256) -> (6, 375, 256)图像特征\n        value = value.permute(2, 0, 1, 3).reshape(bs * self.num_cams, l, self.embed_dims) # (6, 375, 1, 256) -> (1, 6, 375, 256) -> (6, 375, 256)\n        # query: (1, 6, max_len, 256) -> (6, max_len, 256)\n        # key: (6, 375, 256)\n        # value: (6, 375, 256)\n        # ref_p: (1, 6, max_len, 4, 2) -> (6, max_len, 4, 2)投影到图像点的基础参考位置位置，会算出一系列的sampling offsets\n        # spatial_shapes: [[15, 25]]\n        # level_start_index: [0]\n        # queries其实算的是我们reference points当前点邻近相邻的偏移的4个位置的点的集合特征\n        queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,\n                                            reference_points=reference_points_rebatch.view(bs*self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,\n                                            level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims) # (1, 6, max_len, 256)\n        for j in range(bs): # per_batch\n            for i, index_query_per_img in enumerate(indexes): # per_img\n                slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]    # j-th batch, 对应index += queries特征\n        count = bev_mask.sum(-1) > 0                          # (6, 1, 2500, 4) -> (6, 1, 2500)           高度求和\n        count = count.permute(1, 2, 0).sum(-1)                # (6, 1, 2500) -> (1, 2500, 6) -> (1, 2500) 相机维度求和 2500上是否有值的意思\n        count = torch.clamp(count, min=1.0)                   # (1, 2500)\n        slots = slots / count[..., None]                      # 取均值，(1, 2500, 256) / (1, 2500, 1) slot: (1, 2500, 256)\n        slots = self.output_proj(slots)                       # 线性层, (1, 2500, 256)\n        return self.dropout(slots) + inp_residual             # (1, 2500, 256)\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/transformer.py</p><span class=\'hidden-code\' data-code=\'@TRANSFORMER.register_module()\nclass PerceptionTransformer(BaseModule):\n    @auto_fp16(apply_to=(’mlvl_feats’, ’bev_queries’, ’object_query_embed’, ’prev_bev’, ’bev_pos’))\n    def forward(self,\n                mlvl_feats,\n                bev_queries,\n                object_query_embed,\n                bev_h,\n                bev_w,\n                grid_length=[0.512, 0.512],\n                bev_pos=None,\n                reg_branches=None,\n                cls_branches=None,\n                prev_bev=None,\n                **kwargs):\n        bev_embed = self.get_bev_features(        # 有了bev特征之后，怎么得到检测结果\n            mlvl_feats, # (1, 6, 256, 15, 25)\n            bev_queries, # (2500, 256)\n            bev_h, # 50\n            bev_w, # 50\n            grid_length=grid_length, # 2.048\n            bev_pos=bev_pos, # (1, 256, 50, 50)\n            prev_bev=prev_bev, # (1, 2500, 256)\n            **kwargs)  # bev_embed shape: bs, bev_h*bev_w, embed_dims, 1, 2500, 256\n        bs = mlvl_feats[0].size(0)                                                 # 1\n        query_pos, query = torch.split(object_query_embed, self.embed_dims, dim=1) # (900, 256), (900, 256)【上面是空间的查询，这里是物体的查询】\n        query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)                      # (900, 256) -> (1, 900, 256) -> (1, 900, 256)\n        query = query.unsqueeze(0).expand(bs, -1, -1)       # (900, 256) -> (1, 900, 256) -> (1, 900, 256)\n        reference_points = self.reference_points(query_pos) # 线性层 (1, 900, 256) -> (1, 900, 3)  初始参考点\n        reference_points = reference_points.sigmoid()       # sigmoid (1, 900, 3)经过0-1处理\n        init_reference_out = reference_points               # (1, 900, 3) 参考点\n        query = query.permute(1, 0, 2)         # (1, 900, 256) -> (900, 1, 256)\n        query_pos = query_pos.permute(1, 0, 2) # (1, 900, 256) -> (900, 1, 256)\n        bev_embed = bev_embed.permute(1, 0, 2) # (1, 2500, 256) -> (2500, 1, 256)\n        inter_states, inter_references = self.`decoder`(\n            query=query,         # object query, (900, 1, 256)\n            key=None,\n            value=bev_embed,     # (2500, 1, 256)\n            query_pos=query_pos, # (900, 1, 256)\n            reference_points=reference_points, # (1, 900, 3)\n            reg_branches=reg_branches, # 回归\n            cls_branches=cls_branches, # 分类\n            spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device), # [[50, 50]]\n            level_start_index=torch.tensor([0], device=query.device),           # [0]\n            **kwargs)\n        inter_references_out = inter_references\n        ’’’\n        bev_embed:(2500, 1, 256) bev的拉直嵌入\n        inter_states:(6, 900, 1, 256) 内部decoder layer输出的object query\n        init_reference_out:(1, 900, 3) 随机初始化的参考点\n        inter_references_out:(6, 1, 900, 3) 内部decoder layer输出的参考点\n        ’’’\n        return bev_embed, inter_states, init_reference_out, inter_references_out\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/modules/decoder.py</p><span class=\'hidden-code\' data-code=\'@TRANSFORMER_LAYER_SEQUENCE.register_module()\nclass DetectionTransformerDecoder(TransformerLayerSequence):\n    def forward(self,query,*args,reference_points=None,reg_branches=None,key_padding_mask=None,**kwargs):\n        output = query                            # object query, (900, 1, 256)\n        intermediate = []\n        intermediate_reference_points = []\n        for lid, layer in enumerate(self.layers): # 6层 decoder     DetrTransformerDecoderLayer\n            reference_points_input = reference_points[..., :2].unsqueeze(2)  # BS NUM_QUERY NUM_LEVEL 2 (1, 900, 3) -> (1, 900, 2) -> (1, 900, 1, 2) \n            output = layer(                \n                output,                            # (900, 1, 256)\n                *args,\n                reference_points=reference_points_input, # (1, 900, 1, 2) \n                key_padding_mask=key_padding_mask, # None\n                **kwargs)                          # (900, 1, 256) DetrTransformerDecoderLayer\n            output = output.permute(1, 0, 2)       # (900, 1, 256) -> (1, 900, 256)\n            if reg_branches is not None:\n                ’’’\n                Sequential(\n                    (0): Linear(in_features=256, out_features=256, bias=True)\n                    (1): ReLU()\n                    (2): Linear(in_features=256, out_features=256, bias=True)\n                    (3): ReLU()\n                    (4): Linear(in_features=256, out_features=10, bias=True)\n                )\n                ’’’\n                tmp = reg_branches[lid](output) # (1, 900, 256) -> (1, 900, 10) 偏移量\n                assert reference_points.shape[-1] == 3\n                new_reference_points = torch.zeros_like(reference_points) # (1, 900, 3)  # 预测的tmp是偏移量，原始点reference_points + 偏移量tmp = 新坐标\n                new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points[..., :2])    # 新坐标，前两维+偏移\n                new_reference_points[..., 2:3] = tmp[..., 4:5] + inverse_sigmoid(reference_points[..., 2:3]) # 新坐标，第三维+偏移\n                new_reference_points = new_reference_points.sigmoid() # sigmoid处理\n                reference_points = new_reference_points.detach()      # 不参与反向传播\n            output = output.permute(1, 0, 2) # (1, 900, 256) -> (900, 1, 256)\n            if self.return_intermediate:     # 如果返回中间结果，把中间结果存起来\n                intermediate.append(output)\n                intermediate_reference_points.append(reference_points)\n        if self.return_intermediate:\n            return torch.stack(intermediate), torch.stack(intermediate_reference_points) # (6, 900, 1, 256), (6, 1, 900, 3)\n        return output, reference_points\n\'> </span>'}]}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">loss计算</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/bevformer/dense_heads/bevformer_head.py</p><span class=\'hidden-code\' data-code=\'@HEADS.register_module()\nclass BEVFormerHead(DETRHead):\n    def loss_single(self,cls_scores,bbox_preds,gt_bboxes_list,gt_labels_list,gt_bboxes_ignore_list=None):\n        num_imgs = cls_scores.size(0) # 1 batch_size\n        cls_scores_list = [cls_scores[i] for i in range(num_imgs)]\n        bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]\n        # 1.按照batch处理,计算分类和回归的targets\n        cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list,gt_bboxes_list, gt_labels_list,gt_bboxes_ignore_list)\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,num_total_pos, num_total_neg) = cls_reg_targets\n         # 在batch维度进行拼接\n        labels = torch.cat(labels_list, 0)\n        label_weights = torch.cat(label_weights_list, 0)\n        bbox_targets = torch.cat(bbox_targets_list, 0)\n        bbox_weights = torch.cat(bbox_weights_list, 0)\n        # classification loss\n        # 2.计算该层的分类和回归损失\n        # 2.1 classification loss\n        cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n        # construct weighted avg_factor to match with the official DETR repo\n        cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n        if self.sync_cls_avg_factor:\n            cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n        cls_avg_factor = max(cls_avg_factor, 1)\n        loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n        # Compute the average number of gt boxes accross all gpus, for normalization purposes\n        num_total_pos = loss_cls.new_tensor([num_total_pos])\n        num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n        # regression L1 loss\n        # 2.2regression L1 loss\n        bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n        normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)\n        isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n        bbox_weights = bbox_weights * self.code_weights\n        # 计算回归损失 只计算匹配上gt的预测框损失\n        loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan,:10], bbox_weights[isnotnan, :10],\n            avg_factor=num_total_pos)\n        if digit_version(TORCH_VERSION) >= digit_version(’1.8’):\n            loss_cls = torch.nan_to_num(loss_cls) # 将nan替换为0\n            loss_bbox = torch.nan_to_num(loss_bbox)\n        return loss_cls, loss_bbox\n\'> </span>'}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
