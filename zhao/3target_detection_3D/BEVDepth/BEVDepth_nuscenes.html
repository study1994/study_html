<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>BEVDepth_nuscenes</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/exps/nuscenes/mv/bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da_ema.py</p><font size="0"><pre class="language-python"><code class="language-python">from bevdepth.exps.base_cli import run_cli\nfrom bevdepth.exps.nuscenes.mv.bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da import BEVDepthLightningModel  <span style=\'color: red\'>noqa</span>\nif __name__ == \'__main__\':\n    <span style=\'color: green;font-weight: bold;\'>run_cli</span>(BEVDepthLightningModel,\'bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da_ema\',use_ema=True,extra_trainer_config_args={\'epochs\': 20})\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/exps/base_cli.py</p><font size="0"><pre class="language-python"><code class="language-python">def run_cli(model_class=BEVDepthLightningModel,exp_name=\'base_exp\',use_ema=False,extra_trainer_config_args={}):\n    model = <span style=\'color: green;font-weight: bold;\'>model_class</span>(**vars(args))        <span style=\'color: red\'># data_code/3target_detection_3D/project/BEVDepth/model.log</span>\n    if use_ema:                                <span style=\'color: red\'># True</span>\n        train_dataloader = model.train_dataloader()\n        ema_callback = EMACallback(len(train_dataloader.dataset) * args.max_epochs)\n        trainer = pl.Trainer.from_argparse_args(args, callbacks=[ema_callback])\n    else:\n        trainer = pl.Trainer.from_argparse_args(args)\n    if args.evaluate:\n        trainer.test(model, ckpt_path=args.ckpt_path)\n    elif args.predict:\n        predict_step_outputs = trainer.predict(model, ckpt_path=args.ckpt_path)\n        all_pred_results = list()\n        all_img_metas = list()\n        for predict_step_output in predict_step_outputs:\n            for i in range(len(predict_step_output)):\n                all_pred_results.append(predict_step_output[i][:3])\n                all_img_metas.append(predict_step_output[i][3])\n        synchronize()\n        len_dataset = len(model.test_dataloader().dataset)\n        all_pred_results = sum(map(list, zip(*all_gather_object(all_pred_results))),[])[:len_dataset]\n        all_img_metas = sum(map(list, zip(*all_gather_object(all_img_metas))),[])[:len_dataset]\n        model.evaluator._format_bbox(all_pred_results, all_img_metas,os.path.dirname(args.ckpt_path))\n    else:\n        trainer.<span style=\'color: green;font-weight: bold;\'>fit</span>(model)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/exps/nuscenes/base_exp.py</p><font size="0"><pre class="language-python"><code class="language-python">class BEVDepthLightningModel(LightningModule):\n    def training_step(self, batch):\n        (sweep_imgs, mats, _, _, gt_boxes, gt_labels, depth_labels) = batch      <span style=\'color: red\'># torch.Size([2, 2, 6, 3, 256, 704]); ;[torch.Size([33, 9]),torch.Size([33, 9])]; [torch.Size([33]),torch.Size([33])]; torch.Size([2, 1, 6, 256, 704])</span>\n        if torch.cuda.is_available():\n            for key, value in mats.items():\n                mats[key] = value.cuda()\n            sweep_imgs = sweep_imgs.cuda()\n            gt_boxes = [gt_box.cuda() for gt_box in gt_boxes]\n            gt_labels = [gt_label.cuda() for gt_label in gt_labels]\n        preds, depth_preds = <span style=\'color: green;font-weight: bold;\'>self</span>(sweep_imgs, mats)                              <span style=\'color: red\'># torch.Size([2, 2, 6, 3, 256, 704]); </span>\n        if isinstance(self.model, torch.nn.parallel.DistributedDataParallel):\n            targets = self.model.module.get_targets(gt_boxes, gt_labels)\n            detection_loss = self.model.module.loss(targets, preds)\n        else:\n            targets = self.model.get_targets(gt_boxes, gt_labels)\n            detection_loss = self.model.loss(targets, preds)\n        if len(depth_labels.shape) == 5:\n            <span style=\'color: red\'># only key-frame will calculate depth loss</span>\n            depth_labels = depth_labels[:, 0, ...]\n        depth_loss = self.get_depth_loss(depth_labels.cuda(), depth_preds)\n        self.log(\'detection_loss\', detection_loss)\n        self.log(\'depth_loss\', depth_loss)\n        return detection_loss + depth_loss\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/exps/nuscenes/base_exp.py</p><font size="0"><pre class="language-python"><code class="language-python">class BEVDepthLightningModel(LightningModule):\n    def forward(self, sweep_imgs, mats):\n        return self.<span style=\'color: green;font-weight: bold;\'>model</span>(sweep_imgs, mats)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/models/base_bev_depth.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseBEVDepth(nn.Module):\n    def forward(self,x,mats_dict,timestamps=None):\n        if self.is_train_depth and self.training:\n            x, depth_pred = self.<span style=\'color: green;font-weight: bold;\'>backbone</span>(x,mats_dict,timestamps,is_return_depth=True)\n            preds = self.<span style=\'color: green;font-weight: bold;\'>head</span>(x)\n            return preds, depth_pred\n        else:\n            x = self.backbone(x, mats_dict, timestamps)\n            preds = self.head(x)\n            return preds\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/layers/backbones/bevstereo_lss_fpn.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseLSSFPN(nn.Module):\n    def forward(self,sweep_imgs,mats_dict,timestamps=None,is_return_depth=False):\n        batch_size, num_sweeps, num_cams, num_channels, img_height, img_width = sweep_imgs.shape           <span style=\'color: red\'># torch.Size([2, 2, 6, 3, 256, 704]);</span>\n        key_frame_res = self.<span style=\'color: green;font-weight: bold;\'>_forward_single_sweep</span>(0,sweep_imgs[:, 0:1, ...],mats_dict,is_return_depth=is_return_depth)\n        if num_sweeps == 1:\n            return key_frame_res\n        key_frame_feature = key_frame_res[ 0] if is_return_depth else key_frame_res\n        ret_feature_list = [key_frame_feature]\n        for sweep_index in range(1, num_sweeps):\n            with torch.no_grad():\n                feature_map = self.<span style=\'color: green;font-weight: bold;\'>_forward_single_sweep</span>(sweep_index,sweep_imgs[:, sweep_index:sweep_index + 1, ...],mats_dict,is_return_depth=False)\n                ret_feature_list.append(feature_map)\n        if is_return_depth:\n            return torch.cat(ret_feature_list, 1), key_frame_res[1]\n        else:\n            return torch.cat(ret_feature_list, 1)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 7, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/layers/backbones/base_lss_fpn.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseLSSFPN(nn.Module):\n    def _forward_single_sweep(self,sweep_index,sweep_imgs,mats_dict,is_return_depth=False):\n        batch_size, num_sweeps, num_cams, num_channels, img_height, img_width = sweep_imgs.shape\n        img_feats = self.<span style=\'color: green;font-weight: bold;\'>get_cam_feats</span>(sweep_imgs)      <span style=\'color: red\'># torch.Size([2, 1, 6, 3, 256, 704]) -> torch.Size([2, 1, 6, 512, 16, 44])</span>\n        source_features = img_feats[:, 0, ...]            <span style=\'color: red\'># 第0维是原始图片特征</span>\n        depth_feature = self.<span style=\'color: green;font-weight: bold;\'>_forward_depth_net</span>(source_features.reshape(batch_size * num_cams,source_features.shape[2],source_features.shape[3],source_features.shape[4]),mats_dict)   <span style=\'color: red\'># torch.Size([12, 192, 16, 44])</span>\n        depth = depth_feature[:, :self.depth_channels].softmax(dim=1, dtype=depth_feature.dtype)  <span style=\'color: red\'># 112 -> torch.Size([12, 112, 16, 44])</span>\n        geom_xyz = self.<span style=\'color: green;font-weight: bold;\'>get_geometry</span>(                   <span style=\'color: red\'># torch.Size([2, 6, 112, 16, 44, 3])</span>\n            mats_dict[\'sensor2ego_mats\'][:, sweep_index, ...],\n            mats_dict[\'intrin_mats\'][:, sweep_index, ...],\n            mats_dict[\'ida_mats\'][:, sweep_index, ...],\n            mats_dict.get(\'bda_mat\', None),\n        )\n        geom_xyz = ((geom_xyz - (self.voxel_coord - self.voxel_size / 2.0)) / self.voxel_size).int()  <span style=\'color: red\'># voxel_coord=[-50.8,-50.8,-1]+voxel_size=[0.8,0.8,8]->torch.Size([2, 6, 112, 16, 44, 3])</span>\n        if self.training or self.use_da:                                                              <span style=\'color: red\'># elf.use_da=True</span>\n            img_feat_with_depth = depth.unsqueeze(1) * depth_feature[:, self.depth_channels:(self.depth_channels + self.output_channels)].unsqueeze(2)   <span style=\'color: red\'># torch.Size([12, 80, 112, 16, 44])</span>\n            img_feat_with_depth = self.<span style=\'color: green;font-weight: bold;\'>_forward_voxel_net</span>(img_feat_with_depth)\n            img_feat_with_depth = img_feat_with_depth.reshape(                           <span style=\'color: red\'># torch.Size([2, 6, 80, 112, 16, 44])</span>\n                batch_size,\n                num_cams,\n                img_feat_with_depth.shape[1],\n                img_feat_with_depth.shape[2],\n                img_feat_with_depth.shape[3],\n                img_feat_with_depth.shape[4],\n            )\n            img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)          <span style=\'color: red\'># torch.Size([2, 6, 112, 16, 44, 80])</span>\n            feature_map = <span style=\'color: green;font-weight: bold;\'>voxel_pooling_train</span>(geom_xyz,img_feat_with_depth.contiguous(),self.voxel_num.cuda())   <span style=\'color: red\'># ?+?+tensor([128, 128,   1], device=\'cuda:0\')=>torch.Size([2, 80, 128, 128])</span>\n        else:\n            feature_map = voxel_pooling_inference(geom_xyz, depth, depth_feature[:, self.depth_channels:(self.depth_channels + self.output_channels)].contiguous(),self.voxel_num.cuda())\n        if is_return_depth:\n            <span style=\'color: red\'># final_depth has to be fp32, otherwise the depth loss will colapse during the traing process.</span>\n            return feature_map.contiguous(), depth_feature[:, :self.depth_channels].softmax(dim=1)   <span style=\'color: red\'># torch.Size([12, 192, 16, 44])</span>\n        return feature_map.contiguous()\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/layers/backbones/base_lss_fpn.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseLSSFPN(nn.Module):\n    def _forward_depth_net(self, feat, mats_dict):\n        return self.<span style=\'color: green;font-weight: bold;\'>depth_net</span>(feat, mats_dict)\n<span style=\'color: red\'># bevdepth/layers/backbones/bevstereo_lss_fpn.py</span>\nclass DepthNet(nn.Module):\n    def forward(self, x, mats_dict, scale_depth_factor=1000.0):\n        B, _, H, W = x.shape\n        intrins = mats_dict[\'intrin_mats\'][:, 0:1, ..., :3, :3]         <span style=\'color: red\'># torch.Size([2, 1, 6, 3, 3])</span>\n        batch_size = intrins.shape[0]                                   <span style=\'color: red\'># 2</span>\n        num_cams = intrins.shape[2]                                     <span style=\'color: red\'># 6</span>\n        ida = mats_dict[\'ida_mats\'][:, 0:1, ...]                        <span style=\'color: red\'># torch.Size([2, 1, 6, 4, 4])</span>\n        sensor2ego = mats_dict[\'sensor2ego_mats\'][:, 0:1, ..., :3, :]   <span style=\'color: red\'># torch.Size([2, 1, 6, 3, 4])</span>\n        bda = mats_dict[\'bda_mat\'].view(batch_size, 1, 1, 4, 4).repeat(1, 1, num_cams, 1, 1)        <span style=\'color: red\'># torch.Size([2, 1, 6, 4, 4]  参数用于增强</span>\n        mlp_input = torch.cat([torch.stack([\n                        intrins[:, 0:1, ..., 0, 0], intrins[:, 0:1, ..., 1, 1], intrins[:, 0:1, ..., 0, 2], intrins[:, 0:1, ..., 1, 2],\n                        ida[:, 0:1, ..., 0, 0], ida[:, 0:1, ..., 0, 1], ida[:, 0:1, ..., 0, 3], ida[:, 0:1, ..., 1, 0],\n                        ida[:, 0:1, ..., 1, 1], ida[:, 0:1, ..., 1, 3],\n                        bda[:, 0:1, ..., 0, 0], bda[:, 0:1, ..., 0, 1], bda[:, 0:1, ..., 1, 0],  bda[:, 0:1, ..., 1, 1], bda[:, 0:1, ..., 2, 2],\n                    ],\n                    dim=-1,),sensor2ego.view(batch_size, 1, num_cams, -1),],-1,\n        )                    <span style=\'color: red\'># torch.Size([2, 1, 6, 27])</span>\n        mlp_input = self.bn(mlp_input.reshape(-1, mlp_input.shape[-1]))  <span style=\'color: red\'># torch.Size([12, 27])</span>\n        x = self.reduce_conv(x)                                          <span style=\'color: red\'># torch.Size([12, 512, 16, 44])</span>\n        context_se = self.context_mlp(mlp_input)[..., None, None]        <span style=\'color: red\'># torch.Size([12, 512, 1, 1])</span>\n        context = self.context_se(x, context_se)                         <span style=\'color: red\'># torch.Size([12, 512, 16, 44])</span>\n        context = self.context_conv(context)                             <span style=\'color: red\'># torch.Size([12, 80, 16, 44])         #</span>\n        depth_se = self.depth_mlp(mlp_input)[..., None, None]            <span style=\'color: red\'># torch.Size([12, 512, 1, 1])</span>\n        depth = self.depth_se(x, depth_se)                               <span style=\'color: red\'># torch.Size([12, 512, 16, 44])</span>\n        depth = self.depth_conv(depth)                                   <span style=\'color: red\'># torch.Size([12, 112, 16, 44])</span>\n        return torch.cat([depth, context], dim=1)                        <span style=\'color: red\'># torch.Size([12, 192, 16, 44])</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/layers/backbones/base_lss_fpn.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseLSSFPN(nn.Module):\n    def get_geometry(self, sensor2ego_mat, intrin_mat, ida_mat, bda_mat):\n        batch_size, num_cams, _, _ = sensor2ego_mat.shape\n        <span style=\'color: red\'># undo post-transformation</span>\n        <span style=\'color: red\'># B x N x D x H x W x 3</span>\n        points = self.frustum\n        ida_mat = ida_mat.view(batch_size, num_cams, 1, 1, 1, 4, 4)\n        points = ida_mat.inverse().matmul(points.unsqueeze(-1))\n        <span style=\'color: red\'># cam_to_ego</span>\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],points[:, :, :, :, :, 2:]), 5)\n        combine = sensor2ego_mat.matmul(torch.inverse(intrin_mat))\n        points = combine.view(batch_size, num_cams, 1, 1, 1, 4,4).matmul(points)\n        if bda_mat is not None:\n            bda_mat = bda_mat.unsqueeze(1).repeat(1, num_cams, 1, 1).view(batch_size, num_cams, 1, 1, 1, 4, 4)\n            points = (bda_mat @ points).squeeze(-1)\n        else:\n            points = points.squeeze(-1)\n        return points[..., :3]\n</code></pre></font>'}, {'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/layers/backbones/base_lss_fpn.py</p><font size="0"><pre class="language-python"><code class="language-python">class BaseLSSFPN(nn.Module):\n    def _forward_voxel_net(self, img_feat_with_depth):\n        if self.use_da:\n            <span style=\'color: red\'># BEVConv2D [n, c, d, h, w] -> [n, h, c, w, d]</span>\n            img_feat_with_depth = img_feat_with_depth.permute(0, 3, 1, 4,2).contiguous()  <span style=\'color: red\'># [n, c, d, h, w] -> [n, h, c, w, d]  torch.Size([12, 80, 112, 16, 44])-></span>\n            n, h, c, w, d = img_feat_with_depth.shape\n            img_feat_with_depth = img_feat_with_depth.view(-1, c, w, d)                   <span style=\'color: red\'># torch.Size([192, 80, 44, 112])</span>\n            img_feat_with_depth = (self.<span style=\'color: green;font-weight: bold;\'>depth_aggregation_net</span>(img_feat_with_depth).view(n, h, c, w, d).permute(0, 2, 4, 1, 3).contiguous())  <span style=\'color: red\'># torch.Size([192, 80, 44, 112])</span>\n        return img_feat_with_depth\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 9, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">bevdepth/layers/backbones/base_lss_fpn.py</p><font size="0"><pre class="language-python"><code class="language-python">class DepthAggregation(nn.Module):\n    def forward(self, x):\n        x = self.reduce_conv(x)     <span style=\'color: red\'># torch.Size([192, 80, 44, 112])->torch.Size([192, 80, 44, 112])</span>\n        x = self.conv(x) + x        <span style=\'color: red\'># torch.Size([192, 80, 44, 112])</span>\n        x = self.out_conv(x)        <span style=\'color: red\'># torch.Size([192, 80, 44, 112])</span>\n        return x\n</code></pre></font>'}]}]}]}]}]}]}]}]}]})</script></body>
</html>
