<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>BEVDepth_nuscenes</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BEVDepthLightningModel</p>bevdepth/exps/nuscenes/mv/bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da_ema.py<br>\n<span class=\'hidden-code\' data-code=\'from bevdepth.exps.base_cli import run_cli  该函数封装了 PyTorch Lightning 的训练/验证/测试逻辑，支持从命令行传参（如学习率、batch size、ckpt路径等），并自动构建 Trainer 和模型\nfrom bevdepth.exps.nuscenes.mv.bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da import BEVDepthLightningModel  noqa\nif __name__ == ’__main__’:\n    `run_cli`(BEVDepthLightningModel,’bev_stereo_lss_r50_256x704_128x128_20e_cbgs_2key_da_ema’,use_ema=True,extra_trainer_config_args={’epochs’: 20})\n    是否启用 Exponential Moving Average (EMA) 权重。EMA 可提升推理稳定性，常用于检测任务。\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">run_cli</p>bevdepth/exps/base_cli.py<br>\n统一管理训练、验证和预测流程的核心入口函数<br>\n<span class=\'hidden-code\' data-code=\'def run_cli(model_class=BEVDepthLightningModel,exp_name=’base_exp’,use_ema=False,extra_trainer_config_args={}):\n    model = `model_class`(**vars(args))        # data_code/3target_detection_3D/project/BEVDepth/model.log； 用命令行参数 args 实例化模型\n    if use_ema:                                # True\n        train_dataloader = model.train_dataloader()\n        ema_callback = EMACallback(len(train_dataloader.dataset) * args.max_epochs)  # EMA 更新频率：通常 EMA 每 step 更新一次，总更新次数 = num_samples × epochs\n        trainer = pl.Trainer.from_argparse_args(args, callbacks=[ema_callback])\n    else:\n        trainer = pl.Trainer.from_argparse_args(args)  \n    if args.evaluate:\n        trainer.test(model, ckpt_path=args.ckpt_path)  # 加载指定 checkpoint，运行 test_step，计算 mAP 等指标。\n                                                       # 调用 model.test_dataloader() 和 model.test_step()。\n    elif args.predict:\n        predict_step_outputs = trainer.predict(model, ckpt_path=args.ckpt_path)\n        all_pred_results = list()\n        all_img_metas = list()\n        for predict_step_output in predict_step_outputs:\n            for i in range(len(predict_step_output)):\n                all_pred_results.append(predict_step_output[i][:3])\n                all_img_metas.append(predict_step_output[i][3])\n        synchronize()\n        len_dataset = len(model.test_dataloader().dataset)\n        all_pred_results = sum(map(list, zip(*all_gather_object(all_pred_results))),[])[:len_dataset]\n        all_img_metas = sum(map(list, zip(*all_gather_object(all_img_metas))),[])[:len_dataset]\n        model.evaluator._format_bbox(all_pred_results, all_img_metas,os.path.dirname(args.ckpt_path))\n    else:\n        trainer.`fit`(model)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BEVDepthLightningModel:training_step</p>bevdepth/exps/nuscenes/base_exp.py<br>\n定义单个训练 step 的前向传播 + 损失计算<br>\n输入：batch —— 从 train_dataloader() 返回的一个 batch 数据（tuple）。<br>\n输出：标量损失（detection_loss + depth_loss），用于反向传播。<br>\n<span class=\'hidden-code\' data-code=\'class BEVDepthLightningModel(LightningModule):\n    def training_step(self, batch):\n        # sweep_imgs[2, 2, 6, 3, 256, 704]：B=2（batch size）+2sweeps（时序帧，如 key + prev）+ 6 cameras（nuScenes）+ 图像尺寸 256×704\n        # gt_boxes：[N, 9]->(x,y,z,l,w,h,θ,vx,vy)   gt_labels:torch.Size([33])      [torch.Size([33, 9]),torch.Size([33, 9])];\n        # depth_labels[2, 1, 6, 256, 704]：仅 key frame 的深度图 GT，6为camera个数    [torch.Size([33]),torch.Size([33])];\n        (sweep_imgs, mats, _, _, gt_boxes, gt_labels, depth_labels) = batch      \n        if torch.cuda.is_available():\n            for key, value in mats.items():\n                mats[key] = value.cuda()   # 相机变换矩阵字典，包含：sensor2ego_mats + intrin_mats + ida_mats + bda_mat（data augmentation）\n            sweep_imgs = sweep_imgs.cuda()\n            gt_boxes = [gt_box.cuda() for gt_box in gt_boxes]\n            gt_labels = [gt_label.cuda() for gt_label in gt_labels]\n        preds, depth_preds = `self`(sweep_imgs, mats)    # BEV特征图[B,num_sweeps,C,128,128] + 预测的深度概率分布[B*num_cams,D=112,H’=16,W’=44]经softmax表示每个像素在112个深度bin 上的概率\n        if isinstance(self.model, torch.nn.parallel.DistributedDataParallel): # 在 DDP（多卡训练）下，模型被包裹为 DistributedDataParallel，真实模型在 .module 中\n            targets = self.model.module.get_targets(gt_boxes, gt_labels)\n            # 根据 gt_boxes/labels 生成检测 head 所需的监督信号（如 heatmap, offset, height 等）\n            # 输入：list of [N_i, 9], list of [N_i]         输出：dict of target tensors（与 preds 结构对应）\n            detection_loss = self.model.module.loss(targets, preds)\n        else:\n            targets = self.model.get_targets(gt_boxes, gt_labels)\n            detection_loss = self.model.loss(targets, preds)\n        if len(depth_labels.shape) == 5:\n            # only key-frame will calculate depth loss\n            depth_labels = depth_labels[:, 0, ...]                          # depth_labels 原始 shape: [B, 1, N, H, W] → squeeze 第1维 → [B, N, H, W]\n        depth_loss = self.get_depth_loss(depth_labels.cuda(), depth_preds)  # depth_preds 是 [B*N, D, H’, W’]，而 depth_labels 是 [B, N, H, W]\n                                                                            # 对 depth_labels下采样到 H’×W’; 将其转换为 one-hot 或 soft label over D=112 bins + 计算 cross-entropy 或 KL loss\n        self.log(’detection_loss’, detection_loss)\n        self.log(’depth_loss’, depth_loss)\n        return detection_loss + depth_loss\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BEVDepthLightningModel:forward</p>bevdepth/exps/nuscenes/base_exp.py<br>\n定义模型的前向传播逻辑<br>\n(输入) sweep_imgs: 多帧多视角图像，shape 通常是 (B, num_sweeps, num_cams, C, H, W) mats: 相机和变换矩阵字典，包含内参、外参、数据增强矩阵等<br>\n(输出) detection_outputs: BEV 空间中的检测头输出（heatmap, reg, etc.）depth_preds: 每个图像像素的深度分布（soft or hard）<br>\n<span class=\'hidden-code\' data-code=\'class BEVDepthLightningModel(LightningModule):\n    def forward(self, sweep_imgs, mats):\n        return self.`model`(sweep_imgs, mats)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BaseBEVDepth:forward</p>bevdepth/models/base_bev_depth.py<br>\n目的：同时输出 3D 检测结果 和 深度预测，用于联合优化。<br>\nbackbone返回： x: BEV特征图（如[B,C,128,128]）+ depth_pred: 深度概率分布（如 [B*num_cams,D=112,H\', W\']）<br>\nhead：基于BEV特征预测检测结果（heatmap, bbox 等）<br>\n<span class=\'hidden-code\' data-code=\'class BaseBEVDepth(nn.Module):\n    def forward(self,x,mats_dict,timestamps=None):\n        if self.is_train_depth and self.training:\n            x, depth_pred = self.`backbone`(x,mats_dict,timestamps,is_return_depth=True)\n            preds = self.`head`(x)\n            return preds, depth_pred\n        else:\n            x = self.backbone(x, mats_dict, timestamps)\n            preds = self.head(x)\n            return preds\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BaseLSSFPN:forward</p>bevdepth/layers/backbones/bevstereo_lss_fpn.py<br>\n<span class=\'hidden-code\' data-code=\'class BaseLSSFPN(nn.Module):\n    def forward(self,sweep_imgs,mats_dict,timestamps=None,is_return_depth=False):\n        batch_size, num_sweeps, num_cams, num_channels, img_height, img_width = sweep_imgs.shape           # torch.Size([2, 2, 6, 3, 256, 704]);\n        key_frame_res = self.`_forward_single_sweep`(0,sweep_imgs[:, 0:1, ...],mats_dict,is_return_depth=is_return_depth)\n        if num_sweeps == 1:\n            return key_frame_res\n        key_frame_feature = key_frame_res[0] if is_return_depth else key_frame_res\n        ret_feature_list = [key_frame_feature]\n        for sweep_index in range(1, num_sweeps):\n            with torch.no_grad():\n                feature_map = self.`_forward_single_sweep`(sweep_index,sweep_imgs[:, sweep_index:sweep_index + 1, ...],mats_dict,is_return_depth=False)\n                ret_feature_list.append(feature_map)\n        if is_return_depth:\n            return torch.cat(ret_feature_list, 1), key_frame_res[1]\n        else:\n            return torch.cat(ret_feature_list, 1)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 7, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BaseLSSFPN:_forward_single_sweep</p>bevdepth/layers/backbones/base_lss_fpn.py<br>\n负责将单帧多视角图像转换为 BEV（Bird\'s Eye View）特征图，并可选地输出深度预测<br>\nStep 1: 提取图像特征 (e.g., ResNet + FPN)<br>\nStep 2: 深度估计网络<br>\nStep 3: 构建 3D 几何坐标 (Lift)<br>\nStep 4: Splat 到 BEV<br>\n<span class=\'hidden-code\' data-code=\'class BaseLSSFPN(nn.Module):\n    def _forward_single_sweep(self,sweep_index,sweep_imgs,mats_dict,is_return_depth=False):\n        batch_size, num_sweeps, num_cams, num_channels, img_height, img_width = sweep_imgs.shape\n        # Step 1: 提取图像特征 (e.g., ResNet + FPN)\n        img_feats = self.`get_cam_feats`(sweep_imgs)      # torch.Size([2, 1, 6, 3, 256, 704]) -> torch.Size([2, 1, 6, 512, 16, 44])\n        source_features = img_feats[:, 0, ...]            # 第0维是原始图片特征\n        # Step 2: 深度估计网络  得到torch.Size([12, 192, 16, 44]) 192是将图片特征进行卷积得到深度112以及特征80  \n        depth_feature = self.`_forward_depth_net`(source_features.reshape(batch_size * num_cams,source_features.shape[2],source_features.shape[3],source_features.shape[4]),mats_dict)\n        # 分离深度 logits 和图像特征   深度范围 [2.0, 58.0]，间隔 0.5m → (58-2)/0.5 = 112 个 bin。这里的 112 是 预设的深度采样点数量，不是网络预测的“深度值”，而是网格\n        depth = depth_feature[:, :self.depth_channels].softmax(dim=1, dtype=depth_feature.dtype)  # 112 -> torch.Size([12, 112, 16, 44])  ’d_bound’: [2.0, 58.0, 0.5]->112  这里的112是网络生成的\n        # Step 3: 构建 3D 几何坐标 (Lift)     Lift--图像像素 → 3D 空间（通过深度分布）；Splat--将图像特征按深度概率“泼洒”到 3D 网格   Shoot--通过 voxel pooling 得到 BEV 特征\n        geom_xyz = self.`get_geometry`(                        # [B, N, D=112, H=16, W=44, 3] —— 这里的 D=112 是预设的深度采样点（真实物理坐标）\n            mats_dict[’sensor2ego_mats’][:, sweep_index, ...],\n            mats_dict[’intrin_mats’][:, sweep_index, ...],\n            mats_dict[’ida_mats’][:, sweep_index, ...],\n            mats_dict.get(’bda_mat’, None),\n        )\n        # 将连续坐标映射到 voxel 索引\n        geom_xyz = ((geom_xyz - (self.voxel_coord - self.voxel_size / 2.0)) / self.voxel_size).int()  # voxel_coord=[-50.8,-50.8,-1]+voxel_size=[0.8,0.8,8]->torch.Size([2, 6, 112, 16, 44, 3])  图片上的预设点到点云去了\n        # 深度分布（经过softmax处理后的概率分布）与图像特征相乘时【每个特征都有对应深度+位置就知道在3Dvoxel上面的位置】，实际上是在执行一种加权求和的操作； 用深度分布作为权重，对图像特征进行加权\n        if self.training or self.use_da:                                                              # self.use_da=True   # 将深度分布与图像特征相乘（soft splatting）\n            img_feat_with_depth = depth.unsqueeze(1) * depth_feature[:, self.depth_channels:(self.depth_channels + self.output_channels)].unsqueeze(2)   # [B*N, 1, 112, 16, 44] * [B*N, 80, 1, 16, 44] → [B*N, 80, 112, 16, 44]\n            img_feat_with_depth = self.`_forward_voxel_net`(img_feat_with_depth)         # 【bevdet_sttiny没有进行卷积计算】# 可选：通过 voxel_net（如 3D conv）进一步融合\n            img_feat_with_depth = img_feat_with_depth.reshape(                           # torch.Size([2, 6, 80, 112, 16, 44])  80是通道，112是深度，16x44是网络输出的大小\n                batch_size,\n                num_cams,\n                img_feat_with_depth.shape[1],\n                img_feat_with_depth.shape[2],\n                img_feat_with_depth.shape[3],\n                img_feat_with_depth.shape[4],\n            )   \n            img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)          # torch.Size([2, 6, 112, 16, 44, 80])\n            # Voxel Pooling (训练模式：支持梯度回传)\n            feature_map = `voxel_pooling_train`(geom_xyz,img_feat_with_depth.contiguous(),self.voxel_num.cuda())   # ?+?+tensor([128, 128, 1], device=’cuda:0’)=>torch.Size([2, 80, 128, 128])\n        else:   # # 推理模式：使用更高效的 pooling（无梯度）\n            feature_map = voxel_pooling_inference(geom_xyz, depth, depth_feature[:, self.depth_channels:(self.depth_channels + self.output_channels)].contiguous(),self.voxel_num.cuda())\n        if is_return_depth:        # Step 5: 返回结果返回的是 softmax 前的 logits？不，这里再次 softmax（确保 fp32） 注释强调：必须用 fp32 防止 loss collapse\n            # final_depth has to be fp32, otherwise the depth loss will colapse during the traing process.\n            return feature_map.contiguous(), depth_feature[:, :self.depth_channels].softmax(dim=1)   # torch.Size([12, 192, 16, 44])；这里的深度是torch.Size([12, 112, 16, 44])-->torch.Size([12, 112, 16, 44])  用作训练\n        return feature_map.contiguous()\n\'> </span>', 'children': [{'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BaseLSSFPN->DepthNet:forward</p>bevdepth/layers/backbones/base_lss_fpn.py<br>\n相机参数感知的深度预测机制，是LSS架构中“深度监督”能有效提升BEV表征质量的关键所在<br>\n给定图像特征 x 和相机内外参 mats_dict，输出两部分：<br>\n1.深度分布：[B×N, D=112, H, W] —— 用于lift到3D<br>\n3.上下文特征：[B×N, C=80, H, W] —— 用于后续BEV检测<br>\n<span class=\'hidden-code\' data-code=\'class BaseLSSFPN(nn.Module):\n    def _forward_depth_net(self, feat, mats_dict):\n        return self.`depth_net`(feat, mats_dict)\n# bevdepth/layers/backbones/bevstereo_lss_fpn.py\nclass DepthNet(nn.Module):\n    def forward(self, x, mats_dict, scale_depth_factor=1000.0):\n        B, _, H, W = x.shape                                # shape = [B×N, 512, 16, 44]\n        # Step 1: 提取关键相机参数（构建 MLP 输入）\n        intrins = mats_dict[’intrin_mats’][:, 0:1, ..., :3, :3]         # torch.Size([2, 1, 6, 3, 3])\n        batch_size = intrins.shape[0]                                   # 2\n        num_cams = intrins.shape[2]                                     # 6\n        ida = mats_dict[’ida_mats’][:, 0:1, ...]                        # torch.Size([2, 1, 6, 4, 4])\n        sensor2ego = mats_dict[’sensor2ego_mats’][:, 0:1, ..., :3, :]   # torch.Size([2, 1, 6, 3, 4])\n        bda = mats_dict[’bda_mat’].view(batch_size, 1, 1, 4, 4).repeat(1, 1, num_cams, 1, 1)        # torch.Size([2, 1, 6, 4, 4]  参数用于增强\n        mlp_input = torch.cat([torch.stack([                            # torch.Size([2, 1, 6, 27])     \n                        intrins[:, 0:1, ..., 0, 0], intrins[:, 0:1, ..., 1, 1], intrins[:, 0:1, ..., 0, 2], intrins[:, 0:1, ..., 1, 2],  # fx,fy,cx,cy\n                        ida[:, 0:1, ..., 0, 0], ida[:, 0:1, ..., 0, 1], ida[:, 0:1, ..., 0, 3], ida[:, 0:1, ..., 1, 0],                  # scale_x;shear;......\n                        ida[:, 0:1, ..., 1, 1], ida[:, 0:1, ..., 1, 3],                                                                  # trans_x;trans_y;\n                        bda[:, 0:1, ..., 0, 0], bda[:, 0:1, ..., 0, 1], bda[:, 0:1, ..., 1, 0],  bda[:, 0:1, ..., 1, 1], bda[:, 0:1, ..., 2, 2],\n                    ],\n                    dim=-1,),sensor2ego.view(batch_size, 1, num_cams, -1),],-1,\n        )  # 相机矩阵包含大量冗余信息（如齐次坐标的最后一行总是 [0,0,0,1]） ; 直接输入整个 4×4 矩阵效率低，且难以学习 ; 提取物理意义明确的参数（焦距、主点、旋转、平移等）更利于网络理解几何关系\n        # Step 2: 归一化 MLP 输入\n        mlp_input = self.bn(mlp_input.reshape(-1, mlp_input.shape[-1]))  # torch.Size([12, 27])     # 12=BXN=2X6 对相机参数做归一化，提升训练稳定性。\n        # Step 3: 特征压缩\n        x = self.reduce_conv(x)                                          # torch.Size([12, 512, 16, 44])   用于调整通道数或引入非线性\n        # Step 4: 双分支 SE-style 调制（核心创新！）使得深度预测不仅依赖图像内容，还显式利用了相机几何先验，大幅提升深度估计准确性\n        ##########分支1：Context（用于 BEV 检测）--根据当前相机位姿/内参，动态调制图像特征，使其更适合后续 BEV 投影   \n        # # 用一个与相机几何相关的信号，去缩放（scale）或偏移（bias）图像特征的每个通道\n        context_se = self.context_mlp(mlp_input)[..., None, None]        # torch.Size([12, 512, 1, 1])  小型 MLP，将 27 维相机参数映射到 512 维（与特征通道对齐 \n        context = self.context_se(x, context_se)                         # torch.Size([12, 512, 16, 44])\n        context = self.context_conv(context)                             # torch.Size([12, 80, 16, 44])         #\n        ##########分支2：Depth（用于深度预测） 同样使用相机参数调制特征，但专用于深度预测\n        depth_se = self.depth_mlp(mlp_input)[..., None, None]            # torch.Size([12, 512, 1, 1])\n        depth = self.depth_se(x, depth_se)                               # torch.Size([12, 512, 16, 44])\n        depth = self.depth_conv(depth)                                   # torch.Size([12, 112, 16, 44])\n        # Step 5: 拼接输出  depth: 用于 Lift（生成 3D 点) context: 用于 Splat（作为 3D 点的特征）\n        return torch.cat([depth, context], dim=1)                        # torch.Size([12, 192, 16, 44])\n\'> </span>'}, {'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BaseLSSFPN:get_geometry</p>bevdepth/layers/backbones/base_lss_fpn.py<br>\n预定义的图像视锥（frustum）中的每个像素-深度点，             # B x N x D x H x W x 3<br>\n通过相机内外参变换到自车坐标系（ego frame）下的 3D 空间位置  # [B, N, D, H, W, 3]<br>\n<span class=\'hidden-code\' data-code=\'class BaseLSSFPN(nn.Module):\n    def get_geometry(self, sensor2ego_mat, intrin_mat, ida_mat, bda_mat):\n        batch_size, num_cams, _, _ = sensor2ego_mat.shape       # 相机坐标系 → 自车坐标系（ego）的变换矩阵（含旋转+平移）\n        # undo post-transformation  这个 frustum 是 固定的网格，代表“我们假设每个像素可能出现在哪些深度上”\n        # B x N x D x H x W x 3    D=112：深度方向采样数（如 2.0m ~ 58.0m，步长 0.5m） + H=16, W=44：图像特征图的空间分辨率（原始图 256×704 下采样 16 倍）\n        points = self.frustum      # 每个点 frustum[d, h, w] = [u, v, d]： u, v：归一化图像坐标（或像素坐标）+ d：预设的深度值（不是预测的！）\n        Step 1: Undo IDA（图像数据增强逆变换）将 frustum 点从 增强后的图像坐标系 变换回 原始图像坐标系。\n        ida_mat = ida_mat.view(batch_size, num_cams, 1, 1, 1, 4, 4)   \n        points = ida_mat.inverse().matmul(points.unsqueeze(-1))\n        Step 2: 齐次坐标转换   cam_to_ego    先把 frustum 点表示为 齐次像素坐标：[u, v, 1, 1]^T  然后乘以d 得到：[u*d, v*d, d, 1]^T\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],points[:, :, :, :, :, 2:]), 5)\n        Step 3: 应用相机内参逆 + 外参（核心 Lift）通过几何变换，把 2D 像素+深度假设 lift 到 3D ego 空间\n        combine = sensor2ego_mat.matmul(torch.inverse(intrin_mat))\n        points = combine.view(batch_size, num_cams, 1, 1, 1, 4,4).matmul(points)\n        Step 4: 应用 BDA（批次数据增强）\n        if bda_mat is not None:\n            bda_mat = bda_mat.unsqueeze(1).repeat(1, num_cams, 1, 1).view(batch_size, num_cams, 1, 1, 1, 4, 4)\n            points = (bda_mat @ points).squeeze(-1)\n        else:\n            points = points.squeeze(-1)\n        return points[..., :3]  # [B, N, D, H, W, 3]\n\'> </span>'}, {'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">BaseLSSFPN:_forward_voxel_net</p>bevdepth/layers/backbones/base_lss_fpn.py<br>\n利用 3D 卷积或等效操作，在深度维度上聚合上下文信息，增强深度感知能力<br>\nimg_feat_with_depth-->img_feat_with_depth<br>\n<span class=\'hidden-code\' data-code=\'class BaseLSSFPN(nn.Module):\n    def _forward_voxel_net(self, img_feat_with_depth):\n        if self.use_da:\n            # BEVConv2D [n, c, d, h, w] -> [n, h, c, w, d]\n            img_feat_with_depth = img_feat_with_depth.permute(0, 3, 1, 4,2).contiguous()  # [n, c, d, h, w] -> [n, h, c, w, d]  torch.Size([12, 80, 112, 16, 44])->\n            n, h, c, w, d = img_feat_with_depth.shape\n            img_feat_with_depth = img_feat_with_depth.view(-1, c, w, d)                   # torch.Size([192, 80, 44, 112])\n            img_feat_with_depth = (self.`depth_aggregation_net`(img_feat_with_depth).view(n, h, c, w, d).permute(0, 2, 4, 1, 3).contiguous())  # torch.Size([192, 80, 44, 112])\n        return img_feat_with_depth\n\'> </span>', 'children': [{'type': 'heading', 'depth': 9, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">DepthAggregation:forward</p>bevdepth/layers/backbones/base_lss_fpn.py<br>\n<span class=\'hidden-code\' data-code=\'class DepthAggregation(nn.Module):\n    def forward(self, x):\n        x = self.reduce_conv(x)     # torch.Size([192, 80, 44, 112])->torch.Size([192, 80, 44, 112])\n        x = self.conv(x) + x        # torch.Size([192, 80, 44, 112])\n        x = self.out_conv(x)        # torch.Size([192, 80, 44, 112])\n        return x\n\'> </span>'}]}, {'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">voxel_pooling_train</p>负责将 3D 特征点投影到 BEV 格子中，并进行池化操作以生成最终的 BEV 特征图<br>\n输入：<br>\n1，geom_xyz: [B, N, D, H, W, 3]表示从图像平面 lift 到 3D 空间的坐标，其中每个点 (x, y, z) 都对应一个特定的深度、高度和宽度位置。<br>\n2，img_feat_with_depth: [B, N, D, H, W, C] 这是经过 soft splatting 后的特征图，包含了每个像素在不同深度上的特征表示。<br>\n3，voxel_num: [3]表示 BEV 网格的大小 [X, Y, Z]，例如 [128, 128, 1]，分别代表沿 X 轴、Y 轴和 Z 轴的体素数量<br>\n输出：<br>\n1，bev_feature 是形状为 [B, C, Y, X] 的张量，表示 BEV 空间中的特征表示。这里的 C 是特征通道数，Y 和 X 是 BEV 网格的尺寸<br>\n<span class=\'hidden-code\' data-code=\'1. 几何变换后的 3D 坐标 (geom_xyz)\n在 get_geometry 方法中，已经将图像平面上的每一个像素根据其深度假设 lift 到了自车坐标系下的 3D 空间位置。geom_xyz 包含了这些 3D 坐标，形状为 [B, N, D, H, W, 3]，其中每个元素是一个 (x, y, z) 向量，表示该点在自车坐标系中的位置。\n2. 带深度的特征图 (img_feat_with_depth)\nimg_feat_with_depth 是经过 soft splatting 后的结果，它包含了每个像素在不同深度上的特征表示。具体来说，对于每个 (u, v) 像素位置，我们有多个深度假设，每个深度假设都有一个对应的特征向量。因此，它的形状为 [B, N, D, H, W, C]。\n3. BEV 网格 (voxel_num)\nvoxel_num 定义了 BEV 空间的网格大小。例如 [128, 128, 1] 表示沿 X 和 Y 方向各有 128 个体素，Z 方向只有 1 个体素（因为通常 BEV 只考虑地面层）。这决定了 BEV 特征图的分辨率。\n4. voxel_pooling_train 操作\nvoxel_pooling_train 的核心任务是将 img_feat_with_depth 投影到 BEV 空间中，并对重叠的点进行池化操作。具体步骤如下：\n# 初始化 BEV 特征图\nbev_feature = torch.zeros(batch_size, num_channels, *voxel_num[1:])\n# 遍历所有点\nfor b in range(batch_size):\n    for n in range(num_cams):\n        for d in range(depth_levels):\n            for h in range(height):\n                for w in range(width):\n                    x, y, z = grid_idx[b, n, d, h, w]\n                    if 0 <= x < voxel_num[0] and 0 <= y < voxel_num[1]:\n                        bev_feature[b, :, y, x] = max(bev_feature[b, :, y, x], img_feat_with_depth[b, n, d, h, w])\n\'> </span>'}]}]}]}]}]}]}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
