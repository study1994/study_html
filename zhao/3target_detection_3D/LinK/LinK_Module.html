<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>LinK_Module</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">TSELKBlock</p><span class=\'hidden-code\' data-code=\'class TSELKBlock(nn.Module):\n    def forward(self, sct: spconv.SparseConvTensor, stride):\n        st, sct_save = `spconv2ts`(sct)            [41, 1440, 1440]\n        new_st = self.`forward_`(st, stride)\n        new_sct = `ts2spconv`(new_st, sct_save)\n        return new_sct\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">spconv2ts</p><span class=\'hidden-code\' data-code=\'def spconv2ts(sct: spconv.SparseConvTensor):\n    feats = sct.features                                        # torch.Size([137566, 16])\n    coords = torch.index_select(sct.indices, 1,                 # torch.Size([137566, 4])->(B,Z,Y,Z)=>(X,Y,Z,B)?\n             torch.LongTensor([3,2,1,0]).to(sct.indices.device)).contiguous()\n    st = SparseTensor(feats, coords, 1)                         # 这里将自带的tensor转变到torchsparse的tensor\n    sct_save = dict()\n    sct_save[&amp;#39;batch_size&amp;#39;] = sct.batch_size                     # 2\n    sct_save[&amp;#39;benchmark&amp;#39;] = sct.benchmark                       # False\n    sct_save[&amp;#39;benchmark_record&amp;#39;] = sct.benchmark_record         # {}\n    sct_save[&amp;#39;grid&amp;#39;] = sct.grid                                 # tensor([])\n    sct_save[&amp;#39;indice_dict&amp;#39;] = sct.indice_dict                   # {&amp;#39;res0&amp;#39;: `<`spconv.pytorch.core...8910bc4f0`>`}\n    sct_save[&amp;#39;spatial_shape&amp;#39;] = sct.spatial_shape               # [41, 1440, 1440]\n    sct_save[&amp;#39;voxel_num&amp;#39;] = sct.voxel_num                       # None\n    return st, sct_save\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">TSELKBlock-forward_</p><span class=\'hidden-code\' data-code=\'class TSELKBlock(nn.Module):\n    def forward_(self, st: SparseTensor, stride):\n        F_input = self.pre_mix(st.F)       # Linear+LayerNorm => st.F=torch.Size([137566, 16]) -> torch.Size([137566, 16])\n        local_mix = self.local_mix(st)     # spnn.Conv3d(self.inc, self.inc, kernel_size=3, dilation=1,stride=1)\n        if self.baseop == &amp;#39;sin&amp;#39;:\n            ......\n        elif self.baseop == &amp;#39;cos&amp;#39;:\n            pos_weight = self.pos_weight(st.C[:,:3].float())              # torch.Size([137566, 4])  坐标用3维变成16维-线性核-》torch.Size([137566, 16])\n            pos_weight = pos_weight[:,:self.inc//2].repeat([1,2])         # channel grouping  torch.Size([137566, 8])->torch.Size([137566, 16])沿着第二维重复\n            pos_weight_sin = torch.sin(pos_weight)                        # torch.Size([137566, 16])\n            pos_weight_cos = torch.cos(pos_weight)                        # torch.Size([137566, 16])\n            F_weighted_sin = F_input*pos_weight_sin                       # torch.Size([137566, 16])\n            F_weighted_cos = F_input*pos_weight_cos\n            st.F = torch.cat([F_weighted_cos, F_weighted_sin], dim=1).contiguous()     # torch.Size([137566, 32])     K^(0)(a)*f_a+k^(1)(a)*f_a\n            small_st, idx, counts = `large_to_small`(st, stride=stride)    # 应该是7x7下采样得到新的tensor\n            large_st = `small_to_large_v2`(small_st, st, idx, counts)      # torch.Size([137566, 32])\n            new_st_F = large_st.F[:,:self.inc]*pos_weight_cos + large_st.F[:,self.inc:]*pos_weight_sin  # self.inc=16; -->torch.Size([137566, 16])\n        elif self.baseop == &amp;#39;cos_x_alpha&amp;#39;:\n            ......\n        elif self.baseop == &amp;#39;cos_sin&amp;#39;:\n            ......\n        elif self.baseop == &amp;#39;x&amp;#39;:\n            ......\n        new_st_F = self.norm(new_st_F)\n        local_F = self.norm_local(local_mix.F)\n        new_st_F = self.activate(new_st_F+local_F)\n        large_st.F = new_st_F\n        return large_st\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">large_to_small</p><span class=\'hidden-code\' data-code=\'# x: SparseTensor->large, stride: scale of kernel \n# return : SparseTensor->small\ndef large_to_small(large_x, stride):              # torch.Size([137566, 32])，7  分成7快大小s\n    x_C = torch.cat([torch.div(large_x.C[:,:3], stride, rounding_mode=&amp;#39;floor&amp;#39;).int(), large_x.C[:,3:]], dim=1)   # 相当于把x,y,z的位置除以7，可能有不同的点下采样到同一个点  torch.Size([137566, 4])\n    large_x_hash = F.sphash(x_C.to(large_x.F.device))                      # torch.Size([137566])   tensor([383018381322703704, 941813131069407809, ...], device=&amp;#39;cuda:0&amp;#39;)\n    small_x_C = torch.unique(x_C, dim=0)                                   # torch.Size([13740, 4])  \n    small_x_hash = F.sphash(small_x_C.to(large_x.F.device))                # torch.Size([13740])\n    idx_query = F.sphashquery(large_x_hash, small_x_hash)                  # torch.Size([137566])+torch.Size([13740])->torch.Size([137566]) torch.max(idx_query)=13740-1\n    counts = F.spcount(idx_query.int(), len(small_x_hash))                 # torch.Size([13740])  每个索引的数量\n    inserted_feat = F.`spvoxelize`(large_x.F, idx_query, counts)             # (torch.Size([137566, 32]),torch.Size([13740, 4]),torch.Size([13740,])) -> torch.Size([13740, 32])\n    small_x = SparseTensor(inserted_feat, small_x_C, stride)\n    small_x.cmaps = large_x.cmaps\n    small_x.kmaps = large_x.kmaps\n    return small_x, idx_query, counts\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">torchsparse/torchsparse/backend/voxelize/voxelize_cpu.cpp</p><span class=\'hidden-code\' data-code=\'at::Tensor voxelize_forward_cpu(const at::Tensor inputs, const at::Tensor idx, const at::Tensor counts) {\n    int N = inputs.size(0);                    // 137566\n    int c = inputs.size(1);                    // 32\n    int N1 = counts.size(0);                   // 13740\n    at::Tensor out = torch::zeros({N1, c}, at::device(idx.device()).dtype(at::ScalarType::Float));     // 13740,32\n    for (int i = 0; i < N; i++) {\n        int pos = *(idx.data_ptr`<`int`>`() + i);                      // 获取索引张量 idx 中第 i 个位置的值\n        if (*(counts.data_ptr`<`int`>`() + pos) == 0) continue;        // 如果计数张量 counts 中 pos 位置的值为 0，则跳过当前循环\n    #pragma omp parallel for\n        for (int j = 0; j < c; j++) {          // 对于每个特征\n            *(out.data_ptr`<`float`>`() + pos * c + j) += *(inputs.data_ptr`<`float`>`() + i * c + j) / (float)(*(counts.data_ptr`<`int`>`() + pos));  //将 inputs 中的值加到 out 中的对应位置上\n        }\n    }\n    return out;\n}\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">small_to_large_v2</p><span class=\'hidden-code\' data-code=\'# F.sphash()是一个函数，用于将一个点云的坐标和特征编码成一个哈希表。这个哈希表存储了点云中每个点的坐标和特征信息，以及这些点在原始点云中的索引。\n# 它使用了点云的体素化方法，将空间分成了许多小的体素，每个体素内部存储其内包含的点的信息。\n# 这种压缩方式可以大大减少点云的存储空间，并且可以加速点云的相关计算，如点云分割和物体检测。\ndef small_to_large_v2(small_x, large_x, idx, counts):\n    # local offsets to index neighbors         [2^3,3]\n    kernel_size = 3                                                            # offsets[:3] = tensor([[-1, -1, -1], [ 0, -1, -1], [ 1, -1, -1]])  offsets[13]=(0,0,0)\n    offsets = get_kernel_offsets(kernel_size, 1, 1, device=large_x.F.device)   # torch.Size([27, 3])        from torchsparse.nn.utils import get_kernel_offsets  \n    neighbor_hash = F.sphash(small_x.C, offsets)                               # 坐标 torch.Size([13740, 4]) + torch.Size([27, 3])  torch.Size([27, 13740])   tensor([[  99013418389022827,...]\n    small_hash = F.sphash(small_x.C.to(large_x.F.device))                      # 坐标 torch.Size([13740, 4])->torch.Size([13740])\n    idx_query = F.sphashquery(neighbor_hash, small_hash)                       # torch.Size([27, 13740])     # 有13740个bi块  没有的值为-1，有的最大值为13740-1\n    idx_query = idx_query.transpose(0,1).contiguous()                          # torch.Size([13740, 27])\n    idx_query_flat = idx_query.view(-1)\n    f = torch.cat([small_x.F, torch.ones_like(small_x.F[:,:1]).to(small_x.F.device)], dim=1)   # torch.Size([13740, 32])+torch.Size([13740, 1])->torch.Size([13740, 33])\n    f = f*counts.unsqueeze(dim=-1)                                                             # torch.Size([13740, 33])\n    weights = torch.ones(small_x.F.shape[0], kernel_size**3).to(small_x.F.device).float()      # torch.Size([13740, 27])\n    weights[idx_query == -1] = 0                                               # 将本身的坐标\n    new_feat = F.`spdevoxelize`(f, idx_query, weights, kernel_size)            # torch.Size([13740, 33])\n    new_feat = new_feat[:,:-1] / new_feat[:,-1:]                               # torch.Size([13740, 32])\n    large_x.F = new_feat[idx]                                                  # large_x.F.shape = torch.Size([137566, 32])   idx.shape=torch.Size([137566])\n    return large_x\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">torchsparse/torchsparse/backend/devoxelize/devoxelize_cuda.cu</p><span class=\'hidden-code\' data-code=\'// make sure indices is int type     用于在CUDA上执行反体素化（devoxelization）\n// feat: (b,c,s) indices: (N, 3) batch_index: (N, ) -> out: (N, c)\nat::Tensor devoxelize_forward_cuda(const at::Tensor feat, const at::Tensor indices, const at::Tensor weight, int r) {\n  int c = feat.size(1);                     // 33 \n  int N = indices.size(0);                  // 13740\n  at::Tensor out = torch::zeros({N, c}, at::device(feat.device()).dtype(feat.dtype()));      // 13740,33\n  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n      feat.type(), &amp;#39;devoxelize_forward_cuda&amp;#39;, ([&amp;] {                                 // 根据特征张量的数据类型进行分发计算\n        `devoxelize_forward_kernel``<`scalar_t`>``<``<``<`N, c`>``>``>`(                               // 调用 CUDA 核函数进行反体素化操作\n            N, c, r, indices.data_ptr`<`int`>`(), weight.data_ptr`<`scalar_t`>`(),\n            feat.data_ptr`<`scalar_t`>`(), out.data_ptr`<`scalar_t`>`());\n      }));\n  return out;\n}\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">torchsparse/torchsparse/backend/devoxelize/devoxelize_cuda.cu</p><span class=\'hidden-code\' data-code=\'template `<`typename scalar_t`>`\n__global__ void devoxelize_forward_kernel(int N, int c, int r, const int *__restrict__ indices, const scalar_t *__restrict__ weight, const scalar_t *__restrict__ feat, scalar_t *__restrict__ out) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x; // 计算当前线程的全局索引\n    int i = index / c;                                 // 计算当前线程处理的索引张量中的索引\n    int j = index % c;                                 // 计算当前线程处理的特征张量中的通道索引\n    int R = r * r * r;                                 // 计算体素大小      3*3*3\n    if (i < N) {                                       // 确保当前线程在范围内\n        const int *indices_ = indices + R * i;         // 获取当前体素的索引\n        const scalar_t *weight_ = weight + R * i;      // 获取当前体素的权重\n        const scalar_t *feat_ = feat + j;              // 获取当前特征张量位置\n        scalar_t cur_feat;                             // 当前特征值\n        for (int k = 0; k < R; k++) {\n            cur_feat = 0;                              // 初始化当前特征值为 0\n            if (indices_[k] >= 0) cur_feat = feat_[indices_[k] * c]; // 如果索引有效，则更新当前特征值\n            out[i * c + j] += weight_[k] * cur_feat;                 // 根据权重和特征值更新输出张量的值\n        }\n    }\n}\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ts2spconv</p><span class=\'hidden-code\' data-code=\'def ts2spconv(st: SparseTensor, sct_save: dict):\n    features = st.feats                                       # torch.Size([137566, 16])\n    indices = torch.index_select(st.coords, 1, torch.LongTensor([3,2,1,0]).to(st.coords.device)).contiguous()\n    sct = spconv.SparseConvTensor(features,indices,\n                spatial_shape=sct_save[&amp;#39;spatial_shape&amp;#39;],      # [41, 1440, 1440]\n                batch_size=sct_save[&amp;#39;batch_size&amp;#39;],\n                grid=sct_save[&amp;#39;grid&amp;#39;],\n                voxel_num=sct_save[&amp;#39;voxel_num&amp;#39;],\n                indice_dict=sct_save[&amp;#39;indice_dict&amp;#39;],\n                benchmark=sct_save[&amp;#39;benchmark&amp;#39;]\n    )\n    sct.benchmark_record = sct_save[&amp;#39;benchmark_record&amp;#39;]\n    return sct\n\'> </span>'}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
