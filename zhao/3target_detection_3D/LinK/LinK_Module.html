<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>LinK_Module</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">TSELKBlock</p><font size="0"><pre class="language-python"><code class="language-python">class TSELKBlock(nn.Module):\n    def forward(self, sct: spconv.SparseConvTensor, stride):\n        st, sct_save = <span style=\'color: green;font-weight: bold;\'>spconv2ts</span>(sct)            <span style=\'color: red\'>[41, 1440, 1440]</span>\n        new_st = self.<span style=\'color: green;font-weight: bold;\'>forward_</span>(st, stride)\n        new_sct = <span style=\'color: green;font-weight: bold;\'>ts2spconv</span>(new_st, sct_save)\n        return new_sct\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">spconv2ts</p><font size="0"><pre class="language-python"><code class="language-python">def spconv2ts(sct: spconv.SparseConvTensor):\n    feats = sct.features                                        <span style=\'color: red\'># torch.Size([137566, 16])</span>\n    coords = torch.index_select(sct.indices, 1,                 <span style=\'color: red\'># torch.Size([137566, 4])->(B,Z,Y,Z)=>(X,Y,Z,B)?</span>\n             torch.LongTensor([3,2,1,0]).to(sct.indices.device)).contiguous()\n    st = SparseTensor(feats, coords, 1)                         <span style=\'color: red\'># 这里将自带的tensor转变到torchsparse的tensor</span>\n    sct_save = dict()\n    sct_save[\'batch_size\'] = sct.batch_size                     <span style=\'color: red\'># 2</span>\n    sct_save[\'benchmark\'] = sct.benchmark                       <span style=\'color: red\'># False</span>\n    sct_save[\'benchmark_record\'] = sct.benchmark_record         <span style=\'color: red\'># {}</span>\n    sct_save[\'grid\'] = sct.grid                                 <span style=\'color: red\'># tensor([])</span>\n    sct_save[\'indice_dict\'] = sct.indice_dict                   <span style=\'color: red\'># {\'res0\': <spconv.pytorch.core...8910bc4f0>}</span>\n    sct_save[\'spatial_shape\'] = sct.spatial_shape               <span style=\'color: red\'># [41, 1440, 1440]</span>\n    sct_save[\'voxel_num\'] = sct.voxel_num                       <span style=\'color: red\'># None</span>\n    return st, sct_save\n</code></pre></font>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">TSELKBlock-forward_</p><font size="0"><pre class="language-python"><code class="language-python">class TSELKBlock(nn.Module):\n    def forward_(self, st: SparseTensor, stride):\n        F_input = self.pre_mix(st.F)       <span style=\'color: red\'># Linear+LayerNorm => st.F=torch.Size([137566, 16]) -> torch.Size([137566, 16])</span>\n        local_mix = self.local_mix(st)     <span style=\'color: red\'># spnn.Conv3d(self.inc, self.inc, kernel_size=3, dilation=1,stride=1)</span>\n        if self.baseop == \'sin\':\n            ......\n        elif self.baseop == \'cos\':\n            pos_weight = self.pos_weight(st.C[:,:3].float())              <span style=\'color: red\'># torch.Size([137566, 4])  坐标用3维变成16维-线性核-》torch.Size([137566, 16])</span>\n            pos_weight = pos_weight[:,:self.inc//2].repeat([1,2])         <span style=\'color: red\'># channel grouping  torch.Size([137566, 8])->torch.Size([137566, 16])沿着第二维重复</span>\n            pos_weight_sin = torch.sin(pos_weight)                        <span style=\'color: red\'># torch.Size([137566, 16])</span>\n            pos_weight_cos = torch.cos(pos_weight)                        <span style=\'color: red\'># torch.Size([137566, 16])</span>\n            F_weighted_sin = F_input*pos_weight_sin                       <span style=\'color: red\'># torch.Size([137566, 16])</span>\n            F_weighted_cos = F_input*pos_weight_cos\n            st.F = torch.cat([F_weighted_cos, F_weighted_sin], dim=1).contiguous()     <span style=\'color: red\'># torch.Size([137566, 32])     K^(0)(a)*f_a+k^(1)(a)*f_a</span>\n            small_st, idx, counts = <span style=\'color: green;font-weight: bold;\'>large_to_small</span>(st, stride=stride)    <span style=\'color: red\'># 应该是7x7下采样得到新的tensor</span>\n            large_st = <span style=\'color: green;font-weight: bold;\'>small_to_large_v2</span>(small_st, st, idx, counts)      <span style=\'color: red\'># torch.Size([137566, 32])</span>\n            new_st_F = large_st.F[:,:self.inc]*pos_weight_cos + large_st.F[:,self.inc:]*pos_weight_sin  <span style=\'color: red\'># self.inc=16; -->torch.Size([137566, 16])</span>\n        elif self.baseop == \'cos_x_alpha\':\n            ......\n        elif self.baseop == \'cos_sin\':\n            ......\n        elif self.baseop == \'x\':\n            ......\n        new_st_F = self.norm(new_st_F)\n        local_F = self.norm_local(local_mix.F)\n        new_st_F = self.activate(new_st_F+local_F)\n        large_st.F = new_st_F\n        return large_st\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">large_to_small</p><font size="0"><pre class="language-python"><code class="language-python"><span style=\'color: red\'># x: SparseTensor->large, stride: scale of kernel </span>\n<span style=\'color: red\'># return : SparseTensor->small</span>\ndef large_to_small(large_x, stride):              <span style=\'color: red\'># torch.Size([137566, 32])，7  分成7快大小s</span>\n    x_C = torch.cat([torch.div(large_x.C[:,:3], stride, rounding_mode=\'floor\').int(), large_x.C[:,3:]], dim=1)   <span style=\'color: red\'># 相当于把x,y,z的位置除以7，可能有不同的点下采样到同一个点  torch.Size([137566, 4])</span>\n    large_x_hash = F.sphash(x_C.to(large_x.F.device))                      <span style=\'color: red\'># torch.Size([137566])   tensor([383018381322703704, 941813131069407809, ...], device=\'cuda:0\')</span>\n    small_x_C = torch.unique(x_C, dim=0)                                   <span style=\'color: red\'># torch.Size([13740, 4])  </span>\n    small_x_hash = F.sphash(small_x_C.to(large_x.F.device))                <span style=\'color: red\'># torch.Size([13740])</span>\n    idx_query = F.sphashquery(large_x_hash, small_x_hash)                  <span style=\'color: red\'># torch.Size([137566])+torch.Size([13740])->torch.Size([137566]) torch.max(idx_query)=13740-1</span>\n    counts = F.spcount(idx_query.int(), len(small_x_hash))                 <span style=\'color: red\'># torch.Size([13740])  每个索引的数量</span>\n    inserted_feat = F.<span style=\'color: green;font-weight: bold;\'>spvoxelize</span>(large_x.F, idx_query, counts)             <span style=\'color: red\'># (torch.Size([137566, 32]),torch.Size([13740, 4]),torch.Size([13740,])) -> torch.Size([13740, 32])</span>\n    small_x = SparseTensor(inserted_feat, small_x_C, stride)\n    small_x.cmaps = large_x.cmaps\n    small_x.kmaps = large_x.kmaps\n    return small_x, idx_query, counts\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">torchsparse/torchsparse/backend/voxelize/voxelize_cpu.cpp</p><font size="0"><pre class="language-cpp"><code class="language-cpp">at::Tensor voxelize_forward_cpu(const at::Tensor inputs, const at::Tensor idx, const at::Tensor counts) {\n    int N = inputs.size(0);                    <span style=\'color: red\'>// 137566</span>\n    int c = inputs.size(1);                    <span style=\'color: red\'>// 32</span>\n    int N1 = counts.size(0);                   <span style=\'color: red\'>// 13740</span>\n    at::Tensor out = torch::zeros({N1, c}, at::device(idx.device()).dtype(at::ScalarType::Float));     <span style=\'color: red\'>// 13740,32</span>\n    for (int i = 0; i < N; i++) {\n        int pos = *(idx.data_ptr<int>() + i);                      <span style=\'color: red\'>// 获取索引张量 idx 中第 i 个位置的值</span>\n        if (*(counts.data_ptr<int>() + pos) == 0) continue;        <span style=\'color: red\'>// 如果计数张量 counts 中 pos 位置的值为 0，则跳过当前循环</span>\n    #pragma omp parallel for\n        for (int j = 0; j < c; j++) {          <span style=\'color: red\'>// 对于每个特征</span>\n            *(out.data_ptr<float>() + pos * c + j) += *(inputs.data_ptr<float>() + i * c + j) / (float)(*(counts.data_ptr<int>() + pos));  //将 inputs 中的值加到 out 中的对应位置上\n        }\n    }\n    return out;\n}\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">small_to_large_v2</p><font size="0"><pre class="language-python"><code class="language-python"><span style=\'color: red\'># F.sphash()是一个函数，用于将一个点云的坐标和特征编码成一个哈希表。这个哈希表存储了点云中每个点的坐标和特征信息，以及这些点在原始点云中的索引。</span>\n<span style=\'color: red\'># 它使用了点云的体素化方法，将空间分成了许多小的体素，每个体素内部存储其内包含的点的信息。</span>\n<span style=\'color: red\'># 这种压缩方式可以大大减少点云的存储空间，并且可以加速点云的相关计算，如点云分割和物体检测。</span>\ndef small_to_large_v2(small_x, large_x, idx, counts):\n    <span style=\'color: red\'># local offsets to index neighbors         [2^3,3]</span>\n    kernel_size = 3                                                            <span style=\'color: red\'># offsets[:3] = tensor([[-1, -1, -1], [ 0, -1, -1], [ 1, -1, -1]])  offsets[13]=(0,0,0)</span>\n    offsets = get_kernel_offsets(kernel_size, 1, 1, device=large_x.F.device)   <span style=\'color: red\'># torch.Size([27, 3])        from torchsparse.nn.utils import get_kernel_offsets  </span>\n    neighbor_hash = F.sphash(small_x.C, offsets)                               <span style=\'color: red\'># 坐标 torch.Size([13740, 4]) + torch.Size([27, 3])  torch.Size([27, 13740])   tensor([[  99013418389022827,...]</span>\n    small_hash = F.sphash(small_x.C.to(large_x.F.device))                      <span style=\'color: red\'># 坐标 torch.Size([13740, 4])->torch.Size([13740])</span>\n    idx_query = F.sphashquery(neighbor_hash, small_hash)                       <span style=\'color: red\'># torch.Size([27, 13740])    </span>\n    idx_query = idx_query.transpose(0,1).contiguous()                          <span style=\'color: red\'># torch.Size([13740, 27])</span>\n    idx_query_flat = idx_query.view(-1)\n    f = torch.cat([small_x.F, torch.ones_like(small_x.F[:,:1]).to(small_x.F.device)], dim=1)   <span style=\'color: red\'># torch.Size([13740, 32])+torch.Size([13740, 1])->torch.Size([13740, 33])</span>\n    f = f*counts.unsqueeze(dim=-1)                                                             <span style=\'color: red\'># torch.Size([13740, 33])</span>\n    weights = torch.ones(small_x.F.shape[0], kernel_size**3).to(small_x.F.device).float()      <span style=\'color: red\'># torch.Size([13740, 27])</span>\n    weights[idx_query == -1] = 0                                               <span style=\'color: red\'># 将本身的坐标</span>\n    new_feat = F.<span style=\'color: green;font-weight: bold;\'>spdevoxelize</span>(f, idx_query, weights, kernel_size)            <span style=\'color: red\'># torch.Size([13740, 33])</span>\n    new_feat = new_feat[:,:-1] / new_feat[:,-1:]                               <span style=\'color: red\'># torch.Size([13740, 32])</span>\n    large_x.F = new_feat[idx]                                                  <span style=\'color: red\'># large_x.F.shape = torch.Size([137566, 32])   idx.shape=torch.Size([137566])</span>\n    return large_x\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">torchsparse/torchsparse/backend/devoxelize/devoxelize_cuda.cu</p><font size="0"><pre class="language-cpp"><code class="language-cpp"><span style=\'color: red\'>// make sure indices is int type     用于在CUDA上执行反体素化（devoxelization）</span>\n<span style=\'color: red\'>// feat: (b,c,s) indices: (N, 3) batch_index: (N, ) -> out: (N, c)</span>\nat::Tensor devoxelize_forward_cuda(const at::Tensor feat, const at::Tensor indices, const at::Tensor weight, int r) {\n  int c = feat.size(1);                     <span style=\'color: red\'>// 33 </span>\n  int N = indices.size(0);                  <span style=\'color: red\'>// 13740</span>\n  at::Tensor out = torch::zeros({N, c}, at::device(feat.device()).dtype(feat.dtype()));      <span style=\'color: red\'>// 13740,33</span>\n  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n      feat.type(), "devoxelize_forward_cuda", ([&] {                                 <span style=\'color: red\'>// 根据特征张量的数据类型进行分发计算</span>\n        <span style=\'color: green;font-weight: bold;\'>devoxelize_forward_kernel</span><scalar_t><<<N, c>>>(                               <span style=\'color: red\'>// 调用 CUDA 核函数进行反体素化操作</span>\n            N, c, r, indices.data_ptr<int>(), weight.data_ptr<scalar_t>(),\n            feat.data_ptr<scalar_t>(), out.data_ptr<scalar_t>());\n      }));\n  return out;\n}\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">torchsparse/torchsparse/backend/devoxelize/devoxelize_cuda.cu</p><font size="0"><pre class="language-cpp"><code class="language-cpp">template <typename scalar_t>\n__global__ void devoxelize_forward_kernel(int N, int c, int r, const int *__restrict__ indices, const scalar_t *__restrict__ weight, const scalar_t *__restrict__ feat, scalar_t *__restrict__ out) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x; <span style=\'color: red\'>// 计算当前线程的全局索引</span>\n    int i = index / c;                                 <span style=\'color: red\'>// 计算当前线程处理的索引张量中的索引</span>\n    int j = index % c;                                 <span style=\'color: red\'>// 计算当前线程处理的特征张量中的通道索引</span>\n    int R = r * r * r;                                 <span style=\'color: red\'>// 计算体素大小      3*3*3</span>\n    if (i < N) {                                       <span style=\'color: red\'>// 确保当前线程在范围内</span>\n        const int *indices_ = indices + R * i;         <span style=\'color: red\'>// 获取当前体素的索引</span>\n        const scalar_t *weight_ = weight + R * i;      <span style=\'color: red\'>// 获取当前体素的权重</span>\n        const scalar_t *feat_ = feat + j;              <span style=\'color: red\'>// 获取当前特征张量位置</span>\n        scalar_t cur_feat;                             <span style=\'color: red\'>// 当前特征值</span>\n        for (int k = 0; k < R; k++) {\n            cur_feat = 0;                              <span style=\'color: red\'>// 初始化当前特征值为 0</span>\n            if (indices_[k] >= 0) cur_feat = feat_[indices_[k] * c]; <span style=\'color: red\'>// 如果索引有效，则更新当前特征值</span>\n            out[i * c + j] += weight_[k] * cur_feat;                 <span style=\'color: red\'>// 根据权重和特征值更新输出张量的值</span>\n        }\n    }\n}\n</code></pre></font>'}]}]}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">ts2spconv</p><font size="0"><pre class="language-python"><code class="language-python">def ts2spconv(st: SparseTensor, sct_save: dict):\n    features = st.feats                                       <span style=\'color: red\'># torch.Size([137566, 16])</span>\n    indices = torch.index_select(st.coords, 1, torch.LongTensor([3,2,1,0]).to(st.coords.device)).contiguous()\n    sct = spconv.SparseConvTensor(features,indices,\n                spatial_shape=sct_save[\'spatial_shape\'],      <span style=\'color: red\'># [41, 1440, 1440]</span>\n                batch_size=sct_save[\'batch_size\'],\n                grid=sct_save[\'grid\'],\n                voxel_num=sct_save[\'voxel_num\'],\n                indice_dict=sct_save[\'indice_dict\'],\n                benchmark=sct_save[\'benchmark\']\n    )\n    sct.benchmark_record = sct_save[\'benchmark_record\']\n    return sct\n</code></pre></font>'}]}]})</script></body>
</html>
