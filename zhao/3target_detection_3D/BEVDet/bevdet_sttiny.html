<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>bevdet_sttiny</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练数据读取流程</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py:class LoadMultiViewImageFromFiles_BEVDet</p>is_train=True,<br>\ndata_config=dict(cams=[\'CAM_FRONT_LEFT\', \'CAM_FRONT\', \'CAM_FRONT_RIGHT\',CAM_BACK_LEFT\', \'CAM_BACK\', \'CAM_BACK_RIGHT\'],<br>\nNcams=6,input_size=(256, 704),src_size=(900, 1600),resize=(-0.06, 0.11),<br>\nrot=(-5.4, 5.4),flip=True,crop_h=(0.0, 0.0),resize_test=0.04)<br>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">def get_inputs</p>\n<p>cams=[\'CAM_FRONT_LEFT\', \'CAM_FRONT\', \'CAM_FRONT_RIGHT\', \'CAM_BACK_LEFT\', \'CAM_BACK\', \'CAM_BACK_RIGHT\']<br>\n<code>=========================for cam in cams======================================</code><br>\nfilename=\'./data/nuscenes/samples/CAM_FRONT_LEFT/n015-2018-07-24-11-13-19+0800__CAM_FRONT_LEFT__1532402060154844.jpg\'<br>\nimg,size()=(1600, 900),rot+tran雷达到相机坐标系，加上相机内参就是雷达到图像坐标系<br>\n输入sample_augmentation：1600，900，None,None<br></p>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">def sample_augmentation</p>fH, fW=(256, 704)；resize=0.44；np.random.uniform(*self.data_config[\'resize\'])=0.03329829566764521<br>\nself.data_config[\'resize\']=(-0.06, 0.11)；resize_dims=(802, 451)；<br>\ncrop_h=195；crop_w=53；crop=(53, 195, 757, 451)<br>\n返回resize, resize_dims, crop, flip, rotate=(0.50..., (802, 451), (53, 195, 757, 451), 1, 1.33...)<br>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">def get_inputs</p>输入img_transform：img+resize, resize_dims, crop, flip, rotate=(0.50..., (802, 451), (53, 195, 757, 451), 1, 1.33...)<br>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">def img_transform</p>输入img_transform_core：img+resize_dims, crop, flip, rotate=((802, 451), (53, 195, 757, 451), 1, 1.33...)<br>\n得到img.size:(704, 256)<br>\npost-homography transformation(矩阵进行数据增强得到的数据):<br>\npost_rot=tensor([[-0.5014,  0.0117], [ 0.0117,  0.5014]]); post_tran=tensor([ 749.3677, -204.3445])<br>\n最后返回imgs, rots(雷达到相机), trans(雷达到相机), intrins(相机内参), post_rots(图像增强), post_trans(图像增强)<br>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">def get_inputs</p>将返回的post_tran2和post_rot2-》post_tran.shape=(3)；post_rot.shape=(3, 3)<br>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/transforms_3d.py:class GlobalRotScaleTrans</p>rot_range=[-0.3925, 0.3925],scale_ratio_range=[0.95, 1.05],translation_std=[0, 0, 0],update_img2lidar=True<br>\ninput_dict[\'transformation_3d_flow\'] = []<br>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/transforms_3d.py:class GlobalRotScaleTrans</p>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">self._rot_bbox_points(input_dict)</p>noise_rotation=0.017150932573806288;rot_mat_T.shape=torch.Size([3, 3])<br>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/transforms_3d.py:class GlobalRotScaleTrans</p>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">def _scale_bbox_points</p>scale=0.9914661939990523;self.shift_height=False<br>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/transforms_3d.py:class GlobalRotScaleTrans</p>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">def update_transform</p>input_dict[\'img_inputs\'][1].shape=torch.Size([6, 3, 3])表示雷达到相机坐标系的旋转<br>\n这部分是变化的<br>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/transforms_3d.py:class RandomFlip3D</p>sync_2d=False,flip_ratio_bev_horizontal=0.5,flip_ratio_bev_vertical=0.5,update_img2lidar=True<br>'}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def forward_train</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def extract_feat</p>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def extract_img_feat</p>[i.shape for i in img]=[(1, 6, 3,,256, 704]),(1, 6, 3, 3),(1, 6, 3), (1, 6, 3, 3), (1, 6, 3, 3),(1, 6, 3)]<br>\n900x1600下采样，后面分别为rots, trans, intrins, post_rots, post_trans<br>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def image_encoder</p>imgs=（6, 3,,256, 704）<br>\nx->[(6, 384, 16, 44),(6, 768, 8, 22)]->(6, 512, 16, 44)->(1, 6, 512, 16, 44)<br>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def extract_img_feat</p>输入self.img_view_transformer为[(1, 6, 512, 16, 44),(1, 6, 3, 3),(1, 6, 3), (1, 6, 3, 3), (1, 6, 3, 3),(1, 6, 3)]<br>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py：class ViewTransformerLiftSplatShoot：def forward</p>\n<p>x-&gt;(1, 6, 512, 16, 44)-&gt;(6, 512, 16, 44)-<code>self.depthnet</code>&gt;(6, 123, 16, 44)<br>\nself.D=59,self.numC_Trans=64相加得到123<br>\ndepth=（6, 59, 16, 44)<br>\nimg_feat=(6, 64, 16, 44)<br>\nvolume=(6, 64, 59, 16, 44)-&gt;(6, 64, 59, 16, 44)-&gt;(1,6,59,16,44,64)<br>\n输入self.voxel_pooling_accelerated：rots, trans, intrins, post_rots, post_trans, volume<br></p>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py：class ViewTransformerLiftSplatShoot：def voxel_pooling_accelerated</p>B, N, D, H, W, C=1,6,59,16,44,64，去掉特征Nprime=249216<br>\nnx=tensor（128，128，1）在点云的范围x坐标在0-128,y坐标在0-128,z坐标在0-1<br>\nx=(249216,64),代表249216个点，每个点的特征维度为64<br>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py：class ViewTransformerLiftSplatShoot：def get_geometry</p>self.frustum=（59, 16, 44, 3）<br>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py：class ViewTransformerLiftSplatShoot：def voxel_pooling_accelerated</p>geom_feats=（1, 6, 59, 16, 44, 3）这里相对于x没有64维特征，加了x,y,z，3个特征,<br>\nself.bx=（-50.8000, -50.8000,   0.0000）；self.dx=（0.8000,  0.8000, 20.0000），<br>\n(self.bx - self.dx / 2.)=-51.2000, -51.2000, -10.0000，也就是x,y,z为0时对应到了前面这个值<br>\nx范围0-128，0-128*0.8=102.4,-51.2-51.2,<br>\n将x, geom_feats, ranks, idx按照坐标排序<br>\nfinal-（1, 64, 128, 128）<br>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def extract_img_feat</p>输入self.bev_encoder为（1, 64, 128, 128）<br>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def bev_encoder</p>输入self.img_bev_encoder_backbone为（1, 64, 128, 128）<br>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/backbones/resnet.py</p>\n<p>len(self.layers)=3,self.backbone_output_ids=(0,1,2)<br>\n<code>==========================lid=0=================================</code><br>\nx_tmp = layer(x_tmp):（1, 64, 128, 128）-&gt;(1, 128, 64, 64)<br>\n<code>==========================lid=1=================================</code><br>\nx_tmp = layer(x_tmp):(1, 128, 64, 64)-&gt;(1, 128, 32, 32)<br>\n<code>==========================lid=2=================================</code><br>\nx_tmp = layer(x_tmp):(1, 128, 32, 32)-&gt;(1, 128, 16, 16)<br>\n<code>----------------------------------------------------------------</code><br>\n[(1, 128, 64, 64),(1, 128, 32, 32),(1, 128, 16, 16)]<br></p>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def bev_encoder</p>输入self.img_bev_encoder_neck为[(1, 128, 64, 64),(1, 128, 32, 32),(1, 128, 16, 16)]<br>\n输出：[1, 256, 128, 128]<br>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/lss_fpn.py</p>'}]}]}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def extract_feat</p>返回：(img_feats, pts_feats)=（1, 256, 128, 128），None<br>'}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py:def forward_train</p>输入self.forward_pts_train:[（1, 256, 128, 128）]，[LiDARInstance3DBoxes()],[tensor([5, 8, ...,]]<br>'}]})</script></body>
</html>
