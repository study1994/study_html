<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>bevdet_sttiny</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练数据读取流程</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><font size="0"><pre class="language-python"><code class="language-python">class LoadMultiViewImageFromFiles_BEVDet(object):\n    def __init__(self, data_config, is_train=False,sequential=False, aligned=False, trans_only=True):\n        <span style=\'color: red\'># is_train=True,</span>\n        <span style=\'color: red\'># data_config=dict(cams=[\'CAM_FRONT_LEFT\', \'CAM_FRONT\', \'CAM_FRONT_RIGHT\',CAM_BACK_LEFT\', \'CAM_BACK\', \'CAM_BACK_RIGHT\'],</span>\n        <span style=\'color: red\'># Ncams=6,input_size=(256, 704),src_size=(900, 1600),resize=(-0.06, 0.11),</span>\n        <span style=\'color: red\'># rot=(-5.4, 5.4),flip=True,crop_h=(0.0, 0.0),resize_test=0.04)</span>\n    def __call__(self, results):\n        results[\'img_inputs\'] = self.<span style=\'color: green;font-weight: bold;\'>get_inputs</span>(results)\n        return results\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><font size="0"><pre class="language-python"><code class="language-python">class LoadMultiViewImageFromFiles_BEVDet(object):\n    def get_inputs(self,results, flip=None, scale=None):\n        imgs = []\n        rots = []\n        trans = []\n        intrins = []\n        post_rots = []\n        post_trans = []\n        cams = self.choose_cams()\n        for cam in cams:                     <span style=\'color: red\'># cams=[\'CAM_FRONT_LEFT\', \'CAM_FRONT\', \'CAM_FRONT_RIGHT\', \'CAM_BACK_LEFT\', \'CAM_BACK\', \'CAM_BACK_RIGHT\']</span>\n            cam_data = results[\'img_info\'][cam]\n            filename = cam_data[\'data_path\'] <span style=\'color: red\'># filename=\'./data/nuscenes/samples/CAM_FRONT_LEFT/n015-2018-07-24-11-13-19+0800__CAM_FRONT_LEFT__1532402060154844.jpg\'</span>\n            img = Image.open(filename)\n            post_rot = torch.eye(2)\n            post_tran = torch.zeros(2)\n            intrin = torch.Tensor(cam_data[\'cam_intrinsic\'])     <span style=\'color: red\'># img,size()=(1600, 900),rot+tran雷达到相机坐标系，加上相机内参就是雷达到图像坐标系</span>\n            rot = torch.Tensor(cam_data[\'sensor2lidar_rotation\'])\n            tran = torch.Tensor(cam_data[\'sensor2lidar_translation\'])\n            <span style=\'color: red\'># augmentation (resize, crop, horizontal flip, rotate)</span>\n            resize, resize_dims, crop, flip, rotate = self.<span style=\'color: green;font-weight: bold;\'>sample_augmentation</span>(H=img.height,W=img.width,flip=flip,scale=scale) <span style=\'color: red\'># sample_augmentation：1600，900，None,None</span>\n            img, post_rot2, post_tran2 = self.<span style=\'color: green;font-weight: bold;\'>img_transform</span>(img, post_rot, post_tran,resize=resize,resize_dims=resize_dims,crop=crop,flip=flip,rotate=rotate)\n            <span style=\'color: red\'># for convenience, make augmentation matrices 3x3  </span>\n            post_tran = torch.zeros(3)\n            post_rot = torch.eye(3)\n            post_tran[:2] = post_tran2\n            post_rot[:2, :2] = post_rot2\n            imgs.append(self.normalize_img(img))\n            if self.sequential:\n                ......\n            intrins.append(intrin)\n            rots.append(rot)\n            trans.append(tran)\n            post_rots.append(post_rot)\n            post_trans.append(post_tran)\n        if self.sequential:\n            ......\n        imgs,rots,trans,intrins,post_rots,post_trans=(torch.stack(imgs),torch.stack(rots),torch.stack(trans),torch.stack(intrins),torch.stack(post_rots),torch.stack(post_trans))\n        return imgs, rots, trans, intrins, post_rots, post_trans\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><font size="0"><pre class="language-python"><code class="language-python">class LoadMultiViewImageFromFiles_BEVDet(object):\n    def sample_augmentation(self, H , W, flip=None, scale=None):\n        fH, fW = self.data_config[\'input_size\'] <span style=\'color: red\'># fH, fW=(256, 704)；</span>\n        if self.is_train:\n            resize = float(fW)/float(W)               <span style=\'color: red\'># resize=0.44；</span>\n            resize += np.random.uniform(*self.data_config[\'resize\'])  <span style=\'color: red\'># np.random.uniform(*self.data_config[\'resize\'])=0.03329829566764521</span>\n            resize_dims = (int(W * resize), int(H * resize))          <span style=\'color: red\'># self.data_config[\'resize\']=(-0.06, 0.11)；resize_dims=(802, 451)；</span>\n            newW, newH = resize_dims\n            crop_h = int((1 - np.random.uniform(*self.data_config[\'crop_h\'])) * newH) - fH\n            crop_w = int(np.random.uniform(0, max(0, newW - fW)))       <span style=\'color: red\'># crop_h=195；crop_w=53；crop=(53, 195, 757, 451)</span>\n            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n            flip = self.data_config[\'flip\'] and np.random.choice([0, 1])\n            rotate = np.random.uniform(*self.data_config[\'rot\'])      \n        else:\n            resize = float(fW)/float(W)\n            resize += self.data_config.get(\'resize_test\', 0.0)\n            if scale is not None:\n                resize = scale\n            resize_dims = (int(W * resize), int(H * resize))\n            newW, newH = resize_dims\n            crop_h = int((1 - np.mean(self.data_config[\'crop_h\'])) * newH) - fH\n            crop_w = int(max(0, newW - fW) / 2)\n            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n            flip = False if flip is None else flip\n            rotate = 0\n        return resize, resize_dims, crop, flip, rotate       <span style=\'color: red\'># (0.50..., (802, 451), (53, 195, 757, 451), 1, 1.33...)</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><font size="0"><pre class="language-python"><code class="language-python">class LoadMultiViewImageFromFiles_BEVDet(object):\n    def img_transform:\n        <span style=\'color: red\'># 输入img_transform_core：img+resize_dims, crop, flip, rotate=((802, 451), (53, 195, 757, 451), 1, 1.33...)</span>\n        <span style=\'color: red\'># 得到img.size:(704, 256)</span>\n        <span style=\'color: red\'># post-homography transformation(矩阵进行数据增强得到的数据):</span>\n        <span style=\'color: red\'># post_rot=tensor([[-0.5014,  0.0117], [ 0.0117,  0.5014]]); post_tran=tensor([ 749.3677, -204.3445])</span>\n        <span style=\'color: red\'># 最后返回imgs, rots(雷达到相机), trans(雷达到相机), intrins(相机内参), post_rots(图像增强), post_trans(图像增强)</span>\n</code></pre></font>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py：初始化</p><font size="0"><pre class="language-python"><code class="language-python">class ViewTransformerLiftSplatShoot(BaseModule):\n    def __init__(self, grid_config=None, data_config=None,numC_input=512, numC_Trans=64, downsample=16,\n                 accelerate=False, max_drop_point_rate=0.0, use_bev_pool=True,**kwargs):\n        super(ViewTransformerLiftSplatShoot, self).__init__()\n        if grid_config is None:\n            grid_config = {\n                \'xbound\': [-51.2, 51.2, 0.8],\n                \'ybound\': [-51.2, 51.2, 0.8],\n                \'zbound\': [-10.0, 10.0, 20.0],\n                \'dbound\': [1.0, 60.0, 1.0],}\n        self.grid_config = grid_config\n        dx, bx, nx = gen_dx_bx(self.grid_config[\'xbound\'],self.grid_config[\'ybound\'],self.grid_config[\'zbound\'])\n        self.dx = nn.Parameter(dx, requires_grad=False)\n        self.bx = nn.Parameter(bx, requires_grad=False)\n        self.nx = nn.Parameter(nx, requires_grad=False)\n        if data_config is None:\n            data_config = {\'input_size\': (256, 704)}\n        self.data_config = data_config\n        self.downsample = downsample\n        self.frustum = self.create_frustum()\n        self.D, _, _, _ = self.frustum.shape\n        self.numC_input = numC_input\n        self.numC_Trans = numC_Trans\n        self.depthnet = nn.Conv2d(self.numC_input, self.D + self.numC_Trans, kernel_size=1, padding=0)\n        self.geom_feats = None\n        self.accelerate = accelerate\n        self.max_drop_point_rate = max_drop_point_rate\n        self.use_bev_pool = use_bev_pool\n</code></pre></font>'}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py：训练流程</p><font size="0"><pre class="language-python"><code class="language-python">class BEVDet(CenterPoint):\n     def forward_train(self,points,img_metas,gt_bboxes_3d,gt_labels_3d,gt_labels,gt_bboxes,img_inputs,proposals,gt_bboxes_ignore):\n        img_feats, pts_feats = self.<span style=\'color: green;font-weight: bold;\'>extract_feat</span>(points, img=img_inputs, img_metas=img_metas)\n        assert self.with_pts_bbox\n        losses = dict()\n        losses_pts = self.<span style=\'color: green;font-weight: bold;\'>forward_pts_train</span>(img_feats, gt_bboxes_3d,gt_labels_3d, img_metas,gt_bboxes_ignore)\n        losses.update(losses_pts)\n        return losses\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py</p><font size="0"><pre class="language-python"><code class="language-python">class BEVDet(CenterPoint):\n    def extract_img_feat(self, img, img_metas):\n        """Extract features of images."""\n        x = self.<span style=\'color: green;font-weight: bold;\'>image_encoder</span>(img[0])              <span style=\'color: red\'># [i.shape for i in img]=[(1, 6, 3, 256, 704]),(1, 6, 3, 3),(1, 6, 3), (1, 6, 3, 3), (1, 6, 3, 3),(1, 6, 3)]  704,256为输入到网络当中的大小</span>\n        x = self.<span style=\'color: green;font-weight: bold;\'>img_view_transformer</span>([x] + img[1:])# 900x1600下采样，后面分别为rots, trans, intrins, post_rots, post_trans\n        x = self.<span style=\'color: green;font-weight: bold;\'>bev_encoder</span>(x)\n        return [x]\n    def extract_feat(self, points, img, img_metas):\n        """Extract features from images and points."""\n        img_feats = self.<span style=\'color: green;font-weight: bold;\'>extract_img_feat</span>(img, img_metas)\n        pts_feats = None\n        return (img_feats, pts_feats)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py</p><font size="0"><pre class="language-python"><code class="language-python">class BEVDet(CenterPoint):\n    def image_encoder(self,img):\n        imgs = img\n        B, N, C, imH, imW = imgs.shape        <span style=\'color: red\'># (1,6,3,256,704）                           6张图片，704,256为输入到网络当中的大小</span>\n        imgs = imgs.view(B * N, C, imH, imW)  \n        x = self.<span style=\'color: green;font-weight: bold;\'>img_backbone</span>(imgs)         <span style=\'color: red\'># [(6xbn, 384, 16, 44),(6xbn, 768, 8, 22)]  下采样8倍和16倍</span>\n        if self.with_img_neck:\n            x = self.<span style=\'color: green;font-weight: bold;\'>img_neck</span>(x)            <span style=\'color: red\'># (6xbn, 512, 16, 44)                       下采样8倍</span>\n        _, output_dim, ouput_H, output_W = x.shape\n        x = x.view(B, N, output_dim, ouput_H, output_W)     <span style=\'color: red\'># (1, 6, 512, 16, 44)         16x44为网络输出大小</span>\n        return x\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class ViewTransformerLiftSplatShoot(BaseModule):\n    def forward(self, input):\n        x, rots, trans, intrins, post_rots, post_trans = input  <span style=\'color: red\'># [(1, 6, 512, 16, 44),(1, 6, 3, 3),(1, 6, 3), (1, 6, 3, 3), (1, 6, 3, 3),(1, 6, 3)]</span>\n        B, N, C, H, W = x.shape         <span style=\'color: red\'># (1, 6, 512, 16, 44)  </span>\n        x = x.view(B * N, C, H, W)      <span style=\'color: red\'># (6*1, 512, 16, 44)</span>\n        ##########################################################################################################\n        <span style=\'color: red\'># 类似与bevdeep里面的 def get_cam_feats(self, x, d)    </span>\n        x = self.<span style=\'color: green;font-weight: bold;\'>depthnet</span>(x)          <span style=\'color: red\'># (6, 123, 16, 44)    </span>\n        <span style=\'color: red\'># self.depthnet = nn.Conv2d(self.numC_input, self.D + self.numC_Trans, kernel_size=1, padding=0)</span>\n        depth = self.<span style=\'color: green;font-weight: bold;\'>get_depth_dist</span>(x[:, :self.D])           <span style=\'color: red\'># x.softmax(dim=1)->（6, 59, 16, 44)  59是由上面\'dbound\': [1.0, 60.0, 1.0]来的</span>\n        img_feat = x[:, self.D:(self.D + self.numC_Trans)]     <span style=\'color: red\'># (6, 64, 16, 44)</span>\n        <span style=\'color: red\'># Lift</span>\n        volume = depth.unsqueeze(1) * img_feat.unsqueeze(2)        <span style=\'color: red\'># (6, 64, 59, 16, 44)</span>\n        volume = volume.view(B, N, self.numC_Trans, self.D, H, W)  <span style=\'color: red\'># (1，6, 64, 59, 16, 44)</span>\n        volume = volume.permute(0, 1, 3, 4, 5, 2)                  <span style=\'color: red\'># (1, 6, 59, 16, 44, 64)</span>\n        ##########################################################################################################\n        <span style=\'color: red\'># Splat</span>\n        if self.accelerate:               <span style=\'color: red\'># 这个训练出来有问题？？？？</span>\n            bev_feat = self.<span style=\'color: green;font-weight: bold;\'>voxel_pooling_accelerated</span>(rots, trans, intrins, post_rots, post_trans, volume)\n        else:\n            geom = self.get_geometry(rots, trans, intrins, post_rots, post_trans)\n            bev_feat = self.<span style=\'color: green;font-weight: bold;\'>voxel_pooling</span>(geom, volume)\n        return bev_feat\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class ViewTransformerLiftSplatShoot(BaseModule)：\n    def voxel_pooling_accelerated(self, rots, trans, intrins, post_rots, post_trans, x):\n        B, N, D, H, W, C = x.shape\n        Nprime = B * N * D * H * W     <span style=\'color: red\'># B, N, D, H, W, C=1,6,59,16,44,64，去掉特征Nprime=249216;  就是有这么多个点</span>\n        nx = self.nx.to(torch.long)    <span style=\'color: red\'># nx=tensor（128，128，1）在点云的范围x坐标在0-128,y坐标在0-128,z坐标在0-1</span>\n        <span style=\'color: red\'># flatten x</span>\n        x = x.reshape(Nprime, C)       <span style=\'color: red\'># x=(249216,64),代表249216个点，每个点的特征维度为64</span>\n        max = 300\n        <span style=\'color: red\'># flatten indices</span>\n        if self.geom_feats is None:\n            geom_feats = self.<span style=\'color: green;font-weight: bold;\'>get_geometry</span>(rots, trans, intrins, post_rots, post_trans)   <span style=\'color: red\'># 得到原先2D的坐标点的位置在车身坐标系下的3D位置 </span>\n            geom_feats = ((geom_feats - (self.bx - self.dx / 2.)) / self.dx).long()         <span style=\'color: red\'># self.bx=（-50.8000,-50.8000,0.0000）self.dx=（0.8000,0.8000,20.0000）</span>\n            <span style=\'color: red\'># (self.bx - self.dx / 2.)=-51.2000, -51.2000, -10.0000，也就是x,y,z为0时对应到了前面这个值</span>\n            geom_feats = geom_feats.view(Nprime, 3)\n            batch_ix = torch.cat([torch.full([Nprime <span style=\'color: red\'>// B, 1], ix, device=x.device, dtype=torch.long) for ix in range(B)])</span>\n            geom_feats = torch.cat((geom_feats, batch_ix), 1)\n            <span style=\'color: red\'># filter out points that are outside box</span>\n            kept1 = (geom_feats[:, 0] >= 0) & (geom_feats[:, 0] < self.nx[0]) \\\n                    & (geom_feats[:, 1] >= 0) & (geom_feats[:, 1] < self.nx[1]) \\\n                    & (geom_feats[:, 2] >= 0) & (geom_feats[:, 2] < self.nx[2])\n            idx = torch.range(0, x.shape[0] - 1, dtype=torch.long)\n            x = x[kept1]\n            idx = idx[kept1]\n            geom_feats = geom_feats[kept1]\n            <span style=\'color: red\'># get tensors from the same voxel next to each other 将所有的feature基于坐标位置进行排序，在俯视图上相同坐标的feature的ranks值相同</span>\n            ranks = geom_feats[:, 0] * (self.nx[1] * self.nx[2] * B) \\\n                    + geom_feats[:, 1] * (self.nx[2] * B) \\\n                    + geom_feats[:, 2] * B \\\n                    + geom_feats[:, 3]\n            sorts = ranks.argsort()\n            x, geom_feats, ranks, idx = x[sorts], geom_feats[sorts], ranks[sorts], idx[sorts]\n            repeat_id = torch.ones(geom_feats.shape[0], device=geom_feats.device, dtype=geom_feats.dtype)   <span style=\'color: red\'># 假设剩下5000个点</span>\n            curr = 0\n            repeat_id[0] = 0\n            curr_rank = ranks[0]\n            for i in range(1, ranks.shape[0]):   <span style=\'color: red\'># 多个图像点会放到相同的bev位置上</span>\n                if curr_rank == ranks[i]:\n                    curr += 1\n                    repeat_id[i] = curr\n                else:\n                    curr_rank = ranks[i]\n                    curr = 0\n                    repeat_id[i] = curr\n            kept2 = repeat_id < max\n            repeat_id, geom_feats, x, idx = repeat_id[kept2], geom_feats[kept2], x[kept2], idx[kept2]\n            geom_feats = torch.cat([geom_feats, repeat_id.unsqueeze(-1)], dim=-1)\n            self.geom_feats = geom_feats\n            self.idx = idx\n        else:\n            geom_feats = self.geom_feats\n            idx = self.idx\n            x = x[idx]\n        <span style=\'color: red\'># griddify (B x C x Z x X x Y)</span>\n        final = torch.zeros((B, C, nx[2], nx[1], nx[0], max), device=x.device)                                   <span style=\'color: red\'># C这里是64维特征，每个点的特征</span>\n        final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 1], geom_feats[:, 0], geom_feats[:, 4]] = x   <span style=\'color: red\'># 一个bev特征上的点，最多放max个图像点；geom_feats[:, 4]第四维度是重复的id</span>\n        final = final.sum(-1)\n        <span style=\'color: red\'># collapse Z</span>\n        final = torch.cat(final.unbind(dim=2), 1)\n        return final\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class ViewTransformerLiftSplatShoot(BaseModule):\n    def create_frustum(self):\n        <span style=\'color: red\'># make grid in image plane</span>\n        ogfH, ogfW = self.data_config[\'input_size\']                       <span style=\'color: red\'># 模型输入到网络的大小</span>\n        fH, fW = ogfH // self.downsample, ogfW // self.downsample         <span style=\'color: red\'># 特征提取后大小</span>\n        ds = torch.arange(*self.grid_config[\'dbound\'], dtype=torch.float).view(-1, 1, 1).expand(-1, fH, fW)  <span style=\'color: red\'># 1m到60m，1m为间隔生成深度信息</span>\n        D, _, _ = ds.shape\n        xs = torch.linspace(0, ogfW - 1, fW, dtype=torch.float).view(1, 1, fW).expand(D, fH, fW)             <span style=\'color: red\'># 输入到网络原始图片的值0->ofFW取fw个值</span>\n        ys = torch.linspace(0, ogfH - 1, fH, dtype=torch.float).view(1, fH, 1).expand(D, fH, fW)\n        <span style=\'color: red\'># D x H x W x 3  可以认为生成了41xHxW个点，每个点的坐标为(xs,ys,ds),(xs/ds,ys/ds)为图片上的点，而乘以内参k得到相机坐标系下的点</span>\n        <span style=\'color: red\'># </span>\n        frustum = torch.stack((xs, ys, ds), -1)\n        return nn.Parameter(frustum, requires_grad=False)\n    def get_geometry(self, rots, trans, intrins, post_rots, post_trans, offset=None):\n        """Determine the (x,y,z) locations (in the ego frame) of the points in the point cloud.\n        Returns B x N x D x H/downsample x W/downsample x 3    N=6张图片，D为每个像素深度的点数，这里59是由上面\'dbound\': [1.0, 60.0, 1.0]为59 3是x,y,z坐标值\n        """\n        B, N, _ = trans.shape\n        <span style=\'color: red\'># undo post-transformation</span>\n        <span style=\'color: red\'># B x N x D x H x W x 3  post_trans和post_rots为图像增强中使用到的仿射变换参数，因为此处要对视锥中的对应点做同样变换</span>\n        points = self.<span style=\'color: green;font-weight: bold;\'>frustum</span> - post_trans.view(B, N, 1, 1, 1, 3)\n        if offset is not None:\n            _,D,H,W = offset.shape\n            points[:,:,:,:,:,2] = points[:,:,:,:,:,2]+offset.view(B,N,D,H,W)\n        points = torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3).matmul(points.unsqueeze(-1))\n        <span style=\'color: red\'># cam_to_ego 像素坐标系->相机坐标系->车身坐标系</span>\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],\n                            points[:, :, :, :, :, 2:3]\n                            ), 5)\n        if intrins.shape[3]==4:                          <span style=\'color: red\'># for KITTI</span>\n            shift = intrins[:,:,:3,3]\n            points  = points - shift.view(B,N,1,1,1,3,1)\n            intrins = intrins[:,:,:3,:3]\n        combine = rots.matmul(torch.inverse(intrins))\n        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)\n        points += trans.view(B, N, 1, 1, 1, 3)\n        <span style=\'color: red\'># points_numpy = points.detach().cpu().numpy()</span>\n        return points                     <span style=\'color: red\'># 得到原先2D的坐标点的位置在车身坐标系下的3D位置</span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class ViewTransformerLiftSplatShoot(BaseModule):\n    def voxel_pooling(self, geom_feats, x):\n        B, N, D, H, W, C = x.shape\n        Nprime = B * N * D * H * W\n        nx = self.nx.to(torch.long)\n        <span style=\'color: red\'># flatten x</span>\n        x = x.reshape(Nprime, C)\n        <span style=\'color: red\'># flatten indices</span>\n        geom_feats = ((geom_feats - (self.bx - self.dx / 2.)) / self.dx).long()\n        geom_feats = geom_feats.view(Nprime, 3)\n        batch_ix = torch.cat([torch.full([Nprime <span style=\'color: red\'>// B, 1], ix,evice=x.device, dtype=torch.long) for ix in range(B)])</span>\n        geom_feats = torch.cat((geom_feats, batch_ix), 1)\n        <span style=\'color: red\'># filter out points that are outside box</span>\n        kept = (geom_feats[:, 0] >= 0) & (geom_feats[:, 0] < self.nx[0]) \\\n               & (geom_feats[:, 1] >= 0) & (geom_feats[:, 1] < self.nx[1]) \\\n               & (geom_feats[:, 2] >= 0) & (geom_feats[:, 2] < self.nx[2])\n        x = x[kept]\n        geom_feats = geom_feats[kept]\n        if self.max_drop_point_rate > 0.0 and self.training:\n            drop_point_rate = torch.rand(1)*self.max_drop_point_rate\n            kept = torch.rand(x.shape[0])>drop_point_rate\n            x, geom_feats = x[kept], geom_feats[kept]\n        if self.use_bev_pool:\n            final = bev_pool.bev_pool(x, geom_feats, B, self.nx[2], self.nx[0],self.nx[1])\n            final = final.transpose(dim0=-2, dim1=-1)\n        else:\n            <span style=\'color: red\'># get tensors from the same voxel next to each other</span>\n            ranks = geom_feats[:, 0] * (self.nx[1] * self.nx[2] * B) \\\n                    + geom_feats[:, 1] * (self.nx[2] * B) \\\n                    + geom_feats[:, 2] * B \\\n                    + geom_feats[:, 3]\n            sorts = ranks.argsort()\n            x, geom_feats, ranks = x[sorts], geom_feats[sorts], ranks[sorts]\n            <span style=\'color: red\'># cumsum trick</span>\n            x, geom_feats = QuickCumsum.apply(x, geom_feats, ranks)\n            <span style=\'color: red\'># griddify (B x C x Z x X x Y)</span>\n            final = torch.zeros((B, C, nx[2], nx[1], nx[0]), device=x.device)\n            final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 1], geom_feats[:, 0]] = x\n        <span style=\'color: red\'># collapse Z</span>\n        final = torch.cat(final.unbind(dim=2), 1)\n        return final\n</code></pre></font>'}]}]}]}]})</script></body>
</html>
