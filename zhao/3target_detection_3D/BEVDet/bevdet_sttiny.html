<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>bevdet_sttiny</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练数据读取流程</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><span class=\'hidden-code\' data-code=\'class LoadMultiViewImageFromFiles_BEVDet(object):\n    def __init__(self, data_config, is_train=False,sequential=False, aligned=False, trans_only=True):\n        # is_train=True,\n        # data_config=dict(cams=[\'CAM_FRONT_LEFT\', \'CAM_FRONT\', \'CAM_FRONT_RIGHT\',CAM_BACK_LEFT\', \'CAM_BACK\', \'CAM_BACK_RIGHT\'],\n        # Ncams=6,input_size=(256, 704),src_size=(900, 1600),resize=(-0.06, 0.11),\n        # rot=(-5.4, 5.4),flip=True,crop_h=(0.0, 0.0),resize_test=0.04)\n    def __call__(self, results):\n        results[\'img_inputs\'] = self.`get_inputs`(results)\n        return results\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><span class=\'hidden-code\' data-code=\'class LoadMultiViewImageFromFiles_BEVDet(object):\n    def get_inputs(self,results, flip=None, scale=None):\n        imgs = []\n        rots = []\n        trans = []\n        intrins = []\n        post_rots = []\n        post_trans = []\n        cams = self.choose_cams()\n        for cam in cams:                     # cams=[\'CAM_FRONT_LEFT\', \'CAM_FRONT\', \'CAM_FRONT_RIGHT\', \'CAM_BACK_LEFT\', \'CAM_BACK\', \'CAM_BACK_RIGHT\']\n            cam_data = results[\'img_info\'][cam]\n            filename = cam_data[\'data_path\'] # filename=\'./data/nuscenes/samples/CAM_FRONT_LEFT/n015-2018-07-24-11-13-19+0800__CAM_FRONT_LEFT__1532402060154844.jpg\'\n            img = Image.open(filename)\n            post_rot = torch.eye(2)\n            post_tran = torch.zeros(2)\n            intrin = torch.Tensor(cam_data[\'cam_intrinsic\'])     # img,size()=(1600, 900),rot+tran雷达到相机坐标系，加上相机内参就是雷达到图像坐标系\n            rot = torch.Tensor(cam_data[\'sensor2lidar_rotation\'])\n            tran = torch.Tensor(cam_data[\'sensor2lidar_translation\'])\n            # augmentation (resize, crop, horizontal flip, rotate)\n            resize, resize_dims, crop, flip, rotate = self.`sample_augmentation`(H=img.height,W=img.width,flip=flip,scale=scale) # sample_augmentation：1600，900，None,None\n            img, post_rot2, post_tran2 = self.`img_transform`(img, post_rot, post_tran,resize=resize,resize_dims=resize_dims,crop=crop,flip=flip,rotate=rotate)\n            # for convenience, make augmentation matrices 3x3   # img+resize, resize_dims, crop, flip, rotate=(0.50..., (802, 451), (53, 195, 757, 451), 1, 1.33...)\n            post_tran = torch.zeros(3)\n            post_rot = torch.eye(3)\n            post_tran[:2] = post_tran2\n            post_rot[:2, :2] = post_rot2\n            imgs.append(self.normalize_img(img))\n            if self.sequential:\n                ......\n            intrins.append(intrin)\n            rots.append(rot)\n            trans.append(tran)\n            post_rots.append(post_rot)\n            post_trans.append(post_tran)\n        if self.sequential:\n            ......\n        imgs,rots,trans,intrins,post_rots,post_trans=(torch.stack(imgs),torch.stack(rots),torch.stack(trans),torch.stack(intrins),torch.stack(post_rots),torch.stack(post_trans))\n        return imgs, rots, trans, intrins, post_rots, post_trans\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><span class=\'hidden-code\' data-code=\'class LoadMultiViewImageFromFiles_BEVDet(object):\n    def sample_augmentation(self, H , W, flip=None, scale=None):\n        fH, fW = self.data_config[\'input_size\'] # fH, fW=(256, 704)；\n        if self.is_train:\n            resize = float(fW)/float(W)               # resize=0.44；\n            resize += np.random.uniform(*self.data_config[\'resize\'])  # np.random.uniform(*self.data_config[\'resize\'])=0.03329829566764521\n            resize_dims = (int(W * resize), int(H * resize))          # self.data_config[\'resize\']=(-0.06, 0.11)；resize_dims=(802, 451)；\n            newW, newH = resize_dims\n            crop_h = int((1 - np.random.uniform(*self.data_config[\'crop_h\'])) * newH) - fH\n            crop_w = int(np.random.uniform(0, max(0, newW - fW)))       # crop_h=195；crop_w=53；crop=(53, 195, 757, 451)\n            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n            flip = self.data_config[\'flip\'] and np.random.choice([0, 1])\n            rotate = np.random.uniform(*self.data_config[\'rot\'])      \n        else:\n            resize = float(fW)/float(W)\n            resize += self.data_config.get(\'resize_test\', 0.0)\n            if scale is not None:\n                resize = scale\n            resize_dims = (int(W * resize), int(H * resize))\n            newW, newH = resize_dims\n            crop_h = int((1 - np.mean(self.data_config[\'crop_h\'])) * newH) - fH\n            crop_w = int(max(0, newW - fW) / 2)\n            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n            flip = False if flip is None else flip\n            rotate = 0\n        return resize, resize_dims, crop, flip, rotate       # (0.50..., (802, 451), (53, 195, 757, 451), 1, 1.33...)\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/pipelines/loading.py</p><span class=\'hidden-code\' data-code=\'class LoadMultiViewImageFromFiles_BEVDet(object):\n    def img_transform:\n        # 输入img_transform_core：img+resize_dims, crop, flip, rotate=((802, 451), (53, 195, 757, 451), 1, 1.33...)\n        # 得到img.size:(704, 256)\n        # post-homography transformation(矩阵进行数据增强得到的数据):\n        # post_rot=tensor([[-0.5014,  0.0117], [ 0.0117,  0.5014]]); post_tran=tensor([ 749.3677, -204.3445])\n        # 最后返回imgs, rots(雷达到相机), trans(雷达到相机), intrins(相机内参), post_rots(图像增强), post_trans(图像增强)\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py：初始化</p><span class=\'hidden-code\' data-code=\'class ViewTransformerLiftSplatShoot(BaseModule):\n    def __init__(self, grid_config=None, data_config=None,numC_input=512, numC_Trans=64, downsample=16,\n                 accelerate=False, max_drop_point_rate=0.0, use_bev_pool=True,**kwargs):\n        super(ViewTransformerLiftSplatShoot, self).__init__()\n        if grid_config is None:\n            grid_config = {\n                \'xbound\': [-51.2, 51.2, 0.8],\n                \'ybound\': [-51.2, 51.2, 0.8],\n                \'zbound\': [-10.0, 10.0, 20.0],\n                \'dbound\': [1.0, 60.0, 1.0],}\n        self.grid_config = grid_config\n        dx, bx, nx = gen_dx_bx(self.grid_config[\'xbound\'],self.grid_config[\'ybound\'],self.grid_config[\'zbound\'])\n        self.dx = nn.Parameter(dx, requires_grad=False)\n        self.bx = nn.Parameter(bx, requires_grad=False)\n        self.nx = nn.Parameter(nx, requires_grad=False)\n        if data_config is None:\n            data_config = {\'input_size\': (256, 704)}\n        self.data_config = data_config\n        self.downsample = downsample\n        self.frustum = self.create_frustum()\n        self.D, _, _, _ = self.frustum.shape\n        self.numC_input = numC_input\n        self.numC_Trans = numC_Trans\n        self.depthnet = nn.Conv2d(self.numC_input, self.D + self.numC_Trans, kernel_size=1, padding=0)\n        self.geom_feats = None\n        self.accelerate = accelerate\n        self.max_drop_point_rate = max_drop_point_rate\n        self.use_bev_pool = use_bev_pool\n\'> </span>'}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py：训练流程</p><span class=\'hidden-code\' data-code=\'class BEVDet(CenterPoint):\n     def forward_train(self,points,img_metas,gt_bboxes_3d,gt_labels_3d,gt_labels,gt_bboxes,img_inputs,proposals,gt_bboxes_ignore):\n        img_feats, pts_feats = self.`extract_feat`(points, img=img_inputs, img_metas=img_metas)\n        assert self.with_pts_bbox\n        losses = dict()\n        losses_pts = self.`forward_pts_train`(img_feats, gt_bboxes_3d,gt_labels_3d, img_metas,gt_bboxes_ignore)\n        losses.update(losses_pts)\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py</p><span class=\'hidden-code\' data-code=\'class BEVDet(CenterPoint):\n    def extract_img_feat(self, img, img_metas):\n        """Extract features of images."""\n        x = self.`image_encoder`(img[0])              # [i.shape for i in img]=[(1, 6, 3, 256, 704]),(1, 6, 3, 3),(1, 6, 3), (1, 6, 3, 3), (1, 6, 3, 3),(1, 6, 3)]  704,256为输入到网络当中的大小\n        x = self.`img_view_transformer`([x] + img[1:])# 900x1600下采样，后面分别为rots, trans, intrins, post_rots, post_trans\n        x = self.`bev_encoder`(x)\n        return [x]\n    def extract_feat(self, points, img, img_metas):\n        """Extract features from images and points."""\n        img_feats = self.`extract_img_feat`(img, img_metas)\n        pts_feats = None\n        return (img_feats, pts_feats)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/bevdet.py</p><span class=\'hidden-code\' data-code=\'class BEVDet(CenterPoint):\n    def image_encoder(self,img):\n        imgs = img\n        B, N, C, imH, imW = imgs.shape        # (1,6,3,256,704）                           6张图片，704,256为输入到网络当中的大小\n        imgs = imgs.view(B * N, C, imH, imW)  \n        x = self.`img_backbone`(imgs)         # [(6xbn, 384, 16, 44),(6xbn, 768, 8, 22)]  下采样8倍和16倍\n        if self.with_img_neck:\n            x = self.`img_neck`(x)            # (6xbn, 512, 16, 44)                       下采样8倍\n        _, output_dim, ouput_H, output_W = x.shape\n        x = x.view(B, N, output_dim, ouput_H, output_W)     # (1, 6, 512, 16, 44)         16x44为网络输出大小\n        return x\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><span class=\'hidden-code\' data-code=\'class ViewTransformerLiftSplatShoot(BaseModule):\n    def forward(self, input):\n        x, rots, trans, intrins, post_rots, post_trans = input  # [(1, 6, 512, 16, 44),(1, 6, 3, 3),(1, 6, 3), (1, 6, 3, 3), (1, 6, 3, 3),(1, 6, 3)]\n        B, N, C, H, W = x.shape         # (1, 6, 512, 16, 44)   # (B, N, output_dim, ouput_H, output_W)\n        x = x.view(B * N, C, H, W)      # (6*1, 512, 16, 44)\n        ##########################################################################################################\n        # 类似与bevdeep里面的 def get_cam_feats(self, x, d)     # 相当于上面通过一个卷积得到深度图（6, 59, 16, 44)和特征(6, 64, 16, 44)  其中16x44是网络输出大小\n        x = self.`depthnet`(x)          # (6, 123, 16, 44)     # self.D=59,self.numC_Trans=64相加得到123;深度值与每个点的特征\n        # self.depthnet = nn.Conv2d(self.numC_input, self.D + self.numC_Trans, kernel_size=1, padding=0)\n        depth = self.`get_depth_dist`(x[:, :self.D])           # x.softmax(dim=1)->（6, 59, 16, 44)  59是由上面\'dbound\': [1.0, 60.0, 1.0]来的\n        img_feat = x[:, self.D:(self.D + self.numC_Trans)]     # (6, 64, 16, 44)\n        # Lift\n        volume = depth.unsqueeze(1) * img_feat.unsqueeze(2)        # (6, 64, 59, 16, 44)\n        volume = volume.view(B, N, self.numC_Trans, self.D, H, W)  # (1，6, 64, 59, 16, 44)\n        volume = volume.permute(0, 1, 3, 4, 5, 2)                  # (1, 6, 59, 16, 44, 64)\n        ##########################################################################################################\n        # Splat\n        if self.accelerate:               # 这个训练出来有问题？？？？\n            bev_feat = self.`voxel_pooling_accelerated`(rots, trans, intrins, post_rots, post_trans, volume)\n        else:\n            geom = self.get_geometry(rots, trans, intrins, post_rots, post_trans)\n            bev_feat = self.`voxel_pooling`(geom, volume)\n        return bev_feat\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><span class=\'hidden-code\' data-code=\'class ViewTransformerLiftSplatShoot(BaseModule)：\n    def voxel_pooling_accelerated(self, rots, trans, intrins, post_rots, post_trans, x):\n        B, N, D, H, W, C = x.shape\n        Nprime = B * N * D * H * W     # B, N, D, H, W, C=1,6,59,16,44,64，去掉特征Nprime=249216;  就是有这么多个点\n        nx = self.nx.to(torch.long)    # nx=tensor（128，128，1）在点云的范围x坐标在0-128,y坐标在0-128,z坐标在0-1\n        # flatten x\n        x = x.reshape(Nprime, C)       # x=(249216,64),代表249216个点，每个点的特征维度为64\n        max = 300\n        # flatten indices\n        if self.geom_feats is None:\n            geom_feats = self.`get_geometry`(rots, trans, intrins, post_rots, post_trans)   # 得到原先2D的坐标点的位置在车身坐标系下的3D位置  # 也就是我们知道图片每个像素点的深度，就能得到是否能在bev范围内\n            geom_feats = ((geom_feats - (self.bx - self.dx / 2.)) / self.dx).long()         # self.bx=（-50.8000,-50.8000,0.0000）self.dx=（0.8000,0.8000,20.0000）\n            # (self.bx - self.dx / 2.)=-51.2000, -51.2000, -10.0000，也就是x,y,z为0时对应到了前面这个值\n            geom_feats = geom_feats.view(Nprime, 3)\n            batch_ix = torch.cat([torch.full([Nprime // B, 1], ix, device=x.device, dtype=torch.long) for ix in range(B)])\n            geom_feats = torch.cat((geom_feats, batch_ix), 1)\n            # filter out points that are outside box\n            kept1 = (geom_feats[:, 0] `>`= 0) & (geom_feats[:, 0] `<` self.nx[0]) \\\n                    & (geom_feats[:, 1] `>`= 0) & (geom_feats[:, 1] `<` self.nx[1]) \\\n                    & (geom_feats[:, 2] `>`= 0) & (geom_feats[:, 2] `<` self.nx[2])\n            idx = torch.range(0, x.shape[0] - 1, dtype=torch.long)\n            x = x[kept1]\n            idx = idx[kept1]\n            geom_feats = geom_feats[kept1]\n            # get tensors from the same voxel next to each other 将所有的feature基于坐标位置进行排序，在俯视图上相同坐标的feature的ranks值相同\n            ranks = geom_feats[:, 0] * (self.nx[1] * self.nx[2] * B) \\\n                    + geom_feats[:, 1] * (self.nx[2] * B) \\\n                    + geom_feats[:, 2] * B \\\n                    + geom_feats[:, 3]\n            sorts = ranks.argsort()\n            x, geom_feats, ranks, idx = x[sorts], geom_feats[sorts], ranks[sorts], idx[sorts]\n            repeat_id = torch.ones(geom_feats.shape[0], device=geom_feats.device, dtype=geom_feats.dtype)   # 假设剩下5000个点\n            curr = 0\n            repeat_id[0] = 0\n            curr_rank = ranks[0]\n            for i in range(1, ranks.shape[0]):   # 多个图像点会放到相同的bev位置上\n                if curr_rank == ranks[i]:\n                    curr += 1\n                    repeat_id[i] = curr\n                else:\n                    curr_rank = ranks[i]\n                    curr = 0\n                    repeat_id[i] = curr\n            kept2 = repeat_id < max\n            repeat_id, geom_feats, x, idx = repeat_id[kept2], geom_feats[kept2], x[kept2], idx[kept2]\n            geom_feats = torch.cat([geom_feats, repeat_id.unsqueeze(-1)], dim=-1)\n            self.geom_feats = geom_feats\n            self.idx = idx\n        else:\n            geom_feats = self.geom_feats\n            idx = self.idx\n            x = x[idx]\n        # griddify (B x C x Z x X x Y)\n        final = torch.zeros((B, C, nx[2], nx[1], nx[0], max), device=x.device)                                   # C这里是64维特征，每个点的特征\n        final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 1], geom_feats[:, 0], geom_feats[:, 4]] = x   # 一个bev特征上的点，最多放max个图像点；geom_feats[:, 4]第四维度是重复的id\n        final = final.sum(-1)\n        # collapse Z\n        final = torch.cat(final.unbind(dim=2), 1)\n        return final\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><span class=\'hidden-code\' data-code=\'class ViewTransformerLiftSplatShoot(BaseModule):\n    def create_frustum(self):\n        # make grid in image plane\n        ogfH, ogfW = self.data_config[\'input_size\']                       # 模型输入到网络的大小\n        fH, fW = ogfH // self.downsample, ogfW // self.downsample         # 特征提取后大小\n        ds = torch.arange(*self.grid_config[\'dbound\'], dtype=torch.float).view(-1, 1, 1).expand(-1, fH, fW)  # 1m到60m，1m为间隔生成深度信息\n        D, _, _ = ds.shape\n        xs = torch.linspace(0, ogfW - 1, fW, dtype=torch.float).view(1, 1, fW).expand(D, fH, fW)             # 输入到网络原始图片的值0->ofFW取fw个值\n        ys = torch.linspace(0, ogfH - 1, fH, dtype=torch.float).view(1, fH, 1).expand(D, fH, fW)\n        # D x H x W x 3  可以认为生成了41xHxW个点，每个点的坐标为(xs,ys,ds),(xs/ds,ys/ds)为图片上的点，而乘以内参k得到相机坐标系下的点\n        # \n        frustum = torch.stack((xs, ys, ds), -1)\n        return nn.Parameter(frustum, requires_grad=False)\n    def get_geometry(self, rots, trans, intrins, post_rots, post_trans, offset=None):\n        """Determine the (x,y,z) locations (in the ego frame) of the points in the point cloud.\n        Returns B x N x D x H/downsample x W/downsample x 3    N=6张图片，D为每个像素深度的点数，这里59是由上面\'dbound\': [1.0, 60.0, 1.0]为59 3是x,y,z坐标值\n        """\n        B, N, _ = trans.shape\n        # undo post-transformation\n        # B x N x D x H x W x 3  post_trans和post_rots为图像增强中使用到的仿射变换参数，因为此处要对视锥中的对应点做同样变换\n        points = self.`frustum` - post_trans.view(B, N, 1, 1, 1, 3)\n        if offset is not None:\n            _,D,H,W = offset.shape\n            points[:,:,:,:,:,2] = points[:,:,:,:,:,2]+offset.view(B,N,D,H,W)\n        points = torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3).matmul(points.unsqueeze(-1))\n        # cam_to_ego 像素坐标系->相机坐标系->车身坐标系\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],\n                            points[:, :, :, :, :, 2:3]\n                            ), 5)\n        if intrins.shape[3]==4:                          # for KITTI\n            shift = intrins[:,:,:3,3]\n            points  = points - shift.view(B,N,1,1,1,3,1)\n            intrins = intrins[:,:,:3,:3]\n        combine = rots.matmul(torch.inverse(intrins))\n        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)\n        points += trans.view(B, N, 1, 1, 1, 3)\n        # points_numpy = points.detach().cpu().numpy()\n        return points                     # 得到原先2D的坐标点的位置在车身坐标系下的3D位置\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/necks/view_transformer.py</p><span class=\'hidden-code\' data-code=\'class ViewTransformerLiftSplatShoot(BaseModule):\n    def voxel_pooling(self, geom_feats, x):\n        B, N, D, H, W, C = x.shape\n        Nprime = B * N * D * H * W\n        nx = self.nx.to(torch.long)\n        # flatten x\n        x = x.reshape(Nprime, C)\n        # flatten indices\n        geom_feats = ((geom_feats - (self.bx - self.dx / 2.)) / self.dx).long()\n        geom_feats = geom_feats.view(Nprime, 3)\n        batch_ix = torch.cat([torch.full([Nprime // B, 1], ix,evice=x.device, dtype=torch.long) for ix in range(B)])\n        geom_feats = torch.cat((geom_feats, batch_ix), 1)\n        # filter out points that are outside box\n        kept = (geom_feats[:, 0] `>`= 0) & (geom_feats[:, 0] `<` self.nx[0]) \\\n               & (geom_feats[:, 1] `>`= 0) & (geom_feats[:, 1] `<` self.nx[1]) \\\n               & (geom_feats[:, 2] `>`= 0) & (geom_feats[:, 2] `<` self.nx[2])\n        x = x[kept]\n        geom_feats = geom_feats[kept]\n        if self.max_drop_point_rate > 0.0 and self.training:\n            drop_point_rate = torch.rand(1)*self.max_drop_point_rate\n            kept = torch.rand(x.shape[0])>drop_point_rate\n            x, geom_feats = x[kept], geom_feats[kept]\n        if self.use_bev_pool:\n            final = bev_pool.bev_pool(x, geom_feats, B, self.nx[2], self.nx[0],self.nx[1])\n            final = final.transpose(dim0=-2, dim1=-1)\n        else:\n            # get tensors from the same voxel next to each other\n            ranks = geom_feats[:, 0] * (self.nx[1] * self.nx[2] * B) \\\n                    + geom_feats[:, 1] * (self.nx[2] * B) \\\n                    + geom_feats[:, 2] * B \\\n                    + geom_feats[:, 3]\n            sorts = ranks.argsort()\n            x, geom_feats, ranks = x[sorts], geom_feats[sorts], ranks[sorts]\n            # cumsum trick\n            x, geom_feats = QuickCumsum.apply(x, geom_feats, ranks)\n            # griddify (B x C x Z x X x Y)\n            final = torch.zeros((B, C, nx[2], nx[1], nx[0]), device=x.device)\n            final[geom_feats[:, 3], :, geom_feats[:, 2], geom_feats[:, 1], geom_feats[:, 0]] = x\n        # collapse Z\n        final = torch.cat(final.unbind(dim=2), 1)\n        return final\n\'> </span>'}]}]}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
