<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>CenterFusion_radar</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">数据读取</p>nuscenes的图片大小1600,900；输入有3Dbox以及毫米波雷达点+pillar_dim-[1.5, 0.2, 0.2]构成的box<br>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def __getitem__(self, index):\n        opt = self.opt\n        img, anns, img_info, img_path = self.<span style=\'color: green;font-weight: bold;\'>_load_data</span>(index)        <span style=\'color: red\'># img.shape=(900, 1600, 3)；</span>\n        <span style=\'color: red\'># anns[0].keys()=dict_keys([\'id\', \'image_id\', \'category_id\', \'dim\', \'location\', \'depth\', \'occluded\', \'truncated\', \'rotation_y\', \'amodel_center\', </span>\n        <span style=\'color: red\'>#                           \'iscrowd\', \'track_id\', \'attributes\', \'velocity\', \'velocity_cam\', \'bbox\', \'area\', \'alpha\'])； </span>\n        <span style=\'color: red\'># img_info.keys()=dict_keys([\'id\', \'file_name\', \'calib\', \'video_id\', \'frame_id\', \'sensor_id\', \'sample_token\', \'trans_matrix\', \'velocity_trans_matrix\', </span>\n        <span style=\'color: red\'>#                            \'width\', \'height\', \'pose_record_trans\', \'pose_record_rot\', \'cs_record_trans\', \'cs_record_rot\', \'radar_pc\', \'camera_intrinsic\'])； </span>\n        <span style=\'color: red\'># img_info[\'radar_pc\']=18x80,18中的2是深度信息Z；vx_comp是x方向的速度？</span>\n        <span style=\'color: red\'>#  x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_state x_rms y_rms invalid_state pdh0 vx_rms vy_rms；</span>\n        <span style=\'color: red\'># (x is right, z is front)，x,y,z在相机坐标系，能够用相机内参转到图片上</span>\n        height, width = img.shape[0], img.shape[1]\n        new_anns = sorted(anns, key=lambda k: k[\'depth\'], reverse=True)              <span style=\'color: red\'># sort annotations based on depth form far to near</span>\n        c = np.array([img.shape[1] / 2., img.shape[0] / 2.], dtype=np.float32)       <span style=\'color: red\'># Get center and scale from image</span>\n        s = max(img.shape[0], img.shape[1]) * 1.0 if not self.opt.not_max_crop else np.array([img.shape[1], img.shape[0]], np.float32)   <span style=\'color: red\'># s=1600；一个基本的图像尺寸 </span>\n        aug_s, rot, flipped = 1, 0, 0\n        <span style=\'color: red\'># data augmentation for training set</span>\n        if \'train\' in self.split:\n            c, aug_s, rot = self.<span style=\'color: green;font-weight: bold;\'>_get_aug_param</span>(c, s, width, height)     <span style=\'color: red\'># c=array([480.,332.07568], dtype=float32); aug_s=1.0; rot=0; </span>\n            s = s * aug_s\n            if np.random.random() < opt.flip:\n                flipped = 1\n                img = img[:, ::-1, :]\n                anns = self._flip_anns(anns, width)\n        trans_input = <span style=\'color: green;font-weight: bold;\'>get_affine_transform</span>(c, s, rot, [opt.input_w, opt.input_h])    <span style=\'color: red\'># trans_input=array([[0.5,0.0,11.16],[0.0, 0.5, 68.80]])  </span>\n        trans_output = get_affine_transform(c, s, rot, [opt.output_w, opt.output_h])   <span style=\'color: red\'># trans_output=array([[0.125,0.0,2.79],[0.0,0.125,17.2]])</span>\n        inp = self.<span style=\'color: green;font-weight: bold;\'>_get_input</span>(img, trans_input)                         <span style=\'color: red\'># Augment, resize and normalize the imag</span>\n        ret = {\'image\': inp}\n        gt_det = {\'bboxes\': [], \'scores\': [], \'clses\': [], \'cts\': []}\n        <span style=\'color: red\'>#  load point cloud data</span>\n        if opt.pointcloud:             <span style=\'color: red\'># True</span>\n            pc_2d, pc_N, pc_dep, pc_3d = self.<span style=\'color: green;font-weight: bold;\'>_load_pc_data</span>(img, img_info, trans_input, trans_output, flipped)\n            ret.update({\'pc_2d\': pc_2d,\'pc_3d\': pc_3d,\'pc_N\': pc_N,\'pc_dep\': pc_dep })\n        pre_cts, track_ids = None, None\n        if opt.tracking:\n            pre_image, pre_anns, frame_dist, pre_img_info = self._load_pre_data(img_info[\'video_id\'], img_info[\'frame_id\'], img_info[\'sensor_id\'] if \'sensor_id\' in img_info else 1)\n            if flipped:\n                pre_image = pre_image[:, ::-1, :].copy()\n                pre_anns = self._flip_anns(pre_anns, width)\n                if pc_2d is not None:\n                    pc_2d = self._flip_pc(pc_2d,  width)\n            if opt.same_aug_pre and frame_dist != 0:\n                trans_input_pre = trans_input \n                trans_output_pre = trans_output\n            else:\n                c_pre, aug_s_pre, _ = self._get_aug_param(c, s, width, height, disturb=True)\n                s_pre = s * aug_s_pre\n                trans_input_pre = get_affine_transform(c_pre, s_pre, rot, [opt.input_w, opt.input_h])\n                trans_output_pre = get_affine_transform(c_pre, s_pre, rot, [opt.output_w, opt.output_h])\n            pre_img = self._get_input(pre_image, trans_input_pre)\n            pre_hm, pre_cts, track_ids = self._get_pre_dets(pre_anns, trans_input_pre, trans_output_pre)\n            ret[\'pre_img\'] = pre_img\n            if opt.pre_hm:\n                ret[\'pre_hm\'] = pre_hm\n            if opt.pointcloud:\n                pre_pc_2d, pre_pc_N, pre_pc_hm, pre_pc_3d = self._load_pc_data(pre_img, pre_img_info, trans_input_pre, trans_output_pre, flipped)\n                ret[\'pre_pc_2d\'] = pre_pc_2d\n                ret[\'pre_pc_3d\'] = pre_pc_3d\n                ret[\'pre_pc_N\'] = pre_pc_N\n                ret[\'pre_pc_hm\'] = pre_pc_hm\n        <span style=\'color: red\'># init samples</span>\n        self.<span style=\'color: green;font-weight: bold;\'>_init_ret</span>(ret, gt_det)\n        calib = self.<span style=\'color: green;font-weight: bold;\'>_get_calib</span>(img_info, width, height)\n        <span style=\'color: red\'># get velocity transformation matrix</span>\n        if "velocity_trans_matrix" in img_info:\n            velocity_mat = np.array(img_info[\'velocity_trans_matrix\'], dtype=np.float32)  <span style=\'color: red\'># velocity_mat.shape(4, 4);速度的转换矩阵 </span>\n        else:\n            velocity_mat = np.eye(4)\n        num_objs = min(len(anns), self.max_objs)\n        for k in range(num_objs):\n            ann = anns[k]\n            cls_id = int(self.cat_ids[ann[\'category_id\']])\n            if cls_id > self.opt.num_classes or cls_id <= -999:\n                continue\n            bbox, bbox_amodal = self.<span style=\'color: green;font-weight: bold;\'>_get_bbox_output</span>(ann[\'bbox\'], trans_output, height, width)   <span style=\'color: red\'># bbox_amodal是没有clip边界的；2D box转换后的边界值</span>\n            if cls_id <= 0 or (\'iscrowd\' in ann and ann[\'iscrowd\'] > 0):\n                self._mask_ignore_or_crowd(ret, cls_id, bbox)\n                continue\n            self.<span style=\'color: green;font-weight: bold;\'>_add_instance</span>(ret, gt_det, k, cls_id, bbox, bbox_amodal, ann, trans_output, aug_s, calib, pre_cts, track_ids)\n        if self.opt.debug > 0 or self.enable_meta:\n            gt_det = self._format_gt_det(gt_det)\n            meta = {\'c\': c, \'s\': s, \'gt_det\': gt_det, \'img_id\': img_info[\'id\'],\'img_path\': img_path, \'calib\': calib,\n                    \'img_width\': img_info[\'width\'], \'img_height\': img_info[\'height\'],\'flipped\': flipped, \'velocity_mat\':velocity_mat}\n            ret[\'meta\'] = meta\n        ret[\'calib\'] = calib\n        return ret\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _load_data(self, index):\n        coco = self.coco\n        img_dir = self.img_dir\n        img_id = self.images[index]\n        img, anns, img_info, img_path = self.<span style=\'color: green;font-weight: bold;\'>_load_image_anns</span>(img_id, coco, img_dir)\n        return img, anns, img_info, img_path\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _load_image_anns(self, img_id, coco, img_dir):\n        img_info = coco.loadImgs(ids=[img_id])[0]\n        file_name = img_info[\'file_name\']\n        img_path = os.path.join(img_dir, file_name)\n        ann_ids = coco.getAnnIds(imgIds=[img_id])\n        anns = copy.deepcopy(coco.loadAnns(ids=ann_ids))\n        img = cv2.imread(img_path)\n        return img, anns, img_info, img_path\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _get_aug_param(self, c, s, width, height, disturb=False):\n        if (not self.opt.not_rand_crop) and not disturb:\n            aug_s = np.random.choice(np.arange(0.6, 1.4, 0.1))\n            w_border = self._get_border(128, width)\n            h_border = self._get_border(128, height)\n            c[0] = np.random.randint(low=w_border, high=width - w_border)\n            c[1] = np.random.randint(low=h_border, high=height - h_border)\n        else:\n            sf = self.opt.scale\n            cf = self.opt.shift\n            <span style=\'color: red\'># if type(s) == float:</span>\n            <span style=\'color: red\'>#   s = [s, s]</span>\n            temp = np.random.randn()*cf\n            c[0] += s * np.clip(temp, -2*cf, 2*cf)\n            c[1] += s * np.clip(np.random.randn()*cf, -2*cf, 2*cf)\n            aug_s = np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)\n        if np.random.random() < self.opt.aug_rot:\n            rf = self.opt.rotate\n            rot = np.clip(np.random.randn()*rf, -rf*2, rf*2)\n        else:\n            rot = 0    \n        return c, aug_s, rot\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/image.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_affine_transform(center,scale,rot,output_size,shift=np.array([0, 0], dtype=np.float32),inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        scale = np.array([scale, scale], dtype=np.float32)\n    scale_tmp = scale\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n    rot_rad = np.pi * rot / 180\n    src_dir = <span style=\'color: green;font-weight: bold;\'>get_dir</span>([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5], np.float32) + dst_dir\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n    return trans\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/image.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n    return src_result\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/image.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _get_input(self, img, trans_input):          <span style=\'color: red\'># Augment, resize and normalize the image</span>\n        inp = cv2.warpAffine(img, trans_input, (self.opt.input_w, self.opt.input_h),flags=cv2.INTER_LINEAR)\n        inp = (inp.astype(np.float32) / 255.)\n        if \'train\' in self.split and not self.opt.no_color_aug:\n            color_aug(self._data_rng, inp, self._eig_val, self._eig_vec)\n        inp = (inp - self.mean) / self.std\n        inp = inp.transpose(2, 0, 1)\n    return inp\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):                  <span style=\'color: red\'># Load the Radar point cloud data</span>\n    def _load_pc_data(self, img, img_info, inp_trans, out_trans, flipped=0):\n        img_height, img_width = img.shape[0], img.shape[1]\n        radar_pc = np.array(img_info.get(\'radar_pc\', None))\n        if radar_pc is None:\n            return None, None, None, None\n        depth = radar_pc[2,:]         <span style=\'color: red\'># calculate distance to points</span>\n        if self.opt.max_pc_dist > 0:  <span style=\'color: red\'># filter points by distance  过滤距离超过60的雷达点；</span>\n            mask = (depth <= self.opt.max_pc_dist)\n            radar_pc = radar_pc[:,mask]\n            depth = depth[mask]\n        if self.opt.pc_z_offset != 0:  <span style=\'color: red\'># add z offset to radar points</span>\n            radar_pc[1,:] -= self.opt.pc_z_offset\n        \n        <span style=\'color: red\'># map points to the image and filter ones outside</span>\n        pc_2d, mask = <span style=\'color: green;font-weight: bold;\'>map_pointcloud_to_image</span>(radar_pc, np.array(img_info[\'camera_intrinsic\']), img_shape=(img_info[\'width\'],img_info[\'height\']))\n        pc_3d = radar_pc[:,mask]\n        <span style=\'color: red\'># sort points by distance</span>\n        ind = np.argsort(pc_2d[2,:])\n        pc_2d = pc_2d[:,ind]\n        pc_3d = pc_3d[:,ind]\n        <span style=\'color: red\'># flip points if image is flipped</span>\n        if flipped:\n            pc_2d = self._flip_pc(pc_2d,  img_width)\n            pc_3d[0,:] *= -1  <span style=\'color: red\'># flipping the x dimension</span>\n            pc_3d[8,:] *= -1  <span style=\'color: red\'># flipping x velocity (x is right, z is front)</span>\n        pc_2d, pc_3d, pc_dep = self.<span style=\'color: green;font-weight: bold;\'>_process_pc</span>(pc_2d, pc_3d, img, inp_trans, out_trans, img_info)\n        pc_N = np.array(pc_2d.shape[1])\n        <span style=\'color: red\'># pad point clouds with zero to avoid size mismatch error in dataloader</span>\n        n_points = min(self.opt.max_pc, pc_2d.shape[1])          <span style=\'color: red\'># n_points=20，最多1000个</span>\n        pc_z = np.zeros((pc_2d.shape[0], self.opt.max_pc))       <span style=\'color: red\'># pc_z.shape=(3,1000)</span>\n        pc_z[:, :n_points] = pc_2d[:, :n_points]                 <span style=\'color: red\'># 0,1维度在图像坐标系;</span>\n        pc_3dz = np.zeros((pc_3d.shape[0], self.opt.max_pc))     <span style=\'color: red\'># pc_3dz.shape=(18,1000)  </span>\n        pc_3dz[:, :n_points] = pc_3d[:, :n_points]\n        return pc_z, pc_N, pc_dep, pc_3dz                 <span style=\'color: red\'># pc_dep.shape=(3, 112, 200);见pc_hm_feat；</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/pointcloud.py</p><font size="0"><pre class="language-python"><code class="language-python">def map_pointcloud_to_image(pc, cam_intrinsic, img_shape=(1600,900)):  <span style=\'color: red\'># Map point cloud from camera coordinates to the image</span>\n    if isinstance(pc, RadarPointCloud):\n        points = pc.points[:3,:]\n    else:\n        points = pc\n    (width, height) = img_shape\n    depths = points[2, :]                                                  <span style=\'color: red\'># 毫米波雷达在图像坐标系，经过内参可以转到图片上?</span>\n    points = <span style=\'color: green;font-weight: bold;\'>view_points</span>(points[:3, :], cam_intrinsic, normalize=True)   <span style=\'color: red\'># Take the actual picture 到图片上的像素值大小了</span>\n                                                                           <span style=\'color: red\'># from nuscenes.utils.geometry_utils import view_points</span>\n    mask = np.ones(depths.shape[0], dtype=bool)            <span style=\'color: red\'># Remove points that are either outside or behind the camera. </span>\n    mask = np.logical_and(mask, depths > 0)\n    mask = np.logical_and(mask, points[0, :] > 1)\n    mask = np.logical_and(mask, points[0, :] < width - 1)\n    mask = np.logical_and(mask, points[1, :] > 1)\n    mask = np.logical_and(mask, points[1, :] < height - 1)\n    points = points[:, mask]\n    points[2,:] = depths[mask]\n    return points, mask\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _process_pc(self, pc_2d, pc_3d, img, inp_trans, out_trans, img_info):    \n        img_height, img_width = img.shape[0], img.shape[1]\n        <span style=\'color: red\'># transform points</span>\n        mask = None\n        if len(self.opt.pc_feat_lvl) > 0:                         <span style=\'color: red\'># pc_feat=(3,20)图像坐标系下；第3维度是相机坐标系下的深度(距离)，mask=(20,)</span>\n            pc_feat, mask = self.<span style=\'color: green;font-weight: bold;\'>_transform_pc</span>(pc_2d, out_trans, self.opt.output_w, self.opt.output_h)         <span style=\'color: red\'># 去掉数据增强后去掉的点</span>\n            pc_hm_feat = np.zeros((len(self.opt.pc_feat_lvl), self.opt.output_h, self.opt.output_w), np.float32) <span style=\'color: red\'># (3, 112, 200);</span>\n            <span style=\'color: red\'># 相当于20个点，在112x200的特征图里面,self.opt.pc_feat_lvl=[\'pc_dep\', \'pc_vx\', \'pc_vz\'],所以第0维为3，然后每个点的范围由下面的函数决定，</span>\n            <span style=\'color: red\'># 里面的值都是一样的，等于深度，从标注信息里面获得得vx和vy,pc_N=20;pc_3d-(18, 20)相当于有20个点                                                 </span>\n        if mask is not None:\n            pc_N = np.array(sum(mask))\n            pc_2d = pc_2d[:,mask]\n            pc_3d = pc_3d[:,mask]\n        else:\n            pc_N = pc_2d.shape[1]\n        <span style=\'color: red\'># create point cloud pillars</span>\n        if self.opt.pc_roi_method == "pillars":\n            pillar_wh = self.<span style=\'color: green;font-weight: bold;\'>create_pc_pillars</span>(img, img_info, pc_2d, pc_3d, inp_trans, out_trans)    \n        <span style=\'color: red\'># generate point cloud channels</span>\n        for i in range(pc_N-1, -1, -1):                            <span style=\'color: red\'>#  pc_N是毫米波雷达点数量；对于每个而毫米波点  从远到近</span>\n            for feat in self.opt.pc_feat_lvl:                      <span style=\'color: red\'># [\'pc_dep\', \'pc_vx\', \'pc_vz\']</span>\n                point = pc_feat[:,i]                               <span style=\'color: red\'># 在数据增强后图片上的点</span>\n                depth = point[2]\n                ct = np.array([point[0], point[1]])\n                ct_int = ct.astype(np.int32)\n                if self.opt.pc_roi_method == "pillars":\n                    wh = pillar_wh[:,i]\n                    b = [max(ct[1]-wh[1], 0), ct[1], max(ct[0]-wh[0]/2, 0), min(ct[0]+wh[0]/2, self.opt.output_w)]  <span style=\'color: red\'># yc-h,yc,xc-w/2,xc+w/2</span>\n                    b = np.round(b).astype(np.int32)               <span style=\'color: red\'># 雷达点在图片上的投影加上3Dbox在图片上投影的长宽得到2DBOX         </span>\n                elif self.opt.pc_roi_method == "hm":\n                    radius = (1.0 / depth) * self.opt.r_a + self.opt.r_b\n                    radius = gaussian_radius((radius, radius))\n                    radius = max(0, int(radius))\n                    x, y = ct_int[0], ct_int[1]\n                    height, width = pc_hm_feat.shape[1:3]\n                    left, right = min(x, radius), min(width - x, radius + 1)\n                    top, bottom = min(y, radius), min(height - y, radius + 1)\n                    b = np.array([y - top, y + bottom, x - left, x + right])\n                    b = np.round(b).astype(np.int32)            \n                if feat == \'pc_dep\':      <span style=\'color: red\'># pc_hm_feat.shape=(3, 112, 200);第0维度的b范围的值是depth【z前方距离】1维是8那个vx_comp，2维是9那个vy_comp</span>\n                    channel = self.opt.pc_feat_channels[\'pc_dep\']\n                    pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = depth              \n                if feat == \'pc_vx\':\n                    vx = pc_3d[8,i]\n                    channel = self.opt.pc_feat_channels[\'pc_vx\']\n                    pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = vx\n                if feat == \'pc_vz\':\n                    vz = pc_3d[9,i]\n                    channel = self.opt.pc_feat_channels[\'pc_vz\']\n                    pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = vz\n        return pc_2d, pc_3d, pc_hm_feat\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _transform_pc(self, pc_2d, trans, img_width, img_height, filter_out=True):\n        if pc_2d.shape[1] == 0:\n            return pc_2d, []\n        pc_t = np.expand_dims(pc_2d[:2,:].T, 0)   <span style=\'color: red\'># [3,N] -> [1,N,2]</span>\n        t_points = cv2.transform(pc_t, trans)\n        t_points = np.squeeze(t_points,0).T       <span style=\'color: red\'># [1,N,2] -> [2,N]</span>\n        <span style=\'color: red\'># remove points outside image             也就是数据增强，深度还是对应点的深度</span>\n        if filter_out:\n            mask = (t_points[0,:]<img_width) & (t_points[1,:]<img_height) & (0<t_points[0,:]) & (0<t_points[1,:])\n            out = np.concatenate((t_points[:,mask], pc_2d[2:,mask]), axis=0)\n        else:\n            mask = None\n            out = np.concatenate((t_points, pc_2d[2:,:]), axis=0)\n        return out, mask              <span style=\'color: red\'># 图像坐标系下；第3维度是相机坐标系下的深度(距离)</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def create_pc_pillars(self, img, img_info, pc_2d, pc_3d, inp_trans, out_trans):\n        pillar_wh = np.zeros((2, pc_3d.shape[1]))            <span style=\'color: red\'># pillar_wh.shape->(2, 20); </span>\n        boxes_2d = np.zeros((0,8,2))\n        pillar_dim = self.opt.pillar_dims                    <span style=\'color: red\'># pillar_dim-[1.5, 0.2, 0.2]</span>\n        v = np.dot(np.eye(3), np.array([1,0,0]))             <span style=\'color: red\'># v=array([1.,0.,0.])</span>\n        ry = -np.arctan2(v[2], v[0])                         <span style=\'color: red\'># ry=-0.0；</span>\n        for i, center in enumerate(pc_3d[:3,:].T):           <span style=\'color: red\'># Create a 3D pillar at pc location for the full-size image</span>\n            box_3d = compute_box_3d(dim=pillar_dim, location=center, rotation_y=ry)    <span style=\'color: red\'># center=array([-5.25291526,  0.96514001, 10.26383004])</span>\n            <span style=\'color: red\'># 对于每个x,y,z为中心，上面的 pillar_dim 为x,y,z的dim得到相机坐标系的8个顶点box_3d.shape</span>\n            box_2d = project_to_image(box_3d, img_info[\'calib\']).T                     <span style=\'color: red\'># [2x8]      再根据矩阵内参得到图像上的坐标box_2d  </span>\n            <span style=\'color: red\'># save the box for debug plots</span>\n            if self.opt.debug:\n                box_2d_img, m = self._transform_pc(box_2d, inp_trans, self.opt.input_w, self.opt.input_h, filter_out=False)  <span style=\'color: red\'># 到输入尺寸的2Dbox</span>\n                boxes_2d = np.concatenate((boxes_2d, np.expand_dims(box_2d_img.T,0)),0)\n            <span style=\'color: red\'># transform points</span>\n            box_2d_t, m = self._transform_pc(box_2d, out_trans, self.opt.output_w, self.opt.output_h)      <span style=\'color: red\'># 到输出尺寸的2Dbox</span>\n            if box_2d_t.shape[1] <= 1:\n                continue\n            <span style=\'color: red\'># get the bounding box in [xyxy] format</span>\n            bbox = [np.min(box_2d_t[0,:]), np.min(box_2d_t[1,:]), np.max(box_2d_t[0,:]), np.max(box_2d_t[1,:])] <span style=\'color: red\'># format: xyxy</span>\n            <span style=\'color: red\'># store height and width of the 2D box  存储上面得到的图片上的box的长宽；</span>\n            pillar_wh[0,i] = bbox[2] - bbox[0]     <span style=\'color: red\'># 也就说对于每个毫米波雷达点在相机坐标系上构建一定维度的3Dbox映射到图片上的2Dbox的长宽；</span>\n            pillar_wh[1,i] = bbox[3] - bbox[1]\n        return pillar_wh\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _init_ret(self, ret, gt_det):       <span style=\'color: red\'># 这里只是初始化，没做任何赋值操作 </span>\n        max_objs = self.max_objs * self.opt.dense_reg                             <span style=\'color: red\'># max_objs=128x1;</span>\n        ret[\'hm\'] = np.zeros((self.opt.num_classes, self.opt.output_h, self.opt.output_w), np.float32)  <span style=\'color: red\'># ret[\'hm\'].shape=(10, 112, 200);</span>\n        ret[\'ind\'] = np.zeros((max_objs), dtype=np.int64)                         <span style=\'color: red\'># ret[\'ind\'].shape=(128,)</span>\n        ret[\'cat\'] = np.zeros((max_objs), dtype=np.int64)\n        ret[\'mask\'] = np.zeros((max_objs), dtype=np.float32)      \n        if self.opt.pointcloud:                                                  <span style=\'color: red\'># ret[\'pc_hm\'].shape=(3, 112, 200);</span>\n            ret[\'pc_hm\'] = np.zeros((len(self.opt.pc_feat_lvl), self.opt.output_h, self.opt.output_w), np.float32)\n        regression_head_dims = {\'reg\': 2, \'wh\': 2, \'tracking\': 2, \'ltrb\': 4, \'ltrb_amodal\': 4, \'nuscenes_att\': 8, \'velocity\': 3, \n                \'hps\': self.num_joints * 2, \'dep\': 1, \'dim\': 3, \'amodel_offset\': 2 }\n        for head in regression_head_dims:\n            if head in self.opt.heads:\n                ret[head] = np.zeros((max_objs, regression_head_dims[head]), dtype=np.float32)\n                ret[head + \'_mask\'] = np.zeros((max_objs, regression_head_dims[head]), dtype=np.float32)\n                gt_det[head] = []\n            <span style=\'color: red\'># if self.opt.pointcloud:</span>\n            <span style=\'color: red\'>#     ret[\'pc_dep_mask\'] = np.zeros((max_objs, 1), dtype=np.float32)</span>\n            <span style=\'color: red\'>#     ret[\'pc_dep\'] = np.zeros((max_objs, 1), dtype=np.float32)</span>\n            <span style=\'color: red\'>#     gt_det[\'pc_dep\'] = []</span>\n        if \'hm_hp\' in self.opt.heads:\n            num_joints = self.num_joints\n            ret[\'hm_hp\'] = np.zeros((num_joints, self.opt.output_h, self.opt.output_w), dtype=np.float32)\n            ret[\'hm_hp_mask\'] = np.zeros((max_objs * num_joints), dtype=np.float32)\n            ret[\'hp_offset\'] = np.zeros((max_objs * num_joints, 2), dtype=np.float32)\n            ret[\'hp_ind\'] = np.zeros((max_objs * num_joints), dtype=np.int64)\n            ret[\'hp_offset_mask\'] = np.zeros((max_objs * num_joints, 2), dtype=np.float32)\n            ret[\'joint\'] = np.zeros((max_objs * num_joints), dtype=np.int64)\n        if \'rot\' in self.opt.heads:\n            ret[\'rotbin\'] = np.zeros((max_objs, 2), dtype=np.int64)\n            ret[\'rotres\'] = np.zeros((max_objs, 2), dtype=np.float32)\n            ret[\'rot_mask\'] = np.zeros((max_objs), dtype=np.float32)\n            gt_det.update({\'rot\': []})\n        <span style=\'color: red\'># gt_det.keys()=dict_keys([\'bboxes\',\'scores\',\'clses\',\'cts\',\'reg\',\'wh\',\'nuscenes_att\',\'velocity\',\'dep\',\'dim\',\'amodel_offset\',\'rot\']);   </span>\n        <span style=\'color: red\'># ret.keys()=dict_keys([\'image\', \'pc_2d\', \'pc_3d\', \'pc_N\', \'pc_dep\', \'hm\', \'ind\', \'cat\', \'mask\', \'pc_hm\', \'reg\', \'reg_mask\', \'wh\', \'wh_mask\', \'nuscenes_att\', </span>\n        <span style=\'color: red\'># \'nuscenes_att_mask\', \'velocity\', \'velocity_mask\', \'dep\', \'dep_mask\', \'dim\', \'dim_mask\', \'amodel_offset\', \'amodel_offset_mask\', \'rotbin\', \'rotres\', \'rot_mask\']);  </span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _get_calib(self, img_info, width, height):\n        if \'calib\' in img_info:\n            calib = np.array(img_info[\'calib\'], dtype=np.float32)\n        else:\n            calib = np.array([[self.rest_focal_length, 0, width / 2, 0], \n                              [0, self.rest_focal_length, height / 2, 0], \n                              [0, 0, 1, 0]])\n        return calib\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _get_bbox_output(self, bbox, trans_output, height, width):\n        bbox = self._coco_box_to_bbox(bbox).copy()\n        rect = np.array([[bbox[0], bbox[1]], [bbox[0], bbox[3]],\n                         [bbox[2], bbox[3]], [bbox[2], bbox[1]]], dtype=np.float32)\n        for t in range(4):\n            rect[t] =  affine_transform(rect[t], trans_output)\n        bbox[:2] = rect[:, 0].min(), rect[:, 1].min()\n        bbox[2:] = rect[:, 0].max(), rect[:, 1].max()\n        bbox_amodal = copy.deepcopy(bbox)\n        bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, self.opt.output_w - 1)\n        bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, self.opt.output_h - 1)\n        h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]\n        return bbox, bbox_amodal\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _add_instance(self, ret, gt_det, k, cls_id, bbox, bbox_amodal, ann, trans_output,aug_s, calib, pre_cts=None, track_ids=None):\n        h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]         <span style=\'color: red\'># 图片上的2Dbox</span>\n        if h <= 0 or w <= 0:\n            return\n        radius = gaussian_radius((math.ceil(h), math.ceil(w)))\n        radius = max(0, int(radius)) \n        #----------------这些都是正常的2DBOX------------------------\n        ct = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)   <span style=\'color: red\'># 2Dbox的中心点</span>\n        ct_int = ct.astype(np.int32)\n        ret[\'cat\'][k] = cls_id - 1                                            <span style=\'color: red\'># 类别id，从0开始 </span>\n        ret[\'mask\'][k] = 1                                                    <span style=\'color: red\'># 1表示有真值需预测</span>\n        if \'wh\' in ret:\n            ret[\'wh\'][k] = 1. * w, 1. * h                                     <span style=\'color: red\'># 2Dbox-不是毫米波求得 </span>\n            ret[\'wh_mask\'][k] = 1\n        ret[\'ind\'][k] = ct_int[1] * self.opt.output_w + ct_int[0]             <span style=\'color: red\'># 中心点y*xh_out+中心点x 作为索引</span>\n        ret[\'reg\'][k] = ct - ct_int                                           <span style=\'color: red\'># 中心点偏移</span>\n        ret[\'reg_mask\'][k] = 1\n        draw_umich_gaussian(ret[\'hm\'][cls_id - 1], ct_int, radius)            <span style=\'color: red\'># 类别得高斯分布 </span>\n        gt_det[\'bboxes\'].append(np.array([ct[0] - w / 2, ct[1] - h / 2,ct[0] + w / 2, ct[1] + h / 2], dtype=np.float32))\n        gt_det[\'scores\'].append(1)\n        gt_det[\'clses\'].append(cls_id - 1)\n        gt_det[\'cts\'].append(ct)\n        if \'tracking\' in self.opt.heads:\n            if ann[\'track_id\'] in track_ids:\n                pre_ct = pre_cts[track_ids.index(ann[\'track_id\'])]\n                ret[\'tracking_mask\'][k] = 1\n                ret[\'tracking\'][k] = pre_ct - ct_int\n                gt_det[\'tracking\'].append(ret[\'tracking\'][k])\n            else:\n                gt_det[\'tracking\'].append(np.zeros(2, np.float32))\n        if \'ltrb\' in self.opt.heads:\n            ret[\'ltrb\'][k] = bbox[0] - ct_int[0], bbox[1] - ct_int[1], bbox[2] - ct_int[0], bbox[3] - ct_int[1]\n            ret[\'ltrb_mask\'][k] = 1\n        <span style=\'color: red\'># ltrb_amodal is to use the left, top, right, bottom bounding box representation </span>\n        <span style=\'color: red\'># to enable detecting out-of-image bounding box (important for MOT datasets)</span>\n        if \'ltrb_amodal\' in self.opt.heads:\n            ret[\'ltrb_amodal\'][k] = bbox_amodal[0] - ct_int[0], bbox_amodal[1] - ct_int[1], bbox_amodal[2] - ct_int[0], bbox_amodal[3] - ct_int[1]\n            ret[\'ltrb_amodal_mask\'][k] = 1\n            gt_det[\'ltrb_amodal\'].append(bbox_amodal)\n        if \'nuscenes_att\' in self.opt.heads:\n            if (\'attributes\' in ann) and ann[\'attributes\'] > 0:\n                att = int(ann[\'attributes\'] - 1)               <span style=\'color: red\'># att=6；</span>\n                ret[\'nuscenes_att\'][k][att] = 1                <span style=\'color: red\'># ret[\'nuscenes_att\'].shape=(128, 8)有8个att</span>\n                ret[\'nuscenes_att_mask\'][k][self.nuscenes_att_range[att]] = 1\n            gt_det[\'nuscenes_att\'].append(ret[\'nuscenes_att\'][k])\n        if \'velocity\' in self.opt.heads:\n            if (\'velocity_cam\' in ann) and min(ann[\'velocity_cam\']) > -1000:\n                ret[\'velocity\'][k] = np.array(ann[\'velocity_cam\'], np.float32)[:3]  <span style=\'color: red\'># ret[\'velocity\'][k]=array([0.05722534, 0.30095306, 0.06465194], dtype=float32) </span>\n                ret[\'velocity_mask\'][k] = 1\n            gt_det[\'velocity\'].append(ret[\'velocity\'][k])\n        if \'hps\' in self.opt.heads:\n            self._add_hps(ret, k, ann, gt_det, trans_output, ct_int, bbox, h, w)\n        if \'rot\' in self.opt.heads:\n            self.<span style=\'color: green;font-weight: bold;\'>_add_rot</span>(ret, ann, k, gt_det)\n        if \'dep\' in self.opt.heads:\n            if \'depth\' in ann:\n                ret[\'dep_mask\'][k] = 1\n                ret[\'dep\'][k] = ann[\'depth\'] * aug_s\n                gt_det[\'dep\'].append(ret[\'dep\'][k])\n            else:\n                gt_det[\'dep\'].append(2)\n        if \'dim\' in self.opt.heads:\n            if \'dim\' in ann:\n                ret[\'dim_mask\'][k] = 1\n                ret[\'dim\'][k] = ann[\'dim\']\n                gt_det[\'dim\'].append(ret[\'dim\'][k])\n            else:\n                gt_det[\'dim\'].append([1,1,1])\n        \n        if \'amodel_offset\' in self.opt.heads:\n            if \'amodel_center\' in ann:\n                amodel_center = affine_transform(ann[\'amodel_center\'], trans_output)\n                ret[\'amodel_offset_mask\'][k] = 1\n                ret[\'amodel_offset\'][k] = amodel_center - ct_int\n                gt_det[\'amodel_offset\'].append(ret[\'amodel_offset\'][k])\n            else:\n                gt_det[\'amodel_offset\'].append([0, 0])\n        \n        if self.opt.pointcloud:\n            <span style=\'color: red\'># get pointcloud heatmap</span>\n            if self.opt.disable_frustum:\n                ret[\'pc_hm\'] = ret[\'pc_dep\']\n                if opt.normalize_depth:\n                ret[\'pc_hm\'][self.opt.pc_feat_channels[\'pc_dep\']] /= opt.max_pc_dist\n            else:\n                dist_thresh = get_dist_thresh(calib, ct, ann[\'dim\'], ann[\'alpha\'])\n                <span style=\'color: green;font-weight: bold;\'>pc_dep_to_hm</span>(ret[\'pc_hm\'], ret[\'pc_dep\'], ann[\'depth\'], bbox, dist_thresh, self.opt)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class GenericDataset(data.Dataset):\n    def _add_rot(self, ret, ann, k, gt_det):\n        if \'alpha\' in ann:          <span style=\'color: red\'># -pi->pi</span>\n            ret[\'rot_mask\'][k] = 1\n            alpha = ann[\'alpha\']\n            if alpha < np.pi / 6. or alpha > 5 * np.pi / 6.:    <span style=\'color: red\'># [-pi->pi/6]  [5pi/6->pi]   8/6</span>\n                ret[\'rotbin\'][k, 0] = 1                         <span style=\'color: red\'># ret[\'rotbin\'].shape=(128, 2)</span>\n                ret[\'rotres\'][k, 0] = alpha - (-0.5 * np.pi)    <span style=\'color: red\'># ret[\'rotres\'].shape=(128, 2);角度扔到两个区间算</span>\n            if alpha > -np.pi / 6. or alpha < -5 * np.pi / 6.:  <span style=\'color: red\'># [-pi/6->pi]  [-pi->-5pi/6]</span>\n                ret[\'rotbin\'][k, 1] = 1\n                ret[\'rotres\'][k, 1] = alpha - (0.5 * np.pi)\n            gt_det[\'rot\'].append(self._alpha_to_8(ann[\'alpha\']))\n        else:\n            gt_det[\'rot\'].append(self._alpha_to_8(0))\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/pointcloud.py</p><font size="0"><pre class="language-python"><code class="language-python">def pc_dep_to_hm(pc_hm, pc_dep, dep, bbox, dist_thresh, opt):\n    if isinstance(dep, list) and len(dep) > 0:\n        dep = dep[0]\n    ct = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)                       <span style=\'color: red\'># 原先图片上的2Dbox</span>\n    bbox_int = np.array([np.floor(bbox[0]), np.floor(bbox[1]), np.ceil(bbox[2]), np.ceil(bbox[3])], np.int32) <span style=\'color: red\'># format: xyxy</span>\n    roi = pc_dep[:, bbox_int[1]:bbox_int[3]+1, bbox_int[0]:bbox_int[2]+1]            <span style=\'color: red\'># 用2Dbox将原先的值抠出来</span>\n    pc_dep = roi[opt.pc_feat_channels[\'pc_dep\']]\n    pc_vx = roi[opt.pc_feat_channels[\'pc_vx\']]\n    pc_vz = roi[opt.pc_feat_channels[\'pc_vz\']]\n    nonzero_inds = np.nonzero(pc_dep)\n    if len(nonzero_inds[0]) > 0:               <span style=\'color: red\'># 这里进行匹配，将毫米波每个点生成的feat和2Dbox自带的深度匹配</span>\n        <span style=\'color: red\'>#  nonzero_pc_dep = np.exp(-pc_dep[nonzero_inds])</span>\n        nonzero_pc_dep = pc_dep[nonzero_inds]\n        nonzero_pc_vx = pc_vx[nonzero_inds]\n        nonzero_pc_vz = pc_vz[nonzero_inds]\n        <span style=\'color: red\'># Get points within dist threshold</span>\n        within_thresh = (nonzero_pc_dep < dep+dist_thresh) & (nonzero_pc_dep > max(0, dep-dist_thresh))\n        pc_dep_match = nonzero_pc_dep[within_thresh]\n        pc_vx_match = nonzero_pc_vx[within_thresh]\n        pc_vz_match = nonzero_pc_vz[within_thresh]\n        if len(pc_dep_match) > 0:\n            arg_min = np.argmin(pc_dep_match)      <span style=\'color: red\'># 要是匹配上了，优先匹配最近的</span>\n            dist = pc_dep_match[arg_min]\n            vx = pc_vx_match[arg_min]\n            vz = pc_vz_match[arg_min]\n            if opt.normalize_depth:\n                dist /= opt.max_pc_dist\n            w = bbox[2] - bbox[0]\n            w_interval = opt.hm_to_box_ratio*(w)      <span style=\'color: red\'># 范围为中心点往外扩张得长宽的opt.hm_to_box_ratio=0.3倍 </span>\n            w_min = int(ct[0] - w_interval/2.)\n            w_max = int(ct[0] + w_interval/2.)\n            h = bbox[3] - bbox[1]\n            h_interval = opt.hm_to_box_ratio*(h)\n            h_min = int(ct[1] - h_interval/2.)\n            h_max = int(ct[1] + h_interval/2.)\n            pc_hm[opt.pc_feat_channels[\'pc_dep\'],h_min:h_max+1, w_min:w_max+1+1] = dist     <span style=\'color: red\'># pc_hm.shape=(3, 112, 200);</span>\n            pc_hm[opt.pc_feat_channels[\'pc_vx\'],h_min:h_max+1, w_min:w_max+1+1] = vx\n            pc_hm[opt.pc_feat_channels[\'pc_vz\'],h_min:h_max+1, w_min:w_max+1+1] = vz \n</code></pre></font>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练模型</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/model/networks/dla.py</p><font size="0"><pre class="language-python"><code class="language-python">class DLASeg(BaseModel):\n    def __init__(self, num_layers, heads, head_convs, opt):\n        super(DLASeg, self).__init__(heads, head_convs, 1, 64 if num_layers == 34 else 128, opt=opt)\n        <span style=\'color: red\'># heads={\'hm\':10, \'reg\':2, \'wh\':2, \'dep\':1, \'rot\':8, \'dim\':3, \'amodel_offset\':2, \'dep_sec\':1, \'rot_sec\':8, \'nuscenes_att\':8, \'velocity\':3}</span>\n        <span style=\'color: red\'># head_convs={\'hm\': [256], \'reg\': [256], \'wh\': [256], \'dep\': [256], \'rot\': [256], \'dim\': [256], \'amodel_offset\': [256], </span>\n        <span style=\'color: red\'># \'dep_sec\': [256, 256, 256], \'rot_sec\': [256, 256, 256], \'nuscenes_att\': [256, 256, 256], \'velocity\': [256, 256, 256]} </span>\n        down_ratio=4\n        self.opt = opt\n        self.node_type = DLA_NODE[opt.dla_node]\n        print(\'Using node type:\', self.node_type)\n        self.first_level = int(np.log2(down_ratio))\n        self.last_level = 5\n        self.base = globals()[\'dla{}\'.format(num_layers)](pretrained=(opt.load_model == \'\'), opt=opt)\n        channels = self.base.channels\n        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n        self.dla_up = DLAUp(self.first_level, channels[self.first_level:], scales, node_type=self.node_type)\n        out_channel = channels[self.first_level]\n        self.ida_up = IDAUp(out_channel, channels[self.first_level:self.last_level], [2 ** i for i in range(self.last_level - self.first_level)],node_type=self.node_type)\n</code></pre></font>'}]}]})</script></body>
</html>
