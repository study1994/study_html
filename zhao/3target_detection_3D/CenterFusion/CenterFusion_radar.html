<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>CenterFusion_radar</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">数据读取</p>nuscenes的图片大小1600,900；输入有3Dbox以及毫米波雷达点+pillar_dim-[1.5, 0.2, 0.2]构成的box<br>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def __getitem__(self, index):\n        opt = self.opt\n        img, anns, img_info, img_path = self.`_load_data`(index)        # img.shape=(900, 1600, 3)；\n        # anns[0].keys()=dict_keys([&amp;#39;id&amp;#39;, &amp;#39;image_id&amp;#39;, &amp;#39;category_id&amp;#39;, &amp;#39;dim&amp;#39;, &amp;#39;location&amp;#39;, &amp;#39;depth&amp;#39;, &amp;#39;occluded&amp;#39;, &amp;#39;truncated&amp;#39;, &amp;#39;rotation_y&amp;#39;, &amp;#39;amodel_center&amp;#39;, \n        #                           &amp;#39;iscrowd&amp;#39;, &amp;#39;track_id&amp;#39;, &amp;#39;attributes&amp;#39;, &amp;#39;velocity&amp;#39;, &amp;#39;velocity_cam&amp;#39;, &amp;#39;bbox&amp;#39;, &amp;#39;area&amp;#39;, &amp;#39;alpha&amp;#39;])； \n        # img_info.keys()=dict_keys([&amp;#39;id&amp;#39;, &amp;#39;file_name&amp;#39;, &amp;#39;calib&amp;#39;, &amp;#39;video_id&amp;#39;, &amp;#39;frame_id&amp;#39;, &amp;#39;sensor_id&amp;#39;, &amp;#39;sample_token&amp;#39;, &amp;#39;trans_matrix&amp;#39;, &amp;#39;velocity_trans_matrix&amp;#39;, \n        #                            &amp;#39;width&amp;#39;, &amp;#39;height&amp;#39;, &amp;#39;pose_record_trans&amp;#39;, &amp;#39;pose_record_rot&amp;#39;, &amp;#39;cs_record_trans&amp;#39;, &amp;#39;cs_record_rot&amp;#39;, &amp;#39;radar_pc&amp;#39;, &amp;#39;camera_intrinsic&amp;#39;])； \n        # img_info[&amp;#39;radar_pc&amp;#39;]=18x80,18中的2是深度信息Z；vx_comp是x方向的速度？\n        #  x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_state x_rms y_rms invalid_state pdh0 vx_rms vy_rms；\n        # (x is right, z is front)，x,y,z在相机坐标系，能够用相机内参转到图片上\n        height, width = img.shape[0], img.shape[1]\n        new_anns = sorted(anns, key=lambda k: k[&amp;#39;depth&amp;#39;], reverse=True)              # sort annotations based on depth form far to near\n        c = np.array([img.shape[1] / 2., img.shape[0] / 2.], dtype=np.float32)       # Get center and scale from image\n        s = max(img.shape[0], img.shape[1]) * 1.0 if not self.opt.not_max_crop else np.array([img.shape[1], img.shape[0]], np.float32)   # s=1600；一个基本的图像尺寸 \n        aug_s, rot, flipped = 1, 0, 0\n        # data augmentation for training set\n        if &amp;#39;train&amp;#39; in self.split:\n            c, aug_s, rot = self.`_get_aug_param`(c, s, width, height)     # c=array([480.,332.07568], dtype=float32); aug_s=1.0; rot=0; \n            s = s * aug_s\n            if np.random.random() < opt.flip:\n                flipped = 1\n                img = img[:, ::-1, :]\n                anns = self._flip_anns(anns, width)\n        trans_input = `get_affine_transform`(c, s, rot, [opt.input_w, opt.input_h])    # trans_input=array([[0.5,0.0,11.16],[0.0, 0.5, 68.80]])  \n        trans_output = get_affine_transform(c, s, rot, [opt.output_w, opt.output_h])   # trans_output=array([[0.125,0.0,2.79],[0.0,0.125,17.2]])\n        inp = self.`_get_input`(img, trans_input)                         # Augment, resize and normalize the imag\n        ret = {&amp;#39;image&amp;#39;: inp}\n        gt_det = {&amp;#39;bboxes&amp;#39;: [], &amp;#39;scores&amp;#39;: [], &amp;#39;clses&amp;#39;: [], &amp;#39;cts&amp;#39;: []}\n        #  load point cloud data\n        if opt.pointcloud:             # True\n            pc_2d, pc_N, pc_dep, pc_3d = self.`_load_pc_data`(img, img_info, trans_input, trans_output, flipped)\n            ret.update({&amp;#39;pc_2d&amp;#39;: pc_2d,&amp;#39;pc_3d&amp;#39;: pc_3d,&amp;#39;pc_N&amp;#39;: pc_N,&amp;#39;pc_dep&amp;#39;: pc_dep })\n        pre_cts, track_ids = None, None\n        if opt.tracking:\n            pre_image, pre_anns, frame_dist, pre_img_info = self._load_pre_data(img_info[&amp;#39;video_id&amp;#39;], img_info[&amp;#39;frame_id&amp;#39;], img_info[&amp;#39;sensor_id&amp;#39;] if &amp;#39;sensor_id&amp;#39; in img_info else 1)\n            if flipped:\n                pre_image = pre_image[:, ::-1, :].copy()\n                pre_anns = self._flip_anns(pre_anns, width)\n                if pc_2d is not None:\n                    pc_2d = self._flip_pc(pc_2d,  width)\n            if opt.same_aug_pre and frame_dist != 0:\n                trans_input_pre = trans_input \n                trans_output_pre = trans_output\n            else:\n                c_pre, aug_s_pre, _ = self._get_aug_param(c, s, width, height, disturb=True)\n                s_pre = s * aug_s_pre\n                trans_input_pre = get_affine_transform(c_pre, s_pre, rot, [opt.input_w, opt.input_h])\n                trans_output_pre = get_affine_transform(c_pre, s_pre, rot, [opt.output_w, opt.output_h])\n            pre_img = self._get_input(pre_image, trans_input_pre)\n            pre_hm, pre_cts, track_ids = self._get_pre_dets(pre_anns, trans_input_pre, trans_output_pre)\n            ret[&amp;#39;pre_img&amp;#39;] = pre_img\n            if opt.pre_hm:\n                ret[&amp;#39;pre_hm&amp;#39;] = pre_hm\n            if opt.pointcloud:\n                pre_pc_2d, pre_pc_N, pre_pc_hm, pre_pc_3d = self._load_pc_data(pre_img, pre_img_info, trans_input_pre, trans_output_pre, flipped)\n                ret[&amp;#39;pre_pc_2d&amp;#39;] = pre_pc_2d\n                ret[&amp;#39;pre_pc_3d&amp;#39;] = pre_pc_3d\n                ret[&amp;#39;pre_pc_N&amp;#39;] = pre_pc_N\n                ret[&amp;#39;pre_pc_hm&amp;#39;] = pre_pc_hm\n        # init samples\n        self.`_init_ret`(ret, gt_det)\n        calib = self.`_get_calib`(img_info, width, height)\n        # get velocity transformation matrix\n        if &amp;#39;velocity_trans_matrix&amp;#39; in img_info:\n            velocity_mat = np.array(img_info[&amp;#39;velocity_trans_matrix&amp;#39;], dtype=np.float32)  # velocity_mat.shape(4, 4);速度的转换矩阵 \n        else:\n            velocity_mat = np.eye(4)\n        num_objs = min(len(anns), self.max_objs)\n        for k in range(num_objs):\n            ann = anns[k]\n            cls_id = int(self.cat_ids[ann[&amp;#39;category_id&amp;#39;]])\n            if cls_id `>` self.opt.num_classes or cls_id `<`= -999:\n                continue\n            bbox, bbox_amodal = self.`_get_bbox_output`(ann[&amp;#39;bbox&amp;#39;], trans_output, height, width)   # bbox_amodal是没有clip边界的；2D box转换后的边界值\n            if cls_id `<`= 0 or (&amp;#39;iscrowd&amp;#39; in ann and ann[&amp;#39;iscrowd&amp;#39;] `>` 0):\n                self._mask_ignore_or_crowd(ret, cls_id, bbox)\n                continue\n            self.`_add_instance`(ret, gt_det, k, cls_id, bbox, bbox_amodal, ann, trans_output, aug_s, calib, pre_cts, track_ids)\n        if self.opt.debug > 0 or self.enable_meta:\n            gt_det = self._format_gt_det(gt_det)\n            meta = {&amp;#39;c&amp;#39;: c, &amp;#39;s&amp;#39;: s, &amp;#39;gt_det&amp;#39;: gt_det, &amp;#39;img_id&amp;#39;: img_info[&amp;#39;id&amp;#39;],&amp;#39;img_path&amp;#39;: img_path, &amp;#39;calib&amp;#39;: calib,\n                    &amp;#39;img_width&amp;#39;: img_info[&amp;#39;width&amp;#39;], &amp;#39;img_height&amp;#39;: img_info[&amp;#39;height&amp;#39;],&amp;#39;flipped&amp;#39;: flipped, &amp;#39;velocity_mat&amp;#39;:velocity_mat}\n            ret[&amp;#39;meta&amp;#39;] = meta\n        ret[&amp;#39;calib&amp;#39;] = calib\n        return ret\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _load_data(self, index):\n        coco = self.coco\n        img_dir = self.img_dir\n        img_id = self.images[index]\n        img, anns, img_info, img_path = self.`_load_image_anns`(img_id, coco, img_dir)\n        return img, anns, img_info, img_path\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _load_image_anns(self, img_id, coco, img_dir):\n        img_info = coco.loadImgs(ids=[img_id])[0]\n        file_name = img_info[&amp;#39;file_name&amp;#39;]\n        img_path = os.path.join(img_dir, file_name)\n        ann_ids = coco.getAnnIds(imgIds=[img_id])\n        anns = copy.deepcopy(coco.loadAnns(ids=ann_ids))\n        img = cv2.imread(img_path)\n        return img, anns, img_info, img_path\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _get_aug_param(self, c, s, width, height, disturb=False):\n        if (not self.opt.not_rand_crop) and not disturb:\n            aug_s = np.random.choice(np.arange(0.6, 1.4, 0.1))\n            w_border = self._get_border(128, width)\n            h_border = self._get_border(128, height)\n            c[0] = np.random.randint(low=w_border, high=width - w_border)\n            c[1] = np.random.randint(low=h_border, high=height - h_border)\n        else:\n            sf = self.opt.scale\n            cf = self.opt.shift\n            # if type(s) == float:\n            #   s = [s, s]\n            temp = np.random.randn()*cf\n            c[0] += s * np.clip(temp, -2*cf, 2*cf)\n            c[1] += s * np.clip(np.random.randn()*cf, -2*cf, 2*cf)\n            aug_s = np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)\n        if np.random.random() < self.opt.aug_rot:\n            rf = self.opt.rotate\n            rot = np.clip(np.random.randn()*rf, -rf*2, rf*2)\n        else:\n            rot = 0    \n        return c, aug_s, rot\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/image.py</p><span class=\'hidden-code\' data-code=\'def get_affine_transform(center,scale,rot,output_size,shift=np.array([0, 0], dtype=np.float32),inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        scale = np.array([scale, scale], dtype=np.float32)\n    scale_tmp = scale\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n    rot_rad = np.pi * rot / 180\n    src_dir = `get_dir`([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5], np.float32) + dst_dir\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n    return trans\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/image.py</p><span class=\'hidden-code\' data-code=\'def get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n    return src_result\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/image.py</p><span class=\'hidden-code\' data-code=\'def get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _get_input(self, img, trans_input):          # Augment, resize and normalize the image\n        inp = cv2.warpAffine(img, trans_input, (self.opt.input_w, self.opt.input_h),flags=cv2.INTER_LINEAR)\n        inp = (inp.astype(np.float32) / 255.)\n        if &amp;#39;train&amp;#39; in self.split and not self.opt.no_color_aug:\n            color_aug(self._data_rng, inp, self._eig_val, self._eig_vec)\n        inp = (inp - self.mean) / self.std\n        inp = inp.transpose(2, 0, 1)\n    return inp\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):                  # Load the Radar point cloud data\n    def _load_pc_data(self, img, img_info, inp_trans, out_trans, flipped=0):\n        img_height, img_width = img.shape[0], img.shape[1]\n        radar_pc = np.array(img_info.get(&amp;#39;radar_pc&amp;#39;, None))\n        if radar_pc is None:\n            return None, None, None, None\n        depth = radar_pc[2,:]         # calculate distance to points\n        if self.opt.max_pc_dist > 0:  # filter points by distance  过滤距离超过60的雷达点；\n            mask = (depth <= self.opt.max_pc_dist)\n            radar_pc = radar_pc[:,mask]\n            depth = depth[mask]\n        if self.opt.pc_z_offset != 0:  # add z offset to radar points\n            radar_pc[1,:] -= self.opt.pc_z_offset\n        \n        # map points to the image and filter ones outside\n        pc_2d, mask = `map_pointcloud_to_image`(radar_pc, np.array(img_info[&amp;#39;camera_intrinsic&amp;#39;]), img_shape=(img_info[&amp;#39;width&amp;#39;],img_info[&amp;#39;height&amp;#39;]))\n        pc_3d = radar_pc[:,mask]\n        # sort points by distance\n        ind = np.argsort(pc_2d[2,:])\n        pc_2d = pc_2d[:,ind]\n        pc_3d = pc_3d[:,ind]\n        # flip points if image is flipped\n        if flipped:\n            pc_2d = self._flip_pc(pc_2d,  img_width)\n            pc_3d[0,:] *= -1  # flipping the x dimension\n            pc_3d[8,:] *= -1  # flipping x velocity (x is right, z is front)\n        pc_2d, pc_3d, pc_dep = self.`_process_pc`(pc_2d, pc_3d, img, inp_trans, out_trans, img_info)\n        pc_N = np.array(pc_2d.shape[1])\n        # pad point clouds with zero to avoid size mismatch error in dataloader\n        n_points = min(self.opt.max_pc, pc_2d.shape[1])          # n_points=20，最多1000个\n        pc_z = np.zeros((pc_2d.shape[0], self.opt.max_pc))       # pc_z.shape=(3,1000)\n        pc_z[:, :n_points] = pc_2d[:, :n_points]                 # 0,1维度在图像坐标系;\n        pc_3dz = np.zeros((pc_3d.shape[0], self.opt.max_pc))     # pc_3dz.shape=(18,1000)  \n        pc_3dz[:, :n_points] = pc_3d[:, :n_points]\n        return pc_z, pc_N, pc_dep, pc_3dz                 # pc_dep.shape=(3, 112, 200);见pc_hm_feat；\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/pointcloud.py</p><span class=\'hidden-code\' data-code=\'def map_pointcloud_to_image(pc, cam_intrinsic, img_shape=(1600,900)):  # Map point cloud from camera coordinates to the image\n    if isinstance(pc, RadarPointCloud):\n        points = pc.points[:3,:]\n    else:\n        points = pc\n    (width, height) = img_shape\n    depths = points[2, :]                                                  # 毫米波雷达在图像坐标系，经过内参可以转到图片上?\n    points = `view_points`(points[:3, :], cam_intrinsic, normalize=True)   # Take the actual picture 到图片上的像素值大小了\n                                                                           # from nuscenes.utils.geometry_utils import view_points\n    mask = np.ones(depths.shape[0], dtype=bool)            # Remove points that are either outside or behind the camera. \n    mask = np.logical_and(mask, depths > 0)\n    mask = np.logical_and(mask, points[0, :] > 1)\n    mask = np.logical_and(mask, points[0, :] < width - 1)\n    mask = np.logical_and(mask, points[1, :] > 1)\n    mask = np.logical_and(mask, points[1, :] < height - 1)\n    points = points[:, mask]\n    points[2,:] = depths[mask]\n    return points, mask\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _process_pc(self, pc_2d, pc_3d, img, inp_trans, out_trans, img_info):    \n        img_height, img_width = img.shape[0], img.shape[1]\n        # transform points\n        mask = None\n        if len(self.opt.pc_feat_lvl) > 0:                         # pc_feat=(3,20)图像坐标系下；第3维度是相机坐标系下的深度(距离)，mask=(20,)\n            pc_feat, mask = self.`_transform_pc`(pc_2d, out_trans, self.opt.output_w, self.opt.output_h)         # 去掉数据增强后去掉的点\n            pc_hm_feat = np.zeros((len(self.opt.pc_feat_lvl), self.opt.output_h, self.opt.output_w), np.float32) # (3, 112, 200);\n            # 相当于20个点，在112x200的特征图里面,self.opt.pc_feat_lvl=[&amp;#39;pc_dep&amp;#39;, &amp;#39;pc_vx&amp;#39;, &amp;#39;pc_vz&amp;#39;],所以第0维为3，然后每个点的范围由下面的函数决定，\n            # 里面的值都是一样的，等于深度，从标注信息里面获得得vx和vy,pc_N=20;pc_3d-(18, 20)相当于有20个点                                                 \n        if mask is not None:\n            pc_N = np.array(sum(mask))\n            pc_2d = pc_2d[:,mask]\n            pc_3d = pc_3d[:,mask]\n        else:\n            pc_N = pc_2d.shape[1]\n        # create point cloud pillars\n        if self.opt.pc_roi_method == &amp;#39;pillars&amp;#39;:\n            pillar_wh = self.`create_pc_pillars`(img, img_info, pc_2d, pc_3d, inp_trans, out_trans)    \n        # generate point cloud channels\n        for i in range(pc_N-1, -1, -1):                            #  pc_N是毫米波雷达点数量；对于每个而毫米波点  从远到近\n            for feat in self.opt.pc_feat_lvl:                      # [&amp;#39;pc_dep&amp;#39;, &amp;#39;pc_vx&amp;#39;, &amp;#39;pc_vz&amp;#39;]\n                point = pc_feat[:,i]                               # 在数据增强后图片上的点\n                depth = point[2]\n                ct = np.array([point[0], point[1]])\n                ct_int = ct.astype(np.int32)\n                if self.opt.pc_roi_method == &amp;#39;pillars&amp;#39;:\n                    wh = pillar_wh[:,i]\n                    b = [max(ct[1]-wh[1], 0), ct[1], max(ct[0]-wh[0]/2, 0), min(ct[0]+wh[0]/2, self.opt.output_w)]  # yc-h,yc,xc-w/2,xc+w/2\n                    b = np.round(b).astype(np.int32)               # 雷达点在图片上的投影加上3Dbox在图片上投影的长宽得到2DBOX         \n                elif self.opt.pc_roi_method == &amp;#39;hm&amp;#39;:\n                    radius = (1.0 / depth) * self.opt.r_a + self.opt.r_b\n                    radius = gaussian_radius((radius, radius))\n                    radius = max(0, int(radius))\n                    x, y = ct_int[0], ct_int[1]\n                    height, width = pc_hm_feat.shape[1:3]\n                    left, right = min(x, radius), min(width - x, radius + 1)\n                    top, bottom = min(y, radius), min(height - y, radius + 1)\n                    b = np.array([y - top, y + bottom, x - left, x + right])\n                    b = np.round(b).astype(np.int32)            \n                if feat == &amp;#39;pc_dep&amp;#39;:      # pc_hm_feat.shape=(3, 112, 200);第0维度的b范围的值是depth【z前方距离】1维是8那个vx_comp，2维是9那个vy_comp\n                    channel = self.opt.pc_feat_channels[&amp;#39;pc_dep&amp;#39;]\n                    pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = depth              \n                if feat == &amp;#39;pc_vx&amp;#39;:\n                    vx = pc_3d[8,i]\n                    channel = self.opt.pc_feat_channels[&amp;#39;pc_vx&amp;#39;]\n                    pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = vx\n                if feat == &amp;#39;pc_vz&amp;#39;:\n                    vz = pc_3d[9,i]\n                    channel = self.opt.pc_feat_channels[&amp;#39;pc_vz&amp;#39;]\n                    pc_hm_feat[channel, b[0]:b[1], b[2]:b[3]] = vz\n        return pc_2d, pc_3d, pc_hm_feat\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _transform_pc(self, pc_2d, trans, img_width, img_height, filter_out=True):\n        if pc_2d.shape[1] == 0:\n            return pc_2d, []\n        pc_t = np.expand_dims(pc_2d[:2,:].T, 0)   # [3,N] -> [1,N,2]\n        t_points = cv2.transform(pc_t, trans)\n        t_points = np.squeeze(t_points,0).T       # [1,N,2] -> [2,N]\n        # remove points outside image             也就是数据增强，深度还是对应点的深度\n        if filter_out:\n            mask = (t_points[0,:]<img_width) &amp; (t_points[1,:]<img_height) &amp; (0<t_points[0,:]) &amp; (0<t_points[1,:])\n            out = np.concatenate((t_points[:,mask], pc_2d[2:,mask]), axis=0)\n        else:\n            mask = None\n            out = np.concatenate((t_points, pc_2d[2:,:]), axis=0)\n        return out, mask              # 图像坐标系下；第3维度是相机坐标系下的深度(距离)\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def create_pc_pillars(self, img, img_info, pc_2d, pc_3d, inp_trans, out_trans):\n        pillar_wh = np.zeros((2, pc_3d.shape[1]))            # pillar_wh.shape->(2, 20); \n        boxes_2d = np.zeros((0,8,2))\n        pillar_dim = self.opt.pillar_dims                    # pillar_dim-[1.5, 0.2, 0.2]\n        v = np.dot(np.eye(3), np.array([1,0,0]))             # v=array([1.,0.,0.])\n        ry = -np.arctan2(v[2], v[0])                         # ry=-0.0；\n        for i, center in enumerate(pc_3d[:3,:].T):           # Create a 3D pillar at pc location for the full-size image\n            box_3d = compute_box_3d(dim=pillar_dim, location=center, rotation_y=ry)    # center=array([-5.25291526,  0.96514001, 10.26383004])\n            # 对于每个x,y,z为中心，上面的 pillar_dim 为x,y,z的dim得到相机坐标系的8个顶点box_3d.shape\n            box_2d = project_to_image(box_3d, img_info[&amp;#39;calib&amp;#39;]).T                     # [2x8]      再根据矩阵内参得到图像上的坐标box_2d  \n            # save the box for debug plots\n            if self.opt.debug:\n                box_2d_img, m = self._transform_pc(box_2d, inp_trans, self.opt.input_w, self.opt.input_h, filter_out=False)  # 到输入尺寸的2Dbox\n                boxes_2d = np.concatenate((boxes_2d, np.expand_dims(box_2d_img.T,0)),0)\n            # transform points\n            box_2d_t, m = self._transform_pc(box_2d, out_trans, self.opt.output_w, self.opt.output_h)      # 到输出尺寸的2Dbox\n            if box_2d_t.shape[1] <= 1:\n                continue\n            # get the bounding box in [xyxy] format\n            bbox = [np.min(box_2d_t[0,:]), np.min(box_2d_t[1,:]), np.max(box_2d_t[0,:]), np.max(box_2d_t[1,:])] # format: xyxy\n            # store height and width of the 2D box  存储上面得到的图片上的box的长宽；\n            pillar_wh[0,i] = bbox[2] - bbox[0]     # 也就说对于每个毫米波雷达点在相机坐标系上构建一定维度的3Dbox映射到图片上的2Dbox的长宽；\n            pillar_wh[1,i] = bbox[3] - bbox[1]\n        return pillar_wh\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _init_ret(self, ret, gt_det):       # 这里只是初始化，没做任何赋值操作 \n        max_objs = self.max_objs * self.opt.dense_reg                             # max_objs=128x1;\n        ret[&amp;#39;hm&amp;#39;] = np.zeros((self.opt.num_classes, self.opt.output_h, self.opt.output_w), np.float32)  # ret[&amp;#39;hm&amp;#39;].shape=(10, 112, 200);\n        ret[&amp;#39;ind&amp;#39;] = np.zeros((max_objs), dtype=np.int64)                         # ret[&amp;#39;ind&amp;#39;].shape=(128,)\n        ret[&amp;#39;cat&amp;#39;] = np.zeros((max_objs), dtype=np.int64)\n        ret[&amp;#39;mask&amp;#39;] = np.zeros((max_objs), dtype=np.float32)      \n        if self.opt.pointcloud:                                                  # ret[&amp;#39;pc_hm&amp;#39;].shape=(3, 112, 200);\n            ret[&amp;#39;pc_hm&amp;#39;] = np.zeros((len(self.opt.pc_feat_lvl), self.opt.output_h, self.opt.output_w), np.float32)\n        regression_head_dims = {&amp;#39;reg&amp;#39;: 2, &amp;#39;wh&amp;#39;: 2, &amp;#39;tracking&amp;#39;: 2, &amp;#39;ltrb&amp;#39;: 4, &amp;#39;ltrb_amodal&amp;#39;: 4, &amp;#39;nuscenes_att&amp;#39;: 8, &amp;#39;velocity&amp;#39;: 3, \n                &amp;#39;hps&amp;#39;: self.num_joints * 2, &amp;#39;dep&amp;#39;: 1, &amp;#39;dim&amp;#39;: 3, &amp;#39;amodel_offset&amp;#39;: 2 }\n        for head in regression_head_dims:\n            if head in self.opt.heads:\n                ret[head] = np.zeros((max_objs, regression_head_dims[head]), dtype=np.float32)\n                ret[head + &amp;#39;_mask&amp;#39;] = np.zeros((max_objs, regression_head_dims[head]), dtype=np.float32)\n                gt_det[head] = []\n            # if self.opt.pointcloud:\n            #     ret[&amp;#39;pc_dep_mask&amp;#39;] = np.zeros((max_objs, 1), dtype=np.float32)\n            #     ret[&amp;#39;pc_dep&amp;#39;] = np.zeros((max_objs, 1), dtype=np.float32)\n            #     gt_det[&amp;#39;pc_dep&amp;#39;] = []\n        if &amp;#39;hm_hp&amp;#39; in self.opt.heads:\n            num_joints = self.num_joints\n            ret[&amp;#39;hm_hp&amp;#39;] = np.zeros((num_joints, self.opt.output_h, self.opt.output_w), dtype=np.float32)\n            ret[&amp;#39;hm_hp_mask&amp;#39;] = np.zeros((max_objs * num_joints), dtype=np.float32)\n            ret[&amp;#39;hp_offset&amp;#39;] = np.zeros((max_objs * num_joints, 2), dtype=np.float32)\n            ret[&amp;#39;hp_ind&amp;#39;] = np.zeros((max_objs * num_joints), dtype=np.int64)\n            ret[&amp;#39;hp_offset_mask&amp;#39;] = np.zeros((max_objs * num_joints, 2), dtype=np.float32)\n            ret[&amp;#39;joint&amp;#39;] = np.zeros((max_objs * num_joints), dtype=np.int64)\n        if &amp;#39;rot&amp;#39; in self.opt.heads:\n            ret[&amp;#39;rotbin&amp;#39;] = np.zeros((max_objs, 2), dtype=np.int64)\n            ret[&amp;#39;rotres&amp;#39;] = np.zeros((max_objs, 2), dtype=np.float32)\n            ret[&amp;#39;rot_mask&amp;#39;] = np.zeros((max_objs), dtype=np.float32)\n            gt_det.update({&amp;#39;rot&amp;#39;: []})\n        # gt_det.keys()=dict_keys([&amp;#39;bboxes&amp;#39;,&amp;#39;scores&amp;#39;,&amp;#39;clses&amp;#39;,&amp;#39;cts&amp;#39;,&amp;#39;reg&amp;#39;,&amp;#39;wh&amp;#39;,&amp;#39;nuscenes_att&amp;#39;,&amp;#39;velocity&amp;#39;,&amp;#39;dep&amp;#39;,&amp;#39;dim&amp;#39;,&amp;#39;amodel_offset&amp;#39;,&amp;#39;rot&amp;#39;]);   \n        # ret.keys()=dict_keys([&amp;#39;image&amp;#39;, &amp;#39;pc_2d&amp;#39;, &amp;#39;pc_3d&amp;#39;, &amp;#39;pc_N&amp;#39;, &amp;#39;pc_dep&amp;#39;, &amp;#39;hm&amp;#39;, &amp;#39;ind&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;mask&amp;#39;, &amp;#39;pc_hm&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;reg_mask&amp;#39;, &amp;#39;wh&amp;#39;, &amp;#39;wh_mask&amp;#39;, &amp;#39;nuscenes_att&amp;#39;, \n        # &amp;#39;nuscenes_att_mask&amp;#39;, &amp;#39;velocity&amp;#39;, &amp;#39;velocity_mask&amp;#39;, &amp;#39;dep&amp;#39;, &amp;#39;dep_mask&amp;#39;, &amp;#39;dim&amp;#39;, &amp;#39;dim_mask&amp;#39;, &amp;#39;amodel_offset&amp;#39;, &amp;#39;amodel_offset_mask&amp;#39;, &amp;#39;rotbin&amp;#39;, &amp;#39;rotres&amp;#39;, &amp;#39;rot_mask&amp;#39;]);  \n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _get_calib(self, img_info, width, height):\n        if &amp;#39;calib&amp;#39; in img_info:\n            calib = np.array(img_info[&amp;#39;calib&amp;#39;], dtype=np.float32)\n        else:\n            calib = np.array([[self.rest_focal_length, 0, width / 2, 0], \n                              [0, self.rest_focal_length, height / 2, 0], \n                              [0, 0, 1, 0]])\n        return calib\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _get_bbox_output(self, bbox, trans_output, height, width):\n        bbox = self._coco_box_to_bbox(bbox).copy()\n        rect = np.array([[bbox[0], bbox[1]], [bbox[0], bbox[3]],\n                         [bbox[2], bbox[3]], [bbox[2], bbox[1]]], dtype=np.float32)\n        for t in range(4):\n            rect[t] =  affine_transform(rect[t], trans_output)\n        bbox[:2] = rect[:, 0].min(), rect[:, 1].min()\n        bbox[2:] = rect[:, 0].max(), rect[:, 1].max()\n        bbox_amodal = copy.deepcopy(bbox)\n        bbox[[0, 2]] = np.clip(bbox[[0, 2]], 0, self.opt.output_w - 1)\n        bbox[[1, 3]] = np.clip(bbox[[1, 3]], 0, self.opt.output_h - 1)\n        h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]\n        return bbox, bbox_amodal\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _add_instance(self, ret, gt_det, k, cls_id, bbox, bbox_amodal, ann, trans_output,aug_s, calib, pre_cts=None, track_ids=None):\n        h, w = bbox[3] - bbox[1], bbox[2] - bbox[0]         # 图片上的2Dbox\n        if h <= 0 or w <= 0:\n            return\n        radius = gaussian_radius((math.ceil(h), math.ceil(w)))\n        radius = max(0, int(radius)) \n        #----------------这些都是正常的2DBOX------------------------\n        ct = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)   # 2Dbox的中心点\n        ct_int = ct.astype(np.int32)\n        ret[&amp;#39;cat&amp;#39;][k] = cls_id - 1                                            # 类别id，从0开始 \n        ret[&amp;#39;mask&amp;#39;][k] = 1                                                    # 1表示有真值需预测\n        if &amp;#39;wh&amp;#39; in ret:\n            ret[&amp;#39;wh&amp;#39;][k] = 1. * w, 1. * h                                     # 2Dbox-不是毫米波求得 \n            ret[&amp;#39;wh_mask&amp;#39;][k] = 1\n        ret[&amp;#39;ind&amp;#39;][k] = ct_int[1] * self.opt.output_w + ct_int[0]             # 中心点y*xh_out+中心点x 作为索引\n        ret[&amp;#39;reg&amp;#39;][k] = ct - ct_int                                           # 中心点偏移\n        ret[&amp;#39;reg_mask&amp;#39;][k] = 1\n        draw_umich_gaussian(ret[&amp;#39;hm&amp;#39;][cls_id - 1], ct_int, radius)            # 类别得高斯分布 \n        gt_det[&amp;#39;bboxes&amp;#39;].append(np.array([ct[0] - w / 2, ct[1] - h / 2,ct[0] + w / 2, ct[1] + h / 2], dtype=np.float32))\n        gt_det[&amp;#39;scores&amp;#39;].append(1)\n        gt_det[&amp;#39;clses&amp;#39;].append(cls_id - 1)\n        gt_det[&amp;#39;cts&amp;#39;].append(ct)\n        if &amp;#39;tracking&amp;#39; in self.opt.heads:\n            if ann[&amp;#39;track_id&amp;#39;] in track_ids:\n                pre_ct = pre_cts[track_ids.index(ann[&amp;#39;track_id&amp;#39;])]\n                ret[&amp;#39;tracking_mask&amp;#39;][k] = 1\n                ret[&amp;#39;tracking&amp;#39;][k] = pre_ct - ct_int\n                gt_det[&amp;#39;tracking&amp;#39;].append(ret[&amp;#39;tracking&amp;#39;][k])\n            else:\n                gt_det[&amp;#39;tracking&amp;#39;].append(np.zeros(2, np.float32))\n        if &amp;#39;ltrb&amp;#39; in self.opt.heads:\n            ret[&amp;#39;ltrb&amp;#39;][k] = bbox[0] - ct_int[0], bbox[1] - ct_int[1], bbox[2] - ct_int[0], bbox[3] - ct_int[1]\n            ret[&amp;#39;ltrb_mask&amp;#39;][k] = 1\n        # ltrb_amodal is to use the left, top, right, bottom bounding box representation \n        # to enable detecting out-of-image bounding box (important for MOT datasets)\n        if &amp;#39;ltrb_amodal&amp;#39; in self.opt.heads:\n            ret[&amp;#39;ltrb_amodal&amp;#39;][k] = bbox_amodal[0] - ct_int[0], bbox_amodal[1] - ct_int[1], bbox_amodal[2] - ct_int[0], bbox_amodal[3] - ct_int[1]\n            ret[&amp;#39;ltrb_amodal_mask&amp;#39;][k] = 1\n            gt_det[&amp;#39;ltrb_amodal&amp;#39;].append(bbox_amodal)\n        if &amp;#39;nuscenes_att&amp;#39; in self.opt.heads:\n            if (&amp;#39;attributes&amp;#39; in ann) and ann[&amp;#39;attributes&amp;#39;] > 0:\n                att = int(ann[&amp;#39;attributes&amp;#39;] - 1)               # att=6；\n                ret[&amp;#39;nuscenes_att&amp;#39;][k][att] = 1                # ret[&amp;#39;nuscenes_att&amp;#39;].shape=(128, 8)有8个att\n                ret[&amp;#39;nuscenes_att_mask&amp;#39;][k][self.nuscenes_att_range[att]] = 1\n            gt_det[&amp;#39;nuscenes_att&amp;#39;].append(ret[&amp;#39;nuscenes_att&amp;#39;][k])\n        if &amp;#39;velocity&amp;#39; in self.opt.heads:\n            if (&amp;#39;velocity_cam&amp;#39; in ann) and min(ann[&amp;#39;velocity_cam&amp;#39;]) > -1000:\n                ret[&amp;#39;velocity&amp;#39;][k] = np.array(ann[&amp;#39;velocity_cam&amp;#39;], np.float32)[:3]  # ret[&amp;#39;velocity&amp;#39;][k]=array([0.05722534, 0.30095306, 0.06465194], dtype=float32) \n                ret[&amp;#39;velocity_mask&amp;#39;][k] = 1\n            gt_det[&amp;#39;velocity&amp;#39;].append(ret[&amp;#39;velocity&amp;#39;][k])\n        if &amp;#39;hps&amp;#39; in self.opt.heads:\n            self._add_hps(ret, k, ann, gt_det, trans_output, ct_int, bbox, h, w)\n        if &amp;#39;rot&amp;#39; in self.opt.heads:\n            self.`_add_rot`(ret, ann, k, gt_det)\n        if &amp;#39;dep&amp;#39; in self.opt.heads:\n            if &amp;#39;depth&amp;#39; in ann:\n                ret[&amp;#39;dep_mask&amp;#39;][k] = 1\n                ret[&amp;#39;dep&amp;#39;][k] = ann[&amp;#39;depth&amp;#39;] * aug_s\n                gt_det[&amp;#39;dep&amp;#39;].append(ret[&amp;#39;dep&amp;#39;][k])\n            else:\n                gt_det[&amp;#39;dep&amp;#39;].append(2)\n        if &amp;#39;dim&amp;#39; in self.opt.heads:\n            if &amp;#39;dim&amp;#39; in ann:\n                ret[&amp;#39;dim_mask&amp;#39;][k] = 1\n                ret[&amp;#39;dim&amp;#39;][k] = ann[&amp;#39;dim&amp;#39;]\n                gt_det[&amp;#39;dim&amp;#39;].append(ret[&amp;#39;dim&amp;#39;][k])\n            else:\n                gt_det[&amp;#39;dim&amp;#39;].append([1,1,1])\n        \n        if &amp;#39;amodel_offset&amp;#39; in self.opt.heads:\n            if &amp;#39;amodel_center&amp;#39; in ann:\n                amodel_center = affine_transform(ann[&amp;#39;amodel_center&amp;#39;], trans_output)\n                ret[&amp;#39;amodel_offset_mask&amp;#39;][k] = 1\n                ret[&amp;#39;amodel_offset&amp;#39;][k] = amodel_center - ct_int\n                gt_det[&amp;#39;amodel_offset&amp;#39;].append(ret[&amp;#39;amodel_offset&amp;#39;][k])\n            else:\n                gt_det[&amp;#39;amodel_offset&amp;#39;].append([0, 0])\n        \n        if self.opt.pointcloud:\n            # get pointcloud heatmap\n            if self.opt.disable_frustum:\n                ret[&amp;#39;pc_hm&amp;#39;] = ret[&amp;#39;pc_dep&amp;#39;]\n                if opt.normalize_depth:\n                ret[&amp;#39;pc_hm&amp;#39;][self.opt.pc_feat_channels[&amp;#39;pc_dep&amp;#39;]] /= opt.max_pc_dist\n            else:\n                dist_thresh = get_dist_thresh(calib, ct, ann[&amp;#39;dim&amp;#39;], ann[&amp;#39;alpha&amp;#39;])\n                `pc_dep_to_hm`(ret[&amp;#39;pc_hm&amp;#39;], ret[&amp;#39;pc_dep&amp;#39;], ann[&amp;#39;depth&amp;#39;], bbox, dist_thresh, self.opt)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/dataset/generic_dataset.py</p><span class=\'hidden-code\' data-code=\'class GenericDataset(data.Dataset):\n    def _add_rot(self, ret, ann, k, gt_det):\n        if &amp;#39;alpha&amp;#39; in ann:          # -pi->pi\n            ret[&amp;#39;rot_mask&amp;#39;][k] = 1\n            alpha = ann[&amp;#39;alpha&amp;#39;]\n            if alpha `<` np.pi / 6. or alpha `>` 5 * np.pi / 6.:    # [-pi-`>`pi/6]  [5pi/6-`>`pi]   8/6\n                ret[&amp;#39;rotbin&amp;#39;][k, 0] = 1                         # ret[&amp;#39;rotbin&amp;#39;].shape=(128, 2)\n                ret[&amp;#39;rotres&amp;#39;][k, 0] = alpha - (-0.5 * np.pi)    # ret[&amp;#39;rotres&amp;#39;].shape=(128, 2);角度扔到两个区间算\n            if alpha `>` -np.pi / 6. or alpha `<` -5 * np.pi / 6.:  # [-pi/6-`>`pi]  [-pi-`>`-5pi/6]\n                ret[&amp;#39;rotbin&amp;#39;][k, 1] = 1\n                ret[&amp;#39;rotres&amp;#39;][k, 1] = alpha - (0.5 * np.pi)\n            gt_det[&amp;#39;rot&amp;#39;].append(self._alpha_to_8(ann[&amp;#39;alpha&amp;#39;]))\n        else:\n            gt_det[&amp;#39;rot&amp;#39;].append(self._alpha_to_8(0))\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/utils/pointcloud.py</p><span class=\'hidden-code\' data-code=\'def pc_dep_to_hm(pc_hm, pc_dep, dep, bbox, dist_thresh, opt):\n    if isinstance(dep, list) and len(dep) > 0:\n        dep = dep[0]\n    ct = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], dtype=np.float32)                       # 原先图片上的2Dbox\n    bbox_int = np.array([np.floor(bbox[0]), np.floor(bbox[1]), np.ceil(bbox[2]), np.ceil(bbox[3])], np.int32) # format: xyxy\n    roi = pc_dep[:, bbox_int[1]:bbox_int[3]+1, bbox_int[0]:bbox_int[2]+1]            # 用2Dbox将原先的值抠出来\n    pc_dep = roi[opt.pc_feat_channels[&amp;#39;pc_dep&amp;#39;]]\n    pc_vx = roi[opt.pc_feat_channels[&amp;#39;pc_vx&amp;#39;]]\n    pc_vz = roi[opt.pc_feat_channels[&amp;#39;pc_vz&amp;#39;]]\n    nonzero_inds = np.nonzero(pc_dep)\n    if len(nonzero_inds[0]) > 0:               # 这里进行匹配，将毫米波每个点生成的feat和2Dbox自带的深度匹配\n        #  nonzero_pc_dep = np.exp(-pc_dep[nonzero_inds])\n        nonzero_pc_dep = pc_dep[nonzero_inds]\n        nonzero_pc_vx = pc_vx[nonzero_inds]\n        nonzero_pc_vz = pc_vz[nonzero_inds]\n        # Get points within dist threshold\n        within_thresh = (nonzero_pc_dep `<` dep+dist_thresh) &amp; (nonzero_pc_dep `>` max(0, dep-dist_thresh))\n        pc_dep_match = nonzero_pc_dep[within_thresh]\n        pc_vx_match = nonzero_pc_vx[within_thresh]\n        pc_vz_match = nonzero_pc_vz[within_thresh]\n        if len(pc_dep_match) > 0:\n            arg_min = np.argmin(pc_dep_match)      # 要是匹配上了，优先匹配最近的\n            dist = pc_dep_match[arg_min]\n            vx = pc_vx_match[arg_min]\n            vz = pc_vz_match[arg_min]\n            if opt.normalize_depth:\n                dist /= opt.max_pc_dist\n            w = bbox[2] - bbox[0]\n            w_interval = opt.hm_to_box_ratio*(w)      # 范围为中心点往外扩张得长宽的opt.hm_to_box_ratio=0.3倍 \n            w_min = int(ct[0] - w_interval/2.)\n            w_max = int(ct[0] + w_interval/2.)\n            h = bbox[3] - bbox[1]\n            h_interval = opt.hm_to_box_ratio*(h)\n            h_min = int(ct[1] - h_interval/2.)\n            h_max = int(ct[1] + h_interval/2.)\n            pc_hm[opt.pc_feat_channels[&amp;#39;pc_dep&amp;#39;],h_min:h_max+1, w_min:w_max+1+1] = dist     # pc_hm.shape=(3, 112, 200);\n            pc_hm[opt.pc_feat_channels[&amp;#39;pc_vx&amp;#39;],h_min:h_max+1, w_min:w_max+1+1] = vx\n            pc_hm[opt.pc_feat_channels[&amp;#39;pc_vz&amp;#39;],h_min:h_max+1, w_min:w_max+1+1] = vz \n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">训练模型</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">src/lib/model/networks/dla.py</p><span class=\'hidden-code\' data-code=\'class DLASeg(BaseModel):\n    def __init__(self, num_layers, heads, head_convs, opt):\n        super(DLASeg, self).__init__(heads, head_convs, 1, 64 if num_layers == 34 else 128, opt=opt)\n        # heads={&amp;#39;hm&amp;#39;:10, &amp;#39;reg&amp;#39;:2, &amp;#39;wh&amp;#39;:2, &amp;#39;dep&amp;#39;:1, &amp;#39;rot&amp;#39;:8, &amp;#39;dim&amp;#39;:3, &amp;#39;amodel_offset&amp;#39;:2, &amp;#39;dep_sec&amp;#39;:1, &amp;#39;rot_sec&amp;#39;:8, &amp;#39;nuscenes_att&amp;#39;:8, &amp;#39;velocity&amp;#39;:3}\n        # head_convs={&amp;#39;hm&amp;#39;: [256], &amp;#39;reg&amp;#39;: [256], &amp;#39;wh&amp;#39;: [256], &amp;#39;dep&amp;#39;: [256], &amp;#39;rot&amp;#39;: [256], &amp;#39;dim&amp;#39;: [256], &amp;#39;amodel_offset&amp;#39;: [256], \n        # &amp;#39;dep_sec&amp;#39;: [256, 256, 256], &amp;#39;rot_sec&amp;#39;: [256, 256, 256], &amp;#39;nuscenes_att&amp;#39;: [256, 256, 256], &amp;#39;velocity&amp;#39;: [256, 256, 256]} \n        down_ratio=4\n        self.opt = opt\n        self.node_type = DLA_NODE[opt.dla_node]\n        print(&amp;#39;Using node type:&amp;#39;, self.node_type)\n        self.first_level = int(np.log2(down_ratio))\n        self.last_level = 5\n        self.base = globals()[&amp;#39;dla{}&amp;#39;.format(num_layers)](pretrained=(opt.load_model == &amp;#39;&amp;#39;), opt=opt)\n        channels = self.base.channels\n        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n        self.dla_up = DLAUp(self.first_level, channels[self.first_level:], scales, node_type=self.node_type)\n        out_channel = channels[self.first_level]\n        self.ida_up = IDAUp(out_channel, channels[self.first_level:self.last_level], [2 ** i for i in range(self.last_level - self.first_level)],node_type=self.node_type)\n\'> </span>'}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
