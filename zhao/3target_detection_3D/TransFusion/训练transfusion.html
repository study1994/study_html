<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>训练transfusion</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/transfusion.py</p><span class=\'hidden-code\' data-code=\'class TransFusionDetector(MVXTwoStageDetector):\n    def forward_train():\n        img_feats, pts_feats = self.`extract_feat`(points, img=img, img_metas=img_metas)  pts_feats[0].shape=torch.Size([2, 384, 128, 176]);\n        len(img_feats)=5/[12, 256, 128, 232]-[12, 256, 64, 116]-[12, 256, 32, 58]-[12, 256, 16, 29]-[12, 256, 8, 15]\n        losses = dict()\n        if pts_feats:\n            losses_pts = self.`forward_pts_train`(pts_feats, img_feats, gt_bboxes_3d,gt_labels_3d, img_metas,gt_bboxes_ignore)\n            losses.update(losses_pts)\n        if img_feats:\n            losses_img = self.`forward_img_train`(mg_feats,img_metas,gt_bboxes,gt_labels,gt_bboxes_ignore,proposals)\n            losses.update(losses_img)\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/mvx_two_stage.py</p><span class=\'hidden-code\' data-code=\'class MVXTwoStageDetector(Base3DDetector):\n    def extract_feat(self, points, img, img_metas):\n        img_feats = self.`extract_img_feat`(img, img_metas)\n        pts_feats = self.`extract_pts_feat`(points, img_feats, img_metas)\n        return (img_feats, pts_feats)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/transfusion.py</p><span class=\'hidden-code\' data-code=\'class TransFusionDetector(MVXTwoStageDetector):\n    def extract_img_feat(self, img, img_metas):       # img.size()=torch.Size([2, 6, 3, 448, 800])\n        if self.with_img_backbone and img is not None:\n            input_shape = img.shape[-2:]\n            # update real input shape of each single img\n            for img_meta in img_metas:\n                img_meta.update(input_shape=input_shape)\n            if img.dim() == 5 and img.size(0) == 1:\n                img.squeeze_(0)\n            elif img.dim() == 5 and img.size(0) > 1:\n                B, N, C, H, W = img.size()\n                img = img.view(B * N, C, H, W)          # torch.Size([12, 3, 448, 800])-->经过`ResNet`：len(img_feats=4;\n            img_feats = self.img_backbone(img.float())  # ([12, 256, 112, 200]、[12, 512, 56, 100]、[12, 1024, 28, 50]、[12, 2048, 14, 25])  \n        else:\n            return None\n        if self.with_img_neck:                      # 经过`FPN`:len(img_feats)=5;\n            img_feats = self.img_neck(img_feats)    # ([12, 256, 112, 200]、[12, 256, 56, 100]、[12, 256, 28, 50]、[12, 256, 14, 25]、[12, 256, 7, 13]);\n        return img_feats\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/transfusion.py</p><span class=\'hidden-code\' data-code=\'class TransFusionDetector(MVXTwoStageDetector):\n    def extract_pts_feat(self, pts, img_feats, img_metas):\n        &amp;#39;&amp;#39;&amp;#39;`PillarFeatureNet->PointPillarsScatter->SECOND->SECONDFPN`&amp;#39;&amp;#39;&amp;#39;\n        if not self.with_pts_bbox:\n            return None\n        voxels, num_points, coors = self.voxelize(pts)\n        voxel_features = self.pts_voxel_encoder(voxels, num_points, coors)\n        batch_size = coors[-1, 0] + 1\n        x = self.pts_middle_encoder(voxel_features, coors, batch_size)\n        x = self.pts_backbone(x)\n        if self.with_pts_neck:\n            x = self.pts_neck(x)     # len(pts_feats)=1; pts_feats[0].shape=torch.Size([2, 384, 128, 128]);\n        return x\n\'> </span>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/transfusion_head.py</p><span class=\'hidden-code\' data-code=\'class TransFusionHead(nn.Module):\n    def forward_single(self, inputs, img_inputs, img_metas):\n        batch_size = inputs.shape[0]\n        lidar_feat = self.shared_conv(inputs)    # lidar_feat.shape=torch.Size([2, 128, 128, 128])\n        #################################\n        # image to BEV\n        #################################\n        lidar_feat_flatten = lidar_feat.view(batch_size, lidar_feat.shape[1], -1) # torch.Size([2, 128, 128, 176])->torch.Size([2, 128, 22528])\n        bev_pos = self.bev_pos.repeat(batch_size, 1, 1).to(lidar_feat.device)     # torch.Size([2, 16384, 2]);128x128=16384个点，横竖的范围是[0.5,1.5,2.5]\n        if self.fuse_img:                                # 原始图片1600x900-。800x448->200x112 \n            img_feat = self.shared_conv_img(img_inputs)  # [BS*n_views,C,H,W]  torch.Size([12, 256, 112, 200])->torch.Size([2, 128, 112, 1200])\n            img_h, img_w, num_channel = img_inputs.shape[-2], img_inputs.shape[-1], img_feat.shape[1]\n            raw_img_feat = img_feat.view(batch_size, self.num_views, num_channel, img_h, img_w).permute(0, 2, 3, 1, 4) # [BS, C, H, n_views, W]\n            img_feat = raw_img_feat.reshape(batch_size, num_channel, img_h, img_w * self.num_views)                    # [BS, C, H, n_views*W]=(2,128,112,200*6)\n            img_feat_collapsed = img_feat.max(2).values  # torch.Size([2, 128, 1200])在h也就是112做max\n            img_feat_collapsed = self.fc(img_feat_collapsed).view(batch_size, num_channel, img_w * self.num_views)\n            # positional encoding for image guided query initialization\n            if self.img_feat_collapsed_pos is None:                         # torch.Size([1, 1200, 2])   \n                img_feat_collapsed_pos = self.img_feat_collapsed_pos = self.`create_2D_grid`(1, img_feat_collapsed.shape[-1]).to(img_feat.device)\n                # torch.min(img_feat_collapsed_pos[:,:,0])=0.5000, torch.max()=1199.5000, torch.min(img_feat_collapsed_pos[:,:,1])=0.5000, torch.max()=0.5000\n            else:\n                img_feat_collapsed_pos = self.img_feat_collapsed_pos\n            bev_feat = lidar_feat_flatten\n            for idx_view in range(self.num_views):        # self.decoder里面有8个`TransformerDecoderLayer0`     bev_feat=[2, 128, 16384]作为Q，每张图片的img_feat_collapsed[...,v1:v2]作为kv \n                bev_feat = self.decoder[2 + idx_view](bev_feat, img_feat_collapsed[..., img_w * idx_view:img_w * (idx_view + 1)], bev_pos, img_feat_collapsed_pos[:, img_w * idx_view:img_w * (idx_view + 1)])\n        #################################\n        # image guided query initialization\n        #################################\n        if self.initialize_by_heatmap:\n            dense_heatmap = self.heatmap_head(lidar_feat)                                            # torch.Size([2, 10, 128, 128]);10是类别数量   \n            dense_heatmap_img = None\n            if self.fuse_img:\n                dense_heatmap_img = self.heatmap_head_img(bev_feat.view(lidar_feat.shape))           # torch.Size([2, 10, 128, 128])\n                heatmap = (dense_heatmap.detach().sigmoid()+dense_heatmap_img.detach().sigmoid())/2  # torch.Size([2, 10, 128, 128])\n            else:\n                heatmap = dense_heatmap.detach().sigmoid()\n            padding=self.nms_kernel_size//2                                                                     # padding=self.nms_kernel_size//2=3//2=1  \n            local_max = torch.zeros_like(heatmap)\n            # equals to nms radius = voxel_size * out_size_factor * kenel_size\n            local_max_inner = F.max_pool2d(heatmap, kernel_size=self.nms_kernel_size, stride=1, padding=0)      # torch.Size([2, 10, 126, 126])\n            local_max[:, :, padding:(-padding), padding:(-padding)] = local_max_inner                           # local_max[:,8,].shape=torch.Size([2, 128, 128]) \n            for Pedestrian &amp; Traffic_cone in nuScenes\n            if self.test_cfg[&amp;#39;dataset&amp;#39;] == &amp;#39;nuScenes&amp;#39;:\n                local_max[:, 8, ] = F.max_pool2d(heatmap[:, 8], kernel_size=1, stride=1, padding=0)\n                local_max[:, 9, ] = F.max_pool2d(heatmap[:, 9], kernel_size=1, stride=1, padding=0)\n            elif self.test_cfg[&amp;#39;dataset&amp;#39;] == &amp;#39;Waymo&amp;#39;:  # for Pedestrian &amp; Cyclist in Waymo\n                local_max[:, 1, ] = F.max_pool2d(heatmap[:, 1], kernel_size=1, stride=1, padding=0)\n                local_max[:, 2, ] = F.max_pool2d(heatmap[:, 2], kernel_size=1, stride=1, padding=0)\n            elif self.test_cfg[&amp;#39;dataset&amp;#39;] == &amp;#39;pandaset&amp;#39;:\n                local_max[:, 0, ] = F.max_pool2d(heatmap[:, 0], kernel_size=1, stride=1, padding=0)\n                local_max[:, 1, ] = F.max_pool2d(heatmap[:, 1], kernel_size=1, stride=1, padding=0)\n            elif self.test_cfg[&amp;#39;dataset&amp;#39;] == &amp;#39;minieye&amp;#39;:\n                local_max[:, 1, ] = F.max_pool2d(heatmap[:, 1], kernel_size=1, stride=1, padding=0)\n                local_max[:, 2, ] = F.max_pool2d(heatmap[:, 2], kernel_size=1, stride=1, padding=0)\n                local_max[:, 3, ] = F.max_pool2d(heatmap[:, 3], kernel_size=1, stride=1, padding=0)\n            heatmap = heatmap * (heatmap == local_max)\n            heatmap = heatmap.view(batch_size, heatmap.shape[1], -1)         # torch.Size([2, 7, 22528])\n            # top #num_proposals among all classes\n            top_proposals = heatmap.view(batch_size, -1).argsort(dim=-1, descending=True)[..., :self.num_proposals]   # top_proposals.shape=torch.Size([2, 200]);\n            top_proposals_class = top_proposals // heatmap.shape[-1]         # top_proposals_class.shape=torch.Size([2, 200])【self.query_labels】;  top_proposals_class:0-9\n            top_proposals_index = top_proposals % heatmap.shape[-1]          # top_proposals_index.shape=torch.Size([2, 200]);                       top_proposals_index:1502-16176\n            # query_feat.shape=torch.Size([2, 128, 200]);lidar_feat_flatten.gather从(2,128,128,128)的后两个128里面取值 \n            query_feat = lidar_feat_flatten.gather(index=top_proposals_index[:, None, :].expand(-1, lidar_feat_flatten.shape[1], -1), dim=-1)\n            self.query_labels = top_proposals_class\n            # add category embedding\n            one_hot = F.one_hot(top_proposals_class, num_classes=self.num_classes).permute(0, 2, 1)      # one_hot.shape=torch.Size([2, 10, 200]);\n            query_cat_encoding = self.class_encoding(one_hot.float())                                    # query_cat_encoding.shape=torch.Size([2, 128, 200]);\n            query_feat += query_cat_encoding                                                             # query_feat.shape=torch.Size([2, 128, 200]);\n            query_pos = bev_pos.gather(index=top_proposals_index[:, None, :].permute(0, 2, 1).expand(-1, -1, bev_pos.shape[-1]), dim=1)   # query_pos = bev_pos.gather是从bve_pose=torch.Size([2, 16384, 2]里面得到 query_pos=(2,200,2)? \n        else:\n            query_feat = self.query_feat.repeat(batch_size, 1, 1)  # [BS, C, num_proposals]\n            base_xyz = self.query_pos.repeat(batch_size, 1, 1).to(lidar_feat.device)  # [BS, num_proposals, 2]\n        #################################\n        # transformer decoder layer (LiDAR feature as K,V)\n        #################################\n        ret_dicts = []\n        for i in range(self.num_decoder_layers):\n            prefix = &amp;#39;last_&amp;#39; if (i == self.num_decoder_layers - 1) else f&amp;#39;{i}head_&amp;#39;\n            # Transformer Decoder Layer\n            # :param query: B C Pq    :param query_pos: B Pq 3/6\n            query_feat = self.decoder[i](query_feat, lidar_feat_flatten, query_pos, bev_pos)         # torch.Size([2, 128, 200])\n            # Prediction\n            res_layer = self.prediction_heads[i](query_feat)                        # self.prediction_heads[i]=FFN((center)+(height)+(dim)+(rot)+(vel)+(heatmap)) \n            res_layer[&amp;#39;center&amp;#39;] = res_layer[&amp;#39;center&amp;#39;]+query_pos.permute(0,2,1)      # torch.Size([2, 2, 200]) res_layer.keys()=dict_keys([&amp;#39;center&amp;#39;, &amp;#39;height&amp;#39;, &amp;#39;dim&amp;#39;, &amp;#39;rot&amp;#39;, &amp;#39;vel&amp;#39;, &amp;#39;heatmap&amp;#39;])\n            first_res_layer = res_layer\n            if not self.fuse_img:\n                ret_dicts.append(res_layer)\n            # for next level positional embedding\n            query_pos = res_layer[&amp;#39;center&amp;#39;].detach().clone().permute(0, 2, 1)       # query_pos.shape=torch.Size([2, 200, 2])利用中心点更新pose\n        #################################\n        # transformer decoder layer (img feature as K,V)\n        #################################\n        if self.fuse_img:\n            # positional encoding for image fusion\n            img_feat = raw_img_feat.permute(0, 3, 1, 2, 4)                                 # [BS, n_views, C, H,W]--[2, 6, 128, 112, 200]\n            img_feat_flatten = img_feat.view(batch_size, self.num_views, num_channel, -1)  # [BS, n_views, C, H*W]--[2, 6, 128, 22400]\n            if self.img_feat_pos is None:\n                (h, w) = img_inputs.shape[-2], img_inputs.shape[-1]                        # 112,200\n                img_feat_pos = self.create_2D_grid(h, w).to(img_feat_flatten.device)       # [1, 22400, 2]\n            else:\n                img_feat_pos = self.img_feat_pos\n            prev_query_feat = query_feat.detach().clone()                                  # torch.Size([2, 128, 200]);原先雷达的特征\n            query_feat = torch.zeros_like(query_feat)  # create new container for img query feature 原先雷达的特征到真实坐标点的，注意这里为什么x和y的坐标要相等`query_pos_realmetric = ... + self.test_cfg[&amp;#39;pc_range&amp;#39;][0]`\n            query_pos_realmetric = query_pos.permute(0, 2, 1) * self.test_cfg[&amp;#39;out_size_factor&amp;#39;] * self.test_cfg[&amp;#39;voxel_size&amp;#39;][0] + self.test_cfg[&amp;#39;pc_range&amp;#39;][0]\n            query_pos_3d = torch.cat([query_pos_realmetric, res_layer[&amp;#39;height&amp;#39;]], dim=1).detach().clone()   # res_layer[&amp;#39;height&amp;#39;].shape=torch.Size([2, 1, 200]);这个是z坐标，需要减去高的一半回归3D box\n            if &amp;#39;vel&amp;#39; in res_layer:                     # query_pos_3d.shape=torch.Size([2, 3, 200]);  \n                vel = copy.deepcopy(res_layer[&amp;#39;vel&amp;#39;].detach())\n            else:\n                vel = None\n            pred_boxes = self.bbox_coder.`decode`(            # pred_boxes[0].keys()=dict_keys([&amp;#39;bboxes&amp;#39;, &amp;#39;scores&amp;#39;, &amp;#39;labels&amp;#39;]); \n                copy.deepcopy(res_layer[&amp;#39;heatmap&amp;#39;].detach()),\n                copy.deepcopy(res_layer[&amp;#39;rot&amp;#39;].detach()),\n                copy.deepcopy(res_layer[&amp;#39;dim&amp;#39;].detach()),\n                copy.deepcopy(res_layer[&amp;#39;center&amp;#39;].detach()),\n                copy.deepcopy(res_layer[&amp;#39;height&amp;#39;].detach()),\n                vel,\n            )\n            on_the_image_mask = torch.ones([batch_size, self.num_proposals]).to(query_pos_3d.device) * -1   # on_the_image_mask=torch.Size([2, 200]);\n            for sample_idx in range(batch_size if self.fuse_img else 0):\n                lidar2img_rt = query_pos_3d.new_tensor(img_metas[sample_idx][&amp;#39;lidar2img&amp;#39;])      # lidar2img_rt.shape=torch.Size([6, 4, 4]); \n                img_scale_factor = (                                                            # tensor([0.4975/796, 0.4978/448], device=&amp;#39;cuda:0&amp;#39;)  - tensor([0.4740/910, 0.4741/512], device=&amp;#39;cuda:0&amp;#39;)\n                    query_pos_3d.new_tensor(img_metas[sample_idx][&amp;#39;scale_factor&amp;#39;][:2]\n                                            if &amp;#39;scale_factor&amp;#39; in img_metas[sample_idx].keys() else [1.0, 1.0]))      \n                img_flip = img_metas[sample_idx][&amp;#39;flip&amp;#39;] if &amp;#39;flip&amp;#39; in img_metas[sample_idx].keys() else False        # True\n                img_crop_offset = (                                                                                  # 0\n                    query_pos_3d.new_tensor(img_metas[sample_idx][&amp;#39;img_crop_offset&amp;#39;])\n                    if &amp;#39;img_crop_offset&amp;#39; in img_metas[sample_idx].keys() else 0)\n                img_shape = img_metas[sample_idx][&amp;#39;img_shape&amp;#39;][:2]                            # (448, 796)  -  (512, 910)                           1080,1920\n                img_pad_shape = img_metas[sample_idx][&amp;#39;input_shape&amp;#39;][:2]                      # torch.Size([448, 800])  -- torch.Size([512, 928])   900，1600\n                boxes = LiDARInstance3DBoxes(pred_boxes[sample_idx][&amp;#39;bboxes&amp;#39;][:, :7], box_dim=7)\n                query_pos_3d_with_corners = torch.cat([query_pos_3d[sample_idx], boxes.corners.permute(2, 0, 1).view(3, -1)], dim=-1)  # [3, num_proposals]中心点 + [3, num_proposals*8]8个顶点\n                # transform point clouds back to original coordinate system by reverting the data augmentation                         # torch.Size([3, 1800]);200个中心点加`200*8`个角点，前面200个判断在哪个图片上面；\n                # if batch_size == 1:  # skip during inference to save time\n                #     points = query_pos_3d_with_corners.T\n                # else:\n                points = apply_3d_transformation(query_pos_3d_with_corners.T, &amp;#39;LIDAR&amp;#39;, img_metas[sample_idx], reverse=True).detach()  # 在点云上面的点映射回原始图片上 torch.Size([1800, 3])：1800=200+200x8  \n                num_points = points.shape[0]                                # 1800\n                for view_idx in range(self.num_views):\n                    pts_4d = torch.cat([points, points.new_ones(size=(num_points, 1))], dim=-1)\n                    pts_2d = pts_4d @ lidar2img_rt[view_idx].t()     # pts_4d.shape=torch.Size([1800, 4]); pts_2d.shape=torch.Size([1800, 4]);\n                    pts_2d[:, 2] = torch.clamp(pts_2d[:, 2], min=1e-5)\n                    pts_2d[:, 0] /= pts_2d[:, 2]\n                    pts_2d[:, 1] /= pts_2d[:, 2]\n                    # img transformation: scale -> crop -> flip\n                    # the image is resized by img_scale_factor\n                    img_coors = pts_2d[:, 0:2] * img_scale_factor  # Nx2                     tensor([0.4740, 0.4741], device=&amp;#39;cuda:0&amp;#39;)  在缩小的图片上\n                    img_coors -= img_crop_offset                   # 0\n                    # grid sample, the valid grid range should be in [-1,1]\n                    coor_x, coor_y = torch.split(img_coors, 1, dim=1)  # each is Nx1    coor_x.shape=torch.Size([200, 1]); coor_y.shape=torch.Size([200, 1]);\n                    if img_flip:\n                        # by default we take it as horizontal flip\n                        # use img_shape before padding for flip \n                        orig_h, orig_w = img_shape                # 812,910\n                        coor_x = orig_w - coor_x\n                    coor_x, coor_corner_x = coor_x[0:self.num_proposals, :], coor_x[self.num_proposals:, :]     # 预测的中心点，预测的八个顶点\n                    coor_y, coor_corner_y = coor_y[0:self.num_proposals, :], coor_y[self.num_proposals:, :]\n                    coor_corner_x = coor_corner_x.reshape(self.num_proposals, 8, 1)        # torch.Size([1600, 1])=torch.Size([200, 8, 1])\n                    coor_corner_y = coor_corner_y.reshape(self.num_proposals, 8, 1)        # torch.Size([1600, 1])=torch.Size([200, 8, 1])\n                    coor_corner_xy = torch.cat([coor_corner_x, coor_corner_y], dim=-1)     # 这个是预测的200个3Dbox的角点->coor_corner_xy=(200,8,2)\n                    h, w = img_pad_shape\n                    on_the_image = (coor_x `>` 0) * (coor_x `<` w) * (coor_y `>` 0) * (coor_y `<` h)   # on_the_image.shape=torch.Size([200]);\n                    on_the_image = on_the_image.squeeze()\n                    # skip the following computation if no object query fall on current image\n                    if on_the_image.sum() <= 1:\n                        continue\n                    on_the_image_mask[sample_idx, on_the_image] = view_idx\n                    ########################################################################\n                    # import os,cv2\n                    # img_coor = coor_corner_xy[on_the_image]\n                    # data_file = img_metas[sample_idx][&amp;#39;filename&amp;#39;][view_idx].replace(&amp;#39;./data/pandaset/PandaSet&amp;#39;,&amp;#39;./data_input/pandaset_head&amp;#39;)\n                    # img_arr = cv2.imread(data_file)\n                    # for ii in range(img_coor.shape[0]):\n                    #     draw_projected_box3d(img_arr, img_coor[ii].cpu().numpy(), color=(255,255,255), thickness=2)\n                    # save_path = data_file.replace(&amp;#39;/pandaset_head&amp;#39;,&amp;#39;/pandaset_head_res&amp;#39;)\n                    # save_path_root = os.path.dirname(save_path)\n                    # if not os.path.exists(save_path_root):\n                    #     os.makedirs(save_path_root)\n                    # cv2.imwrite(save_path,img_arr)\n                    ########################################################################\n                    # add spatial constraint\n                    center_ys = (coor_y[on_the_image] / self.out_size_factor_img)           # 4\n                    center_xs = (coor_x[on_the_image] / self.out_size_factor_img)\n                    centers = torch.cat([center_xs, center_ys], dim=-1).int()  # center on the feature map  \n                    # centers.shape=torch.Size([33, 2])->corners.shape=torch.Size([33, 2]);表示在这个图像中有33个点在这图片里面，其中2表示醉最大的x，y范围\n                    corners = (coor_corner_xy[on_the_image].max(1).values - coor_corner_xy[on_the_image].min(1).values) / self.out_size_factor_img         # 8个顶点-4\n                    radius = torch.ceil(corners.norm(dim=-1, p=2) / 2).int()  # radius of the minimum circumscribed circle of the wireframe\n                    sigma = (radius * 2 + 1) / 6.0\n                    distance = (centers[:, None, :] - (img_feat_pos - 0.5)).norm(dim=-1) ** 2       # sigma/radius.shape=torch.Size([33]); distance.shape=torch.Size([33, 22400]) \n                    gaussian_mask = (-distance / (2 * sigma[:, None] ** 2)).exp()\n                    gaussian_mask[gaussian_mask < torch.finfo(torch.float32).eps] = 0\n                    attn_mask = gaussian_mask                                                       # attn_mask.shape=torch.Size([33, 22400]);\n                    query_feat_view = prev_query_feat[sample_idx, :, on_the_image]\n                    query_pos_view = torch.cat([center_xs, center_ys], dim=-1)                      # query_feat_view.shape=torch.Size([128, 33]); query_pos_view.shape=torch.Size([33, 2]);\n                    query_feat_view = self.`decoder`[self.num_decoder_layers](query_feat_view[None], img_feat_flatten[sample_idx:sample_idx + 1, view_idx], query_pos_view[None], img_feat_pos, attn_mask=attn_mask.log())\n                    query_feat[sample_idx, :, on_the_image] = query_feat_view.clone()      # img_feat_flatten[sample_idx:sample_idx + 1, view_idx].shape=torch.Size([1, 128, 22400]);\n            self.on_the_image_mask = (on_the_image_mask != -1)\n            res_layer = self.prediction_heads[self.num_decoder_layers](torch.cat([query_feat, prev_query_feat], dim=1))\n            res_layer[&amp;#39;center&amp;#39;] = res_layer[&amp;#39;center&amp;#39;] + query_pos.permute(0, 2, 1)\n            for key, value in res_layer.items():\n                pred_dim = value.shape[1]\n                res_layer[key][~self.on_the_image_mask.unsqueeze(1).repeat(1, pred_dim, 1)] = first_res_layer[key][~self.on_the_image_mask.unsqueeze(1).repeat(1, pred_dim, 1)]\n            ret_dicts.append(res_layer)\n        if self.initialize_by_heatmap:\n            ret_dicts[0][&amp;#39;query_heatmap_score&amp;#39;] = heatmap.gather(index=top_proposals_index[:, None, :].expand(-1, self.num_classes, -1), dim=-1)  # [bs, num_classes, num_proposals]\n            if self.fuse_img:\n                ret_dicts[0][&amp;#39;dense_heatmap&amp;#39;] = dense_heatmap_img\n            else:\n                ret_dicts[0][&amp;#39;dense_heatmap&amp;#39;] = dense_heatmap\n        if self.auxiliary is False:\n            # only return the results of last decoder layer\n            return [ret_dicts[-1]]\n        # return all the layer&amp;#39;s results for auxiliary superivison\n        new_res = {}                   # new_res.keys()=dict_keys([&amp;#39;center&amp;#39;, &amp;#39;height&amp;#39;, &amp;#39;dim&amp;#39;, &amp;#39;rot&amp;#39;, &amp;#39;vel&amp;#39;, &amp;#39;heatmap&amp;#39;, &amp;#39;query_heatmap_score&amp;#39;, &amp;#39;dense_heatmap&amp;#39;])\n        for key in ret_dicts[0].keys():                # new_res[&amp;#39;center&amp;#39;].shape=torch.Size([2, 2, 200])； new_res[&amp;#39;heatmap&amp;#39;].shape=torch.Size([2, 10, 200])；\n            if key not in [&amp;#39;dense_heatmap&amp;#39;, &amp;#39;dense_heatmap_old&amp;#39;, &amp;#39;query_heatmap_score&amp;#39;]:\n                new_res[key] = torch.cat([ret_dict[key] for ret_dict in ret_dicts], dim=-1)\n            else:\n                new_res[key] = ret_dicts[0][key]       # new_res[&amp;#39;query_heatmap_score&amp;#39;].shape=torch.Size([2, 10, 200])；new_res[&amp;#39;dense_heatmap&amp;#39;].shape=torch.Size([2, 10, 128, 128]) \n        return [new_res]\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/transfusion_head.py</p><span class=\'hidden-code\' data-code=\'class TransFusionHead(nn.Module):\n    def create_2D_grid(self, x_size, y_size):     # (1,1200)\n        meshgrid = [[0, x_size - 1, x_size], [0, y_size - 1, y_size]]\n        batch_y, batch_x = torch.meshgrid(*[torch.linspace(it[0], it[1], it[2]) for it in meshgrid])\n        batch_x = batch_x + 0.5\n        batch_y = batch_y + 0.5\n        coord_base = torch.cat([batch_x[None], batch_y[None]], dim=0)[None]\n        coord_base = coord_base.view(1, 2, -1).permute(0, 2, 1)\n        return coord_base\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/core/bbox/coders/transfusion_bbox_coder.py</p><span class=\'hidden-code\' data-code=\'class TransFusionBBoxCoder(BaseBBoxCoder):\n    def decode(self, heatmap, rot, dim, center, height, vel, filter=False):\n        # class label\n        final_preds = heatmap.max(1, keepdims=False).indices        # torch.Size([1, 7, 200])->torch.Size([1, 200])\n        final_scores = heatmap.max(1, keepdims=False).values\n        # change size to real world metric\n        center[:, 0, :] = center[:, 0, :] * self.out_size_factor * self.voxel_size[0] + self.pc_range[0]      # *4*0.2+(-70.4)\n        center[:, 1, :] = center[:, 1, :] * self.out_size_factor * self.voxel_size[1] + self.pc_range[1]      # *4*0.2+(-51.2)\n        # center[:, 2, :] = center[:, 2, :] * (self.post_center_range[5] - self.post_center_range[2]) + self.post_center_range[2]\n        dim[:, 0, :] = dim[:, 0, :].exp()\n        dim[:, 1, :] = dim[:, 1, :].exp()\n        dim[:, 2, :] = dim[:, 2, :].exp()\n        height = height - dim[:, 2:3, :] * 0.5  # gravity center to bottom center\n        rots, rotc = rot[:, 0:1, :], rot[:, 1:2, :]\n        rot = torch.atan2(rots, rotc)\n        if vel is None:\n            final_box_preds = torch.cat([center, height, dim, rot], dim=1).permute(0, 2, 1)\n        else:\n            final_box_preds = torch.cat([center, height, dim, rot, vel], dim=1).permute(0, 2, 1)\n\'> </span>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_targets_single</p>输入：gt_bboxes_3d, gt_labels_3d, preds_dict(模型预测的结果), batch_idx<br>\nboxes_dict[0].keys()=dict_keys([\'bboxes\', \'scores\', \'labels\']);boxes_dict[0][\'bboxes\'].shape=torch.Size([200, 9]);<br>\nnum_layer=1;bboxes_tensor_layer.shape=torch.Size([200, 9]);score_layer.shape=torch.Size([1, 7, 200]);<br>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">class HungarianAssigner3D</p>mmdet3d/core/bbox/assigners/hungarian_assigner.py:def assign<br>\nnum_gts=73;num_bboxes=200;<br>\n1. assign -1 by default<br>\nassigned_gt_inds.shape=torch.Size([200])<br>\n2. compute the weighted costs<br>\ncls_pred[0].T.shape=torch.Size([200, 7]);gt_labels.shape=torch.Size([73]);<br>\ncls_cost.shape=torch.Size([200, 73])<br>\n3. do Hungarian matching on CPU using linear_sum_assignment<br>\n4. assign backgrounds and foregrounds<br>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_targets_single_1</p>bbox_targets.shape=torch.Size([200, 10]);【x,y,z,w,h,l,raw_cos,raw_sin,vx,vy】<br>\nlabels[None].shape=(1, 200);<br>'}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
