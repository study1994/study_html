<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>训练transfusion</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/transfusion.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransFusionDetector(MVXTwoStageDetector):\n    def forward_train():\n        img_feats, pts_feats = self.<span style=\'color: green;font-weight: bold;\'>extract_feat</span>(points, img=img, img_metas=img_metas)  <span style=\'color: red\'>pts_feats[0].shape=torch.Size([2, 384, 128, 176]);</span>\n        <span style=\'color: red\'>len(img_feats)=5/[12, 256, 128, 232]-[12, 256, 64, 116]-[12, 256, 32, 58]-[12, 256, 16, 29]-[12, 256, 8, 15]</span>\n        losses = dict()\n        if pts_feats:\n            losses_pts = self.<span style=\'color: green;font-weight: bold;\'>forward_pts_train</span>(pts_feats, img_feats, gt_bboxes_3d,gt_labels_3d, img_metas,gt_bboxes_ignore)\n            losses.update(losses_pts)\n        if img_feats:\n            losses_img = self.<span style=\'color: green;font-weight: bold;\'>forward_img_train</span>(mg_feats,img_metas,gt_bboxes,gt_labels,gt_bboxes_ignore,proposals)\n            losses.update(losses_img)\n        return losses\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/mvx_two_stage.py</p><font size="0"><pre class="language-python"><code class="language-python">class MVXTwoStageDetector(Base3DDetector):\n    def extract_feat(self, points, img, img_metas):\n        img_feats = self.<span style=\'color: green;font-weight: bold;\'>extract_img_feat</span>(img, img_metas)\n        pts_feats = self.<span style=\'color: green;font-weight: bold;\'>extract_pts_feat</span>(points, img_feats, img_metas)\n        return (img_feats, pts_feats)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/transfusion.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransFusionDetector(MVXTwoStageDetector):\n    def extract_img_feat(self, img, img_metas):       <span style=\'color: red\'># img.size()=torch.Size([2, 6, 3, 448, 800])</span>\n        if self.with_img_backbone and img is not None:\n            input_shape = img.shape[-2:]\n            <span style=\'color: red\'># update real input shape of each single img</span>\n            for img_meta in img_metas:\n                img_meta.update(input_shape=input_shape)\n            if img.dim() == 5 and img.size(0) == 1:\n                img.squeeze_(0)\n            elif img.dim() == 5 and img.size(0) > 1:\n                B, N, C, H, W = img.size()\n                img = img.view(B * N, C, H, W)          <span style=\'color: red\'># torch.Size([12, 3, 448, 800])-->经过<span style=\'color: green;font-weight: bold;\'>ResNet</span>：len(img_feats=4;</span>\n            img_feats = self.img_backbone(img.float())  <span style=\'color: red\'># ([12, 256, 112, 200]、[12, 512, 56, 100]、[12, 1024, 28, 50]、[12, 2048, 14, 25])  </span>\n        else:\n            return None\n        if self.with_img_neck:                      <span style=\'color: red\'># 经过<span style=\'color: green;font-weight: bold;\'>FPN</span>:len(img_feats)=5;</span>\n            img_feats = self.img_neck(img_feats)    <span style=\'color: red\'># ([12, 256, 112, 200]、[12, 256, 56, 100]、[12, 256, 28, 50]、[12, 256, 14, 25]、[12, 256, 7, 13]);</span>\n        return img_feats\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/detectors/transfusion.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransFusionDetector(MVXTwoStageDetector):\n    def extract_pts_feat(self, pts, img_feats, img_metas):\n        """<span style=\'color: green;font-weight: bold;\'>PillarFeatureNet->PointPillarsScatter->SECOND->SECONDFPN</span>"""\n        if not self.with_pts_bbox:\n            return None\n        voxels, num_points, coors = self.voxelize(pts)\n        voxel_features = self.pts_voxel_encoder(voxels, num_points, coors)\n        batch_size = coors[-1, 0] + 1\n        x = self.pts_middle_encoder(voxel_features, coors, batch_size)\n        x = self.pts_backbone(x)\n        if self.with_pts_neck:\n            x = self.pts_neck(x)     <span style=\'color: red\'># len(pts_feats)=1; pts_feats[0].shape=torch.Size([2, 384, 128, 128]);</span>\n        return x\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/transfusion_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransFusionHead(nn.Module):\n    def forward_single(self, inputs, img_inputs, img_metas):\n        batch_size = inputs.shape[0]\n        lidar_feat = self.shared_conv(inputs)    <span style=\'color: red\'># lidar_feat.shape=torch.Size([2, 128, 128, 128])</span>\n        #################################\n        <span style=\'color: red\'># image to BEV</span>\n        #################################\n        lidar_feat_flatten = lidar_feat.view(batch_size, lidar_feat.shape[1], -1) <span style=\'color: red\'># torch.Size([2, 128, 128, 176])->torch.Size([2, 128, 22528])</span>\n        bev_pos = self.bev_pos.repeat(batch_size, 1, 1).to(lidar_feat.device)     <span style=\'color: red\'># torch.Size([2, 16384, 2]);128x128=16384个点，横竖的范围是[0.5,1.5,2.5]</span>\n        if self.fuse_img:                                <span style=\'color: red\'># 原始图片1600x900-。800x448->200x112 </span>\n            img_feat = self.shared_conv_img(img_inputs)  <span style=\'color: red\'># [BS*n_views,C,H,W]  torch.Size([12, 256, 112, 200])->torch.Size([2, 128, 112, 1200])</span>\n            img_h, img_w, num_channel = img_inputs.shape[-2], img_inputs.shape[-1], img_feat.shape[1]\n            raw_img_feat = img_feat.view(batch_size, self.num_views, num_channel, img_h, img_w).permute(0, 2, 3, 1, 4) <span style=\'color: red\'># [BS, C, H, n_views, W]</span>\n            img_feat = raw_img_feat.reshape(batch_size, num_channel, img_h, img_w * self.num_views)                    <span style=\'color: red\'># [BS, C, H, n_views*W]=(2,128,112,200*6)</span>\n            img_feat_collapsed = img_feat.max(2).values  <span style=\'color: red\'># torch.Size([2, 128, 1200])在h也就是112做max</span>\n            img_feat_collapsed = self.fc(img_feat_collapsed).view(batch_size, num_channel, img_w * self.num_views)\n            <span style=\'color: red\'># positional encoding for image guided query initialization</span>\n            if self.img_feat_collapsed_pos is None:                         <span style=\'color: red\'># torch.Size([1, 1200, 2])   </span>\n                img_feat_collapsed_pos = self.img_feat_collapsed_pos = self.<span style=\'color: green;font-weight: bold;\'>create_2D_grid</span>(1, img_feat_collapsed.shape[-1]).to(img_feat.device)\n                <span style=\'color: red\'># torch.min(img_feat_collapsed_pos[:,:,0])=0.5000, torch.max()=1199.5000, torch.min(img_feat_collapsed_pos[:,:,1])=0.5000, torch.max()=0.5000</span>\n            else:\n                img_feat_collapsed_pos = self.img_feat_collapsed_pos\n            bev_feat = lidar_feat_flatten\n            for idx_view in range(self.num_views):        <span style=\'color: red\'># self.decoder里面有8个<span style=\'color: green;font-weight: bold;\'>TransformerDecoderLayer0</span>     bev_feat=[2, 128, 16384]作为Q，每张图片的img_feat_collapsed[...,v1:v2]作为kv </span>\n                bev_feat = self.decoder[2 + idx_view](bev_feat, img_feat_collapsed[..., img_w * idx_view:img_w * (idx_view + 1)], bev_pos, img_feat_collapsed_pos[:, img_w * idx_view:img_w * (idx_view + 1)])\n        #################################\n        <span style=\'color: red\'># image guided query initialization</span>\n        #################################\n        if self.initialize_by_heatmap:\n            dense_heatmap = self.heatmap_head(lidar_feat)                                            <span style=\'color: red\'># torch.Size([2, 10, 128, 128]);10是类别数量   </span>\n            dense_heatmap_img = None\n            if self.fuse_img:\n                dense_heatmap_img = self.heatmap_head_img(bev_feat.view(lidar_feat.shape))           <span style=\'color: red\'># torch.Size([2, 10, 128, 128])</span>\n                heatmap = (dense_heatmap.detach().sigmoid()+dense_heatmap_img.detach().sigmoid())/2  <span style=\'color: red\'># torch.Size([2, 10, 128, 128])</span>\n            else:\n                heatmap = dense_heatmap.detach().sigmoid()\n            padding=self.nms_kernel_size//2                                                                     <span style=\'color: red\'># padding=self.nms_kernel_size//2=3//2=1  </span>\n            local_max = torch.zeros_like(heatmap)\n            <span style=\'color: red\'># equals to nms radius = voxel_size * out_size_factor * kenel_size</span>\n            local_max_inner = F.max_pool2d(heatmap, kernel_size=self.nms_kernel_size, stride=1, padding=0)      <span style=\'color: red\'># torch.Size([2, 10, 126, 126])</span>\n            local_max[:, :, padding:(-padding), padding:(-padding)] = local_max_inner                           <span style=\'color: red\'># local_max[:,8,].shape=torch.Size([2, 128, 128]) </span>\n            for Pedestrian & Traffic_cone in nuScenes\n            if self.test_cfg[\'dataset\'] == \'nuScenes\':\n                local_max[:, 8, ] = F.max_pool2d(heatmap[:, 8], kernel_size=1, stride=1, padding=0)\n                local_max[:, 9, ] = F.max_pool2d(heatmap[:, 9], kernel_size=1, stride=1, padding=0)\n            elif self.test_cfg[\'dataset\'] == \'Waymo\':  <span style=\'color: red\'># for Pedestrian & Cyclist in Waymo</span>\n                local_max[:, 1, ] = F.max_pool2d(heatmap[:, 1], kernel_size=1, stride=1, padding=0)\n                local_max[:, 2, ] = F.max_pool2d(heatmap[:, 2], kernel_size=1, stride=1, padding=0)\n            elif self.test_cfg[\'dataset\'] == \'pandaset\':\n                local_max[:, 0, ] = F.max_pool2d(heatmap[:, 0], kernel_size=1, stride=1, padding=0)\n                local_max[:, 1, ] = F.max_pool2d(heatmap[:, 1], kernel_size=1, stride=1, padding=0)\n            elif self.test_cfg[\'dataset\'] == \'minieye\':\n                local_max[:, 1, ] = F.max_pool2d(heatmap[:, 1], kernel_size=1, stride=1, padding=0)\n                local_max[:, 2, ] = F.max_pool2d(heatmap[:, 2], kernel_size=1, stride=1, padding=0)\n                local_max[:, 3, ] = F.max_pool2d(heatmap[:, 3], kernel_size=1, stride=1, padding=0)\n            heatmap = heatmap * (heatmap == local_max)\n            heatmap = heatmap.view(batch_size, heatmap.shape[1], -1)         <span style=\'color: red\'># torch.Size([2, 7, 22528])</span>\n            <span style=\'color: red\'># top #num_proposals among all classes</span>\n            top_proposals = heatmap.view(batch_size, -1).argsort(dim=-1, descending=True)[..., :self.num_proposals]   <span style=\'color: red\'># top_proposals.shape=torch.Size([2, 200]);</span>\n            top_proposals_class = top_proposals // heatmap.shape[-1]         <span style=\'color: red\'># top_proposals_class.shape=torch.Size([2, 200])【self.query_labels】;  top_proposals_class:0-9</span>\n            top_proposals_index = top_proposals % heatmap.shape[-1]          <span style=\'color: red\'># top_proposals_index.shape=torch.Size([2, 200]);                       top_proposals_index:1502-16176</span>\n            <span style=\'color: red\'># query_feat.shape=torch.Size([2, 128, 200]);lidar_feat_flatten.gather从(2,128,128,128)的后两个128里面取值 </span>\n            query_feat = lidar_feat_flatten.gather(index=top_proposals_index[:, None, :].expand(-1, lidar_feat_flatten.shape[1], -1), dim=-1)\n            self.query_labels = top_proposals_class\n            <span style=\'color: red\'># add category embedding</span>\n            one_hot = F.one_hot(top_proposals_class, num_classes=self.num_classes).permute(0, 2, 1)      <span style=\'color: red\'># one_hot.shape=torch.Size([2, 10, 200]);</span>\n            query_cat_encoding = self.class_encoding(one_hot.float())                                    <span style=\'color: red\'># query_cat_encoding.shape=torch.Size([2, 128, 200]);</span>\n            query_feat += query_cat_encoding                                                             <span style=\'color: red\'># query_feat.shape=torch.Size([2, 128, 200]);</span>\n            query_pos = bev_pos.gather(index=top_proposals_index[:, None, :].permute(0, 2, 1).expand(-1, -1, bev_pos.shape[-1]), dim=1)   <span style=\'color: red\'># query_pos = bev_pos.gather是从bve_pose=torch.Size([2, 16384, 2]里面得到 query_pos=(2,200,2)? </span>\n        else:\n            query_feat = self.query_feat.repeat(batch_size, 1, 1)  <span style=\'color: red\'># [BS, C, num_proposals]</span>\n            base_xyz = self.query_pos.repeat(batch_size, 1, 1).to(lidar_feat.device)  <span style=\'color: red\'># [BS, num_proposals, 2]</span>\n        #################################\n        <span style=\'color: red\'># transformer decoder layer (LiDAR feature as K,V)</span>\n        #################################\n        ret_dicts = []\n        for i in range(self.num_decoder_layers):\n            prefix = \'last_\' if (i == self.num_decoder_layers - 1) else f\'{i}head_\'\n            <span style=\'color: red\'># Transformer Decoder Layer</span>\n            <span style=\'color: red\'># :param query: B C Pq    :param query_pos: B Pq 3/6</span>\n            query_feat = self.decoder[i](query_feat, lidar_feat_flatten, query_pos, bev_pos)         <span style=\'color: red\'># torch.Size([2, 128, 200])</span>\n            <span style=\'color: red\'># Prediction</span>\n            res_layer = self.prediction_heads[i](query_feat)                        <span style=\'color: red\'># self.prediction_heads[i]=FFN((center)+(height)+(dim)+(rot)+(vel)+(heatmap)) </span>\n            res_layer[\'center\'] = res_layer[\'center\']+query_pos.permute(0,2,1)      <span style=\'color: red\'># torch.Size([2, 2, 200]) res_layer.keys()=dict_keys([\'center\', \'height\', \'dim\', \'rot\', \'vel\', \'heatmap\'])</span>\n            first_res_layer = res_layer\n            if not self.fuse_img:\n                ret_dicts.append(res_layer)\n            <span style=\'color: red\'># for next level positional embedding</span>\n            query_pos = res_layer[\'center\'].detach().clone().permute(0, 2, 1)       <span style=\'color: red\'># query_pos.shape=torch.Size([2, 200, 2])利用中心点更新pose</span>\n        #################################\n        <span style=\'color: red\'># transformer decoder layer (img feature as K,V)</span>\n        #################################\n        if self.fuse_img:\n            <span style=\'color: red\'># positional encoding for image fusion</span>\n            img_feat = raw_img_feat.permute(0, 3, 1, 2, 4)                                 <span style=\'color: red\'># [BS, n_views, C, H,W]--[2, 6, 128, 112, 200]</span>\n            img_feat_flatten = img_feat.view(batch_size, self.num_views, num_channel, -1)  <span style=\'color: red\'># [BS, n_views, C, H*W]--[2, 6, 128, 22400]</span>\n            if self.img_feat_pos is None:\n                (h, w) = img_inputs.shape[-2], img_inputs.shape[-1]                        <span style=\'color: red\'># 112,200</span>\n                img_feat_pos = self.create_2D_grid(h, w).to(img_feat_flatten.device)       <span style=\'color: red\'># [1, 22400, 2]</span>\n            else:\n                img_feat_pos = self.img_feat_pos\n            prev_query_feat = query_feat.detach().clone()                                  <span style=\'color: red\'># torch.Size([2, 128, 200]);原先雷达的特征</span>\n            query_feat = torch.zeros_like(query_feat)  <span style=\'color: red\'># create new container for img query feature 原先雷达的特征到真实坐标点的，注意这里为什么x和y的坐标要相等<span style=\'color: green;font-weight: bold;\'>query_pos_realmetric = ... + self.test_cfg[\'pc_range\'][0]</span></span>\n            query_pos_realmetric = query_pos.permute(0, 2, 1) * self.test_cfg[\'out_size_factor\'] * self.test_cfg[\'voxel_size\'][0] + self.test_cfg[\'pc_range\'][0]\n            query_pos_3d = torch.cat([query_pos_realmetric, res_layer[\'height\']], dim=1).detach().clone()   <span style=\'color: red\'># res_layer[\'height\'].shape=torch.Size([2, 1, 200]);这个是z坐标，需要减去高的一半回归3D box</span>\n            if \'vel\' in res_layer:                     <span style=\'color: red\'># query_pos_3d.shape=torch.Size([2, 3, 200]);  </span>\n                vel = copy.deepcopy(res_layer[\'vel\'].detach())\n            else:\n                vel = None\n            pred_boxes = self.bbox_coder.<span style=\'color: green;font-weight: bold;\'>decode</span>(            <span style=\'color: red\'># pred_boxes[0].keys()=dict_keys([\'bboxes\', \'scores\', \'labels\']); </span>\n                copy.deepcopy(res_layer[\'heatmap\'].detach()),\n                copy.deepcopy(res_layer[\'rot\'].detach()),\n                copy.deepcopy(res_layer[\'dim\'].detach()),\n                copy.deepcopy(res_layer[\'center\'].detach()),\n                copy.deepcopy(res_layer[\'height\'].detach()),\n                vel,\n            )\n            on_the_image_mask = torch.ones([batch_size, self.num_proposals]).to(query_pos_3d.device) * -1   <span style=\'color: red\'># on_the_image_mask=torch.Size([2, 200]);</span>\n            for sample_idx in range(batch_size if self.fuse_img else 0):\n                lidar2img_rt = query_pos_3d.new_tensor(img_metas[sample_idx][\'lidar2img\'])      <span style=\'color: red\'># lidar2img_rt.shape=torch.Size([6, 4, 4]); </span>\n                img_scale_factor = (                                                            <span style=\'color: red\'># tensor([0.4975/796, 0.4978/448], device=\'cuda:0\')  - tensor([0.4740/910, 0.4741/512], device=\'cuda:0\')</span>\n                    query_pos_3d.new_tensor(img_metas[sample_idx][\'scale_factor\'][:2]\n                                            if \'scale_factor\' in img_metas[sample_idx].keys() else [1.0, 1.0]))      \n                img_flip = img_metas[sample_idx][\'flip\'] if \'flip\' in img_metas[sample_idx].keys() else False        <span style=\'color: red\'># True</span>\n                img_crop_offset = (                                                                                  <span style=\'color: red\'># 0</span>\n                    query_pos_3d.new_tensor(img_metas[sample_idx][\'img_crop_offset\'])\n                    if \'img_crop_offset\' in img_metas[sample_idx].keys() else 0)\n                img_shape = img_metas[sample_idx][\'img_shape\'][:2]                            <span style=\'color: red\'># (448, 796)  -  (512, 910)                           1080,1920</span>\n                img_pad_shape = img_metas[sample_idx][\'input_shape\'][:2]                      <span style=\'color: red\'># torch.Size([448, 800])  -- torch.Size([512, 928])   900，1600</span>\n                boxes = LiDARInstance3DBoxes(pred_boxes[sample_idx][\'bboxes\'][:, :7], box_dim=7)\n                query_pos_3d_with_corners = torch.cat([query_pos_3d[sample_idx], boxes.corners.permute(2, 0, 1).view(3, -1)], dim=-1)  <span style=\'color: red\'># [3, num_proposals]中心点 + [3, num_proposals*8]8个顶点</span>\n                <span style=\'color: red\'># transform point clouds back to original coordinate system by reverting the data augmentation                        </span>\n                <span style=\'color: red\'># if batch_size == 1: </span>\n                <span style=\'color: red\'>#     points = query_pos_3d_with_corners.T</span>\n                <span style=\'color: red\'># else:</span>\n                points = apply_3d_transformation(query_pos_3d_with_corners.T, \'LIDAR\', img_metas[sample_idx], reverse=True).detach()  <span style=\'color: red\'># 在点云上面的点映射回原始图片上 torch.Size([1800, 3])：1800=200+200x8  </span>\n                num_points = points.shape[0]                                <span style=\'color: red\'># 1800</span>\n                for view_idx in range(self.num_views):\n                    pts_4d = torch.cat([points, points.new_ones(size=(num_points, 1))], dim=-1)\n                    pts_2d = pts_4d @ lidar2img_rt[view_idx].t()     <span style=\'color: red\'># pts_4d.shape=torch.Size([1800, 4]); pts_2d.shape=torch.Size([1800, 4]);</span>\n                    pts_2d[:, 2] = torch.clamp(pts_2d[:, 2], min=1e-5)\n                    pts_2d[:, 0] /= pts_2d[:, 2]\n                    pts_2d[:, 1] /= pts_2d[:, 2]\n                    <span style=\'color: red\'># img transformation: scale -> crop -> flip</span>\n                    <span style=\'color: red\'># the image is resized by img_scale_factor</span>\n                    img_coors = pts_2d[:, 0:2] * img_scale_factor  <span style=\'color: red\'># Nx2                     tensor([0.4740, 0.4741], device=\'cuda:0\')  在缩小的图片上</span>\n                    img_coors -= img_crop_offset                   <span style=\'color: red\'># 0</span>\n                    <span style=\'color: red\'># grid sample, the valid grid range should be in [-1,1]</span>\n                    coor_x, coor_y = torch.split(img_coors, 1, dim=1)  <span style=\'color: red\'># each is Nx1    coor_x.shape=torch.Size([200, 1]); coor_y.shape=torch.Size([200, 1]);</span>\n                    if img_flip:\n                        <span style=\'color: red\'># by default we take it as horizontal flip</span>\n                        <span style=\'color: red\'># use img_shape before padding for flip </span>\n                        orig_h, orig_w = img_shape                <span style=\'color: red\'># 812,910</span>\n                        coor_x = orig_w - coor_x\n                    coor_x, coor_corner_x = coor_x[0:self.num_proposals, :], coor_x[self.num_proposals:, :]     <span style=\'color: red\'># 预测的中心点，预测的八个顶点</span>\n                    coor_y, coor_corner_y = coor_y[0:self.num_proposals, :], coor_y[self.num_proposals:, :]\n                    coor_corner_x = coor_corner_x.reshape(self.num_proposals, 8, 1)        <span style=\'color: red\'># torch.Size([1600, 1])=torch.Size([200, 8, 1])</span>\n                    coor_corner_y = coor_corner_y.reshape(self.num_proposals, 8, 1)        <span style=\'color: red\'># torch.Size([1600, 1])=torch.Size([200, 8, 1])</span>\n                    coor_corner_xy = torch.cat([coor_corner_x, coor_corner_y], dim=-1)     <span style=\'color: red\'># 这个是预测的200个3Dbox的角点->coor_corner_xy=(200,8,2)</span>\n                    h, w = img_pad_shape\n                    on_the_image = (coor_x > 0) * (coor_x < w) * (coor_y > 0) * (coor_y < h)   <span style=\'color: red\'># on_the_image.shape=torch.Size([200]);</span>\n                    on_the_image = on_the_image.squeeze()\n                    <span style=\'color: red\'># skip the following computation if no object query fall on current image</span>\n                    if on_the_image.sum() <= 1:\n                        continue\n                    on_the_image_mask[sample_idx, on_the_image] = view_idx\n                    ########################################################################\n                    <span style=\'color: red\'># import os,cv2</span>\n                    <span style=\'color: red\'># img_coor = coor_corner_xy[on_the_image]</span>\n                    <span style=\'color: red\'># data_file = img_metas[sample_idx][\'filename\'][view_idx].replace("./data/pandaset/PandaSet","./data_input/pandaset_head")</span>\n                    <span style=\'color: red\'># img_arr = cv2.imread(data_file)</span>\n                    <span style=\'color: red\'># for ii in range(img_coor.shape[0]):</span>\n                    <span style=\'color: red\'>#     draw_projected_box3d(img_arr, img_coor[ii].cpu().numpy(), color=(255,255,255), thickness=2)</span>\n                    <span style=\'color: red\'># save_path = data_file.replace("/pandaset_head","/pandaset_head_res")</span>\n                    <span style=\'color: red\'># save_path_root = os.path.dirname(save_path)</span>\n                    <span style=\'color: red\'># if not os.path.exists(save_path_root):</span>\n                    <span style=\'color: red\'>#     os.makedirs(save_path_root)</span>\n                    <span style=\'color: red\'># cv2.imwrite(save_path,img_arr)</span>\n                    ########################################################################\n                    <span style=\'color: red\'># add spatial constraint</span>\n                    center_ys = (coor_y[on_the_image] / self.out_size_factor_img)           <span style=\'color: red\'># 4</span>\n                    center_xs = (coor_x[on_the_image] / self.out_size_factor_img)\n                    centers = torch.cat([center_xs, center_ys], dim=-1).int()  <span style=\'color: red\'># center on the feature map  </span>\n                    <span style=\'color: red\'># centers.shape=torch.Size([33, 2])->corners.shape=torch.Size([33, 2]);表示在这个图像中有33个点在这图片里面，其中2表示醉最大的x，y范围</span>\n                    corners = (coor_corner_xy[on_the_image].max(1).values - coor_corner_xy[on_the_image].min(1).values) / self.out_size_factor_img         <span style=\'color: red\'># 8个顶点-4</span>\n                    radius = torch.ceil(corners.norm(dim=-1, p=2) / 2).int()  <span style=\'color: red\'># radius of the minimum circumscribed circle of the wireframe</span>\n                    sigma = (radius * 2 + 1) / 6.0\n                    distance = (centers[:, None, :] - (img_feat_pos - 0.5)).norm(dim=-1) ** 2       <span style=\'color: red\'># sigma/radius.shape=torch.Size([33]); distance.shape=torch.Size([33, 22400]) </span>\n                    gaussian_mask = (-distance / (2 * sigma[:, None] ** 2)).exp()\n                    gaussian_mask[gaussian_mask < torch.finfo(torch.float32).eps] = 0\n                    attn_mask = gaussian_mask                                                       <span style=\'color: red\'># attn_mask.shape=torch.Size([33, 22400]);</span>\n                    query_feat_view = prev_query_feat[sample_idx, :, on_the_image]\n                    query_pos_view = torch.cat([center_xs, center_ys], dim=-1)                      <span style=\'color: red\'># query_feat_view.shape=torch.Size([128, 33]); query_pos_view.shape=torch.Size([33, 2]);</span>\n                    query_feat_view = self.<span style=\'color: green;font-weight: bold;\'>decoder</span>[self.num_decoder_layers](query_feat_view[None], img_feat_flatten[sample_idx:sample_idx + 1, view_idx], query_pos_view[None], img_feat_pos, attn_mask=attn_mask.log())\n                    query_feat[sample_idx, :, on_the_image] = query_feat_view.clone()      <span style=\'color: red\'># img_feat_flatten[sample_idx:sample_idx + 1, view_idx].shape=torch.Size([1, 128, 22400]);</span>\n            self.on_the_image_mask = (on_the_image_mask != -1)\n            res_layer = self.prediction_heads[self.num_decoder_layers](torch.cat([query_feat, prev_query_feat], dim=1))\n            res_layer[\'center\'] = res_layer[\'center\'] + query_pos.permute(0, 2, 1)\n            for key, value in res_layer.items():\n                pred_dim = value.shape[1]\n                res_layer[key][~self.on_the_image_mask.unsqueeze(1).repeat(1, pred_dim, 1)] = first_res_layer[key][~self.on_the_image_mask.unsqueeze(1).repeat(1, pred_dim, 1)]\n            ret_dicts.append(res_layer)\n        if self.initialize_by_heatmap:\n            ret_dicts[0][\'query_heatmap_score\'] = heatmap.gather(index=top_proposals_index[:, None, :].expand(-1, self.num_classes, -1), dim=-1)  <span style=\'color: red\'># [bs, num_classes, num_proposals]</span>\n            if self.fuse_img:\n                ret_dicts[0][\'dense_heatmap\'] = dense_heatmap_img\n            else:\n                ret_dicts[0][\'dense_heatmap\'] = dense_heatmap\n        if self.auxiliary is False:\n            <span style=\'color: red\'># only return the results of last decoder layer</span>\n            return [ret_dicts[-1]]\n        <span style=\'color: red\'># return all the layer\'s results for auxiliary superivison</span>\n        new_res = {}                   <span style=\'color: red\'># new_res.keys()=dict_keys([\'center\', \'height\', \'dim\', \'rot\', \'vel\', \'heatmap\', \'query_heatmap_score\', \'dense_heatmap\'])</span>\n        for key in ret_dicts[0].keys():                <span style=\'color: red\'># new_res[\'center\'].shape=torch.Size([2, 2, 200])； new_res[\'heatmap\'].shape=torch.Size([2, 10, 200])；</span>\n            if key not in [\'dense_heatmap\', \'dense_heatmap_old\', \'query_heatmap_score\']:\n                new_res[key] = torch.cat([ret_dict[key] for ret_dict in ret_dicts], dim=-1)\n            else:\n                new_res[key] = ret_dicts[0][key]       <span style=\'color: red\'># new_res[\'query_heatmap_score\'].shape=torch.Size([2, 10, 200])；new_res[\'dense_heatmap\'].shape=torch.Size([2, 10, 128, 128]) </span>\n        return [new_res]\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/transfusion_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransFusionHead(nn.Module):\n    def create_2D_grid(self, x_size, y_size):     <span style=\'color: red\'># (1,1200)</span>\n        meshgrid = [[0, x_size - 1, x_size], [0, y_size - 1, y_size]]\n        batch_y, batch_x = torch.meshgrid(*[torch.linspace(it[0], it[1], it[2]) for it in meshgrid])\n        batch_x = batch_x + 0.5\n        batch_y = batch_y + 0.5\n        coord_base = torch.cat([batch_x[None], batch_y[None]], dim=0)[None]\n        coord_base = coord_base.view(1, 2, -1).permute(0, 2, 1)\n        return coord_base\n</code></pre></font>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/core/bbox/coders/transfusion_bbox_coder.py</p><font size="0"><pre class="language-python"><code class="language-python">class TransFusionBBoxCoder(BaseBBoxCoder):\n    def decode(self, heatmap, rot, dim, center, height, vel, filter=False):\n        <span style=\'color: red\'># class label</span>\n        final_preds = heatmap.max(1, keepdims=False).indices        <span style=\'color: red\'># torch.Size([1, 7, 200])->torch.Size([1, 200])</span>\n        final_scores = heatmap.max(1, keepdims=False).values\n        <span style=\'color: red\'># change size to real world metric</span>\n        center[:, 0, :] = center[:, 0, :] * self.out_size_factor * self.voxel_size[0] + self.pc_range[0]      <span style=\'color: red\'># *4*0.2+(-70.4)</span>\n        center[:, 1, :] = center[:, 1, :] * self.out_size_factor * self.voxel_size[1] + self.pc_range[1]      <span style=\'color: red\'># *4*0.2+(-51.2)</span>\n        <span style=\'color: red\'># center[:, 2, :] = center[:, 2, :] * (self.post_center_range[5] - self.post_center_range[2]) + self.post_center_range[2]</span>\n        dim[:, 0, :] = dim[:, 0, :].exp()\n        dim[:, 1, :] = dim[:, 1, :].exp()\n        dim[:, 2, :] = dim[:, 2, :].exp()\n        height = height - dim[:, 2:3, :] * 0.5  <span style=\'color: red\'># gravity center to bottom center</span>\n        rots, rotc = rot[:, 0:1, :], rot[:, 1:2, :]\n        rot = torch.atan2(rots, rotc)\n        if vel is None:\n            final_box_preds = torch.cat([center, height, dim, rot], dim=1).permute(0, 2, 1)\n        else:\n            final_box_preds = torch.cat([center, height, dim, rot, vel], dim=1).permute(0, 2, 1)\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_targets_single</p>输入：gt_bboxes_3d, gt_labels_3d, preds_dict(模型预测的结果), batch_idx<br>\nboxes_dict[0].keys()=dict_keys([\'bboxes\', \'scores\', \'labels\']);boxes_dict[0][\'bboxes\'].shape=torch.Size([200, 9]);<br>\nnum_layer=1;bboxes_tensor_layer.shape=torch.Size([200, 9]);score_layer.shape=torch.Size([1, 7, 200]);<br>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">class HungarianAssigner3D</p>mmdet3d/core/bbox/assigners/hungarian_assigner.py:def assign<br>\nnum_gts=73;num_bboxes=200;<br>\n1. assign -1 by default<br>\nassigned_gt_inds.shape=torch.Size([200])<br>\n2. compute the weighted costs<br>\ncls_pred[0].T.shape=torch.Size([200, 7]);gt_labels.shape=torch.Size([73]);<br>\ncls_cost.shape=torch.Size([200, 73])<br>\n3. do Hungarian matching on CPU using linear_sum_assignment<br>\n4. assign backgrounds and foregrounds<br>'}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">get_targets_single_1</p>bbox_targets.shape=torch.Size([200, 10]);【x,y,z,w,h,l,raw_cos,raw_sin,vx,vy】<br>\nlabels[None].shape=(1, 200);<br>'}]}]})</script></body>
</html>
