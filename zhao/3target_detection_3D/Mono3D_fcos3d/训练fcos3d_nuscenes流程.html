<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>训练fcos3d_nuscenes流程</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">相关资料</p>\n<p><a href="https://gitee.com/zhao-study/data_code/blob/master/3target_detection_3D/project/Mono3D_fcos3d/fcos3d_r101_caffe_fpn_gn-head_dcn_2x8_1x_nus-mono3d.py">fcos3d_r101_caffe_fpn_gn-head_dcn_2x8_1x_nus-mono3d.py</a><br></p>'}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">读取数据流程</p>init_初始化<br>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">site-packages/mmdet/datasets/coco.py:def load_annotations</p>len(data_infos)=168780; data_infos[0].keys()=dict_keys([\'file_name\', \'id\', \'token\', \'cam2ego_rotation\', \'cam2ego_translation\', \'ego2global_rotation\', \'ego2global_translation\', \'cam_intrinsic\', \'width\', \'height\', \'filename\'])<br>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">site-packages/mmdet/datasets/custom.py:def __getitem__:</p>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">site-packages/mmdet/datasets/custom.py:def prepare_train_img:</p>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">site-packages/mmdet/datasets/coco.py:def get_ann_info:</p>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/nuscenes_mono_dataset.py:def _parse_ann_info:需要写的代码</p>ann_info[0]<br>\n<span class=\'hidden-code\' data-code=\'\'file_name\':\'samples/CAM_FRONT_RIGHT/n008-2018-08-30-15-16-55-0400__CAM_FRONT_RIGHT__1535656619770482.jpg\'\n\'image_id\':\'39016ec655864188a592c6d571a2bace\'\n\'area\':14319.15253166274\n\'category_name\':\'car\'\n\'category_id\':0\n\'bbox\':[1423.8459064796646, 359.9941725586372, 176.15409352033544, 81.2876513142723]\n\'iscrowd\':0\n\'bbox_cam3d\':[17.917394889231144, -1.2885081395237552, 31.393943046675236, 4.761, 1.9, 1.946, 2.9612160312161246]\n\'velo_cam3d\':[0.0, 0.0]\n\'center2d\':[1535.0500238983427, 400.373187237498, 31.393943046675236]\n\'attribute_name\':\'vehicle.parked\'\n\'attribute_id\':6\n\'segmentation\':[]\n\'id\':902923\nlen():14\n\'> </span>'}, {'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/datasets/nuscenes_mono_dataset.py:def _parse_ann_info:需要写的代码</p>img_info<br>\n<span class=\'hidden-code\' data-code=\'\'file_name\':\'samples/CAM_FRONT_RIGHT/n008-2018-08-30-15-16-55-0400__CAM_FRONT_RIGHT__1535656619770482.jpg\'\n\'id\':\'39016ec655864188a592c6d571a2bace\'\n\'token\':\'e1d1e1806a7f4263ba59af7f749ef326\'\n\'cam2ego_rotation\':[0.20335173766558642, -0.19146333228946724, 0.6785710044972951, -0.6793609166212989]\n\'cam2ego_translation\':[1.58082565783, -0.499078711449, 1.51749368405]\n\'ego2global_rotation\':[0.9422984872595677, 0.017476920676708387, 0.0017391228598700533, -0.3343128678403938]\n\'ego2global_translation\':[1337.007196856983, 867.4962688666, 0.0]\n\'cam_intrinsic\':[[1256.7485116440405, 0.0, 817.7887570959712], [0.0, 1256.7485116440403, 451.9541780095127], [0.0, 0.0, 1.0]]\n\'width\':1600\n\'height\':900\n\'filename\':\'samples/CAM_FRONT_RIGHT/n008-2018-08-30-15-16-55-0400__CAM_FRONT_RIGHT__1535656619770482.jpg\'\nlen():11\n\'> </span>'}]}]}]}]}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">输入网络</p>图片大小800x480;`gt_bboxes`是在该分辨率下的2Dbox框<br>\n`gt_bboxes_3d`相机坐标系的gt，通过内参cam_intrinsic，得到图片1600x900大小下的八个顶点<br>\n`centers2d`在像素坐标系下的中心点，对应图片大小为1600x900,`depth`深度是相机坐标系的那个point_2d[..., 2:3]<br>\n<span class=\'hidden-code\' data-code=\'point_2d_res = point_2d[..., :2] / point_2d[..., 2:3]\nif with_depth:\n    points_2d_depth = np.concatenate([point_2d_res, point_2d[..., 2:3]],axis=-1)\n    return points_2d_depth\n\'> </span>'}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">head流程</p><span class=\'hidden-code\' data-code=\'class SingleStageMono3DDetector(SingleStageDetector):                    提取特征后\n    def forward_train(self,\n                      img,                    torch.Size([2, 3, 928, 1600])\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_3d,\n                      gt_labels_3d,\n                      centers2d,\n                      depths,\n                      attr_labels=None,\n                      gt_bboxes_ignore=None):\n        x = self.extract_feat(img)  [torch.Size([2, 256, 116, 200]), torch.Size([2, 256, 58, 100]), torch.Size([2, 256, 29, 50]), torch.Size([2, 256, 15, 25]), torch.Size([2, 256, 8, 13])]\n        losses = self.bbox_head.forward_train(x, img_metas, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, gt_bboxes_ignore)\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead(AnchorFreeMono3DHead):\n    def forward(self, feats):\n        # len(feats)==5;;self.strides=[8, 16, 32, 64, 128];\n        # 下降范围[8, 16, 32, 64, 128],regress_ranges=((-1, 48), (48, 96), (96, 192), (192, 384), (384, INF))\n        # 对应模型输入大小\n        return multi_apply(self.`forward_single`, feats, self.scales, self.strides)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead(AnchorFreeMono3DHead):\n    def forward_single(self, x, scale, stride):  # torch.Size([2, 256, 116, 200])\n        # torch.Size([2, 10, 116, 200]);10个类别，torch.Size([2, 9, 116, 200])，torch.Size([2, 2, 116, 200])，。。。，torch.Size([2, 256, 116, 200])，torch.Size([2, 256, 116, 200])\n        cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat = super().`forward_single`(x)          \n        if self.centerness_on_reg:                                          # True\n            clone_reg_feat = reg_feat.clone()\n            for conv_centerness_prev_layer in self.conv_centerness_prev:\n                clone_reg_feat = conv_centerness_prev_layer(clone_reg_feat) # reg_feat->torch.Size([2, 64, 60, 100])->centerness=(bs,1,h,w)\n            centerness = self.conv_centerness(clone_reg_feat)               # torch.Size([2, 1, 116, 200])\n        else:\n            clone_cls_feat = cls_feat.clone()\n            for conv_centerness_prev_layer in self.conv_centerness_prev:\n                clone_cls_feat = conv_centerness_prev_layer(clone_cls_feat)\n            centerness = self.conv_centerness(clone_cls_feat)\n        bbox_pred = self.bbox_coder.`decode`(bbox_pred, scale, stride, self.training, cls_score)         # ->torch.Size([2, 9, 116, 200])\n        return cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, cls_feat, reg_feat\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/anchor_free_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class AnchorFreeMono3DHead(BaseMono3DDenseHead):\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_layer in self.cls_convs:\n            cls_feat = cls_layer(cls_feat)\n        # clone the cls_feat for reusing the feature map afterwards\n        clone_cls_feat = cls_feat.clone()\n        for conv_cls_prev_layer in self.conv_cls_prev:\n            clone_cls_feat = conv_cls_prev_layer(clone_cls_feat)\n        cls_score = self.conv_cls(clone_cls_feat)\n        for reg_layer in self.reg_convs:\n            reg_feat = reg_layer(reg_feat)\n        bbox_pred = []\n        for i in range(len(self.group_reg_dims)):\n            # clone the reg_feat for reusing the feature map afterwards\n            clone_reg_feat = reg_feat.clone()\n            if len(self.reg_branch[i]) > 0:\n                for conv_reg_prev_layer in self.conv_reg_prevs[i]:\n                    clone_reg_feat = conv_reg_prev_layer(clone_reg_feat)\n            bbox_pred.append(self.conv_regs[i](clone_reg_feat))\n        bbox_pred = torch.cat(bbox_pred, dim=1)\n        dir_cls_pred = None\n        if self.use_direction_classifier:\n            clone_reg_feat = reg_feat.clone()\n            for conv_dir_cls_prev_layer in self.conv_dir_cls_prev:\n                clone_reg_feat = conv_dir_cls_prev_layer(clone_reg_feat)\n            dir_cls_pred = self.conv_dir_cls(clone_reg_feat)\n        attr_pred = None\n        if self.pred_attrs:\n            # clone the cls_feat for reusing the feature map afterwards\n            clone_cls_feat = cls_feat.clone()\n            for conv_attr_prev_layer in self.conv_attr_prev:\n                clone_cls_feat = conv_attr_prev_layer(clone_cls_feat)\n            attr_pred = self.conv_attr(clone_cls_feat)\n        # (bs,n_class,h,w),(bs,7,h,w)中心偏移+深度+WHL,(bs,2,h,w),None,(bs,256,h,w),reg_feat=(bs,256,h,w)\n        return cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/core/bbox/coders/fcos3d_bbox_coder.py</p><span class=\'hidden-code\' data-code=\'class FCOS3DBBoxCoder(BaseBBoxCoder):\n    def decode(self, bbox, scale, stride, training, cls_score=None):\n        # scale the bbox of different level only apply to offset, depth and size prediction\n        scale_offset, scale_depth, scale_size = scale[0:3]           # Scale()\n        # 13维度 2-中心点offset,1-深度,3-长宽高,\n        clone_bbox = bbox.clone()\n        bbox[:, :2] = scale_offset(clone_bbox[:, :2]).float()        # 预测的3Dbox\n        bbox[:, 2] = scale_depth(clone_bbox[:, 2]).float()\n        bbox[:, 3:6] = scale_size(clone_bbox[:, 3:6]).float()\n        if self.base_depths is None:                # True\n            bbox[:, 2] = bbox[:, 2].exp()           # 走这里\n        elif len(self.base_depths) == 1:            # only single prior\n            mean = self.base_depths[0][0]\n            std = self.base_depths[0][1]\n            bbox[:, 2] = mean + bbox.clone()[:, 2] * std\n        else:                                       # multi-class priors\n            indices = cls_score.max(dim=1)[1]       # torch.Size([2, 116, 200])；2为batch size\n            depth_priors = cls_score.new_tensor(self.base_depths)[indices, :].permute(0, 3, 1, 2)      # torch.Size([2, 2, 116, 200])\n            mean = depth_priors[:, 0]               # torch.Size([2, 116, 200])，2为batch_size\n            std = depth_priors[:, 1]\n            bbox[:, 2] = mean + bbox.clone()[:, 2] * std\n        bbox[:, 3:6] = bbox[:, 3:6].exp()\n        if self.base_dims is not None:              # None\n            indices = cls_score.max(dim=1)[1]\n            size_priors = cls_score.new_tensor(self.base_dims)[indices, :].permute(0, 3, 1, 2)\n            bbox[:, 3:6] = size_priors * bbox.clone()[:, 3:6]\n        if self.norm_on_bbox:           # True\n            if not training:            # Note that this line is conducted only when testing\n                bbox[:, :2] *= stride   # 预测的偏移量等于中心点的偏移量/strip\n        return bbox\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">损失流程</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead(AnchorFreeMono3DHead):\n    def loss(self,cls_scores,bbox_preds,dir_cls_preds,attr_preds,centernesses,gt_bboxes,gt_labels,gt_bboxes_3d,gt_labels_3d,centers2d,depths,attr_labels,img_metas,gt_bboxes_ignore=None):\n        assert len(cls_scores) == len(bbox_preds) == len(centernesses) == len(attr_preds)\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        # [torch.Size([116, 200]), torch.Size([58, 100]), torch.Size([29, 50]), torch.Size([15, 25]), torch.Size([8, 13])]\n        all_level_points = self.`get_points`(featmap_sizes, bbox_preds[0].dtype,bbox_preds[0].device)\n        # [torch.Size([23200, 2]), torch.Size([5800, 2]), torch.Size([1450, 2]), torch.Size([375, 2]), torch.Size([104, 2])]\n        labels_3d, bbox_targets_3d, centerness_targets, attr_targets = self.`get_targets`(all_level_points, gt_bboxes, gt_labels, gt_bboxes_3d,gt_labels_3d, centers2d, depths, attr_labels)\n        num_imgs = cls_scores[0].size(0)                                 # 2\n        # flatten cls_scores, bbox_preds, dir_cls_preds and centerness\n        flatten_cls_scores = [cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels) for cls_score in cls_scores ]    # [torch.Size([46400, 10]), torch.Size([11600, 10]), torch.Size([2900, 10]), torch.Size([750, 10]), torch.Size([208, 10])],其中46400=2*116*200\n        flatten_bbox_preds = [bbox_pred.permute(0, 2, 3, 1).reshape(-1, sum(self.group_reg_dims)) for bbox_pred in bbox_preds]  # [torch.Size([46400, 9]), torch.Size([11600, 9]), torch.Size([2900, 9]), torch.Size([750, 9]), torch.Size([208, 9])]\n        flatten_dir_cls_preds = [dir_cls_pred.permute(0, 2, 3, 1).reshape(-1, 2) for dir_cls_pred in dir_cls_preds]     # [torch.Size([46400, 2]), torch.Size([11600, 2]), torch.Size([2900, 2]), torch.Size([750, 2]), torch.Size([208, 2])]\n        flatten_centerness = [centerness.permute(0, 2, 3, 1).reshape(-1) for centerness in centernesses]\n        flatten_cls_scores = torch.cat(flatten_cls_scores)           # torch.Size([61858, 10])   模型预测的\n        flatten_bbox_preds = torch.cat(flatten_bbox_preds)           # torch.Size([61858, 9])\n        flatten_dir_cls_preds = torch.cat(flatten_dir_cls_preds)     # torch.Size([61858, 2])\n        flatten_centerness = torch.cat(flatten_centerness)           # torch.Size([61858])\n        #################################下面呢为gt生成的#######################################\n        flatten_labels_3d = torch.cat(labels_3d)                         # torch.Size([61858])范围0-10，10为背景  用gt label生成的\n        flatten_bbox_targets_3d = torch.cat(bbox_targets_3d)             # torch.Size([61858, 9])\n        flatten_centerness_targets = torch.cat(centerness_targets)       # torch.Size([61858])范围0-1，离中心点越大，置信度越大\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        bg_class_ind = self.num_classes                             # 10个类别\n        pos_inds = ((flatten_labels_3d `>`= 0) & (flatten_labels_3d `<` bg_class_ind)).nonzero().reshape(-1)  # torch.Size([61858])-`>`torch.Size([66, 1])-`>`torch.Size([66]) 不是背景的索引值范围0-`>`61858-1\n        num_pos = len(pos_inds)                                     # 66\n        loss_cls = self.`loss_cls`(flatten_cls_scores, flatten_labels_3d, avg_factor=num_pos + num_imgs)  # avoid num_pos is 0  FocalLoss\n        pos_bbox_preds = flatten_bbox_preds[pos_inds]             # 模型预测 torch.Size([66, 9])\n        pos_dir_cls_preds = flatten_dir_cls_preds[pos_inds]       # torch.Size([66, 2])\n        pos_centerness = flatten_centerness[pos_inds]             # torch.Size([66])\n        if self.pred_attrs:\n            flatten_attr_preds = [attr_pred.permute(0, 2, 3, 1).reshape(-1, self.num_attrs) for attr_pred in attr_preds]\n            flatten_attr_preds = torch.cat(flatten_attr_preds)\n            flatten_attr_targets = torch.cat(attr_targets)\n            pos_attr_preds = flatten_attr_preds[pos_inds]\n        if num_pos > 0:\n            pos_bbox_targets_3d = flatten_bbox_targets_3d[pos_inds]\n            pos_centerness_targets = flatten_centerness_targets[pos_inds]\n            if self.pred_attrs:\n                pos_attr_targets = flatten_attr_targets[pos_inds]\n            bbox_weights = pos_centerness_targets.new_ones(len(pos_centerness_targets), sum(self.group_reg_dims))\n            equal_weights = pos_centerness_targets.new_ones(pos_centerness_targets.shape)\n            code_weight = self.train_cfg.get(\'code_weight\', None)\n            if code_weight:\n                assert len(code_weight) == sum(self.group_reg_dims)\n                bbox_weights = bbox_weights * bbox_weights.new_tensor(code_weight)\n            if self.use_direction_classifier:\n                pos_dir_cls_targets = self.`get_direction_target`(pos_bbox_targets_3d, self.dir_offset, self.dir_limit_offset, one_hot=False)\n            if self.diff_rad_by_sin:\n                pos_bbox_preds, pos_bbox_targets_3d = self.add_sin_difference(pos_bbox_preds, pos_bbox_targets_3d)\n            loss_offset = self.loss_bbox(pos_bbox_preds[:, :2],pos_bbox_targets_3d[:, :2],weight=bbox_weights[:, :2],avg_factor=equal_weights.sum())\n            loss_depth = self.loss_bbox(pos_bbox_preds[:, 2],pos_bbox_targets_3d[:, 2],weight=bbox_weights[:, 2],avg_factor=equal_weights.sum())\n            loss_size = self.loss_bbox(pos_bbox_preds[:, 3:6],pos_bbox_targets_3d[:, 3:6],weight=bbox_weights[:, 3:6],avg_factor=equal_weights.sum())\n            loss_rotsin = self.loss_bbox(pos_bbox_preds[:, 6],pos_bbox_targets_3d[:, 6],weight=bbox_weights[:, 6],avg_factor=equal_weights.sum())\n            loss_velo = None\n            if self.pred_velo:\n                loss_velo = self.loss_bbox(pos_bbox_preds[:, 7:9],pos_bbox_targets_3d[:, 7:9],weight=bbox_weights[:, 7:9],avg_factor=equal_weights.sum())\n            loss_centerness = self.loss_centerness(pos_centerness,pos_centerness_targets)\n            # direction classification loss\n            loss_dir = None\n            if self.use_direction_classifier:\n                loss_dir = self.loss_dir(pos_dir_cls_preds,pos_dir_cls_targets,equal_weights,avg_factor=equal_weights.sum())\n            # attribute classification loss\n            loss_attr = None\n            if self.pred_attrs:\n                loss_attr = self.loss_attr(pos_attr_preds,pos_attr_targets,pos_centerness_targets,avg_factor=pos_centerness_targets.sum())\n        else:\n            # need absolute due to possible negative delta x/y\n            loss_offset = pos_bbox_preds[:, :2].sum()\n            loss_depth = pos_bbox_preds[:, 2].sum()\n            loss_size = pos_bbox_preds[:, 3:6].sum()\n            loss_rotsin = pos_bbox_preds[:, 6].sum()\n            loss_velo = None\n            if self.pred_velo:\n                loss_velo = pos_bbox_preds[:, 7:9].sum()\n            loss_centerness = pos_centerness.sum()\n            loss_dir = None\n            if self.use_direction_classifier:\n                loss_dir = pos_dir_cls_preds.sum()\n            loss_attr = None\n            if self.pred_attrs:\n                loss_attr = pos_attr_preds.sum()\n        loss_dict = dict(loss_cls=loss_cls,loss_offset=loss_offset,loss_depth=loss_depth,loss_size=loss_size,loss_rotsin=loss_rotsin,loss_centerness=loss_centerness)\n        if loss_velo is not None:\n            loss_dict[\'loss_velo\'] = loss_velo\n        if loss_dir is not None:\n            loss_dict[\'loss_dir\'] = loss_dir\n        if loss_attr is not None:\n            loss_dict[\'loss_attr\'] = loss_attr\n        return loss_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/anchor_free_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class AnchorFreeMono3DHead(BaseMono3DDenseHead):\n    def get_points(self, featmap_sizes, dtype, device, flatten=False):\n        mlvl_points = []                     # dtype=torch.float32,strides=[8, 16, 32, 64, 128]\n        for i in range(len(featmap_sizes)):\n            mlvl_points.append(self.`_get_points_single`(featmap_sizes[i], self.strides[i],dtype, device, flatten))\n        return mlvl_points                  # len(mlvl_points)=5; [torch.Size([23200, 2]), torch.Size([5800, 2]), torch.Size([1450, 2]), torch.Size([375, 2]), torch.Size([104, 2])]\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class AnchorFreeMono3DHead(BaseMono3DDenseHead):\n    def _get_points_single(self,featmap_size,stride,dtype,device,flatten=False):\n        """Get points according to feature map sizes."""\n        y, x = super().`_get_points_single`(featmap_size, stride, dtype, device)                      # torch.Size([58, 100]);8\n        # points.shape=(5800,2)        tensor([[ 8.,  8.],[24.,  8.],[40.,  8.],[1560.,  920.],[1576.,  920.],[1592.,  920.]], device=\'cuda:0\') 输入尺寸是928, 1600\n        points = torch.stack((x.reshape(-1) * stride, y.reshape(-1) * stride),dim=-1) + stride // 2\n        return points\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/anchor_free_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class AnchorFreeMono3DHead(BaseMono3DDenseHead):\n    def _get_points_single(self,featmap_size,stride,dtype,device,flatten=False):\n        """Get points of a single scale level."""\n        h, w = featmap_size                                        # torch.Size([116, 200])\n        x_range = torch.arange(w, dtype=dtype, device=device)      # torch.tensor([1,2,...,199])\n        y_range = torch.arange(h, dtype=dtype, device=device)      # torch.tensor([1,2,...,115])\n        y, x = torch.meshgrid(y_range, x_range)                    # torch.Size([116, 200]);torch.Size([116, 200])\n        """\n        y[:3,:3]\n        tensor([[0., 0., 0.],\n                [1., 1., 1.],\n                [2., 2., 2.]], device=\'cuda:0\')\n        x[:3,:3]\n        tensor([[0., 1., 2.],\n                [0., 1., 2.],\n                [0., 1., 2.]], device=\'cuda:0\')\n        """\n        if flatten:                                                # False\n            y = y.flatten()\n            x = x.flatten()\n        return y, x\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead(AnchorFreeMono3DHead):\n    def get_targets(self, points, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, attr_labels_list):\n        assert len(points) == len(self.regress_ranges)        # ((-1, 48), (48, 96), (96, 192), (192, 384), (384, 100000000.0))\n        num_levels = len(points)                              # 5\n        # expand regress ranges to align with points\n        expanded_regress_ranges = [points[i].new_tensor(self.regress_ranges[i])[None].expand_as(points[i]) for i in range(num_levels)]\n        # concat all levels points and regress ranges        # [torch.Size([23200, 2]), torch.Size([5800, 2]), torch.Size([1450, 2]), torch.Size([375, 2]), torch.Size([104, 2])]\n        concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)        # expanded_regress_ranges[0][:2] = tensor([[-1., 48.], [-1., 48.]], device=\'cuda:0\')\n        concat_points = torch.cat(points, dim=0)                                 # torch.Size([30929, 2]) 映射到输入图片，不是会原图\n        # the number of points per img, per lvl\n        num_points = [center.size(0) for center in points]                       # [23200, 5800, 1450, 375, 104]\n        if attr_labels_list is None:\n            attr_labels_list = [gt_labels.new_full(gt_labels.shape, self.attr_background_label) for gt_labels in gt_labels_list]\n        # get labels and bbox_targets of each image\n        _, _, labels_3d_list, bbox_targets_3d_list, centerness_targets_list, attr_targets_list = multi_apply(self.`_get_target_single`,gt_bboxes_list,\n                gt_labels_list,gt_bboxes_3d_list,gt_labels_3d_list,centers2d_list,depths_list,attr_labels_list,points=concat_points,regress_ranges=concat_regress_ranges,num_points_per_lvl=num_points)\n        # split to per img, per level\n        labels_3d_list = [labels_3d.split(num_points, 0) for labels_3d in labels_3d_list]\n        bbox_targets_3d_list = [bbox_targets_3d.split(num_points, 0) for bbox_targets_3d in bbox_targets_3d_list]\n        centerness_targets_list = [centerness_targets.split(num_points, 0) for centerness_targets in centerness_targets_list]\n        attr_targets_list = [attr_targets.split(num_points, 0) for attr_targets in attr_targets_list]\n        # concat per level image\n        concat_lvl_labels_3d = []\n        concat_lvl_bbox_targets_3d = []\n        concat_lvl_centerness_targets = []\n        concat_lvl_attr_targets = []\n        for i in range(num_levels):\n            concat_lvl_labels_3d.append(torch.cat([labels[i] for labels in labels_3d_list]))\n            concat_lvl_centerness_targets.append(torch.cat([centerness_targets[i] for centerness_targets in centerness_targets_list]))\n            bbox_targets_3d = torch.cat([bbox_targets_3d[i] for bbox_targets_3d in bbox_targets_3d_list])\n            concat_lvl_attr_targets.append(torch.cat([attr_targets[i] for attr_targets in attr_targets_list]))\n            if self.norm_on_bbox:\n                bbox_targets_3d[:, :2] = bbox_targets_3d[:, :2] / self.strides[i]\n            concat_lvl_bbox_targets_3d.append(bbox_targets_3d)\n        # torch.Size([46400]), torch.Size([11600]), torch.Size([2900]), torch.Size([750]), torch.Size([208])\n        # [torch.Size([46400, 9]), torch.Size([11600, 9]), torch.Size([2900, 9]), torch.Size([750, 9]), torch.Size([208, 9])]\n        # [torch.Size([46400]), torch.Size([11600]), torch.Size([2900]), torch.Size([750]), torch.Size([208])]\n        return concat_lvl_labels_3d, concat_lvl_bbox_targets_3d, concat_lvl_centerness_targets, concat_lvl_attr_targets\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead(AnchorFreeMono3DHead):        # 对于每张图片  不是每个feature\n    def _get_target_single(self, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, points, regress_ranges, num_points_per_lvl):\n        """Compute regression and classification targets for a single image."""\n        num_points = points.size(0)              # 30929 所有点\n        num_gts = gt_labels.size(0)              # 2  tensor([7, 5], device=\'cuda:0\')\n        if not isinstance(gt_bboxes_3d, torch.Tensor):\n            gt_bboxes_3d = gt_bboxes_3d.tensor.to(gt_bboxes.device)       # torch.Size([2, 9])\n        if num_gts == 0:\n            return gt_labels.new_full((num_points,), self.background_label), gt_bboxes.new_zeros((num_points, 4)), \\\n                   gt_labels_3d.new_full((num_points,), self.background_label), \\\n                   gt_bboxes_3d.new_zeros((num_points, self.bbox_code_size)), \\\n                   gt_bboxes_3d.new_zeros((num_points,)), \\\n                   attr_labels.new_full((num_points,), self.attr_background_label)           \n        # change orientation to local yaw\n        gt_bboxes_3d[..., 6] = -torch.atan2(gt_bboxes_3d[..., 0], gt_bboxes_3d[..., 2]) + gt_bboxes_3d[..., 6]   # 到我们看到的角度\n        areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0]) * (gt_bboxes[:, 3] - gt_bboxes[:, 1])                        # tensor([102058.8984,  31237.2988], device=\'cuda:0\')\n        areas = areas[None].repeat(num_points, 1)                                                                # torch.Size([30929, 2])    2是lable num\n        regress_ranges = regress_ranges[:, None, :].expand(num_points, num_gts, 2)                               # torch.Size([30929, 2, 2]) 第1个2是label num\n        gt_bboxes = gt_bboxes[None].expand(num_points, num_gts, 4)                               # torch.Size([30929, 2, 4])\n        centers2d = centers2d[None].expand(num_points, num_gts, 2)                               # torch.Size([30929, 2, 2])\n        gt_bboxes_3d = gt_bboxes_3d[None].expand(num_points, num_gts,self.bbox_code_size)        # torch.Size([30929, 2, 9])\n        depths = depths[None, :, None].expand(num_points, num_gts, 1)                            # torch.Size([30929, 2, 1])\n        xs, ys = points[:, 0], points[:, 1]                                                      # feature上面投影到输入大小的点  points[:3]=tensor([[ 4.,  4.],[12.,  4.],[20.,  4.]]) 从最大的featurex方向算中心点\n        xs = xs[:, None].expand(num_points, num_gts)                                             # torch.Size([30929, 2])                                                   \n        ys = ys[:, None].expand(num_points, num_gts) \n        delta_xs = (xs - centers2d[..., 0])[..., None]\n        delta_ys = (ys - centers2d[..., 1])[..., None]\n        bbox_targets_3d = torch.cat((delta_xs, delta_ys, depths, gt_bboxes_3d[..., 3:]), dim=-1)  # torch.Size([30929, 2, 9])  中心点差距(在输入图片上计算),深度，长宽高，看到角度，vx，vy\n        left = xs - gt_bboxes[..., 0]\n        right = gt_bboxes[..., 2] - xs\n        top = ys - gt_bboxes[..., 1]\n        bottom = gt_bboxes[..., 3] - ys\n        bbox_targets = torch.stack((left, top, right, bottom), -1)                               # 特征点对2Dbox前后左右的差距  torch.Size([30929, 2, 4])\n        assert self.center_sampling is True, \'Setting center_sampling to False has not been implemented for FCOS3D.\'\n        # ------------------------condition1: inside a `center bbox`------------------------   这个是算3D投影点\n        radius = self.center_sample_radius                          # 1.5\n        center_xs = centers2d[..., 0]                               # 2D投影中心点\n        center_ys = centers2d[..., 1]\n        center_gts = torch.zeros_like(gt_bboxes)                    # torch.Size([30929, 2, 4])全是0\n        stride = center_xs.new_zeros(center_xs.shape)               # torch.Size([30929, 2])全是0 2是box的数量\n        # project the points on current lvl back to the `original` sizes\n        lvl_begin = 0\n        for lvl_idx, num_points_lvl in enumerate(num_points_per_lvl):   # [23200, 5800, 1450, 375, 104]  对应的strid值是 [8, 16, 32, 64, 128]*1.5 = [12, 24, 48, 96, 192]\n            lvl_end = lvl_begin + num_points_lvl\n            stride[lvl_begin:lvl_end] = self.strides[lvl_idx] * radius\n            lvl_begin = lvl_end\n        center_gts[..., 0] = center_xs - stride                    # 以中心点投影为中心，stride*0.5为左右上下得到2Dbox\n        center_gts[..., 1] = center_ys - stride\n        center_gts[..., 2] = center_xs + stride\n        center_gts[..., 3] = center_ys + stride\n        cb_dist_left = xs - center_gts[..., 0]                     # 在这个box里面的feature map点\n        cb_dist_right = center_gts[..., 2] - xs\n        cb_dist_top = ys - center_gts[..., 1]\n        cb_dist_bottom = center_gts[..., 3] - ys\n        center_bbox = torch.stack((cb_dist_left, cb_dist_top, cb_dist_right, cb_dist_bottom), -1)\n        inside_gt_bbox_mask = center_bbox.min(-1)[0] > 0           # 要是这个feature map点在里面就预测 应该最多9个点\n        # ------------------------condition2: limit the regression range for each location------------------------  这个是算feature上的点距离2Dbox\n        max_regress_distance = bbox_targets.max(-1)[0]             # 特征点对2Dbox前后左右最大的差距\n        inside_regress_range = ((max_regress_distance `>`= regress_ranges[..., 0]) & (max_regress_distance `<`= regress_ranges[..., 1]))\n        # center-based criterion to deal with ambiguity\n        dists = torch.sqrt(torch.sum(bbox_targets_3d[..., :2]**2, dim=-1))         # torch.Size([30929, 2])\n        dists[inside_gt_bbox_mask == 0] = INF\n        dists[inside_regress_range == 0] = INF\n        min_dist, min_dist_inds = dists.min(dim=1)                                 # torch.Size([30929])； tensor([0, 0, 0,  ..., 0, 0, 0], device=\'cuda:0\') 每个点最小的距离，最小的索引\n        labels = gt_labels[min_dist_inds]                                          # torch.Size([30929]) 每个点的2D lable\n        labels_3d = gt_labels_3d[min_dist_inds]\n        attr_labels = attr_labels[min_dist_inds]\n        labels[min_dist == INF] = self.background_label                            # set as BG\n        labels_3d[min_dist == INF] = self.background_label                         # set as BG\n        attr_labels[min_dist == INF] = self.attr_background_label\n        bbox_targets = bbox_targets[range(num_points), min_dist_inds]              # 前后左右距离\n        bbox_targets_3d = bbox_targets_3d[range(num_points), min_dist_inds]        # torch.Size([30929, 4])\n        relative_dists = torch.sqrt(torch.sum(bbox_targets_3d[..., :2]**2,dim=-1)) / (1.414 * stride[:, 0])     # torch.Size([30929])\n        # [N, 1] / [N, 1]\n        centerness_targets = torch.exp(-self.centerness_alpha * relative_dists)    # torch.Size([30929])\n        # 要是没有box，全是-1，0          ， -1       ， 0             ，     0           ，   -1\n        return      labels, bbox_targets, labels_3d, bbox_targets_3d, centerness_targets, attr_labels\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">/sdb/zzhu/anaconda3/envs/mmcv151/lib/python3.7/site-packages/mmdet/models/losses/focal_loss.py</p><span class=\'hidden-code\' data-code=\'class FocalLoss(nn.Module):\n    def __init__(self,use_sigmoid=True,gamma=2.0,alpha=0.25,reduction=\'mean\',loss_weight=1.0,activated=False):\n        super(FocalLoss, self).__init__()        # 跟默认一样\n        assert use_sigmoid is True, \'Only sigmoid focal loss supported now.\'\n        self.use_sigmoid = use_sigmoid\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n        self.activated = activated\n    def forward(self,pred,target,weight=None,avg_factor=None,reduction_override=None): \n        # torch.Size([61858, 10])，torch.Size([61858])，None,68,None\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (reduction_override if reduction_override else self.reduction)     # \'mean\n        if self.use_sigmoid:\n            if self.activated:\n                calculate_loss_func = py_focal_loss_with_prob\n            else:\n                if torch.cuda.is_available() and pred.is_cuda:\n                    calculate_loss_func = sigmoid_focal_loss              # 这里\n                else:\n                    num_classes = pred.size(1)\n                    target = F.one_hot(target, num_classes=num_classes + 1)\n                    target = target[:, :num_classes]\n                    calculate_loss_func = py_sigmoid_focal_loss\n            loss_cls = self.loss_weight * calculate_loss_func(pred,target,weight,gamma=self.gamma,alpha=self.alpha,reduction=reduction,avg_factor=avg_factor)\n        else:\n            raise NotImplementedError\n        return loss_cls\n\'> </span>'}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead:\n    def get_direction_target(reg_targets,dir_offset=0,dir_limit_offset=0.0,num_bins=2,one_hot=True):\n        rot_gt = reg_targets[..., 6]\n        offset_rot = limit_period(rot_gt - dir_offset, dir_limit_offset,2 * np.pi)\n        dir_cls_targets = torch.floor(offset_rot / (2 * np.pi / num_bins)).long()\n        dir_cls_targets = torch.clamp(dir_cls_targets, min=0, max=num_bins - 1)\n        if one_hot:\n            dir_targets = torch.zeros(*list(dir_cls_targets.shape),num_bins,dtype=reg_targets.dtype,device=dir_cls_targets.device)\n            dir_targets.scatter_(dir_cls_targets.unsqueeze(dim=-1).long(), 1.0)\n            dir_cls_targets = dir_targets\n        return dir_cls_targets\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">预测得到box</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead(AnchorFreeMono3DHead):\n    def get_bboxes(self,cls_scores,bbox_preds,dir_cls_preds,attr_preds,centernesses,img_metas,cfg=None,rescale=None):\n        assert len(cls_scores) == len(bbox_preds) == len(dir_cls_preds) == len(centernesses) == len(attr_preds)\n        # (N, num_points * num_classes, H, W)\n        # (N, num_points * 4, H, W)\n        # num_points * 2. (bin = 2)\n        # (N, num_points * 1, H, W)\n        # (N, num_points * num_attrs, H, W)\n        num_levels = len(cls_scores)\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype,bbox_preds[0].device)\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n            bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n            if self.use_direction_classifier:\n                dir_cls_pred_list = [dir_cls_preds[i][img_id].detach() for i in range(num_levels)]\n            else:\n                dir_cls_pred_list = [cls_scores[i][img_id].new_full([2, *cls_scores[i][img_id].shape[1:]], 0).detach() for i in range(num_levels)]\n            if self.pred_attrs:\n                attr_pred_list = [attr_preds[i][img_id].detach() for i in range(num_levels)]\n            else:\n                attr_pred_list = [cls_scores[i][img_id].new_full([self.num_attrs, *cls_scores[i][img_id].shape[1:]],self.attr_background_label).detach() for i in range(num_levels)]\n            centerness_pred_list = [centernesses[i][img_id].detach() for i in range(num_levels)]\n            input_meta = img_metas[img_id]\n            det_bboxes = self.`_get_bboxes_single`(cls_score_list, bbox_pred_list, dir_cls_pred_list,\n                attr_pred_list, centerness_pred_list, mlvl_points, input_meta,cfg, rescale)\n            result_list.append(det_bboxes)\n        return result_list\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdet3d/models/dense_heads/fcos_mono3d_head.py</p><span class=\'hidden-code\' data-code=\'class FCOSMono3DHead(AnchorFreeMono3DHead):\n    def _get_bboxes_single(self,cls_scores,bbox_preds,dir_cls_preds,attr_preds,centernesses,mlvl_points,input_meta,cfg,rescale=False):\n        view = np.array(input_meta[\'cam2img\'])\n        scale_factor = input_meta[\'scale_factor\']\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)  # (num_points*num_classes,H,W);(num_points*bbox_code_size,H,W);(num_total_points,2)\n        mlvl_centers2d = []\n        mlvl_bboxes = []\n        mlvl_scores = []\n        mlvl_dir_scores = []\n        mlvl_attr_scores = []\n        mlvl_centerness = []\n        for cls_score, bbox_pred, dir_cls_pred, attr_pred, centerness, points in zip(cls_scores, bbox_preds, dir_cls_preds, attr_preds, centernesses, mlvl_points):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            scores = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n            dir_cls_pred = dir_cls_pred.permute(1, 2, 0).reshape(-1, 2)\n            dir_cls_score = torch.max(dir_cls_pred, dim=-1)[1]\n            attr_pred = attr_pred.permute(1, 2, 0).reshape(-1, self.num_attrs)\n            attr_score = torch.max(attr_pred, dim=-1)[1]\n            centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n            bbox_pred = bbox_pred.permute(1, 2,0).reshape(-1,sum(self.group_reg_dims))\n            bbox_pred = bbox_pred[:, :self.bbox_code_size]\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                max_scores, _ = (scores * centerness[:, None]).max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                points = points[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n                dir_cls_pred = dir_cls_pred[topk_inds, :]\n                centerness = centerness[topk_inds]\n                dir_cls_score = dir_cls_score[topk_inds]\n                attr_score = attr_score[topk_inds]\n            # change the offset to actual center predictions\n            bbox_pred[:, :2] = points - bbox_pred[:, :2]\n            if rescale:\n                bbox_pred[:, :2] /= bbox_pred[:, :2].new_tensor(scale_factor)\n            pred_center2d = bbox_pred[:, :3].clone()\n            bbox_pred[:, :3] = points_img2cam(bbox_pred[:, :3], view)\n            mlvl_centers2d.append(pred_center2d)\n            mlvl_bboxes.append(bbox_pred)\n            mlvl_scores.append(scores)\n            mlvl_dir_scores.append(dir_cls_score)\n            mlvl_attr_scores.append(attr_score)\n            mlvl_centerness.append(centerness)\n        mlvl_centers2d = torch.cat(mlvl_centers2d)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        mlvl_dir_scores = torch.cat(mlvl_dir_scores)\n        # change local yaw to global yaw for 3D nms\n        cam2img = mlvl_centers2d.new_zeros((4, 4))\n        cam2img[:view.shape[0], :view.shape[1]] = mlvl_centers2d.new_tensor(view)\n        mlvl_bboxes = self.bbox_coder.decode_yaw(mlvl_bboxes, mlvl_centers2d,mlvl_dir_scores,self.dir_offset, cam2img)\n        mlvl_bboxes_for_nms = xywhr2xyxyr(input_meta[\'box_type_3d\'](mlvl_bboxes, box_dim=self.bbox_code_size,origin=(0.5, 0.5, 0.5)).bev)\n        mlvl_scores = torch.cat(mlvl_scores)\n        padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n        # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n        # BG cat_id: num_class\n        mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n        mlvl_attr_scores = torch.cat(mlvl_attr_scores)\n        mlvl_centerness = torch.cat(mlvl_centerness)\n        # no scale_factors in box3d_multiclass_nms\n        # Then we multiply it from outside\n        mlvl_nms_scores = mlvl_scores * mlvl_centerness[:, None]\n        results = box3d_multiclass_nms(mlvl_bboxes, mlvl_bboxes_for_nms,mlvl_nms_scores, cfg.score_thr,cfg.max_per_img, cfg, mlvl_dir_scores,mlvl_attr_scores)\n        bboxes, scores, labels, dir_scores, attrs = results\n        attrs = attrs.to(labels.dtype)  # change data type to int\n        bboxes = input_meta[\'box_type_3d\'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        # Note that the predictions use origin (0.5, 0.5, 0.5)\n        # Due to the ground truth centers2d are the gravity center of objects\n        # v0.10.0 fix inplace operation to the input tensor of cam_box3d\n        # So here we also need to add origin=(0.5, 0.5, 0.5)\n        if not self.pred_attrs:\n            attrs = None\n        return bboxes, scores, labels, attrs\n\'> </span>'}]}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
