<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>bev_depth_parkdata_10cls</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">初始化</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">exps/bev_depth_parkdata_10cls.py</p><span class=\'hidden-code\' data-code=\'class BEVDepthLightningModel(LightningModule):\n    def __init__(...):\n        super().__init__()\n        self.save_hyperparameters()\n        self.gpus = gpus\n        self.eval_interval = eval_interval                 # 1\n        self.batch_size_per_device = batch_size_per_device # 8\n        self.data_root = data_root                         # ../data/parkdata\n        self.basic_lr_per_img = 2e-4 / 64                  # 最初2e-4 / 64\n        self.class_names = class_names                     # [\'car\']\n        self.backbone_conf = backbone_conf\n        self.head_conf = head_conf\n        self.ida_aug_conf = ida_aug_conf                   # {\'resize_dim\': (640, 400), \'final_dim\': (640, 384), \'H\': 800, \'W\': 1280}\n        self.bda_aug_conf = bda_aug_conf                   # {\'rot_lim\': (-22.5, 22.5), \'scale_lim\': (0.95, 1.05), \'flip_dx_ratio\': 0.5, \'flip_dy_ratio\': 0.5}\n        mmcv.mkdir_or_exist(default_root_dir)              # \'./outputs/bev_depth_parkdata_3task_384x640_120x88_1key\'\n        self.default_root_dir = default_root_dir\n        self.model = `BEVPark10CLS`(self.backbone_conf,self.head_conf)\n        self.mode = \'valid\'\n        self.img_conf = img_conf                           # img_mean img_std to_rgb=True   {\'img_mean\': [0.0, 0.0, 0.0], \'img_std\': [255.0, 255.0, 255.0], \'to_rgb\': True}\n        self.data_use_cbgs = False\n        self.num_sweeps = 1\n        self.sweep_idxes = []\n        self.key_idxes = []\n        #self.data_return_depth = False\n        self.model_use_ema = True\n        self.downsample_factor = self.backbone_conf[\'downsample_factor\']    # 8\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/bev_park10cls.py</p><span class=\'hidden-code\' data-code=\'class BEVPark10CLS(nn.Module):\n    def __init__(self, backbone_conf, head_conf):\n        super(BEVPark10CLS, self).__init__()\n        self.backbone = `SampleFPN10`(**backbone_conf)\n        self.head = `BEV10CLSHead`(**head_conf)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/backbones/sample_fpn_10.py</p><span class=\'hidden-code\' data-code=\'class SampleFPN10(nn.Module):\n    def __init__():\n        super(SampleFPN10, self).__init__()\n        # \'x_bound\': [-16.36, 16.36, 28*scale],  x向前,y朝左\n        # \'y_bound\': [-12, 12, 28*scale],\n        # \'z_bound\': [-1, 3, 4], \n        self.XMIN, self.XMAX, self.vox_size_X = x_bound[:3]\n        self.YMIN, self.YMAX, self.vox_size_Y = y_bound[:3]\n        self.ZMIN, self.ZMAX, self.vox_size_Z = z_bound[:3]\n        self.X = round((x_bound[1]-x_bound[0])/x_bound[2]) # 120\n        self.Y = round((y_bound[1]-y_bound[0])/y_bound[2]) # 88\n        self.Z = round((z_bound[1]-z_bound[0])/z_bound[2]) #  4\n        \n        self.downsample_factor = downsample_factor         # 8 (640,384)下采样8倍, ->80,48\n        self.output_channels = output_channels             # 128\n        # ResNet50\n        self.img_backbone = build_backbone(img_backbone_conf)\n        self.img_backbone.init_weights()\n        \n        # SECONDFPN\n        self.img_neck = build_neck(img_neck_conf)\n        self.img_neck.init_weights()\n        \n        # ResNet18\n        self.bevimg_backbone = build_backbone(bevimg_backbone_conf)\n        self.bevimg_backbone.init_weights()\n        \n        self.bevimg_neck = build_neck(bevimg_neck_conf)\n        self.bevimg_neck.init_weights()\n        \n        \n        fpn_out_channels = img_neck_conf[\'out_channels\']       # [128, 128, 128, 128]\n        fpnout_C = int(sum(fpn_out_channels))                  # 512\n        \n        bev_out_channels = bevimg_neck_conf[\'out_channels\']    # [64, 64, 64, 64]\n        bevout_C = int(sum(bev_out_channels))                  # 256\n        \n        self.do_fpn_compress = do_fpn_compress                 # True\n        self.do_bev_compress = do_bev_compress                 # True\n        \n        if do_fpn_compress:                                    # 将512->128\n            self.fpn_compressor = nn.Sequential(\n                nn.Conv2d(fpnout_C, fpnout_C//4, kernel_size=1,bias=False),\n                nn.BatchNorm2d(fpnout_C//4),\n                nn.ReLU(inplace=True)\n            )\n            for m in self.fpn_compressor.modules():\n                if isinstance(m, nn.Conv2d):\n                    torch.nn.init.kaiming_normal_(m.weight)\n        \n        bev_inchannel = fpnout_C//4*self.Z + bevout_C         # 512//4*4+256=768-->768\n        if do_bev_compress:\n            self.bev_compressor = nn.Sequential(\n                nn.Conv2d(bev_inchannel, self.output_channels, kernel_size=1, bias=False),\n                nn.BatchNorm2d(self.output_channels),\n                nn.ReLU(),\n            )\n            for m in self.bev_compressor.modules():\n                if isinstance(m, nn.Conv2d):\n                    torch.nn.init.kaiming_normal_(m.weight)\n        \n        self.cams = cams\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/heads/bev10cls_sample_head.py</p><span class=\'hidden-code\' data-code=\'class BEV10CLSHead(ParkBEVCenterHead):\n    def __init__():\n        self.trunk = build_backbone(bev_backbone_conf)   # {\'type\': \'ResNet\', \'in_channels\': 128, \'depth\': 18, \'num_stages\': 3, \'strides\': (1, 2, 2), \'dilations\': (1, 1, 1), \'out_indices\': [0, 1, 2], \'norm_eval\': False, \'base_channels\': 160}\n        self.trunk.init_weights()\n        del self.trunk.maxpool\n        \n        self.neck = build_neck(bev_neck_conf)            # {\'type\': \'SECONDFPN\', \'in_channels\': [128, 160, 320, 640], \'upsample_strides\': [1, 2, 4, 8], \'out_channels\': [64, 64, 64, 64]}\n        self.neck.init_weights()\n        \n        self.gaussian_overlap = gaussian_overlap         # 0.1\n        self.min_radius = min_radius                     # 2\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        \n        self.debug = True\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdetection3d/mmdet3d/models/dense_heads/centerpoint_head.py</p><span class=\'hidden-code\' data-code=\'class ParkBEVCenterHead(BaseModule):\n    def __init__():\n        num_classes = [len(t[\'class_names\']) for t in tasks]        # [1, 2, 1, 5, 1]\n        self.class_names = [t[\'class_names\'] for t in tasks]        # [[\'car\'], [\'wheel_2\', \'rider\'], [\'people\'], [\'cone\', \'wsign\', \'stone\', \'piles\', \'lock\'], [\'"pillar"\']]\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.norm_bbox = norm_bbox        # True\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n        self.bbox_coder = build_bbox_coder(bbox_coder)\n        self.num_anchor_per_locs = [n for n in num_classes]\n        self.fp16_enabled = False\n        self.code_size = bbox_coder[\'code_size\']\n        # a shared convolution\n        self.shared_conv = ConvModule(\n            in_channels,                   # 256\n            share_conv_channel,            # 64\n            kernel_size=3,\n            padding=1,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            bias=bias)\n        self.task_heads = nn.ModuleList()\n        for num_cls in num_classes:\n            heads = copy.deepcopy(common_heads)\n            heads.update(dict(heatmap=(num_cls, num_heatmap_convs))) #加入heatmap\n            separate_head.update(\n                in_channels=share_conv_channel, heads=heads, num_cls=num_cls)\n            self.task_heads.append(builder.build_head(separate_head))\n\'> </span>'}]}]}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">前向推理</p>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">exps/bev_depth_parkdata_10cls.py</p><span class=\'hidden-code\' data-code=\'class BEVDepthLightningModel(LightningModule):\n    def training_step(self, batch):\n        (sweep_imgs,sweep_bevimgs,mats, img_metas, gt_boxes, gt_labels,gt_sjzs,gt_xrs) = batch\n        if torch.cuda.is_available():\n            for key, value in mats.items():\n                mats[key] = value.cuda()\n            sweep_imgs = sweep_imgs.cuda()\n            sweep_bevimgs = sweep_bevimgs.cuda()\n            gt_boxes = [gt_box.cuda() for gt_box in gt_boxes]\n            gt_labels = [gt_label.cuda() for gt_label in gt_labels]\n            gt_sjzs = [gt_sjz.cuda() for gt_sjz in gt_sjzs]\n            gt_xrs = [gt_xr.cuda() for gt_xr in gt_xrs]\n            \n        preds = `self`(sweep_imgs,sweep_bevimgs,mats)        # preds: ([每个任务对应的{\'reg\':,\'height\':,\'...\'}],[{}])\n        if isinstance(self.model, torch.nn.parallel.DistributedDataParallel):\n            targets = self.model.module.get_targets(gt_boxes, gt_labels)  # type: ignore\n            detection_loss = self.model.module.loss(targets, preds)  # type: ignore\n        else:\n            targets = self.model.`get_targets`(gt_boxes, gt_labels) # tuple(list[Tensor],list[tensor],..)          # heatmaps, anno_boxes, inds, masks\n            detection_loss = self.model.`loss`(targets, preds)\n        self.log(\'car_loss\',detection_loss, prog_bar=True)\n        loss = detection_loss\n        return loss\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">exps/bev_depth_parkdata_10cls.py</p><span class=\'hidden-code\' data-code=\'class BEVDepthLightningModel(LightningModule):\n    def forward(self,sweep_imgs,sweep_bevimgs,mats):\n        x = self.`backbone`(x,sweep_bevimgs,mats_dict)    # torch.Size([10, 1, 4, 3, 384, 640])+torch.Size([10, 1, 3, 480, 352])-->torch.Size([10, 128, 120, 88])\n        preds = self.`head`(x)                            # ([{每个任务:{\'reg\':,\'height\':,\'...\'}}],[{}])\n        return preds\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/backbones/sample_fpn_10.py</p><span class=\'hidden-code\' data-code=\'class SampleFPN10(nn.Module)::\n    def _forward_single_sweep(self,sweep_imgs,sweep_bevimg,mats_dict):\n        img_feats = self.`get_cam_feats`(sweep_imgs)            # torch.Size([bs, 4, 3, 384, 640])-> [bs,4,512,48,80]\n        bevimg_feat = self.`get_bevimg_feat`(sweep_bevimg)      # torch.Size([bx, 3, 480, 352])   -> b,256,120,88\n        \n        bev_fea = self.`get_bev_fea`(img_feats.float(),mats_dict["points"])       # b,128*4,120,88\n        bev_fea = torch.cat([bevimg_feat,bev_fea.float()],dim=1) # b,256+512,120,88\n        \n        if self.do_bev_compress:                                 # torch.Size([10, 128, 120, 88])\n            bev_fea = self.bev_compressor(bev_fea)\n        \n        return bev_fea.float() # -> bs,128,120,88\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/backbones/sample_fpn_10.py</p><span class=\'hidden-code\' data-code=\'class SampleFPN10(nn.Module)::\n    def get_cam_feats(self, imgs):\n        batch_size, num_cams, num_channels, imH, imW = imgs.shape                   # imgH:384 imgW:640\n        imgs = imgs.flatten().view(batch_size * num_cams,num_channels, imH, imW)\n        bonefea = self.img_backbone(imgs)\n        img_feats = self.img_neck(bonefea)[0]                                       # 下采样8倍 -> [b,512,48,80]\n        # self.img_backbone(imgs)出来: [b,256,96,160],[b,512,48,80],[b,1024,24,40],[b,2048,12,20]\n        if self.do_fpn_compress:\n            img_feats = self.fpn_compressor(img_feats)                             # 512压缩到128\n        img_feats = img_feats.reshape(batch_size, num_cams,img_feats.shape[1], img_feats.shape[2],img_feats.shape[3])\n        return img_feats\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/backbones/sample_fpn_10.py</p><span class=\'hidden-code\' data-code=\'class SampleFPN10(nn.Module)::\n    def get_bevimg_feat(self,sweep_bevimg):\n        """Get feature maps from images."""\n        batch_size, num_channels, imH, imW = sweep_bevimg.shape           # torch.Size([bs, 3, 480, 352])\n        # imgH:480 imgW:352\n        sweep_bevimg = sweep_bevimg.flatten().view(batch_size, num_channels, imH, imW)\n        bonebevfea = self.bevimg_backbone(sweep_bevimg)\n        bevimg_feats = self.bevimg_neck(bonebevfea)[0]                    # 下采样4倍 -> [b,256,120,88]\n        # self.bevimg_backbone(imgs)出来: [b,64,120,88],[b,128,60,44],[b,256,30,22],[b,512,15,22]\n        bevimg_feats = bevimg_feats.reshape(batch_size,bevimg_feats.shape[1], bevimg_feats.shape[2],bevimg_feats.shape[3])\n        return bevimg_feats                                               # b,256,120,88\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/backbones/sample_fpn_10.py</p><span class=\'hidden-code\' data-code=\'class SampleFPN10(nn.Module)::\n    def get_bev_fea(self,source_features,points):\n        # source_features: [bs,4,512,48,80]              \n        bs,num_cams,C,fh,fw = source_features.shape              # torch.Size([10, 4, 128, 48, 80])\n        # points = torch.from_numpy(copy.deepcopy(self.mapcors)) # 4,n,3\n        # scale = float(1/(2*self.downsample_factor))            # 因为原始输入scale已经缩小了一半\n        # points[:,:,:2] = points[:,:,:2] * scale\n        # points = points.unsqueeze(0).repeat(bs,1,1,1).contiguous() # b,4,n,3\n        \n        EPS = 1e-6\n        x,y,z = points[:,:,:,0],points[:,:,:,1],points[:,:,:,2]  # torch.Size([10, 4, 42240])\n        \n        # 获取有效的点位,过滤掉不在截锥范围内的\n        x_valid = (x`>`=0).bool() & (x`<`fw).bool()\n        y_valid = (y`>`=0).bool() & (y`<`fh).bool()\n        z_valid = (z>0.0).bool()\n        valid_fea = (x_valid & y_valid & z_valid).reshape(bs,num_cams,1,self.X,self.Y,self.Z).float()\n        valid_fea = valid_fea.to(source_features.device)        # torch.Size([10, 4, 1, 120, 88, 4])    值要么为1，要么为0\n        \n        # 对x,y进行归一化，满足-1到1之间。\n        grid_x = 2.0*(x/float(fw-1)) - 1.0 # b,4,n           fw = 80         2D空间的范围\n        grid_y = 2.0*(y/float(fh-1)) - 1.0 # b,4,n           fh = 48\n        grid_x = torch.clamp(grid_x, min=-2.0, max=2.0)   # b,4,n\n        grid_y = torch.clamp(grid_y, min=-2.0, max=2.0)   # b,4,n\n        \n        # 获取grid_sample满足的坐标\n        grid_z = torch.zeros_like(x)  # b,4,n\n        xyz_pix = torch.stack([grid_x,grid_y,grid_z],axis=3)            # b,4,n,3       torch.Size([10, 4, 42240, 3])\n        xyz_pix = xyz_pix.reshape([bs*num_cams,self.X,self.Y,self.Z,3]) # b,x,y,z,3     # 120,88,4  3D空间的范围\n        xyz_pix = xyz_pix.to(source_features.device)                    # torch.Size([40, 120, 88, 4, 3])\n        # grid_sample采样对应的图像特征点\n        # print(\'source_features dtype\',source_features.dtype) # torch.float16\n        source_features = source_features.reshape([bs*num_cams,C,1,fh,fw]) # b,C,1,fh,fw   torch.Size([10, 4, 128, 48, 80])\n        bev_fea_ = F.grid_sample(source_features,xyz_pix.float(),align_corners=False)\n        # grid_sample (-1,-1)对应采样特征图的左上角. (1,1)对应采样特征图的右下角。\n        # 其采样结果特征图的顺序对应xyz_pix的顺序坐标,而xyz_pix的顺序对应原始的定义格子顺序\n        # 利用b*4,X,Y,Z,3 从 b*4,C,1,fh,fw 获取 b*4,C,X,Y,Z\n        # 其每个格子投影到每个相机的图像面对应的点为xyz_pix。3维度的顺序必须对应特征的fw,fh,1\n        bev_fea_ = bev_fea_.contiguous().reshape([bs,num_cams,C,self.X,self.Y,self.Z]) # fp16\n        bev_fea_ = bev_fea_ * valid_fea # fp32\n        \n        # 对bev_fea做相机的均值处理,消除相机维度。\n        mask_fea = (torch.abs(bev_fea_)>0).float()\n        fea_prob = bev_fea_ * mask_fea                                           # torch.Size([10, 4, 128, 120, 88, 4])\n        camsum = torch.sum(fea_prob,dim=1,keepdim=False)\n        nonzeronum = EPS+torch.sum(mask_fea, dim=1, keepdim=False)\n        bev_fea_ = camsum/nonzeronum # bs,c,x,y,z\n        # collapse z\n        bev_fea = bev_fea_.permute(0,1,4,3,2).reshape(bs,C*self.Z,self.X,self.Y) # fp32  torch.Size([10, 512, 120, 88])\n        return bev_fea.float().contiguous() # bs,C,X,Y(X对应特征的H维度)\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/heads/bev10cls_sample_head.py</p><span class=\'hidden-code\' data-code=\'class BEV10CLSHead(ParkBEVCenterHead):\n    def forward_offical(self,x):\n        # FPN\n        trunk_outs = [x]                  # x: B,128,120,88\n        if self.trunk.deep_stem:          # False\n            x = self.trunk.stem(x)\n        else:\n            x = self.trunk.conv1(x)       # torch.Size([bs, 160, 60, 44])\n            x = self.trunk.norm1(x)\n            x = self.trunk.relu(x)        # B,160,60,44\n        for i, layer_name in enumerate(self.trunk.res_layers):\n            res_layer = getattr(self.trunk, layer_name)\n            x = res_layer(x)\n            if i in self.trunk.out_indices:\n                trunk_outs.append(x)             # 依次加入 [B,160,60,44], [B, 320, 30,22],[B, 640, 15,11]\n        fpn_output = self.neck(trunk_outs)       # B,256,120,88\n        ret_values = super().`forward`(fpn_output) # ([{每个任务:{\'reg\':,\'height\':,\'...\'}}],[{}])\n        return ret_values        \n    @autocast(False)\n    def forward(self, x):\n        return self.forward_offical(x)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdetection3d/mmdet3d/models/dense_heads/centerpoint_head.py</p><span class=\'hidden-code\' data-code=\'class ParkBEVCenterHead(BaseModule):\n    def forward_single(self, x):\n        ret_dicts = []\n        x = self.shared_conv(x)\n        for task in self.task_heads:                        # task: 是一个SeparateHead类 reg 2 dim 2 rot 2 heatmap 1\n            ret_dicts.append(task(x))\n        """\n        [(key,value.shape) for ret_dict in ret_dicts for key,value in ret_dict.items()]\n        # (\'reg\', torch.Size([10, 2, 120, 88]))\n        # (\'dim\', torch.Size([10, 2, 120, 88]))\n        # (\'rot\', torch.Size([10, 2, 120, 88]))\n        # (\'heatmap\', torch.Size([10, 5, 120, 88]))        # 假设这个组5个类别\n        """\n        return ret_dicts                                   # list列表存了三个dict,每个dict负责一个目标任务。\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/bev_park10cls.py</p><span class=\'hidden-code\' data-code=\'class BEVPark10CLS(nn.Module):\n    def get_targets(self, gt_boxes, gt_labels):\n        return self.head.`get_targets`(gt_boxes, gt_labels)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">mmdetection3d/mmdet3d/models/dense_heads/centerpoint_head.py</p><span class=\'hidden-code\' data-code=\'class CenterHead(BaseModule):\n    def get_targets(self, gt_bboxes_3d, gt_labels_3d):\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">layers/heads/bev10cls_sample_head.py</p><span class=\'hidden-code\' data-code=\'class BEV10CLSHead(ParkBEVCenterHead):\n    def get_targets_single(self, gt_bboxes_3d, gt_labels_3d):\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">models/bev_park10cls.py</p><span class=\'hidden-code\' data-code=\'class BEVPark10CLS(nn.Module):\n    def loss(self, targets, preds_dicts)\n\'> </span>'}]}]}]})</script>
    <script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
  </body>
</html>
