<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>maptr_nano_r18</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def __getitem__(self, idx):\n        idx=0               <span style=\'color: red\'>用来debug</span>\n        if self.test_mode:\n            return self.prepare_test_data(idx)\n        while True:\n            data = self.<span style=\'color: green;font-weight: bold;\'>prepare_train_data</span>(idx)\n            if data is None:\n                idx = self._rand_another(idx)\n                continue\n            return data\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def prepare_train_data(self, index):\n        data_queue = []\n        prev_indexs_list = list(range(index-self.queue_length, index))   <span style=\'color: red\'># [0-1,0]->[-1]</span>\n        random.shuffle(prev_indexs_list)\n        prev_indexs_list = sorted(prev_indexs_list[1:], reverse=True)    <span style=\'color: red\'># []</span>\n        input_dict = self.<span style=\'color: green;font-weight: bold;\'>get_data_info</span>(index)\n        if input_dict is None:\n            return None\n        frame_idx = input_dict[\'frame_idx\']\n        scene_token = input_dict[\'scene_token\']         <span style=\'color: red\'># \'0d2cc345342a460e94ff54748338ac22\'</span>\n        self.<span style=\'color: green;font-weight: bold;\'>pre_pipeline</span>(input_dict)\n        example = self.<span style=\'color: green;font-weight: bold;\'>pipeline</span>(input_dict)           <span style=\'color: red\'># dict_keys([\'img_metas\', \'gt_bboxes_3d\', \'gt_labels_3d\', \'img\'])                 </span>\n        example = self.<span style=\'color: green;font-weight: bold;\'>vectormap_pipeline</span>(example,input_dict)\n        if self.filter_empty_gt and (example is None or ~(example[\'gt_labels_3d\']._data != -1).any()):\n            return None\n        data_queue.insert(0, example)\n        for i in prev_indexs_list:\n            i = max(0, i)\n            input_dict = self.get_data_info(i)\n            if input_dict is None:\n                return None\n            if input_dict[\'frame_idx\'] < frame_idx and input_dict[\'scene_token\'] == scene_token:\n                self.pre_pipeline(input_dict)\n                example = self.pipeline(input_dict)\n                example = self.vectormap_pipeline(example,input_dict)\n                if self.filter_empty_gt and (example is None or ~(example[\'gt_labels_3d\']._data != -1).any()):\n                    return None\n                frame_idx = input_dict[\'frame_idx\']\n            data_queue.insert(0, copy.deepcopy(example))\n        return self.<span style=\'color: green;font-weight: bold;\'>union2one</span>(data_queue)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def get_data_info(self, index):\n        <span style=\'color: red\'># ......</span>\n        if not self.test_mode:\n            annos = self.get_ann_info(index)         <span style=\'color: red\'># {\'gt_bboxes_3d\':torch.Size([44, 7]),\'gt_labels_3d\':(44,),\'gt_names\':(44,)}</span>\n            input_dict[\'ann_info\'] = annos\n        rotation = Quaternion(input_dict[\'ego2global_rotation\'])\n        translation = input_dict[\'ego2global_translation\']     <span style=\'color: red\'># [2234.259847599768, 857.4061194452617, 0.0]</span>\n        can_bus = input_dict[\'can_bus\']             <span style=\'color: red\'># (18,)</span>\n        can_bus[:3] = translation\n        can_bus[3:7] = rotation\n        patch_angle = quaternion_yaw(rotation) / np.pi * 180\n        if patch_angle < 0:\n            patch_angle += 360\n        can_bus[-2] = patch_angle / 180 * np.pi\n        can_bus[-1] = patch_angle\n        lidar2ego = np.eye(4)\n        lidar2ego[:3,:3] = Quaternion(input_dict[\'lidar2ego_rotation\']).rotation_matrix\n        lidar2ego[:3, 3] = input_dict[\'lidar2ego_translation\']\n        ego2global = np.eye(4)\n        ego2global[:3,:3] = Quaternion(input_dict[\'ego2global_rotation\']).rotation_matrix\n        ego2global[:3, 3] = input_dict[\'ego2global_translation\']\n        lidar2global = ego2global @ lidar2ego\n        input_dict[\'lidar2global\'] = lidar2global\n        return input_dict\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">input_dict</p><font size="0"><pre class="language-json"><code class="language-json">{\n    "sample_idx":"f9878012c3f6412184c294c13ba4bac3",\n    "pts_filename":"./data/nuscenes/samples/LIDAR_TOP/n008-2018-05-21-11-06-59-0400__LIDAR_TOP__1526915243047392.pcd.bin",\n    "lidar_path":"./data/nuscenes/samples/LIDAR_TOP/n008-2018-05-21-11-06-59-0400__LIDAR_TOP__1526915243047392.pcd.bin",\n    "sweeps":[],\n    "ego2global_translation":[2234.259847599768, 857.4061194452617, 0.0],\n    "ego2global_rotation":\n    [0.9997238940978229, 8.80899569529068e-05, 0.006940903896707341, 0.022448867747433807],\n    "lidar2ego_translation":[0.891067, 0.0, 1.84292],\n    "lidar2ego_rotation":[0.7043825600303035, 0.002529989518177017, -0.0013265948325242168, -0.7098147986794471],\n    "prev_idx":"",\n    "next_idx":"dee909131941447b98da1f253c64c698",\n    "scene_token":"0d2cc345342a460e94ff54748338ac22",\n    "can_bus":[ 2.23425985e+03, 8.57406119e+02, 0.00000000e+00, 9.99723894e-01,\n                9.99723894e-01, 9.99723894e-01, 9.99723894e-01, 0.00000000e+00,\n                0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n                0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n                4.49059735e-02, 2.57292276e+00],\n    "frame_idx":0,\n    "timestamp":1526915243047392,\n    "map_location":"boston-seaport",\n    "lidar2ego":[[-7.6776166e-03,  9.9995559e-01, -5.4605086e-03,  8.9106703e-01],\n        [-9.9996907e-01, -7.6868986e-03, -1.6808877e-03,  0.0000000e+00],\n        [-1.7227875e-03,  5.4474343e-03,  9.9998367e-01,  1.8429199e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00]],\n    "camera2ego":"[array([[-0.0047123 ,...e=float32), array([[-8.1085944e-...e=float32), array([[ 8.0958527e-...e=float32), array([[ 1.7801417e-...e=float32), array([[ 0.94571775,...e=float32), array([[-9.3496680e-...e=float32)]",\n    "camera_intrinsics":"[array([[1.2628093e+0...e=float32), array([[1.2641254e+0...e=float32), array([[1.2564720e+0...e=float32), array([[798.12427,  ...e=float32), array([[1.2582339e+0...e=float32), array([[1.2594298e+0...e=float32)]",\n    "lidar2global":[[ 3.71896725e-02,  9.99272051e-01,  8.50266984e-03,2.23517551e+03],\n                    [-9.99306010e-01,  3.72061570e-02, -1.78879957e-03,8.57446366e+02],\n                    [-2.10384908e-03, -8.43024420e-03,  9.99962252e-01,1.83037972e+00],\n                    [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,1.00000000e+00]],\n    "img_filename":["./data/nuscenes/samp...012465.jpg", "./data/nuscenes/samp...019956.jpg", "./data/nuscenes/samp...004917.jpg", "./data/nuscenes/samp...037570.jpg", "./data/nuscenes/samp...047295.jpg", "./data/nuscenes/samp...027813.jpg"],\n    "lidar2img":"[array([[ 1.26035909e...000e+00]]), array([[ 1.36185706e...000e+00]]), array([[ 1.10470170e...000e+00]]), array([[-8.04936510e...000e+00]]), array([[-1.11865462e...000e+00]]), array([[ 2.68536904e...000e+00]])]",\n    "cam_intrinsic":"[array([[1.26280936e+...000e+00]]), array([[1.26412537e+...000e+00]]), array([[1.25647208e+...000e+00]]), array([[798.12426527...       ]]), array([[1.25823385e+...000e+00]]), array([[1.25942976e+...000e+00]])]",\n    "lidar2cam":"[array([[ 0.99996887,...       ]]), array([[ 5.91260254e...000e+00]]), array([[ 0.58042157,...       ]]), array([[-0.99994693,...       ]]), array([[-3.32162498e...000e+00]]), array([[-0.34739422,...       ]])]",\n    "ann_info":{\n        "gt_bboxes_3d": "LiDARInstance3DBoxes...00e+00]]))", \n        "gt_labels_3d": "array([ 1,  8,  0,  ...,  0,  0])", \n        "gt_names": "array([\'truck\', \'ped\')"\n    },\n}\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def vectormap_pipeline(self, example, input_dict):\n        lidar2ego = np.eye(4)\n        lidar2ego[:3,:3] = Quaternion(input_dict[\'lidar2ego_rotation\']).rotation_matrix\n        lidar2ego[:3, 3] = input_dict[\'lidar2ego_translation\']\n        ego2global = np.eye(4)\n        ego2global[:3,:3] = Quaternion(input_dict[\'ego2global_rotation\']).rotation_matrix\n        ego2global[:3, 3] = input_dict[\'ego2global_translation\']\n        lidar2global = ego2global @ lidar2ego\n        lidar2global_translation = list(lidar2global[:3,3])\n        lidar2global_rotation = list(Quaternion(matrix=lidar2global).q)\n        location = input_dict[\'map_location\']                             <span style=\'color: red\'># \'boston-seaport\'</span>\n        ego2global_translation = input_dict[\'ego2global_translation\']\n        ego2global_rotation = input_dict[\'ego2global_rotation\']\n        anns_results = self.vector_map.<span style=\'color: green;font-weight: bold;\'>gen_vectorized_samples</span>(location, lidar2global_translation, lidar2global_rotation)\n        \'\'\'\n        anns_results, type: dict\n            \'gt_vecs_pts_loc\': list[num_vecs], vec with num_points*2 coordinates\n            \'gt_vecs_pts_num\': list[num_vecs], vec with num_points\n            \'gt_vecs_label\': list[num_vecs], vec with cls index\n        \'\'\'\n        gt_vecs_label = to_tensor(anns_results[\'gt_vecs_label\'])             <span style=\'color: red\'># tensor([0, 0, 0, 1, 2, 2, 2])</span>\n        if isinstance(anns_results[\'gt_vecs_pts_loc\'], LiDARInstanceLines):\n            gt_vecs_pts_loc = anns_results[\'gt_vecs_pts_loc\']               <span style=\'color: red\'># <projects.mmdet3d_plugin.datasets.nuscenes_map_dataset.LiDARInstanceLines></span>\n        else:\n            gt_vecs_pts_loc = to_tensor(anns_results[\'gt_vecs_pts_loc\'])\n            try:\n                gt_vecs_pts_loc = gt_vecs_pts_loc.flatten(1).to(dtype=torch.float32)\n            except:\n                <span style=\'color: red\'># empty tensor, will be passed in train, but we preserve it for test</span>\n                gt_vecs_pts_loc = gt_vecs_pts_loc\n        example[\'gt_labels_3d\'] = DC(gt_vecs_label, cpu_only=False)\n        example[\'gt_bboxes_3d\'] = DC(gt_vecs_pts_loc, cpu_only=True)\n        return example\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def gen_vectorized_samples(self, location, lidar2global_translation, lidar2global_rotation):\n        map_pose = lidar2global_translation[:2]                 <span style=\'color: red\'># [2235.175513920431, 857.4463660941551]</span>\n        rotation = Quaternion(lidar2global_rotation)\n        patch_box = (map_pose[0], map_pose[1], self.patch_size[0], self.patch_size[1])  <span style=\'color: red\'># (2235.175513920431, 857.4463660941551, 60.0, 30.0)</span>\n        patch_angle = quaternion_yaw(rotation) / np.pi * 180    <span style=\'color: red\'># -87.86869252174084</span>\n        vectors = [] \n        for vec_class in self.vec_classes:                      <span style=\'color: red\'># [\'divider\', \'ped_crossing\', \'boundary\']</span>\n            if vec_class == \'divider\':\n                line_geom = self.<span style=\'color: green;font-weight: bold;\'>get_map_geom</span>(patch_box, patch_angle, self.line_classes, location)   <span style=\'color: red\'># [(\'road_divider\',[<shapely.geometry.linestring.LineString...]),(\'lane_divider\',[<shapely.geometry.linestring.LineString...>,<shapely.geometry.linestring.LineString...>])]</span>\n                line_instances_dict = self.<span style=\'color: green;font-weight: bold;\'>line_geoms_to_instances</span>(line_geom)     \n                for line_type, instances in line_instances_dict.items():\n                    for instance in instances:\n                        vectors.append((instance, self.CLASS2LABEL.get(line_type, -1)))\n            elif vec_class == \'ped_crossing\':\n                ped_geom = self.get_map_geom(patch_box, patch_angle, self.ped_crossing_classes, location)\n                ped_instance_list = self.<span style=\'color: green;font-weight: bold;\'>ped_poly_geoms_to_instances</span>(ped_geom)\n                for instance in ped_instance_list:\n                    vectors.append((instance, self.CLASS2LABEL.get(\'ped_crossing\', -1)))\n            elif vec_class == \'boundary\':\n                polygon_geom = self.get_map_geom(patch_box, patch_angle, self.polygon_classes, location)\n                poly_bound_list = self.<span style=\'color: green;font-weight: bold;\'>poly_geoms_to_instances</span>(polygon_geom)\n                for contour in poly_bound_list:\n                    vectors.append((contour, self.CLASS2LABEL.get(\'contours\', -1)))\n            else:\n                raise ValueError(f\'WRONG vec_class: {vec_class}\')\n        <span style=\'color: red\'># filtered_vectors = []</span>\n        <span style=\'color: red\'># gt_pts_loc_3d = []</span>\n        <span style=\'color: red\'># gt_pts_num_3d = []</span>\n        gt_labels = []\n        gt_instance = []\n        for instance, type in vectors:\n            if type != -1: \n                gt_instance.append(instance)          <span style=\'color: red\'># [shapely.geometry.linestring.LineString,...,]</span>\n                gt_labels.append(type)                <span style=\'color: red\'># [0, 0, 0, 1, 2, 2, 2]</span>\n        gt_instance = LiDARInstanceLines(gt_instance,self.sample_dist, self.num_samples, self.padding, self.fixed_num,self.padding_value, patch_size=self.patch_size)\n        anns_results = dict(gt_vecs_pts_loc=gt_instance,gt_vecs_label=gt_labels,)\n        return anns_results\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def get_map_geom(self, patch_box, patch_angle, layer_names, location):\n        map_geom = []\n        for layer_name in layer_names:\n            if layer_name in self.line_classes:\n                geoms = self.<span style=\'color: green;font-weight: bold;\'>get_divider_line</span>(patch_box, patch_angle, layer_name, location)\n                map_geom.append((layer_name, geoms))\n            elif layer_name in self.polygon_classes:\n                geoms = self.get_contour_line(patch_box, patch_angle, layer_name, location)\n                map_geom.append((layer_name, geoms))\n            elif layer_name in self.ped_crossing_classes:\n                geoms = self.get_ped_crossing_line(patch_box, patch_angle, location)\n                map_geom.append((layer_name, geoms))\n        return map_geom                 <span style=\'color: red\'># [<shapely.geometry.linestring.LineString object at 0x7f9018f08fd0>,<shapely.geometry.linestring.LineString object at 0x7f9018f08430>]</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def get_divider_line(self,patch_box,patch_angle,layer_name,location):\n        if layer_name is \'traffic_light\':\n            return None\n        patch_x = patch_box[0]       <span style=\'color: red\'># 2235.175513920431</span>\n        patch_y = patch_box[1]       <span style=\'color: red\'># 857.4463660941551</span>\n        patch = self.map_explorer[location].get_patch_coord(patch_box, patch_angle)       <span style=\'color: red\'># <shapely.geometry.polygon.Polygon object at 0x7f90f5f11f10></span>\n        line_list = []\n        records = getattr(self.map_explorer[location].map_api, layer_name)                <span style=\'color: red\'># \'road_divider\'->[...,...,...] len=377</span>\n        for record in records:                                                            \n            line = self.map_explorer[location].map_api.extract_line(record[\'line_token\']) <span style=\'color: red\'># <shapely.geometry.linestring.LineString object at 0x7f90191027f0></span>\n            if line.is_empty:  <span style=\'color: red\'># Skip lines without nodes.</span>\n                continue\n            new_line = line.intersection(patch)\n            if not new_line.is_empty:\n                new_line = affinity.rotate(new_line, -patch_angle, origin=(patch_x, patch_y), use_radians=False)\n                new_line = affinity.affine_transform(new_line,[1.0, 0.0, 0.0, 1.0, -patch_x, -patch_y])\n                line_list.append(new_line)\n        return line_list                   <span style=\'color: red\'># <shapely.geometry.linestring.LineString object at 0x7f90f51910a0></span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def line_geoms_to_instances(self, line_geom):\n        line_instances_dict = dict()\n        for line_type, a_type_of_lines in line_geom:\n            one_type_instances = self._one_type_line_geom_to_instances(a_type_of_lines)\n            line_instances_dict[line_type] = one_type_instances\n        return line_instances_dict\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class VectorizedLocalMap(object):\n    def union2one(self, queue):\n        imgs_list = [each[\'img\'].data for each in queue]      <span style=\'color: red\'># [torch.Size([6, 3, 192, 320])]</span>\n        metas_map = {}\n        prev_pos = None\n        prev_angle = None\n        for i, each in enumerate(queue):\n            metas_map[i] = each[\'img_metas\'].data\n            if i == 0:\n                metas_map[i][\'prev_bev\'] = False\n                prev_pos = copy.deepcopy(metas_map[i][\'can_bus\'][:3])\n                prev_angle = copy.deepcopy(metas_map[i][\'can_bus\'][-1])\n                metas_map[i][\'can_bus\'][:3] = 0\n                metas_map[i][\'can_bus\'][-1] = 0\n            else:\n                metas_map[i][\'prev_bev\'] = True\n                tmp_pos = copy.deepcopy(metas_map[i][\'can_bus\'][:3])\n                tmp_angle = copy.deepcopy(metas_map[i][\'can_bus\'][-1])\n                metas_map[i][\'can_bus\'][:3] -= prev_pos\n                metas_map[i][\'can_bus\'][-1] -= prev_angle\n                prev_pos = copy.deepcopy(tmp_pos)\n                prev_angle = copy.deepcopy(tmp_angle)\n        queue[-1][\'img\'] = DC(torch.stack(imgs_list),cpu_only=False, stack=True)\n        queue[-1][\'img_metas\'] = DC(metas_map, cpu_only=True)\n        queue = queue[-1]\n        return queue\n</code></pre></font>'}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/detectors/maptr.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTR(MVXTwoStageDetector):\n    def forward_train(......):\n        lidar_feat = None\n        if self.modality == \'fusion\':\n            lidar_feat = self.extract_lidar_feat(points)\n        \n        len_queue = img.size(1)\n        prev_img = img[:, :-1, ...]         <span style=\'color: red\'>tensor([], device=\'cuda:0\', size=(2, 0, 6, 3, 192, 320))</span>\n        img = img[:, -1, ...]               <span style=\'color: red\'>torch.Size([2, 6, 3, 192, 320])</span>\n        prev_img_metas = copy.deepcopy(img_metas)\n        <span style=\'color: red\'>prev_bev = self.obtain_history_bev(prev_img, prev_img_metas)</span>\n        <span style=\'color: red\'>import pdb;pdb.set_trace()</span>\n        prev_bev = self.<span style=\'color: green;font-weight: bold;\'>obtain_history_bev</span>(prev_img, prev_img_metas) if len_queue>1 else None     <span style=\'color: red\'>多帧融合的时候，历史的bev信息</span>\n        img_metas = [each[len_queue-1] for each in img_metas]                   <span style=\'color: red\'>[{},{}]</span>\n        img_feats = self.<span style=\'color: green;font-weight: bold;\'>extract_feat</span>(img=img, img_metas=img_metas)\n        losses = dict()\n        losses_pts = self.<span style=\'color: green;font-weight: bold;\'>forward_pts_train</span>(img_feats, lidar_feat, gt_bboxes_3d,gt_labels_3d, img_metas,gt_bboxes_ignore, prev_bev)\n        losses.update(losses_pts)\n        return losses\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/detectors/maptr.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTR(MVXTwoStageDetector):\n    def extract_feat(self, img, img_metas=None, len_queue=None):\n        img_feats = self.<span style=\'color: green;font-weight: bold;\'>extract_img_feat</span>(img, img_metas, len_queue=len_queue)        <span style=\'color: red\'># torch.Size([2, 6, 3, 192, 320])->torch.Size([2, 6, 256, 6, 10])  下采样了32倍</span>\n        return img_feats\n</code></pre></font>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/detectors/maptr.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTR(MVXTwoStageDetector):\n    def forward_pts_train(self,pts_feats,lidar_feat,gt_bboxes_3d,gt_labels_3d,img_metas,gt_bboxes_ignore=None,prev_bev=None):\n        outs = self.<span style=\'color: green;font-weight: bold;\'>pts_bbox_head</span>(pts_feats, lidar_feat, img_metas, prev_bev)\n        loss_inputs = [gt_bboxes_3d, gt_labels_3d, outs]\n        losses = self.pts_bbox_head.<span style=\'color: green;font-weight: bold;\'>loss</span>(*loss_inputs, img_metas=img_metas)\n        return losses\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRHead(DETRHead):\n    @force_fp32(apply_to=(\'mlvl_feats\', \'prev_bev\'))\n    def forward(self, mlvl_feats, lidar_feat, img_metas, prev_bev=None,  only_bev=False):\n        bs, num_cam, _, _, _ = mlvl_feats[0].shape\n        dtype = mlvl_feats[0].dtype\n        <span style=\'color: red\'># import pdb;pdb.set_trace()</span>\n        if self.query_embed_type == \'all_pts\':\n            object_query_embeds = self.query_embedding.weight.to(dtype)\n        elif self.query_embed_type == \'instance_pts\':                     <span style=\'color: red\'># True                    100个实例，每个实例20个点    </span>\n            pts_embeds = self.pts_embedding.weight.unsqueeze(0)           <span style=\'color: red\'># self.pts_embedding = nn.Embedding(self.num_pts_per_vec, self.embed_dims * 2)   torch.Size([1, 20, 512])    </span>\n            instance_embeds = self.instance_embedding.weight.unsqueeze(1) <span style=\'color: red\'># self.instance_embedding = nn.Embedding(self.num_vec, self.embed_dims * 2)      torch.Size([100, 1, 512])</span>\n            object_query_embeds = (pts_embeds + instance_embeds).flatten(0, 1).to(dtype)          <span style=\'color: red\'># torch.Size([100, 20, 512]) -> torch.Size([2000, 512])</span>\n        if self.bev_embedding is not None:                                <span style=\'color: red\'># Embedding(3200, 256)  self.bev_embedding = nn.Embedding(self.bev_h * self.bev_w, self.embed_dims)</span>\n            bev_queries = self.bev_embedding.weight.to(dtype)             <span style=\'color: red\'># torch.Size([3200, 256])           </span>\n            bev_mask = torch.zeros((bs, self.bev_h, self.bev_w),device=bev_queries.device).to(dtype)    <span style=\'color: red\'># torch.Size([2, 80, 40])</span>\n            bev_pos = self.positional_encoding(bev_mask).to(dtype)        <span style=\'color: red\'># LearnedPositionalEncoding(num_feats=128, row_num_embed=80, col_num_embed=40) --》torch.Size([2, 256, 80, 40])</span>\n        else:\n            bev_queries = None\n            bev_mask = None\n            bev_pos = None\n        if only_bev:  <span style=\'color: red\'># only use encoder to obtain BEV features, TODO: refine the workaround  False</span>\n            return self.transformer.get_bev_features(mlvl_feats,lidar_feat,bev_queries,self.bev_h,self.bev_w,\n                grid_length=(self.real_h / self.bev_h,self.real_w / self.bev_w),\n                bev_pos=bev_pos,img_metas=img_metas,prev_bev=prev_bev,)\n        else:\n            outputs = self.<span style=\'color: green;font-weight: bold;\'>transformer</span>(mlvl_feats,                                 <span style=\'color: red\'># [torch.Size([2, 6, 256, 6, 10])]</span>\n                lidar_feat, bev_queries, object_query_embeds,                        <span style=\'color: red\'># None, torch.Size([3200, 256]),  torch.Size([2000, 512])</span>\n                self.bev_h,self.bev_w,                                               <span style=\'color: red\'># 80,40</span>\n                grid_length=(self.real_h / self.bev_h,self.real_w / self.bev_w),     <span style=\'color: red\'># (60/80,30/40)->((0.75, 0.75))</span>\n                bev_pos=bev_pos,                                                     <span style=\'color: red\'># torch.Size([2, 256, 80, 40])</span>\n                reg_branches=self.reg_branches if self.with_box_refine else None,    <span style=\'color: red\'># True   一系列Linear+ReLU</span>\n                cls_branches=self.cls_branches if self.as_two_stage else None,       <span style=\'color: red\'># False</span>\n                img_metas=img_metas,\n                prev_bev=prev_bev                                                    <span style=\'color: red\'># None</span>\n        )\n        bev_embed, hs, init_reference, inter_references = outputs    <span style=\'color: red\'># torch.Size([3200, 2, 256]);  torch.Size([2, 2000, 2, 256]);  torch.Size([2, 2000, 2]);  torch.Size([2, 2, 2000, 2])</span>\n        hs = hs.permute(0, 2, 1, 3)                                  <span style=\'color: red\'># torch.Size([2, 2, 2000, 256])</span>\n        outputs_classes = []\n        outputs_coords = []\n        outputs_pts_coords = []\n        for lvl in range(hs.shape[0]):\n            if lvl == 0:\n                reference = init_reference\n            else:\n                reference = inter_references[lvl - 1]\n            reference = inverse_sigmoid(reference)                  <span style=\'color: red\'># torch.Size([2, 2000, 2])->torch.Size([2, 2000, 2])</span>\n            outputs_class = self.cls_branches[lvl](hs[lvl].view(bs,self.num_vec, self.num_pts_per_vec,-1).mean(2))  <span style=\'color: red\'># Linear+LayerNorm+ReLU->torch.Size([2, 100, 3])</span>\n            tmp = self.reg_branches[lvl](hs[lvl])                   <span style=\'color: red\'># torch.Size([2, 2000, 2])</span>\n            <span style=\'color: red\'># TODO: check the shape of reference                    </span>\n            assert reference.shape[-1] == 2\n            tmp[..., 0:2] += reference[..., 0:2]\n            tmp = tmp.sigmoid() <span style=\'color: red\'># cx,cy,w,h                       </span>\n            outputs_coord, outputs_pts_coord = self.<span style=\'color: green;font-weight: bold;\'>transform_box</span>(tmp)     <span style=\'color: red\'># 最外围box，但是没用到，用点对点的匹配</span>\n            outputs_classes.append(outputs_class)\n            outputs_coords.append(outputs_coord)\n            outputs_pts_coords.append(outputs_pts_coord)\n        outputs_classes = torch.stack(outputs_classes)         <span style=\'color: red\'># 每一层回归一次，增加收敛速度</span>\n        outputs_coords = torch.stack(outputs_coords)\n        outputs_pts_coords = torch.stack(outputs_pts_coords)\n        outs = {\n            \'bev_embed\': bev_embed,                  <span style=\'color: red\'># torch.Size([3200, 2, 256])</span>\n            \'all_cls_scores\': outputs_classes,       <span style=\'color: red\'># torch.Size([2, 2, 100, 3])</span>\n            \'all_bbox_preds\': outputs_coords,        <span style=\'color: red\'># torch.Size([2, 2, 100, 4])</span>\n            \'all_pts_preds\': outputs_pts_coords,     <span style=\'color: red\'># torch.Size([2, 2, 100, 20, 2])</span>\n            \'enc_cls_scores\': None,\n            \'enc_bbox_preds\': None,\n            \'enc_pts_preds\': None\n        }\n        return outs\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRPerceptionTransformer(BaseModule):\n    def forward(self, mlvl_feats, lidar_feat, bev_queries, object_query_embed, bev_h, bev_w, grid_length=[0.512, 0.512], bev_pos=None, reg_branches=None, cls_branches=None, prev_bev=None, **kwargs):\n        bev_embed = self.<span style=\'color: green;font-weight: bold;\'>get_bev_features</span>(mlvl_feats,lidar_feat,bev_queries,bev_h,bev_w,grid_length=grid_length,bev_pos=bev_pos,prev_bev=prev_bev,**kwargs)  <span style=\'color: red\'># bev_embed shape: bs, bev_h*bev_w, embed_dims</span>\n        bs = mlvl_feats[0].size(0)\n        query_pos, query = torch.split(object_query_embed, self.embed_dims, dim=1)   <span style=\'color: red\'># torch.Size([2000, 512])->torch.Size([2000, 256]) + torch.Size([2000, 256])</span>\n        query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)                        <span style=\'color: red\'># torch.Size([2, 2000, 256])</span>\n        query = query.unsqueeze(0).expand(bs, -1, -1)                                <span style=\'color: red\'># torch.Size([2, 2000, 256])</span>\n        reference_points = self.reference_points(query_pos)      <span style=\'color: red\'># self.reference_points = nn.Linear(self.embed_dims, 2) 2是指(x,y)  torch.Size([2, 2000, 2])</span>\n        reference_points = reference_points.sigmoid()            <span style=\'color: red\'># 0->1</span>\n        init_reference_out = reference_points\n        query = query.permute(1, 0, 2)                <span style=\'color: red\'># torch.Size([2000, 2, 256])</span>\n        query_pos = query_pos.permute(1, 0, 2)        <span style=\'color: red\'># torch.Size([2000, 2, 256])</span>\n        bev_embed = bev_embed.permute(1, 0, 2)        <span style=\'color: red\'># torch.Size([3200, 2, 256])</span>\n        inter_states, inter_references = self.<span style=\'color: green;font-weight: bold;\'>decoder</span>(        <span style=\'color: red\'># torch.Size([2, 2000, 2, 256]); torch.Size([2, 2, 2000, 2])</span>\n            query=query,\n            key=None,\n            value=bev_embed,\n            query_pos=query_pos,\n            reference_points=reference_points,\n            reg_branches=reg_branches,\n            cls_branches=cls_branches,\n            spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device),\n            level_start_index=torch.tensor([0], device=query.device),\n            **kwargs)\n        inter_references_out = inter_references\n        return bev_embed, inter_states, init_reference_out, inter_references_out   <span style=\'color: red\'># torch.Size([3200, 2, 256]);  torch.Size([2, 2000, 2, 256]);  torch.Size([2, 2000, 2]);  torch.Size([2, 2, 2000, 2])</span>\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRPerceptionTransformer(BaseModule):\n    def get_bev_features(self, mlvl_feats, lidar_feat, bev_queries, bev_h, bev_w, grid_length=[0.512, 0.512], bev_pos=None, prev_bev=None, **kwargs):\n        if self.use_attn_bev:     <span style=\'color: red\'># True</span>\n            bev_embed = self.<span style=\'color: green;font-weight: bold;\'>attn_bev_encode</span>(mlvl_feats, bev_queries, bev_h, bev_w, grid_length=grid_length, bev_pos=bev_pos, prev_bev=prev_bev, **kwargs)  <span style=\'color: red\'># torch.Size([2, 3200, 256])</span>\n        else:\n            bev_embed = self.lss_bev_encode(mlvl_feats,\n                prev_bev=prev_bev,\n                **kwargs)\n        if lidar_feat is not None:      <span style=\'color: red\'># None</span>\n            bs = mlvl_feats[0].size(0)\n            bev_embed = bev_embed.view(bs, bev_h, bev_w, -1).permute(0,3,1,2).contiguous()\n            lidar_feat = lidar_feat.permute(0,1,3,2).contiguous() <span style=\'color: red\'># B C H W</span>\n            lidar_feat = nn.functional.interpolate(lidar_feat, size=(bev_h,bev_w), mode=\'bicubic\', align_corners=False)\n            fused_bev = self.fuser([bev_embed, lidar_feat])\n            fused_bev = fused_bev.flatten(2).permute(0,2,1).contiguous()\n            bev_embed = fused_bev\n        return bev_embed\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/transformer.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRPerceptionTransformer(BaseModule):\n    def attn_bev_encode(self,mlvl_feats,bev_queries,bev_h,bev_w,grid_length=[0.512, 0.512],bev_pos=None,prev_bev=None,**kwargs):\n        bs = mlvl_feats[0].size(0)\n        bev_queries = bev_queries.unsqueeze(1).repeat(1, bs, 1)     <span style=\'color: red\'># torch.Size([3200, 256])->torch.Size([3200, 2, 256])</span>\n        bev_pos = bev_pos.flatten(2).permute(2, 0, 1)               <span style=\'color: red\'># torch.Size([3200, 2, 256])</span>\n        <span style=\'color: red\'># obtain rotation angle and shift with ego motion</span>\n        delta_x = np.array([each[\'can_bus\'][0] for each in kwargs[\'img_metas\']])    <span style=\'color: red\'># array([0., 0.])</span>\n        delta_y = np.array([each[\'can_bus\'][1] for each in kwargs[\'img_metas\']])    <span style=\'color: red\'># array([0., 0.])</span>\n        ego_angle = np.array([each[\'can_bus\'][-2] / np.pi * 180 for each in kwargs[\'img_metas\']])      <span style=\'color: red\'># array([2.57292276, 2.57292276])</span>\n        grid_length_y = grid_length[0]                                              <span style=\'color: red\'># 0.75,0.75</span>\n        grid_length_x = grid_length[1]\n        translation_length = np.sqrt(delta_x ** 2 + delta_y ** 2)                   <span style=\'color: red\'># array([0., 0.])</span>\n        translation_angle = np.arctan2(delta_y, delta_x) / np.pi * 180              <span style=\'color: red\'># array([0., 0.])</span>\n        bev_angle = ego_angle - translation_angle                                   <span style=\'color: red\'># array([2.57292276, 2.57292276])</span>\n        shift_y = translation_length * np.cos(bev_angle / 180 * np.pi) / grid_length_y / bev_h  <span style=\'color: red\'># array([0., 0.])</span>\n        shift_x = translation_length * np.sin(bev_angle / 180 * np.pi) / grid_length_x / bev_w  <span style=\'color: red\'># array([0., 0.])</span>\n        shift_y = shift_y * self.use_shift                                          <span style=\'color: red\'># array([0., 0.])</span>\n        shift_x = shift_x * self.use_shift                                          <span style=\'color: red\'># array([0., 0.])</span>\n        shift = bev_queries.new_tensor([shift_x, shift_y]).permute(1, 0)  <span style=\'color: red\'># xy, bs -> bs, xy     torch.Size([2, 2])</span>\n        if prev_bev is not None:                                                    <span style=\'color: red\'># None</span>\n            if prev_bev.shape[1] == bev_h * bev_w:\n                prev_bev = prev_bev.permute(1, 0, 2)\n            if self.rotate_prev_bev:\n                for i in range(bs):\n                    <span style=\'color: red\'># num_prev_bev = prev_bev.size(1)</span>\n                    rotation_angle = kwargs[\'img_metas\'][i][\'can_bus\'][-1]\n                    tmp_prev_bev = prev_bev[:, i].reshape(bev_h, bev_w, -1).permute(2, 0, 1)\n                    tmp_prev_bev = rotate(tmp_prev_bev, rotation_angle,center=self.rotate_center)\n                    tmp_prev_bev = tmp_prev_bev.permute(1, 2, 0).reshape(bev_h * bev_w, 1, -1)\n                    prev_bev[:, i] = tmp_prev_bev[:, 0]\n        <span style=\'color: red\'># add can bus signals</span>\n        can_bus = bev_queries.new_tensor([each[\'can_bus\'] for each in kwargs[\'img_metas\']])  <span style=\'color: red\'># [:, :]  torch.Size([2, 18])</span>\n        can_bus = self.can_bus_mlp(can_bus)[None, :, :]                          <span style=\'color: red\'># torch.Size([1, 2, 256])</span>\n        bev_queries = bev_queries + can_bus * self.use_can_bus                   <span style=\'color: red\'># torch.Size([3200, 2, 256])+torch.Size([1, 2, 256])->torch.Size([3200, 2, 256])</span>\n        feat_flatten = []\n        spatial_shapes = []\n        for lvl, feat in enumerate(mlvl_feats):\n            bs, num_cam, c, h, w = feat.shape                <span style=\'color: red\'># torch.Size([2, 6, 256, 6, 10])</span>\n            spatial_shape = (h, w)\n            feat = feat.flatten(3).permute(1, 0, 3, 2)       <span style=\'color: red\'># torch.Size([6, 2, 60, 256])</span>\n            if self.use_cams_embeds:\n                feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)      <span style=\'color: red\'># # torch.Size([6, 2, 60, 256]) + torch.Size([6, 1, 1, 256]) -></span>\n            feat = feat + self.level_embeds[None, None, lvl:lvl + 1, :].to(feat.dtype)  <span style=\'color: red\'># torch.Size([6, 2, 60, 256])</span>\n            spatial_shapes.append(spatial_shape)             <span style=\'color: red\'># (6, 10)</span>\n            feat_flatten.append(feat)\n        feat_flatten = torch.cat(feat_flatten, 2)            <span style=\'color: red\'># torch.Size([6, 2, 60, 256])</span>\n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=bev_pos.device)      <span style=\'color: red\'># tensor([[ 6, 10]], device=\'cuda:0\')</span>\n        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))   <span style=\'color: red\'># tensor([0], device=\'cuda:0\')</span>\n        feat_flatten = feat_flatten.permute(0, 2, 1, 3)  <span style=\'color: red\'># (num_cam, H*W, bs, embed_dims)    torch.Size([6, 60, 2, 256])</span>\n        bev_embed = self.encoder(                       <span style=\'color: red\'># BEVFormerEncoder</span>\n            bev_queries,\n            feat_flatten,\n            feat_flatten,\n            bev_h=bev_h,\n            bev_w=bev_w,\n            bev_pos=bev_pos,\n            spatial_shapes=spatial_shapes,\n            level_start_index=level_start_index,\n            prev_bev=prev_bev,\n            shift=shift,\n            **kwargs\n        ) \n        return bev_embed                                <span style=\'color: red\'># torch.Size([2, 3200, 256])</span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/decoder.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRDecoder(TransformerLayerSequence):\n    def forward(self, query, *args, reference_points=None, reg_branches=None, key_padding_mask=None, **kwargs):\n        output = query                                                       <span style=\'color: red\'># torch.Size([2000, 2, 256])</span>\n        intermediate = []\n        intermediate_reference_points = []\n        for lid, layer in enumerate(self.layers):                            <span style=\'color: red\'># len(self.layers)=2</span>\n            reference_points_input = reference_points[..., :2].unsqueeze(2)  <span style=\'color: red\'># BS NUM_QUERY NUM_LEVEL 2                          torch.Size([2, 2000, 1, 2])</span>\n            output = layer(output,*args,reference_points=reference_points_input,key_padding_mask=key_padding_mask,**kwargs)    <span style=\'color: red\'># DETR decoder->DetrTransformerDecoderLayer=torch.Size([2000, 2, 256])</span>\n            output = output.permute(1, 0, 2)                                 <span style=\'color: red\'># torch.Size([2, 2000, 256])</span>\n            if reg_branches is not None:\n                tmp = reg_branches[lid](output)                             <span style=\'color: red\'># 一系列 Linear+ReLU = torch.Size([2, 2000, 256])->torch.Size([2, 2000, 2])</span>\n                assert reference_points.shape[-1] == 2\n                new_reference_points = torch.zeros_like(reference_points)\n                new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points[..., :2])       <span style=\'color: red\'># tmp相当于瞄点加后面代码再次细化</span>\n                <span style=\'color: red\'># new_reference_points[..., 2:3] = tmp[..., 4:5] + inverse_sigmoid(reference_points[..., 2:3])</span>\n                new_reference_points = new_reference_points.sigmoid()                                           <span style=\'color: red\'># 更新</span>\n                reference_points = new_reference_points.detach()\n            output = output.permute(1, 0, 2)                               <span style=\'color: red\'># torch.Size([2000, 2, 256])</span>\n            if self.return_intermediate:                                   <span style=\'color: red\'># True</span>\n                intermediate.append(output)                                <span style=\'color: red\'># [torch.Size([2000, 2, 256]),torch.Size([2000, 2, 256])]</span>\n                intermediate_reference_points.append(reference_points)     <span style=\'color: red\'># [torch.Size([2, 2000, 2]),torch.Size([2, 2000, 2])]</span>\n        if self.return_intermediate:\n            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n        return output, reference_points\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRHead(DETRHead):\n    def transform_box(self, pts, y_first=False):        \n        pts_reshape = pts.view(pts.shape[0], self.num_vec,self.num_pts_per_vec,2)  <span style=\'color: red\'># torch.Size([2, 2000, 2])->torch.Size([2, 100, 20, 2])</span>\n        pts_y = pts_reshape[:, :, :, 0] if y_first else pts_reshape[:, :, :, 1]\n        pts_x = pts_reshape[:, :, :, 1] if y_first else pts_reshape[:, :, :, 0]\n        if self.transform_method == \'minmax\':\n            xmin = pts_x.min(dim=2, keepdim=True)[0]\n            xmax = pts_x.max(dim=2, keepdim=True)[0]\n            ymin = pts_y.min(dim=2, keepdim=True)[0]\n            ymax = pts_y.max(dim=2, keepdim=True)[0]\n            bbox = torch.cat([xmin, ymin, xmax, ymax], dim=2)\n            bbox = bbox_xyxy_to_cxcywh(bbox)\n        else:\n            raise NotImplementedError\n        return bbox, pts_reshape             <span style=\'color: red\'># torch.Size([2, 100, 4]), torch.Size([2, 100, 20, 2])</span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRHead(DETRHead):\n    def loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_bboxes_ignore=None, img_metas=None):\n        gt_vecs_list = copy.deepcopy(gt_bboxes_list)\n        <span style=\'color: red\'># import pdb;pdb.set_trace()</span>\n        all_cls_scores = preds_dicts[\'all_cls_scores\']\n        all_bbox_preds = preds_dicts[\'all_bbox_preds\']\n        all_pts_preds  = preds_dicts[\'all_pts_preds\']\n        enc_cls_scores = preds_dicts[\'enc_cls_scores\']    <span style=\'color: red\'># None没用到</span>\n        enc_bbox_preds = preds_dicts[\'enc_bbox_preds\']\n        enc_pts_preds  = preds_dicts[\'enc_pts_preds\']\n        num_dec_layers = len(all_cls_scores)\n        device = gt_labels_list[0].device\n        gt_bboxes_list = [gt_bboxes.bbox.to(device) for gt_bboxes in gt_vecs_list]     <span style=\'color: red\'># [torch.Size([7, 4]),torch.Size([7, 4])]</span>\n        gt_pts_list = [gt_bboxes.fixed_num_sampled_points.to(device) for gt_bboxes in gt_vecs_list]  <span style=\'color: red\'># [torch.Size([7, 20, 2]),torch.Size([7, 20, 2])]  每个batch有7个实例，每个实例20个点，每个点x,y</span>\n        if self.gt_shift_pts_pattern == \'v0\':\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points.to(device) for gt_bboxes in gt_vecs_list]\n        elif self.gt_shift_pts_pattern == \'v1\':\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points_v1.to(device) for gt_bboxes in gt_vecs_list]\n        elif self.gt_shift_pts_pattern == \'v2\':           <span style=\'color: red\'># 代码v2？</span>\n            gt_shifts_pts_list = [gt_bboxes.<span style=\'color: green;font-weight: bold;\'>shift_fixed_num_sampled_points_v2</span>.to(device) for gt_bboxes in gt_vecs_list]  <span style=\'color: red\'># torch.Size([7, 19, 20, 2])</span>\n        elif self.gt_shift_pts_pattern == \'v3\':           <span style=\'color: red\'># 论文是v3</span>\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points_v3.to(device) for gt_bboxes in gt_vecs_list]\n        elif self.gt_shift_pts_pattern == \'v4\':\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points_v4.to(device) for gt_bboxes in gt_vecs_list]\n        else:\n            raise NotImplementedError\n        all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]           <span style=\'color: red\'># num_dec_layers=2</span>\n        all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n        all_gt_pts_list = [gt_pts_list for _ in range(num_dec_layers)]\n        all_gt_shifts_pts_list = [gt_shifts_pts_list for _ in range(num_dec_layers)]\n        all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n        <span style=\'color: red\'># import pdb;pdb.set_trace()</span>\n        losses_cls, losses_bbox, losses_iou, losses_pts, losses_dir = multi_apply(self.<span style=\'color: green;font-weight: bold;\'>loss_single</span>, all_cls_scores, all_bbox_preds,all_pts_preds,\n            all_gt_bboxes_list, all_gt_labels_list,all_gt_shifts_pts_list,all_gt_bboxes_ignore_list)\n        loss_dict = dict()\n        <span style=\'color: red\'># loss of proposal generated from encode feature map.</span>\n        if enc_cls_scores is not None:                <span style=\'color: red\'># None</span>\n            binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n            <span style=\'color: red\'># TODO bug here</span>\n            enc_loss_cls, enc_losses_bbox, enc_losses_iou, enc_losses_pts, enc_losses_dir = \\\n                self.loss_single(enc_cls_scores, enc_bbox_preds, enc_pts_preds,gt_bboxes_list, binary_labels_list, gt_pts_list,gt_bboxes_ignore)\n            loss_dict[\'enc_loss_cls\'] = enc_loss_cls\n            loss_dict[\'enc_loss_bbox\'] = enc_losses_bbox\n            loss_dict[\'enc_losses_iou\'] = enc_losses_iou\n            loss_dict[\'enc_losses_pts\'] = enc_losses_pts\n            loss_dict[\'enc_losses_dir\'] = enc_losses_dir\n        <span style=\'color: red\'># loss from the last decoder layer</span>\n        loss_dict[\'loss_cls\'] = losses_cls[-1]\n        loss_dict[\'loss_bbox\'] = losses_bbox[-1]\n        loss_dict[\'loss_iou\'] = losses_iou[-1]\n        loss_dict[\'loss_pts\'] = losses_pts[-1]\n        loss_dict[\'loss_dir\'] = losses_dir[-1]\n        <span style=\'color: red\'># loss from other decoder layers</span>\n        num_dec_layer = 0\n        for loss_cls_i, loss_bbox_i, loss_iou_i, loss_pts_i, loss_dir_i in zip(losses_cls[:-1],losses_bbox[:-1],losses_iou[:-1],losses_pts[:-1],losses_dir[:-1]):\n            loss_dict[f\'d{num_dec_layer}.loss_cls\'] = loss_cls_i\n            loss_dict[f\'d{num_dec_layer}.loss_bbox\'] = loss_bbox_i\n            loss_dict[f\'d{num_dec_layer}.loss_iou\'] = loss_iou_i\n            loss_dict[f\'d{num_dec_layer}.loss_pts\'] = loss_pts_i\n            loss_dict[f\'d{num_dec_layer}.loss_dir\'] = loss_dir_i\n            num_dec_layer += 1\n        return loss_dict\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><font size="0"><pre class="language-python"><code class="language-python">class LiDARInstanceLines(object):\n    @property\n    def shift_fixed_num_sampled_points_v2(self):\n        """\n        return  [instances_num, num_shifts, fixed_num, 2]      <span style=\'color: red\'># v3顺时针一次，逆时针一次</span>\n        """\n        assert len(self.instance_list) != 0                <span style=\'color: red\'># [shapely.geometry.linestring.LineString,....]  len->7</span>\n        instances_list = []\n        for instance in self.instance_list:\n            distances = np.linspace(0, instance.length, self.fixed_num)   <span style=\'color: red\'># 20 -> (20,)</span>\n            poly_pts = np.array(list(instance.coords))    <span style=\'color: red\'># (13, 2)</span>\n            start_pts = poly_pts[0]                       <span style=\'color: red\'># (2,)</span>\n            end_pts = poly_pts[-1]                        <span style=\'color: red\'># (2,)</span>\n            is_poly = np.equal(start_pts, end_pts)\n            is_poly = is_poly.all()                       <span style=\'color: red\'># 没有闭环</span>\n            shift_pts_list = []\n            pts_num, coords_num = poly_pts.shape         <span style=\'color: red\'># (4, 2)</span>\n            shift_num = pts_num - 1                      <span style=\'color: red\'># 3</span>\n            final_shift_num = self.fixed_num - 1         <span style=\'color: red\'># 20-1=19</span>\n            if is_poly:                                                      <span style=\'color: red\'># 多边形</span>\n                pts_to_shift = poly_pts[:-1,:]\n                for shift_right_i in range(shift_num):\n                    shift_pts = np.roll(pts_to_shift,shift_right_i,axis=0)   <span style=\'color: red\'># 0.1.2+步长1->1.2.0</span>\n                    pts_to_concat = shift_pts[0]\n                    pts_to_concat = np.expand_dims(pts_to_concat,axis=0)\n                    shift_pts = np.concatenate((shift_pts,pts_to_concat),axis=0)\n                    shift_instance = LineString(shift_pts)\n                    shift_sampled_points = np.array([list(shift_instance.interpolate(distance).coords) for distance in distances]).reshape(-1, 2)\n                    shift_pts_list.append(shift_sampled_points)\n                <span style=\'color: red\'># import pdb;pdb.set_trace() </span>\n            else:                                                            <span style=\'color: red\'># 不是多边形，正反两次</span>\n                sampled_points = np.array([list(instance.interpolate(distance).coords) for distance in distances]).reshape(-1, 2)  <span style=\'color: red\'># (20, 2)</span>\n                flip_sampled_points = np.flip(sampled_points, axis=0)        <span style=\'color: red\'># (20, 2)</span>\n                shift_pts_list.append(sampled_points)\n                shift_pts_list.append(flip_sampled_points)\n            \n            multi_shifts_pts = np.stack(shift_pts_list,axis=0)      <span style=\'color: red\'># (2, 20, 2)</span>\n            shifts_num,_,_ = multi_shifts_pts.shape\n            if shifts_num > final_shift_num:     <span style=\'color: red\'># False</span>\n                index = np.random.choice(multi_shifts_pts.shape[0], final_shift_num, replace=False)\n                multi_shifts_pts = multi_shifts_pts[index]\n            \n            multi_shifts_pts_tensor = to_tensor(multi_shifts_pts)      <span style=\'color: red\'># torch.Size([2, 20, 2])</span>\n            multi_shifts_pts_tensor = multi_shifts_pts_tensor.to(dtype=torch.float32)\n            \n            multi_shifts_pts_tensor[:,:,0] = torch.clamp(multi_shifts_pts_tensor[:,:,0], min=-self.max_x,max=self.max_x)  <span style=\'color: red\'># -15-》15  -30-》30</span>\n            multi_shifts_pts_tensor[:,:,1] = torch.clamp(multi_shifts_pts_tensor[:,:,1], min=-self.max_y,max=self.max_y)\n            <span style=\'color: red\'># if not is_poly:</span>\n            if multi_shifts_pts_tensor.shape[0] < final_shift_num:    <span style=\'color: red\'># 2<19</span>\n                padding = torch.full([final_shift_num-multi_shifts_pts_tensor.shape[0],self.fixed_num,2], self.padding_value)   <span style=\'color: red\'># torch.Size([17, 20, 2])</span>\n                multi_shifts_pts_tensor = torch.cat([multi_shifts_pts_tensor,padding],dim=0)                                    <span style=\'color: red\'># torch.Size([19, 20, 2])</span>\n            instances_list.append(multi_shifts_pts_tensor)\n        instances_tensor = torch.stack(instances_list, dim=0)\n        instances_tensor = instances_tensor.to(\n                            dtype=torch.float32)\n        return instances_tensor              <span style=\'color: red\'># torch.Size([7, 19, 20, 2])</span>\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">/sdb/zzhu/code_study/MapTR/projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRHead(DETRHead):\n    def loss_single(self, cls_scores, bbox_preds, pts_preds, gt_bboxes_list, gt_labels_list, gt_shifts_pts_list, gt_bboxes_ignore_list=None):\n        num_imgs = cls_scores.size(0)                                 <span style=\'color: red\'># 2</span>\n        cls_scores_list = [cls_scores[i] for i in range(num_imgs)]    <span style=\'color: red\'># [torch.Size([100, 3]),torch.Size([100, 3])]</span>\n        bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]    <span style=\'color: red\'># [torch.Size([100, 4]),torch.Size([100, 4])]</span>\n        pts_preds_list = [pts_preds[i] for i in range(num_imgs)]      <span style=\'color: red\'># [torch.Size([100, 20, 2]),torch.Size([100, 20, 2])]</span>\n        <span style=\'color: red\'># import pdb;pdb.set_trace()</span>\n        cls_reg_targets = self.<span style=\'color: green;font-weight: bold;\'>get_targets</span>(cls_scores_list, bbox_preds_list,pts_preds_list,gt_bboxes_list, gt_labels_list,gt_shifts_pts_list,gt_bboxes_ignore_list)\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pts_targets_list, pts_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n        <span style=\'color: red\'># import pdb;pdb.set_trace()</span>\n        labels = torch.cat(labels_list, 0)\n        label_weights = torch.cat(label_weights_list, 0)\n        bbox_targets = torch.cat(bbox_targets_list, 0)\n        bbox_weights = torch.cat(bbox_weights_list, 0)\n        pts_targets = torch.cat(pts_targets_list, 0)\n        pts_weights = torch.cat(pts_weights_list, 0)\n        <span style=\'color: red\'># classification loss</span>\n        cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n        <span style=\'color: red\'># construct weighted avg_factor to match with the official DETR repo</span>\n        cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n        if self.sync_cls_avg_factor:\n            cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n        cls_avg_factor = max(cls_avg_factor, 1)\n        loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n        <span style=\'color: red\'># Compute the average number of gt boxes accross all gpus, for normalization purposes</span>\n        num_total_pos = loss_cls.new_tensor([num_total_pos])\n        num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n        <span style=\'color: red\'># import pdb;pdb.set_trace()</span>\n        <span style=\'color: red\'># regression L1 loss</span>\n        bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n        normalized_bbox_targets = normalize_2d_bbox(bbox_targets, self.pc_range)\n        <span style=\'color: red\'># normalized_bbox_targets = bbox_targets</span>\n        isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n        bbox_weights = bbox_weights * self.code_weights\n        loss_bbox = self.loss_bbox(\n            bbox_preds[isnotnan, :4], normalized_bbox_targets[isnotnan,:4], bbox_weights[isnotnan, :4],\n            avg_factor=num_total_pos)\n        <span style=\'color: red\'># num_samples, num_order, num_pts, num_coords</span>\n        normalized_pts_targets = normalize_2d_pts(pts_targets, self.pc_range)\n        <span style=\'color: red\'># num_samples, num_pts, num_coords</span>\n        pts_preds = pts_preds.reshape(-1, pts_preds.size(-2),pts_preds.size(-1))\n        if self.num_pts_per_vec != self.num_pts_per_gt_vec:\n            pts_preds = pts_preds.permute(0,2,1)\n            pts_preds = F.interpolate(pts_preds, size=(self.num_pts_per_gt_vec), mode=\'linear\', align_corners=True)\n            pts_preds = pts_preds.permute(0,2,1).contiguous()\n        loss_pts = self.loss_pts(pts_preds[isnotnan,:,:], normalized_pts_targets[isnotnan,:,:], pts_weights[isnotnan,:,:], avg_factor=num_total_pos)\n        dir_weights = pts_weights[:, :-self.dir_interval,0]\n        denormed_pts_preds = denormalize_2d_pts(pts_preds, self.pc_range)\n        denormed_pts_preds_dir = denormed_pts_preds[:,self.dir_interval:,:] - denormed_pts_preds[:,:-self.dir_interval,:]\n        pts_targets_dir = pts_targets[:, self.dir_interval:,:] - pts_targets[:,:-self.dir_interval,:]\n        <span style=\'color: red\'># dir_weights = pts_weights[:, indice,:-1,0]</span>\n        loss_dir = self.loss_dir(denormed_pts_preds_dir[isnotnan,:,:], pts_targets_dir[isnotnan,:,:],dir_weights[isnotnan,:],avg_factor=num_total_pos)\n        bboxes = denormalize_2d_bbox(bbox_preds, self.pc_range)\n        <span style=\'color: red\'># regression IoU loss, defaultly GIoU loss</span>\n        loss_iou = self.loss_iou(bboxes[isnotnan, :4], bbox_targets[isnotnan, :4], bbox_weights[isnotnan, :4], avg_factor=num_total_pos)\n        return loss_cls, loss_bbox, loss_iou, loss_pts, loss_dir\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">/sdb/zzhu/code_study/MapTR/projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRHead(DETRHead):\n    def get_targets(self, cls_scores_list, bbox_preds_list, pts_preds_list, gt_bboxes_list, gt_labels_list, gt_shifts_pts_list, gt_bboxes_ignore_list=None):\n        assert gt_bboxes_ignore_list is None, \'Only supports for gt_bboxes_ignore setting to None.\'\n        num_imgs = len(cls_scores_list)\n        gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n        (labels_list, label_weights_list, bbox_targets_list,bbox_weights_list, pts_targets_list, pts_weights_list,pos_inds_list, neg_inds_list) = multi_apply(\n            self.<span style=\'color: green;font-weight: bold;\'>_get_target_single</span>, cls_scores_list, bbox_preds_list,pts_preds_list,gt_labels_list, gt_bboxes_list, gt_shifts_pts_list, gt_bboxes_ignore_list)\n        num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n        num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n        return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pts_targets_list, pts_weights_list, num_total_pos, num_total_neg)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">/sdb/zzhu/code_study/MapTR/projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRHead(DETRHead):\n    def _get_target_single(self, cls_score, bbox_pred, pts_pred, gt_labels, gt_bboxes, gt_shifts_pts, gt_bboxes_ignore=None):\n        num_bboxes = bbox_pred.size(0)     <span style=\'color: red\'># 100</span>\n        <span style=\'color: red\'># assigner and sampler</span>\n        gt_c = gt_bboxes.shape[-1]         <span style=\'color: red\'># 4</span>\n        assign_result, order_index = self.assigner.<span style=\'color: green;font-weight: bold;\'>assign</span>(bbox_pred, cls_score, pts_pred,gt_bboxes, gt_labels, gt_shifts_pts, gt_bboxes_ignore)\n        sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n        pos_inds = sampling_result.pos_inds        <span style=\'color: red\'># torch.Size([7])->tensor([ 2,  5, 19, 20, 26, 55, 59], device=\'cuda:0\')</span>\n        neg_inds = sampling_result.neg_inds        <span style=\'color: red\'># torch.Size([93])-> </span>\n        <span style=\'color: red\'># label targets</span>\n        labels = gt_bboxes.new_full((num_bboxes,), self.num_classes,dtype=torch.long)\n        labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n        label_weights = gt_bboxes.new_ones(num_bboxes)\n        <span style=\'color: red\'># bbox targets</span>\n        bbox_targets = torch.zeros_like(bbox_pred)[..., :gt_c]\n        bbox_weights = torch.zeros_like(bbox_pred)\n        bbox_weights[pos_inds] = 1.0\n        <span style=\'color: red\'># pts targets</span>\n        <span style=\'color: red\'># pts_targets = torch.zeros_like(pts_pred)</span>\n        <span style=\'color: red\'># num_query, num_order, num_points, num_coords</span>\n        if order_index is None:\n            assigned_shift = gt_labels[sampling_result.pos_assigned_gt_inds]\n        else:\n            assigned_shift = order_index[sampling_result.pos_inds, sampling_result.pos_assigned_gt_inds]\n        pts_targets = pts_pred.new_zeros((pts_pred.size(0),pts_pred.size(1), pts_pred.size(2)))\n        pts_weights = torch.zeros_like(pts_targets)\n        pts_weights[pos_inds] = 1.0\n        <span style=\'color: red\'># DETR</span>\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n        pts_targets[pos_inds] = gt_shifts_pts[sampling_result.pos_assigned_gt_inds,assigned_shift,:,:]\n        return (labels, label_weights, bbox_targets, bbox_weights,pts_targets, pts_weights,pos_inds, neg_inds)\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 7, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/assigners/maptr_assigner.py</p><font size="0"><pre class="language-python"><code class="language-python">class MapTRAssigner(BaseAssigner):\n    def assign(self, bbox_pred, cls_pred, pts_pred, gt_bboxes, gt_labels, gt_pts, gt_bboxes_ignore=None, eps=1e-7):\n        num_gts, num_bboxes = gt_bboxes.size(0), bbox_pred.size(0)        <span style=\'color: red\'># 7,100</span>\n        <span style=\'color: red\'># 1. assign -1 by default</span>\n        assigned_gt_inds = bbox_pred.new_full((num_bboxes, ), -1, dtype=torch.long)  <span style=\'color: red\'># torch.Size([100])</span>\n        assigned_labels = bbox_pred.new_full((num_bboxes, ), -1, dtype=torch.long)   <span style=\'color: red\'># torch.Size([100])</span>\n        if num_gts == 0 or num_bboxes == 0:      <span style=\'color: red\'># No ground truth or boxes, return empty assignment</span>\n            if num_gts == 0:                     <span style=\'color: red\'># No ground truth, assign all to background</span>\n                assigned_gt_inds[:] = 0\n            return AssignResult(num_gts, assigned_gt_inds, None, labels=assigned_labels), None\n        <span style=\'color: red\'># 2. compute the weighted costs classification and bboxcost.</span>\n        cls_cost = self.cls_cost(cls_pred, gt_labels)        <span style=\'color: red\'># torch.Size([100, 3])  torch.Size([7])->tensor([0, 0, 0, 1, 2, 2, 2], device=\'cuda:0\')  torch.Size([100, 7])</span>\n        <span style=\'color: red\'># regression L1 cost</span>\n        normalized_gt_bboxes = normalize_2d_bbox(gt_bboxes, self.pc_range)    <span style=\'color: red\'># [-15.0, -30.0, -2.0, 15.0, 30.0, 2.0] -> torch.Size([7, 4])</span>\n        <span style=\'color: red\'># normalized_gt_bboxes = gt_bboxes</span>\n        <span style=\'color: red\'># import pdb;pdb.set_trace()</span>\n        reg_cost = self.reg_cost(bbox_pred[:, :4], normalized_gt_bboxes[:, :4])   <span style=\'color: red\'># torch.Size([100, 4]) torch.Size([7, 4]) -> torch.Size([100, 7])</span>\n        _, num_orders, num_pts_per_gtline, num_coords = gt_pts.shape              <span style=\'color: red\'># torch.Size([7, 19, 20, 2])</span>\n        normalized_gt_pts = normalize_2d_pts(gt_pts, self.pc_range)               <span style=\'color: red\'># torch.Size([7, 19, 20, 2])</span>\n        num_pts_per_predline = pts_pred.size(1)                                   <span style=\'color: red\'># 20</span>\n        if num_pts_per_predline != num_pts_per_gtline:                            <span style=\'color: red\'># False</span>\n            pts_pred_interpolated = F.interpolate(pts_pred.permute(0,2,1),size=(num_pts_per_gtline),mode=\'linear\', align_corners=True)  \n            pts_pred_interpolated = pts_pred_interpolated.permute(0,2,1).contiguous()\n        else:\n            pts_pred_interpolated = pts_pred\n        <span style=\'color: red\'># num_q, num_pts, 2 <-> num_gt, num_pts, 2</span>\n        pts_cost_ordered = self.pts_cost(pts_pred_interpolated, normalized_gt_pts)  <span style=\'color: red\'># torch.Size([100, 20, 2])+torch.Size([7, 19, 20, 2]) -> torch.Size([100, 133])其中133=19*7  7个实例</span>\n        pts_cost_ordered = pts_cost_ordered.view(num_bboxes, num_gts, num_orders)   <span style=\'color: red\'># torch.Size([100, 7, 19])</span>\n        pts_cost, order_index = torch.min(pts_cost_ordered, 2)                      <span style=\'color: red\'># torch.Size([100, 7]) torch.Size([100, 7])</span>\n        \n        bboxes = denormalize_2d_bbox(bbox_pred, self.pc_range)\n        iou_cost = self.iou_cost(bboxes, gt_bboxes)\n        <span style=\'color: red\'># weighted sum of above three costs</span>\n        cost = cls_cost + reg_cost + iou_cost + pts_cost\n        \n        <span style=\'color: red\'># 3. do Hungarian matching on CPU using linear_sum_assignment</span>\n        cost = cost.detach().cpu()\n        if linear_sum_assignment is None:\n            raise ImportError(\'Please run "pip install scipy" to install scipy first.\')\n        matched_row_inds, matched_col_inds = linear_sum_assignment(cost)           <span style=\'color: red\'># 7->array([ 2,  5, 19, 20, 26, 55, 59])  ; 7->array([6, 0, 3, 5, 2, 4, 1])</span>\n        matched_row_inds = torch.from_numpy(matched_row_inds).to(bbox_pred.device)\n        matched_col_inds = torch.from_numpy(matched_col_inds).to(bbox_pred.device)\n        <span style=\'color: red\'># 4. assign backgrounds and foregrounds</span>\n        <span style=\'color: red\'># assign all indices to backgrounds first</span>\n        assigned_gt_inds[:] = 0\n        <span style=\'color: red\'># assign foregrounds based on matching results</span>\n        assigned_gt_inds[matched_row_inds] = matched_col_inds + 1\n        assigned_labels[matched_row_inds] = gt_labels[matched_col_inds]\n        <span style=\'color: red\'># 7, torch.Size([100])-范围0->7, None, torch.Size([100])-范围-1-2  torch.Size([100, 7])</span>\n        return AssignResult(num_gts, assigned_gt_inds, None, labels=assigned_labels), order_index  \n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/assigners/maptr_assigner.py</p><font size="0"><pre class="language-python"><code class="language-python">def normalize_2d_bbox(bboxes, pc_range):  <span style=\'color: red\'># torch.Size([7, 4])</span>\n    patch_h = pc_range[4]-pc_range[1]     <span style=\'color: red\'># 60</span>\n    patch_w = pc_range[3]-pc_range[0]     <span style=\'color: red\'># 30</span>\n    cxcywh_bboxes = bbox_xyxy_to_cxcywh(bboxes)     <span style=\'color: red\'># torch.Size([7, 4])</span>\n    cxcywh_bboxes[...,0:1] = cxcywh_bboxes[..., 0:1] - pc_range[0]\n    cxcywh_bboxes[...,1:2] = cxcywh_bboxes[...,1:2] - pc_range[1]\n    factor = bboxes.new_tensor([patch_w, patch_h,patch_w,patch_h])\n    normalized_bboxes = cxcywh_bboxes / factor\n    return normalized_bboxes     <span style=\'color: red\'># 归一化到了0-1</span>\n</code></pre></font>'}]}]}]}]}]}]}]}]})</script></body>
</html>
