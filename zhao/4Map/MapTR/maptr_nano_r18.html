<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>maptr_nano_r18</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def __getitem__(self, idx):\n        idx=0               用来debug\n        if self.test_mode:\n            return self.prepare_test_data(idx)\n        while True:\n            data = self.`prepare_train_data`(idx)\n            if data is None:\n                idx = self._rand_another(idx)\n                continue\n            return data\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def prepare_train_data(self, index):\n        data_queue = []\n        prev_indexs_list = list(range(index-self.queue_length, index))   # [0-1,0]->[-1]\n        random.shuffle(prev_indexs_list)\n        prev_indexs_list = sorted(prev_indexs_list[1:], reverse=True)    # []\n        input_dict = self.`get_data_info`(index)\n        if input_dict is None:\n            return None\n        frame_idx = input_dict[&amp;#39;frame_idx&amp;#39;]\n        scene_token = input_dict[&amp;#39;scene_token&amp;#39;]         # &amp;#39;0d2cc345342a460e94ff54748338ac22&amp;#39;\n        self.`pre_pipeline`(input_dict)\n        example = self.`pipeline`(input_dict)           # dict_keys([&amp;#39;img_metas&amp;#39;, &amp;#39;gt_bboxes_3d&amp;#39;, &amp;#39;gt_labels_3d&amp;#39;, &amp;#39;img&amp;#39;])                 \n        example = self.`vectormap_pipeline`(example,input_dict)\n        if self.filter_empty_gt and (example is None or ~(example[&amp;#39;gt_labels_3d&amp;#39;]._data != -1).any()):\n            return None\n        data_queue.insert(0, example)\n        for i in prev_indexs_list:\n            i = max(0, i)\n            input_dict = self.get_data_info(i)\n            if input_dict is None:\n                return None\n            if input_dict[&amp;#39;frame_idx&amp;#39;] < frame_idx and input_dict[&amp;#39;scene_token&amp;#39;] == scene_token:\n                self.pre_pipeline(input_dict)\n                example = self.pipeline(input_dict)\n                example = self.vectormap_pipeline(example,input_dict)\n                if self.filter_empty_gt and (example is None or ~(example[&amp;#39;gt_labels_3d&amp;#39;]._data != -1).any()):\n                    return None\n                frame_idx = input_dict[&amp;#39;frame_idx&amp;#39;]\n            data_queue.insert(0, copy.deepcopy(example))\n        return self.`union2one`(data_queue)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def get_data_info(self, index):\n        # ......\n        if not self.test_mode:\n            annos = self.get_ann_info(index)         # {&amp;#39;gt_bboxes_3d&amp;#39;:torch.Size([44, 7]),&amp;#39;gt_labels_3d&amp;#39;:(44,),&amp;#39;gt_names&amp;#39;:(44,)}\n            input_dict[&amp;#39;ann_info&amp;#39;] = annos\n        rotation = Quaternion(input_dict[&amp;#39;ego2global_rotation&amp;#39;])\n        translation = input_dict[&amp;#39;ego2global_translation&amp;#39;]     # [2234.259847599768, 857.4061194452617, 0.0]\n        can_bus = input_dict[&amp;#39;can_bus&amp;#39;]              # (18,)\n        can_bus[:3] = translation\n        can_bus[3:7] = rotation\n        patch_angle = quaternion_yaw(rotation) / np.pi * 180\n        if patch_angle < 0:\n            patch_angle += 360\n        can_bus[-2] = patch_angle / 180 * np.pi\n        can_bus[-1] = patch_angle                    # 也没用到\n        lidar2ego = np.eye(4)\n        lidar2ego[:3,:3] = Quaternion(input_dict[&amp;#39;lidar2ego_rotation&amp;#39;]).rotation_matrix\n        lidar2ego[:3, 3] = input_dict[&amp;#39;lidar2ego_translation&amp;#39;]\n        ego2global = np.eye(4)\n        ego2global[:3,:3] = Quaternion(input_dict[&amp;#39;ego2global_rotation&amp;#39;]).rotation_matrix\n        ego2global[:3, 3] = input_dict[&amp;#39;ego2global_translation&amp;#39;]\n        lidar2global = ego2global @ lidar2ego          # (雷达->全局) == (雷达->自车) @ (自车->全局)\n        input_dict[&amp;#39;lidar2global&amp;#39;] = lidar2global\n        return input_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">input_dict</p><span class=\'hidden-code\' data-code=\'{\n    &amp;#39;sample_idx&amp;#39;:&amp;#39;f9878012c3f6412184c294c13ba4bac3&amp;#39;,\n    &amp;#39;pts_filename&amp;#39;:&amp;#39;./data/nuscenes/samples/LIDAR_TOP/n008-2018-05-21-11-06-59-0400__LIDAR_TOP__1526915243047392.pcd.bin&amp;#39;,\n    &amp;#39;lidar_path&amp;#39;:&amp;#39;./data/nuscenes/samples/LIDAR_TOP/n008-2018-05-21-11-06-59-0400__LIDAR_TOP__1526915243047392.pcd.bin&amp;#39;,\n    &amp;#39;sweeps&amp;#39;:[],\n    &amp;#39;ego2global_translation&amp;#39;:[2234.259847599768, 857.4061194452617, 0.0],\n    &amp;#39;ego2global_rotation&amp;#39;: [0.9997238940978229, 8.80899569529068e-05, 0.006940903896707341, 0.022448867747433807],\n    &amp;#39;lidar2ego_translation&amp;#39;:[0.891067, 0.0, 1.84292],\n    &amp;#39;lidar2ego_rotation&amp;#39;:[0.7043825600303035, 0.002529989518177017, -0.0013265948325242168, -0.7098147986794471],\n    &amp;#39;prev_idx&amp;#39;:&amp;#39;&amp;#39;,\n    &amp;#39;next_idx&amp;#39;:&amp;#39;dee909131941447b98da1f253c64c698&amp;#39;,\n    &amp;#39;scene_token&amp;#39;:&amp;#39;0d2cc345342a460e94ff54748338ac22&amp;#39;,\n    &amp;#39;can_bus&amp;#39;:[ 2.23425985e+03, 8.57406119e+02, 0.00000000e+00, 9.99723894e-01,\n                9.99723894e-01, 9.99723894e-01, 9.99723894e-01, 0.00000000e+00,\n                0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n                0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n                4.49059735e-02, 2.57292276e+00],\n    &amp;#39;frame_idx&amp;#39;:0,\n    &amp;#39;timestamp&amp;#39;:1526915243047392,\n    &amp;#39;map_location&amp;#39;:&amp;#39;boston-seaport&amp;#39;,\n    &amp;#39;lidar2ego&amp;#39;:[[-7.6776166e-03,  9.9995559e-01, -5.4605086e-03,  8.9106703e-01],\n                 [-9.9996907e-01, -7.6868986e-03, -1.6808877e-03,  0.0000000e+00],\n                 [-1.7227875e-03,  5.4474343e-03,  9.9998367e-01,  1.8429199e+00],\n                 [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00]],\n    &amp;#39;camera2ego&amp;#39;:&amp;#39;[array([[-0.0047123 ,...e=float32), array([[-8.1085944e-...e=float32), array([[ 8.0958527e-...e=float32), array([[ 1.7801417e-...e=float32), array([[ 0.94571775,...e=float32), array([[-9.3496680e-...e=float32)]&amp;#39;,\n    &amp;#39;camera_intrinsics&amp;#39;:&amp;#39;[array([[1.2628093e+0...e=float32), array([[1.2641254e+0...e=float32), array([[1.2564720e+0...e=float32), array([[798.12427,  ...e=float32), array([[1.2582339e+0...e=float32), array([[1.2594298e+0...e=float32)]&amp;#39;,\n    &amp;#39;lidar2global&amp;#39;:[[ 3.71896725e-02,  9.99272051e-01,  8.50266984e-03,2.23517551e+03],\n                    [-9.99306010e-01,  3.72061570e-02, -1.78879957e-03,8.57446366e+02],\n                    [-2.10384908e-03, -8.43024420e-03,  9.99962252e-01,1.83037972e+00],\n                    [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,1.00000000e+00]],\n    &amp;#39;img_filename&amp;#39;:[&amp;#39;./data/nuscenes/samp...012465.jpg&amp;#39;, &amp;#39;./data/nuscenes/samp...019956.jpg&amp;#39;, &amp;#39;./data/nuscenes/samp...004917.jpg&amp;#39;, &amp;#39;./data/nuscenes/samp...037570.jpg&amp;#39;, &amp;#39;./data/nuscenes/samp...047295.jpg&amp;#39;, &amp;#39;./data/nuscenes/samp...027813.jpg&amp;#39;],\n    &amp;#39;lidar2img&amp;#39;:&amp;#39;[array([[ 1.26035909e...000e+00]]), array([[ 1.36185706e...000e+00]]), array([[ 1.10470170e...000e+00]]), array([[-8.04936510e...000e+00]]), array([[-1.11865462e...000e+00]]), array([[ 2.68536904e...000e+00]])]&amp;#39;,\n    &amp;#39;cam_intrinsic&amp;#39;:&amp;#39;[array([[1.26280936e+...000e+00]]), array([[1.26412537e+...000e+00]]), array([[1.25647208e+...000e+00]]), array([[798.12426527...       ]]), array([[1.25823385e+...000e+00]]), array([[1.25942976e+...000e+00]])]&amp;#39;,\n    &amp;#39;lidar2cam&amp;#39;:&amp;#39;[array([[ 0.99996887,...       ]]), array([[ 5.91260254e...000e+00]]), array([[ 0.58042157,...       ]]), array([[-0.99994693,...       ]]), array([[-3.32162498e...000e+00]]), array([[-0.34739422,...       ]])]&amp;#39;,\n    &amp;#39;ann_info&amp;#39;:{\n        &amp;#39;gt_bboxes_3d&amp;#39;: &amp;#39;LiDARInstance3DBoxes...00e+00]]))&amp;#39;, \n        &amp;#39;gt_labels_3d&amp;#39;: &amp;#39;array([ 1,  8,  0,  ...,  0,  0])&amp;#39;, \n        &amp;#39;gt_names&amp;#39;: &amp;#39;array([&amp;#39;truck&amp;#39;, &amp;#39;ped&amp;#39;)&amp;#39;\n    },\n}\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def vectormap_pipeline(self, example, input_dict):\n        lidar2ego = np.eye(4)\n        lidar2ego[:3,:3] = Quaternion(input_dict[&amp;#39;lidar2ego_rotation&amp;#39;]).rotation_matrix\n        lidar2ego[:3, 3] = input_dict[&amp;#39;lidar2ego_translation&amp;#39;]\n        ego2global = np.eye(4)\n        ego2global[:3,:3] = Quaternion(input_dict[&amp;#39;ego2global_rotation&amp;#39;]).rotation_matrix\n        ego2global[:3, 3] = input_dict[&amp;#39;ego2global_translation&amp;#39;]\n        lidar2global = ego2global @ lidar2ego\n        lidar2global_translation = list(lidar2global[:3,3])\n        lidar2global_rotation = list(Quaternion(matrix=lidar2global).q)\n        location = input_dict[&amp;#39;map_location&amp;#39;]                                # &amp;#39;boston-seaport&amp;#39;; 用在下面第三行  波斯顿海港\n        ego2global_translation = input_dict[&amp;#39;ego2global_translation&amp;#39;]\n        ego2global_rotation = input_dict[&amp;#39;ego2global_rotation&amp;#39;]\n        anns_results = self.vector_map.`gen_vectorized_samples`(location, lidar2global_translation, lidar2global_rotation)   # anns_results.keys() = gt_vecs_pts_loc + gt_vecs_label\n        gt_vecs_label = to_tensor(anns_results[&amp;#39;gt_vecs_label&amp;#39;])             # tensor([0, 0, 0, 1, 2, 2, 2])\n        if isinstance(anns_results[&amp;#39;gt_vecs_pts_loc&amp;#39;], LiDARInstanceLines):\n            gt_vecs_pts_loc = anns_results[&amp;#39;gt_vecs_pts_loc&amp;#39;]                # `<`projects.mmdet3d_plugin.datasets.nuscenes_map_dataset.LiDARInstanceLines`>`\n        else:\n            gt_vecs_pts_loc = to_tensor(anns_results[&amp;#39;gt_vecs_pts_loc&amp;#39;])\n            try:\n                gt_vecs_pts_loc = gt_vecs_pts_loc.flatten(1).to(dtype=torch.float32)\n            except:\n                # empty tensor, will be passed in train, but we preserve it for test\n                gt_vecs_pts_loc = gt_vecs_pts_loc\n        example[&amp;#39;gt_labels_3d&amp;#39;] = DC(gt_vecs_label, cpu_only=False)\n        example[&amp;#39;gt_bboxes_3d&amp;#39;] = DC(gt_vecs_pts_loc, cpu_only=True)\n        return example\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def gen_vectorized_samples(self, location, lidar2global_translation, lidar2global_rotation):      # &amp;#39;boston-seaport&amp;#39;\n        map_pose = lidar2global_translation[:2]                                                       # [2235.175513920431, 857.4463660941551]\n        rotation = Quaternion(lidar2global_rotation)\n        patch_box = (map_pose[0], map_pose[1], self.patch_size[0], self.patch_size[1])                # (2235.175513920431, 857.4463660941551, 60.0, 30.0)\n        patch_angle = quaternion_yaw(rotation) / np.pi * 180                                          # -87.86869252174084\n        vectors = [] \n        for vec_class in self.vec_classes:                                                            # [&amp;#39;divider&amp;#39;, &amp;#39;ped_crossing&amp;#39;, &amp;#39;boundary&amp;#39;]\n            if vec_class == &amp;#39;divider&amp;#39;:\n                line_geom = self.`get_map_geom`(patch_box, patch_angle, self.line_classes, location)  # [(&amp;#39;road_divider&amp;#39;,[`<`shapely.geometry.linestring.LineString...]),(&amp;#39;lane_divider&amp;#39;,[`<`shapely.geometry.linestring.LineString...`>`,`<`shapely.geometry.linestring.LineString...`>`])]\n                line_instances_dict = self.`line_geoms_to_instances`(line_geom)     \n                for line_type, instances in line_instances_dict.items():\n                    for instance in instances:\n                        vectors.append((instance, self.CLASS2LABEL.get(line_type, -1)))\n            elif vec_class == &amp;#39;ped_crossing&amp;#39;:\n                ped_geom = self.get_map_geom(patch_box, patch_angle, self.ped_crossing_classes, location)\n                ped_instance_list = self.`ped_poly_geoms_to_instances`(ped_geom)\n                for instance in ped_instance_list:\n                    vectors.append((instance, self.CLASS2LABEL.get(&amp;#39;ped_crossing&amp;#39;, -1)))\n            elif vec_class == &amp;#39;boundary&amp;#39;:\n                polygon_geom = self.get_map_geom(patch_box, patch_angle, self.polygon_classes, location)\n                poly_bound_list = self.`poly_geoms_to_instances`(polygon_geom)\n                for contour in poly_bound_list:\n                    vectors.append((contour, self.CLASS2LABEL.get(&amp;#39;contours&amp;#39;, -1)))\n            else:\n                raise ValueError(f&amp;#39;WRONG vec_class: {vec_class}&amp;#39;)\n        # filtered_vectors = []\n        # gt_pts_loc_3d = []\n        # gt_pts_num_3d = []\n        gt_labels = []\n        gt_instance = []\n        for instance, type in vectors:\n            if type != -1: \n                gt_instance.append(instance)          # [shapely.geometry.linestring.LineString,...,]\n                gt_labels.append(type)                # [0, 0, 0, 1, 2, 2, 2]\n        gt_instance = LiDARInstanceLines(gt_instance,self.sample_dist, self.num_samples, self.padding, self.fixed_num,self.padding_value, patch_size=self.patch_size)\n        anns_results = dict(gt_vecs_pts_loc=gt_instance,gt_vecs_label=gt_labels,)\n        return anns_results\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def get_map_geom(self, patch_box, patch_angle, layer_names, location):                         # &amp;#39;boston-seaport&amp;#39;\n        map_geom = []\n        for layer_name in layer_names:\n            if layer_name in self.line_classes:\n                geoms = self.`get_divider_line`(patch_box, patch_angle, layer_name, location)\n                map_geom.append((layer_name, geoms))\n            elif layer_name in self.polygon_classes:\n                geoms = self.get_contour_line(patch_box, patch_angle, layer_name, location)\n                map_geom.append((layer_name, geoms))\n            elif layer_name in self.ped_crossing_classes:\n                geoms = self.get_ped_crossing_line(patch_box, patch_angle, location)\n                map_geom.append((layer_name, geoms))\n        return map_geom                 # [`<`shapely.geometry.linestring.LineString object at 0x7f9018f08fd0`>`,`<`shapely.geometry.linestring.LineString object at 0x7f9018f08430`>`]\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def get_divider_line(self,patch_box,patch_angle,layer_name,location):                 # &amp;#39;boston-seaport&amp;#39;\n        if layer_name is &amp;#39;traffic_light&amp;#39;:\n            return None\n        patch_x = patch_box[0]       # 2235.175513920431\n        patch_y = patch_box[1]       # 857.4463660941551\n        patch = self.map_explorer[location].get_patch_coord(patch_box, patch_angle)       # `<`shapely.geometry.polygon.Polygon object at 0x7f90f5f11f10`>`\n        line_list = []\n        records = getattr(self.map_explorer[location].map_api, layer_name)                # &amp;#39;road_divider&amp;#39;->[...,...,...] len=377\n        for record in records:                                                            \n            line = self.map_explorer[location].map_api.extract_line(record[&amp;#39;line_token&amp;#39;]) # `<`shapely.geometry.linestring.LineString object at 0x7f90191027f0`>`\n            if line.is_empty:        # Skip lines without nodes.\n                continue\n            new_line = line.intersection(patch)\n            if not new_line.is_empty:\n                new_line = affinity.rotate(new_line, -patch_angle, origin=(patch_x, patch_y), use_radians=False)\n                new_line = affinity.affine_transform(new_line,[1.0, 0.0, 0.0, 1.0, -patch_x, -patch_y])\n                line_list.append(new_line)\n        return line_list                   # `<`shapely.geometry.linestring.LineString object at 0x7f90f51910a0`>`\n\'> </span>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def line_geoms_to_instances(self, line_geom):\n        line_instances_dict = dict()\n        for line_type, a_type_of_lines in line_geom:\n            one_type_instances = self._one_type_line_geom_to_instances(a_type_of_lines)\n            line_instances_dict[line_type] = one_type_instances\n        return line_instances_dict\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class VectorizedLocalMap(object):\n    def union2one(self, queue):\n        imgs_list = [each[&amp;#39;img&amp;#39;].data for each in queue]      # [torch.Size([6, 3, 192, 320])]\n        metas_map = {}\n        prev_pos = None\n        prev_angle = None\n        for i, each in enumerate(queue):\n            metas_map[i] = each[&amp;#39;img_metas&amp;#39;].data\n            if i == 0:\n                metas_map[i][&amp;#39;prev_bev&amp;#39;] = False\n                prev_pos = copy.deepcopy(metas_map[i][&amp;#39;can_bus&amp;#39;][:3])\n                prev_angle = copy.deepcopy(metas_map[i][&amp;#39;can_bus&amp;#39;][-1])\n                metas_map[i][&amp;#39;can_bus&amp;#39;][:3] = 0\n                metas_map[i][&amp;#39;can_bus&amp;#39;][-1] = 0\n            else:\n                metas_map[i][&amp;#39;prev_bev&amp;#39;] = True\n                tmp_pos = copy.deepcopy(metas_map[i][&amp;#39;can_bus&amp;#39;][:3])\n                tmp_angle = copy.deepcopy(metas_map[i][&amp;#39;can_bus&amp;#39;][-1])\n                metas_map[i][&amp;#39;can_bus&amp;#39;][:3] -= prev_pos\n                metas_map[i][&amp;#39;can_bus&amp;#39;][-1] -= prev_angle\n                prev_pos = copy.deepcopy(tmp_pos)\n                prev_angle = copy.deepcopy(tmp_angle)\n        queue[-1][&amp;#39;img&amp;#39;] = DC(torch.stack(imgs_list),cpu_only=False, stack=True)\n        queue[-1][&amp;#39;img_metas&amp;#39;] = DC(metas_map, cpu_only=True)\n        queue = queue[-1]\n        return queue\n\'> </span>'}]}]}, {'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/detectors/maptr.py</p><span class=\'hidden-code\' data-code=\'class MapTR(MVXTwoStageDetector):\n    def forward_train(......):\n        lidar_feat = None\n        if self.modality == &amp;#39;fusion&amp;#39;:\n            lidar_feat = self.extract_lidar_feat(points)\n        \n        len_queue = img.size(1)\n        prev_img = img[:, :-1, ...]         tensor([], device=&amp;#39;cuda:0&amp;#39;, size=(2, 0, 6, 3, 192, 320))\n        img = img[:, -1, ...]               torch.Size([2, 6, 3, 192, 320])\n        prev_img_metas = copy.deepcopy(img_metas)\n        prev_bev = self.obtain_history_bev(prev_img, prev_img_metas)\n        import pdb;pdb.set_trace()\n        prev_bev = self.`obtain_history_bev`(prev_img, prev_img_metas) if len_queue>1 else None     多帧融合的时候，历史的bev信息\n        img_metas = [each[len_queue-1] for each in img_metas]                   [{},{}]\n        img_feats = self.`extract_feat`(img=img, img_metas=img_metas)\n        losses = dict()\n        losses_pts = self.`forward_pts_train`(img_feats, lidar_feat, gt_bboxes_3d,gt_labels_3d, img_metas,gt_bboxes_ignore, prev_bev)\n        losses.update(losses_pts)\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/detectors/maptr.py</p><span class=\'hidden-code\' data-code=\'class MapTR(MVXTwoStageDetector):\n    def extract_feat(self, img, img_metas=None, len_queue=None):\n        img_feats = self.`extract_img_feat`(img, img_metas, len_queue=len_queue)        # torch.Size([2, 6, 3, 192, 320])->torch.Size([2, 6, 256, 6, 10])  下采样了32倍\n        return img_feats\n\'> </span>'}, {'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/detectors/maptr.py</p><span class=\'hidden-code\' data-code=\'class MapTR(MVXTwoStageDetector):\n    def forward_pts_train(self,pts_feats,lidar_feat,gt_bboxes_3d,gt_labels_3d,img_metas,gt_bboxes_ignore=None,prev_bev=None):\n        outs = self.`pts_bbox_head`(pts_feats, lidar_feat, img_metas, prev_bev)\n        loss_inputs = [gt_bboxes_3d, gt_labels_3d, outs]\n        losses = self.pts_bbox_head.`loss`(*loss_inputs, img_metas=img_metas)\n        return losses\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><span class=\'hidden-code\' data-code=\'class MapTRHead(DETRHead):\n    @force_fp32(apply_to=(&amp;#39;mlvl_feats&amp;#39;, &amp;#39;prev_bev&amp;#39;))\n    def forward(self, mlvl_feats, lidar_feat, img_metas, prev_bev=None,  only_bev=False):\n        bs, num_cam, _, _, _ = mlvl_feats[0].shape\n        dtype = mlvl_feats[0].dtype\n        # import pdb;pdb.set_trace()\n        if self.query_embed_type == &amp;#39;all_pts&amp;#39;:\n            object_query_embeds = self.query_embedding.weight.to(dtype)\n        elif self.query_embed_type == &amp;#39;instance_pts&amp;#39;:                     # True                    100个实例，每个实例20个点，图像特征大小6x10，投到bev特征上大小80*40，对应真实范围   前后60米，左右30米 \n            pts_embeds = self.pts_embedding.weight.unsqueeze(0)           # self.pts_embedding = nn.Embedding(self.num_pts_per_vec, self.embed_dims * 2)   torch.Size([1, 20, 512])    \n            instance_embeds = self.instance_embedding.weight.unsqueeze(1) # self.instance_embedding = nn.Embedding(self.num_vec, self.embed_dims * 2)      torch.Size([100, 1, 512])\n            object_query_embeds = (pts_embeds + instance_embeds).flatten(0, 1).to(dtype)          # torch.Size([100, 20, 512]) -> torch.Size([2000, 512])        用于decode\n        if self.bev_embedding is not None:                                # Embedding(3200, 256)  self.bev_embedding = nn.Embedding(self.bev_h * self.bev_w, self.embed_dims)\n            bev_queries = self.bev_embedding.weight.to(dtype)             # torch.Size([3200, 256])                                                              用于encode\n            bev_mask = torch.zeros((bs, self.bev_h, self.bev_w),device=bev_queries.device).to(dtype)    # torch.Size([2, 80, 40])\n            bev_pos = self.positional_encoding(bev_mask).to(dtype)        # LearnedPositionalEncoding(num_feats=128, row_num_embed=80, col_num_embed=40) --》torch.Size([2, 256, 80, 40])\n        else:\n            bev_queries = None\n            bev_mask = None\n            bev_pos = None\n        if only_bev:  # only use encoder to obtain BEV features, TODO: refine the workaround  False\n            return self.transformer.get_bev_features(mlvl_feats,lidar_feat,bev_queries,self.bev_h,self.bev_w,\n                grid_length=(self.real_h / self.bev_h,self.real_w / self.bev_w),\n                bev_pos=bev_pos,img_metas=img_metas,prev_bev=prev_bev,)\n        else:\n            outputs = self.`transformer`(mlvl_feats,                                 # [torch.Size([2, 6, 256, 6, 10])]\n                lidar_feat, bev_queries, object_query_embeds,                        # None, torch.Size([3200, 256]),  torch.Size([2000, 512])\n                self.bev_h,self.bev_w,                                               # 80,40\n                grid_length=(self.real_h / self.bev_h,self.real_w / self.bev_w),     # (60/80,30/40)->((0.75, 0.75))   真是范围前后60米，左右30米\n                bev_pos=bev_pos,                                                     # torch.Size([2, 256, 80, 40])\n                reg_branches=self.reg_branches if self.with_box_refine else None,    # True   一系列Linear+ReLU\n                cls_branches=self.cls_branches if self.as_two_stage else None,       # False\n                img_metas=img_metas,\n                prev_bev=prev_bev                                                    # None\n        )\n        # encode得到的bev特征，输入到分类和回归分支的特征，最后一层的结果，每层的结果\n        bev_embed, hs, init_reference, inter_references = outputs    # torch.Size([3200, 2, 256]);  torch.Size([2, 2000, 2, 256]);  torch.Size([2, 2000, 2]);  torch.Size([2, 2, 2000, 2])\n        hs = hs.permute(0, 2, 1, 3)                                  # torch.Size([2, 2, 2000, 256])\n        outputs_classes = []\n        outputs_coords = []\n        outputs_pts_coords = []\n        for lvl in range(hs.shape[0]):\n            if lvl == 0:\n                reference = init_reference\n            else:\n                reference = inter_references[lvl - 1]\n            reference = inverse_sigmoid(reference)                  # torch.Size([2, 2000, 2])->torch.Size([2, 2000, 2])\n            outputs_class = self.cls_branches[lvl](hs[lvl].view(bs,self.num_vec, self.num_pts_per_vec,-1).mean(2))  # Linear+LayerNorm+ReLU->torch.Size([2, 100, 3])\n            tmp = self.reg_branches[lvl](hs[lvl])                   # torch.Size([2, 2000, 2])  这里再预测偏移？\n            # TODO: check the shape of reference                    \n            assert reference.shape[-1] == 2\n            tmp[..., 0:2] += reference[..., 0:2]\n            tmp = tmp.sigmoid() # cx,cy,w,h                        # torch.Size([2, 2000, 2])\n            outputs_coord, outputs_pts_coord = self.`transform_box`(tmp)     # 最外围box，但是没用到，用点对点的匹配\n            outputs_classes.append(outputs_class)\n            outputs_coords.append(outputs_coord)\n            outputs_pts_coords.append(outputs_pts_coord)\n        outputs_classes = torch.stack(outputs_classes)         # 每一层回归一次，增加收敛速度\n        outputs_coords = torch.stack(outputs_coords)\n        outputs_pts_coords = torch.stack(outputs_pts_coords)\n        outs = {\n            &amp;#39;bev_embed&amp;#39;: bev_embed,                  # torch.Size([3200, 2, 256])\n            &amp;#39;all_cls_scores&amp;#39;: outputs_classes,       # torch.Size([2, 2, 100, 3])\n            &amp;#39;all_bbox_preds&amp;#39;: outputs_coords,        # torch.Size([2, 2, 100, 4])\n            &amp;#39;all_pts_preds&amp;#39;: outputs_pts_coords,     # torch.Size([2, 2, 100, 20, 2])\n            &amp;#39;enc_cls_scores&amp;#39;: None,\n            &amp;#39;enc_bbox_preds&amp;#39;: None,\n            &amp;#39;enc_pts_preds&amp;#39;: None\n        }\n        return outs\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/transformer.py</p><span class=\'hidden-code\' data-code=\'class MapTRPerceptionTransformer(BaseModule):\n    def forward(self, mlvl_feats, lidar_feat, bev_queries, object_query_embed, bev_h, bev_w, grid_length=[0.512, 0.512], bev_pos=None, reg_branches=None, cls_branches=None, prev_bev=None, **kwargs):\n        bev_embed = self.`get_bev_features`(mlvl_feats,lidar_feat,bev_queries,bev_h,bev_w,grid_length=grid_length,bev_pos=bev_pos,prev_bev=prev_bev,**kwargs)  # bev_embed shape: bs, bev_h*bev_w, embed_dims\n        bs = mlvl_feats[0].size(0)\n        query_pos, query = torch.split(object_query_embed, self.embed_dims, dim=1)   # torch.Size([2000, 512])->torch.Size([2000, 256]) + torch.Size([2000, 256])\n        query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)                        # torch.Size([2, 2000, 256])        nn.Linear+sigmoid 用于生成x偏移？\n        query = query.unsqueeze(0).expand(bs, -1, -1)                                # torch.Size([2, 2000, 256])\n        reference_points = self.reference_points(query_pos)      # self.reference_points = nn.Linear(self.embed_dims, 2) 2是指(x,y)  torch.Size([2, 2000, 2])\n        reference_points = reference_points.sigmoid()            # 0->1\n        init_reference_out = reference_points\n        query = query.permute(1, 0, 2)                # torch.Size([2000, 2, 256])\n        query_pos = query_pos.permute(1, 0, 2)        # torch.Size([2000, 2, 256])\n        bev_embed = bev_embed.permute(1, 0, 2)        # torch.Size([3200, 2, 256])\n        inter_states, inter_references = self.`decoder`(        # torch.Size([2, 2000, 2, 256]); torch.Size([2, 2, 2000, 2])\n            query=query,                                        # Q查询特征\n            key=None,\n            value=bev_embed,                                    # encoding得到的bev特征\n            query_pos=query_pos,\n            reference_points=reference_points,\n            reg_branches=reg_branches,\n            cls_branches=cls_branches,\n            spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device),\n            level_start_index=torch.tensor([0], device=query.device),\n            **kwargs)\n        inter_references_out = inter_references\n        return bev_embed, inter_states, init_reference_out, inter_references_out   # torch.Size([3200, 2, 256]);  torch.Size([2, 2000, 2, 256]);  torch.Size([2, 2000, 2]);  torch.Size([2, 2, 2000, 2])\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/transformer.py</p><span class=\'hidden-code\' data-code=\'class MapTRPerceptionTransformer(BaseModule):\n    def get_bev_features(self, mlvl_feats, lidar_feat, bev_queries, bev_h, bev_w, grid_length=[0.512, 0.512], bev_pos=None, prev_bev=None, **kwargs):\n        if self.use_attn_bev:     # True\n            bev_embed = self.`attn_bev_encode`(mlvl_feats, bev_queries, bev_h, bev_w, grid_length=grid_length, bev_pos=bev_pos, prev_bev=prev_bev, **kwargs)  # torch.Size([2, 3200, 256]) 3200=h*w=80*40\n        else:\n            bev_embed = self.lss_bev_encode(mlvl_feats,\n                prev_bev=prev_bev,\n                **kwargs)\n        if lidar_feat is not None:      # None\n            bs = mlvl_feats[0].size(0)\n            bev_embed = bev_embed.view(bs, bev_h, bev_w, -1).permute(0,3,1,2).contiguous()\n            lidar_feat = lidar_feat.permute(0,1,3,2).contiguous() # B C H W\n            lidar_feat = nn.functional.interpolate(lidar_feat, size=(bev_h,bev_w), mode=&amp;#39;bicubic&amp;#39;, align_corners=False)\n            fused_bev = self.fuser([bev_embed, lidar_feat])\n            fused_bev = fused_bev.flatten(2).permute(0,2,1).contiguous()\n            bev_embed = fused_bev\n        return bev_embed\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/transformer.py</p><span class=\'hidden-code\' data-code=\'class MapTRPerceptionTransformer(BaseModule):\n    def attn_bev_encode(self,mlvl_feats,bev_queries,bev_h,bev_w,grid_length=[0.512, 0.512],bev_pos=None,prev_bev=None,**kwargs):\n        bs = mlvl_feats[0].size(0)\n        bev_queries = bev_queries.unsqueeze(1).repeat(1, bs, 1)     # torch.Size([3200, 256])->torch.Size([3200, 2, 256])   3200=bev_h*bev_w，bs=2\n        bev_pos = bev_pos.flatten(2).permute(2, 0, 1)               # torch.Size([3200, 2, 256])  LearnedPositionalEncoding\n        # obtain rotation angle and shift with ego motion\n        delta_x = np.array([each[&amp;#39;can_bus&amp;#39;][0] for each in kwargs[&amp;#39;img_metas&amp;#39;]])    # array([0., 0.])\n        delta_y = np.array([each[&amp;#39;can_bus&amp;#39;][1] for each in kwargs[&amp;#39;img_metas&amp;#39;]])    # array([0., 0.])\n        ego_angle = np.array([each[&amp;#39;can_bus&amp;#39;][-2] / np.pi * 180 for each in kwargs[&amp;#39;img_metas&amp;#39;]])      # array([2.57292276, 2.57292276])\n        grid_length_y = grid_length[0]                                              # 0.75,0.75\n        grid_length_x = grid_length[1]\n        translation_length = np.sqrt(delta_x ** 2 + delta_y ** 2)                   # array([0., 0.])\n        translation_angle = np.arctan2(delta_y, delta_x) / np.pi * 180              # array([0., 0.])\n        bev_angle = ego_angle - translation_angle                                   # array([2.57292276, 2.57292276])\n        shift_y = translation_length * np.cos(bev_angle / 180 * np.pi) / grid_length_y / bev_h  # array([0., 0.])\n        shift_x = translation_length * np.sin(bev_angle / 180 * np.pi) / grid_length_x / bev_w  # array([0., 0.])\n        shift_y = shift_y * self.use_shift                                          # array([0., 0.])\n        shift_x = shift_x * self.use_shift                                          # array([0., 0.])\n        shift = bev_queries.new_tensor([shift_x, shift_y]).permute(1, 0)  # xy, bs -> bs, xy     torch.Size([2, 2])\n        if prev_bev is not None:                                                    # None\n            if prev_bev.shape[1] == bev_h * bev_w:\n                prev_bev = prev_bev.permute(1, 0, 2)\n            if self.rotate_prev_bev:\n                for i in range(bs):\n                    # num_prev_bev = prev_bev.size(1)\n                    rotation_angle = kwargs[&amp;#39;img_metas&amp;#39;][i][&amp;#39;can_bus&amp;#39;][-1]\n                    tmp_prev_bev = prev_bev[:, i].reshape(bev_h, bev_w, -1).permute(2, 0, 1)\n                    tmp_prev_bev = rotate(tmp_prev_bev, rotation_angle,center=self.rotate_center)\n                    tmp_prev_bev = tmp_prev_bev.permute(1, 2, 0).reshape(bev_h * bev_w, 1, -1)\n                    prev_bev[:, i] = tmp_prev_bev[:, 0]\n        # add can bus signals\n        can_bus = bev_queries.new_tensor([each[&amp;#39;can_bus&amp;#39;] for each in kwargs[&amp;#39;img_metas&amp;#39;]])  # [:, :]  torch.Size([2, 18])\n        can_bus = self.can_bus_mlp(can_bus)[None, :, :]                          # torch.Size([1, 2, 256])\n        bev_queries = bev_queries + can_bus * self.use_can_bus                   # torch.Size([3200, 2, 256])+torch.Size([1, 2, 256])->torch.Size([3200, 2, 256])\n        feat_flatten = []\n        spatial_shapes = []\n        for lvl, feat in enumerate(mlvl_feats):\n            bs, num_cam, c, h, w = feat.shape                # torch.Size([2, 6, 256, 6, 10])\n            spatial_shape = (h, w)\n            feat = feat.flatten(3).permute(1, 0, 3, 2)       # torch.Size([6, 2, 60, 256])\n            if self.use_cams_embeds:\n                feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)      # # torch.Size([6, 2, 60, 256]) + torch.Size([6, 1, 1, 256]) -> # torch.Size([6, 2, 60, 256])\n            feat = feat + self.level_embeds[None, None, lvl:lvl + 1, :].to(feat.dtype)  # torch.Size([6, 2, 60, 256])\n            spatial_shapes.append(spatial_shape)             # (6, 10)\n            feat_flatten.append(feat)\n        feat_flatten = torch.cat(feat_flatten, 2)                                                      # torch.Size([6, 2, 60, 256])              图像特征\n        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=bev_pos.device)      # tensor([[ 6, 10]], device=&amp;#39;cuda:0&amp;#39;)      图像大小\n        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))   # tensor([0], device=&amp;#39;cuda:0&amp;#39;)\n        feat_flatten = feat_flatten.permute(0, 2, 1, 3)  # (num_cam, H*W, bs, embed_dims)    torch.Size([6, 60, 2, 256])\n        bev_embed = self.encoder(                       # BEVFormerEncoder\n            bev_queries,                                # Q  torch.Size([3200, 2, 256])\n            feat_flatten,                               # K\n            feat_flatten,                               # V\n            bev_h=bev_h,\n            bev_w=bev_w,\n            bev_pos=bev_pos,                            # torch.Size([3200, 2, 256])  LearnedPositionalEncoding\n            spatial_shapes=spatial_shapes,\n            level_start_index=level_start_index,\n            prev_bev=prev_bev,\n            shift=shift,\n            **kwargs\n        ) \n        return bev_embed                                # torch.Size([2, 3200, 256])\n\'> </span>'}]}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/modules/decoder.py</p><span class=\'hidden-code\' data-code=\'class MapTRDecoder(TransformerLayerSequence):\n    def forward(self, query, *args, reference_points=None, reg_branches=None, key_padding_mask=None, **kwargs):                # args包括query、key、value、query_pos\n        output = query                                                       # torch.Size([2000, 2, 256])  2000为100个实例，每个实例20个点\n        intermediate = []\n        intermediate_reference_points = []\n        for lid, layer in enumerate(self.layers):                            # len(self.layers)=2\n            reference_points_input = reference_points[..., :2].unsqueeze(2)  # BS NUM_QUERY NUM_LEVEL 2                          torch.Size([2, 2000, 1, 2])\n            output = layer(output,*args,reference_points=reference_points_input,key_padding_mask=key_padding_mask,**kwargs)    # DETR decoder->DetrTransformerDecoderLayer=torch.Size([2000, 2, 256])\n            output = output.permute(1, 0, 2)                                 # torch.Size([2, 2000, 256])\n            if reg_branches is not None:\n                tmp = reg_branches[lid](output)                             # 一系列 Linear+ReLU = torch.Size([2, 2000, 256])->torch.Size([2, 2000, 2])\n                assert reference_points.shape[-1] == 2\n                new_reference_points = torch.zeros_like(reference_points)\n                new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points[..., :2])       # tmp相当于瞄点加后面代码再次细化\n                # new_reference_points[..., 2:3] = tmp[..., 4:5] + inverse_sigmoid(reference_points[..., 2:3])\n                new_reference_points = new_reference_points.sigmoid()                                           # 更新\n                reference_points = new_reference_points.detach()\n            output = output.permute(1, 0, 2)                               # torch.Size([2000, 2, 256])\n            if self.return_intermediate:                                   # True\n                intermediate.append(output)                                # [torch.Size([2000, 2, 256]),torch.Size([2000, 2, 256])]\n                intermediate_reference_points.append(reference_points)     # [torch.Size([2, 2000, 2]),torch.Size([2, 2000, 2])]\n        if self.return_intermediate:\n            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n        return output, reference_points\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><span class=\'hidden-code\' data-code=\'class MapTRHead(DETRHead):\n    def transform_box(self, pts, y_first=False):        \n        pts_reshape = pts.view(pts.shape[0], self.num_vec,self.num_pts_per_vec,2)  # torch.Size([2, 2000, 2])->torch.Size([2, 100, 20, 2])\n        pts_y = pts_reshape[:, :, :, 0] if y_first else pts_reshape[:, :, :, 1]\n        pts_x = pts_reshape[:, :, :, 1] if y_first else pts_reshape[:, :, :, 0]\n        if self.transform_method == &amp;#39;minmax&amp;#39;:\n            xmin = pts_x.min(dim=2, keepdim=True)[0]\n            xmax = pts_x.max(dim=2, keepdim=True)[0]\n            ymin = pts_y.min(dim=2, keepdim=True)[0]\n            ymax = pts_y.max(dim=2, keepdim=True)[0]\n            bbox = torch.cat([xmin, ymin, xmax, ymax], dim=2)\n            bbox = bbox_xyxy_to_cxcywh(bbox)\n        else:\n            raise NotImplementedError\n        return bbox, pts_reshape             # torch.Size([2, 100, 4]), torch.Size([2, 100, 20, 2])\n\'> </span>'}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><span class=\'hidden-code\' data-code=\'class MapTRHead(DETRHead):\n    def loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_bboxes_ignore=None, img_metas=None):\n        gt_vecs_list = copy.deepcopy(gt_bboxes_list)\n        # import pdb;pdb.set_trace()\n        all_cls_scores = preds_dicts[&amp;#39;all_cls_scores&amp;#39;]\n        all_bbox_preds = preds_dicts[&amp;#39;all_bbox_preds&amp;#39;]\n        all_pts_preds  = preds_dicts[&amp;#39;all_pts_preds&amp;#39;]\n        enc_cls_scores = preds_dicts[&amp;#39;enc_cls_scores&amp;#39;]    # None没用到\n        enc_bbox_preds = preds_dicts[&amp;#39;enc_bbox_preds&amp;#39;]\n        enc_pts_preds  = preds_dicts[&amp;#39;enc_pts_preds&amp;#39;]\n        num_dec_layers = len(all_cls_scores)\n        device = gt_labels_list[0].device\n        gt_bboxes_list = [gt_bboxes.bbox.to(device) for gt_bboxes in gt_vecs_list]                   # [torch.Size([7, 4]),torch.Size([7, 4])]\n        gt_pts_list = [gt_bboxes.fixed_num_sampled_points.to(device) for gt_bboxes in gt_vecs_list]  # [torch.Size([7, 20, 2]),torch.Size([7, 20, 2])]  每个batch有7个实例，每个实例20个点，每个点x,y\n        if self.gt_shift_pts_pattern == &amp;#39;v0&amp;#39;:\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points.to(device) for gt_bboxes in gt_vecs_list]\n        elif self.gt_shift_pts_pattern == &amp;#39;v1&amp;#39;:\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points_v1.to(device) for gt_bboxes in gt_vecs_list]\n        elif self.gt_shift_pts_pattern == &amp;#39;v2&amp;#39;:           # 代码v2？\n            gt_shifts_pts_list = [gt_bboxes.`shift_fixed_num_sampled_points_v2`.to(device) for gt_bboxes in gt_vecs_list]  # torch.Size([7, 19, 20, 2])\n        elif self.gt_shift_pts_pattern == &amp;#39;v3&amp;#39;:           # 论文是v3\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points_v3.to(device) for gt_bboxes in gt_vecs_list]\n        elif self.gt_shift_pts_pattern == &amp;#39;v4&amp;#39;:\n            gt_shifts_pts_list = [gt_bboxes.shift_fixed_num_sampled_points_v4.to(device) for gt_bboxes in gt_vecs_list]\n        else:\n            raise NotImplementedError\n        all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]           # num_dec_layers=2\n        all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n        all_gt_pts_list = [gt_pts_list for _ in range(num_dec_layers)]\n        all_gt_shifts_pts_list = [gt_shifts_pts_list for _ in range(num_dec_layers)]\n        all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n        # import pdb;pdb.set_trace()\n        losses_cls, losses_bbox, losses_iou, losses_pts, losses_dir = multi_apply(self.`loss_single`, all_cls_scores, all_bbox_preds,all_pts_preds,\n            all_gt_bboxes_list, all_gt_labels_list,all_gt_shifts_pts_list,all_gt_bboxes_ignore_list)\n        loss_dict = dict()\n        # loss of proposal generated from encode feature map.\n        if enc_cls_scores is not None:                # None\n            binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n            # TODO bug here\n            enc_loss_cls, enc_losses_bbox, enc_losses_iou, enc_losses_pts, enc_losses_dir = \\\n                self.loss_single(enc_cls_scores, enc_bbox_preds, enc_pts_preds,gt_bboxes_list, binary_labels_list, gt_pts_list,gt_bboxes_ignore)\n            loss_dict[&amp;#39;enc_loss_cls&amp;#39;] = enc_loss_cls\n            loss_dict[&amp;#39;enc_loss_bbox&amp;#39;] = enc_losses_bbox\n            loss_dict[&amp;#39;enc_losses_iou&amp;#39;] = enc_losses_iou\n            loss_dict[&amp;#39;enc_losses_pts&amp;#39;] = enc_losses_pts\n            loss_dict[&amp;#39;enc_losses_dir&amp;#39;] = enc_losses_dir\n        # loss from the last decoder layer\n        loss_dict[&amp;#39;loss_cls&amp;#39;] = losses_cls[-1]\n        loss_dict[&amp;#39;loss_bbox&amp;#39;] = losses_bbox[-1]\n        loss_dict[&amp;#39;loss_iou&amp;#39;] = losses_iou[-1]\n        loss_dict[&amp;#39;loss_pts&amp;#39;] = losses_pts[-1]\n        loss_dict[&amp;#39;loss_dir&amp;#39;] = losses_dir[-1]\n        # loss from other decoder layers\n        num_dec_layer = 0\n        for loss_cls_i, loss_bbox_i, loss_iou_i, loss_pts_i, loss_dir_i in zip(losses_cls[:-1],losses_bbox[:-1],losses_iou[:-1],losses_pts[:-1],losses_dir[:-1]):\n            loss_dict[f&amp;#39;d{num_dec_layer}.loss_cls&amp;#39;] = loss_cls_i\n            loss_dict[f&amp;#39;d{num_dec_layer}.loss_bbox&amp;#39;] = loss_bbox_i\n            loss_dict[f&amp;#39;d{num_dec_layer}.loss_iou&amp;#39;] = loss_iou_i\n            loss_dict[f&amp;#39;d{num_dec_layer}.loss_pts&amp;#39;] = loss_pts_i\n            loss_dict[f&amp;#39;d{num_dec_layer}.loss_dir&amp;#39;] = loss_dir_i\n            num_dec_layer += 1\n        return loss_dict\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/datasets/nuscenes_map_dataset.py</p><span class=\'hidden-code\' data-code=\'class LiDARInstanceLines(object):\n    @property\n    def shift_fixed_num_sampled_points_v2(self):\n        &amp;#39;&amp;#39;&amp;#39;\n        return  [instances_num, num_shifts, fixed_num, 2]      # v3顺时针一次，逆时针一次\n        &amp;#39;&amp;#39;&amp;#39;\n        assert len(self.instance_list) != 0                               # [shapely.geometry.linestring.LineString,....]  len->7\n        instances_list = []\n        for instance in self.instance_list:\n            distances = np.linspace(0, instance.length, self.fixed_num)   # 20 -> (20,) 如果是13个点0-13之间划分20个\n            poly_pts = np.array(list(instance.coords))                    # (13, 2)      13个点\n            start_pts = poly_pts[0]                                       # (2,)         第一个点\n            end_pts = poly_pts[-1]                                        # (2,)         最后一个点\n            is_poly = np.equal(start_pts, end_pts)\n            is_poly = is_poly.all()                                       # 没有闭环，闭环多边形\n            shift_pts_list = []\n            pts_num, coords_num = poly_pts.shape                          # (13, 2)\n            shift_num = pts_num - 1                                       # 12\n            final_shift_num = self.fixed_num - 1                          # 20-1=19\n            if is_poly:                                                   # 多边形\n                pts_to_shift = poly_pts[:-1,:]                            # (12,2) 12个点        \n                for shift_right_i in range(shift_num):\n                    shift_pts = np.roll(pts_to_shift,shift_right_i,axis=0)              # 沿着给定轴滚动数组元素。超出最后位置的元素将会滚动到第一个位置  pts_to_shift[-shift_right:]+pts_to_shift[:-shift_right]\n                    pts_to_concat = shift_pts[0]                                        # (2,)\n                    pts_to_concat = np.expand_dims(pts_to_concat,axis=0)                # (1,2)\n                    shift_pts = np.concatenate((shift_pts,pts_to_concat),axis=0)        # 接到后面形成循环\n                    shift_instance = LineString(shift_pts)\n                    shift_sampled_points = np.array([list(shift_instance.interpolate(distance).coords) for distance in distances]).reshape(-1, 2)\n                    shift_pts_list.append(shift_sampled_points)\n                # import pdb;pdb.set_trace() \n            else:                                                         # 不是多边形，正反两次\n                sampled_points = np.array([list(instance.interpolate(distance).coords) for distance in distances]).reshape(-1, 2)  # (20, 2)\n                flip_sampled_points = np.flip(sampled_points, axis=0)     # (20, 2)\n                shift_pts_list.append(sampled_points)\n                shift_pts_list.append(flip_sampled_points)\n            \n            multi_shifts_pts = np.stack(shift_pts_list,axis=0)            # (2, 20, 2)\n            shifts_num,_,_ = multi_shifts_pts.shape\n            if shifts_num > final_shift_num:                              # False\n                index = np.random.choice(multi_shifts_pts.shape[0], final_shift_num, replace=False)   # 要是没有19个值，怎么采集些\n                multi_shifts_pts = multi_shifts_pts[index]\n            \n            multi_shifts_pts_tensor = to_tensor(multi_shifts_pts)         # torch.Size([2, 20, 2])\n            multi_shifts_pts_tensor = multi_shifts_pts_tensor.to(dtype=torch.float32)\n            \n            multi_shifts_pts_tensor[:,:,0] = torch.clamp(multi_shifts_pts_tensor[:,:,0], min=-self.max_x,max=self.max_x)  # -15-》15  -30-》30\n            multi_shifts_pts_tensor[:,:,1] = torch.clamp(multi_shifts_pts_tensor[:,:,1], min=-self.max_y,max=self.max_y)\n            # if not is_poly:\n            if multi_shifts_pts_tensor.shape[0] < final_shift_num:    # 2<19\n                padding = torch.full([final_shift_num-multi_shifts_pts_tensor.shape[0],self.fixed_num,2], self.padding_value)   # torch.Size([17, 20, 2])\n                multi_shifts_pts_tensor = torch.cat([multi_shifts_pts_tensor,padding],dim=0)                                    # torch.Size([19, 20, 2])\n            instances_list.append(multi_shifts_pts_tensor)\n        instances_tensor = torch.stack(instances_list, dim=0)\n        instances_tensor = instances_tensor.to(dtype=torch.float32) \n        return instances_tensor                                       # torch.Size([7, 19, 20, 2])  7个实列，每个实例最多重复19个顺序，每个顺序20个点\n\'> </span>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">/sdb/zzhu/code_study/MapTR/projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><span class=\'hidden-code\' data-code=\'class MapTRHead(DETRHead):\n    def loss_single(self, cls_scores, bbox_preds, pts_preds, gt_bboxes_list, gt_labels_list, gt_shifts_pts_list, gt_bboxes_ignore_list=None):\n        num_imgs = cls_scores.size(0)                                 # 2\n        cls_scores_list = [cls_scores[i] for i in range(num_imgs)]    # [torch.Size([100, 3]),torch.Size([100, 3])]\n        bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]    # [torch.Size([100, 4]),torch.Size([100, 4])]\n        pts_preds_list = [pts_preds[i] for i in range(num_imgs)]      # [torch.Size([100, 20, 2]),torch.Size([100, 20, 2])]\n        # import pdb;pdb.set_trace()\n        cls_reg_targets = self.`get_targets`(cls_scores_list, bbox_preds_list,pts_preds_list,gt_bboxes_list, gt_labels_list,gt_shifts_pts_list,gt_bboxes_ignore_list)\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pts_targets_list, pts_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n        # import pdb;pdb.set_trace()\n        labels = torch.cat(labels_list, 0)\n        label_weights = torch.cat(label_weights_list, 0)\n        bbox_targets = torch.cat(bbox_targets_list, 0)\n        bbox_weights = torch.cat(bbox_weights_list, 0)\n        pts_targets = torch.cat(pts_targets_list, 0)\n        pts_weights = torch.cat(pts_weights_list, 0)\n        # classification loss\n        cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n        # construct weighted avg_factor to match with the official DETR repo\n        cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n        if self.sync_cls_avg_factor:\n            cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n        cls_avg_factor = max(cls_avg_factor, 1)\n        loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n        # Compute the average number of gt boxes accross all gpus, for normalization purposes\n        num_total_pos = loss_cls.new_tensor([num_total_pos])\n        num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n        # import pdb;pdb.set_trace()\n        # regression L1 loss\n        bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n        normalized_bbox_targets = normalize_2d_bbox(bbox_targets, self.pc_range)\n        # normalized_bbox_targets = bbox_targets\n        isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n        bbox_weights = bbox_weights * self.code_weights\n        loss_bbox = self.loss_bbox(\n            bbox_preds[isnotnan, :4], normalized_bbox_targets[isnotnan,:4], bbox_weights[isnotnan, :4],\n            avg_factor=num_total_pos)\n        # num_samples, num_order, num_pts, num_coords\n        normalized_pts_targets = normalize_2d_pts(pts_targets, self.pc_range)\n        # num_samples, num_pts, num_coords\n        pts_preds = pts_preds.reshape(-1, pts_preds.size(-2),pts_preds.size(-1))\n        if self.num_pts_per_vec != self.num_pts_per_gt_vec:\n            pts_preds = pts_preds.permute(0,2,1)\n            pts_preds = F.interpolate(pts_preds, size=(self.num_pts_per_gt_vec), mode=&amp;#39;linear&amp;#39;, align_corners=True)\n            pts_preds = pts_preds.permute(0,2,1).contiguous()\n        loss_pts = self.loss_pts(pts_preds[isnotnan,:,:], normalized_pts_targets[isnotnan,:,:], pts_weights[isnotnan,:,:], avg_factor=num_total_pos)\n        dir_weights = pts_weights[:, :-self.dir_interval,0]\n        denormed_pts_preds = denormalize_2d_pts(pts_preds, self.pc_range)\n        denormed_pts_preds_dir = denormed_pts_preds[:,self.dir_interval:,:] - denormed_pts_preds[:,:-self.dir_interval,:]\n        pts_targets_dir = pts_targets[:, self.dir_interval:,:] - pts_targets[:,:-self.dir_interval,:]\n        # dir_weights = pts_weights[:, indice,:-1,0]\n        loss_dir = self.loss_dir(denormed_pts_preds_dir[isnotnan,:,:], pts_targets_dir[isnotnan,:,:],dir_weights[isnotnan,:],avg_factor=num_total_pos)\n        bboxes = denormalize_2d_bbox(bbox_preds, self.pc_range)\n        # regression IoU loss, defaultly GIoU loss\n        loss_iou = self.loss_iou(bboxes[isnotnan, :4], bbox_targets[isnotnan, :4], bbox_weights[isnotnan, :4], avg_factor=num_total_pos)\n        return loss_cls, loss_bbox, loss_iou, loss_pts, loss_dir\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">/sdb/zzhu/code_study/MapTR/projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><span class=\'hidden-code\' data-code=\'class MapTRHead(DETRHead):\n    def get_targets(self, cls_scores_list, bbox_preds_list, pts_preds_list, gt_bboxes_list, gt_labels_list, gt_shifts_pts_list, gt_bboxes_ignore_list=None):\n        assert gt_bboxes_ignore_list is None, &amp;#39;Only supports for gt_bboxes_ignore setting to None.&amp;#39;\n        num_imgs = len(cls_scores_list)\n        gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n        (labels_list, label_weights_list, bbox_targets_list,bbox_weights_list, pts_targets_list, pts_weights_list,pos_inds_list, neg_inds_list) = multi_apply(\n            self.`_get_target_single`, cls_scores_list, bbox_preds_list,pts_preds_list,gt_labels_list, gt_bboxes_list, gt_shifts_pts_list, gt_bboxes_ignore_list)\n        num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n        num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n        return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pts_targets_list, pts_weights_list, num_total_pos, num_total_neg)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">/sdb/zzhu/code_study/MapTR/projects/mmdet3d_plugin/maptr/dense_heads/maptr_head.py</p><span class=\'hidden-code\' data-code=\'class MapTRHead(DETRHead):\n    def _get_target_single(self, cls_score, bbox_pred, pts_pred, gt_labels, gt_bboxes, gt_shifts_pts, gt_bboxes_ignore=None):\n        num_bboxes = bbox_pred.size(0)     # 100\n        # assigner and sampler\n        gt_c = gt_bboxes.shape[-1]         # 4\n        assign_result, order_index = self.assigner.`assign`(bbox_pred, cls_score, pts_pred,gt_bboxes, gt_labels, gt_shifts_pts, gt_bboxes_ignore)\n        sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n        pos_inds = sampling_result.pos_inds        # torch.Size([7])->tensor([ 2,  5, 19, 20, 26, 55, 59], device=&amp;#39;cuda:0&amp;#39;)\n        neg_inds = sampling_result.neg_inds        # torch.Size([93])-> \n        # label targets\n        labels = gt_bboxes.new_full((num_bboxes,), self.num_classes,dtype=torch.long)\n        labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n        label_weights = gt_bboxes.new_ones(num_bboxes)\n        # bbox targets\n        bbox_targets = torch.zeros_like(bbox_pred)[..., :gt_c]\n        bbox_weights = torch.zeros_like(bbox_pred)\n        bbox_weights[pos_inds] = 1.0\n        # pts targets\n        # pts_targets = torch.zeros_like(pts_pred)\n        # num_query, num_order, num_points, num_coords\n        if order_index is None:\n            assigned_shift = gt_labels[sampling_result.pos_assigned_gt_inds]\n        else:\n            assigned_shift = order_index[sampling_result.pos_inds, sampling_result.pos_assigned_gt_inds]\n        pts_targets = pts_pred.new_zeros((pts_pred.size(0),pts_pred.size(1), pts_pred.size(2)))\n        pts_weights = torch.zeros_like(pts_targets)\n        pts_weights[pos_inds] = 1.0\n        # DETR\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n        pts_targets[pos_inds] = gt_shifts_pts[sampling_result.pos_assigned_gt_inds,assigned_shift,:,:]\n        return (labels, label_weights, bbox_targets, bbox_weights,pts_targets, pts_weights,pos_inds, neg_inds)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 7, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/assigners/maptr_assigner.py</p><span class=\'hidden-code\' data-code=\'class MapTRAssigner(BaseAssigner):\n    def assign(self, bbox_pred, cls_pred, pts_pred, gt_bboxes, gt_labels, gt_pts, gt_bboxes_ignore=None, eps=1e-7):\n        num_gts, num_bboxes = gt_bboxes.size(0), bbox_pred.size(0)        # 7,100\n        # 1. assign -1 by default\n        assigned_gt_inds = bbox_pred.new_full((num_bboxes, ), -1, dtype=torch.long)  # torch.Size([100])\n        assigned_labels = bbox_pred.new_full((num_bboxes, ), -1, dtype=torch.long)   # torch.Size([100])\n        if num_gts == 0 or num_bboxes == 0:      # No ground truth or boxes, return empty assignment\n            if num_gts == 0:                     # No ground truth, assign all to background\n                assigned_gt_inds[:] = 0\n            return AssignResult(num_gts, assigned_gt_inds, None, labels=assigned_labels), None\n        # 2. compute the weighted costs classification and bboxcost.\n        cls_cost = self.cls_cost(cls_pred, gt_labels)        # torch.Size([100, 3])  torch.Size([7])->tensor([0, 0, 0, 1, 2, 2, 2], device=&amp;#39;cuda:0&amp;#39;)  torch.Size([100, 7])\n        # regression L1 cost\n        normalized_gt_bboxes = normalize_2d_bbox(gt_bboxes, self.pc_range)    # [-15.0, -30.0, -2.0, 15.0, 30.0, 2.0] -> torch.Size([7, 4])\n        # normalized_gt_bboxes = gt_bboxes\n        # import pdb;pdb.set_trace()\n        reg_cost = self.reg_cost(bbox_pred[:, :4], normalized_gt_bboxes[:, :4])   # torch.Size([100, 4]) torch.Size([7, 4]) -> torch.Size([100, 7])\n        _, num_orders, num_pts_per_gtline, num_coords = gt_pts.shape              # torch.Size([7, 19, 20, 2])\n        normalized_gt_pts = normalize_2d_pts(gt_pts, self.pc_range)               # torch.Size([7, 19, 20, 2])\n        num_pts_per_predline = pts_pred.size(1)                                   # 20\n        if num_pts_per_predline != num_pts_per_gtline:                            # False\n            pts_pred_interpolated = F.interpolate(pts_pred.permute(0,2,1),size=(num_pts_per_gtline),mode=&amp;#39;linear&amp;#39;, align_corners=True)  \n            pts_pred_interpolated = pts_pred_interpolated.permute(0,2,1).contiguous()\n        else:\n            pts_pred_interpolated = pts_pred\n        # num_q, num_pts, 2 `<`-`>` num_gt, num_pts, 2\n        pts_cost_ordered = self.pts_cost(pts_pred_interpolated, normalized_gt_pts)  # torch.Size([100, 20, 2])+torch.Size([7, 19, 20, 2]) -> torch.Size([100, 133])其中133=19*7  7个实例\n        pts_cost_ordered = pts_cost_ordered.view(num_bboxes, num_gts, num_orders)   # torch.Size([100, 7, 19])\n        pts_cost, order_index = torch.min(pts_cost_ordered, 2)                      # torch.Size([100, 7]) torch.Size([100, 7])\n        \n        bboxes = denormalize_2d_bbox(bbox_pred, self.pc_range)\n        iou_cost = self.iou_cost(bboxes, gt_bboxes)\n        # weighted sum of above three costs\n        cost = cls_cost + reg_cost + iou_cost + pts_cost\n        \n        # 3. do Hungarian matching on CPU using linear_sum_assignment\n        cost = cost.detach().cpu()\n        if linear_sum_assignment is None:\n            raise ImportError(&amp;#39;Please run &amp;#39;pip install scipy&amp;#39; to install scipy first.&amp;#39;)\n        matched_row_inds, matched_col_inds = linear_sum_assignment(cost)           # 7->array([ 2,  5, 19, 20, 26, 55, 59])  ; 7->array([6, 0, 3, 5, 2, 4, 1])\n        matched_row_inds = torch.from_numpy(matched_row_inds).to(bbox_pred.device)\n        matched_col_inds = torch.from_numpy(matched_col_inds).to(bbox_pred.device)\n        # 4. assign backgrounds and foregrounds\n        # assign all indices to backgrounds first\n        assigned_gt_inds[:] = 0\n        # assign foregrounds based on matching results\n        assigned_gt_inds[matched_row_inds] = matched_col_inds + 1\n        assigned_labels[matched_row_inds] = gt_labels[matched_col_inds]\n        # 7, torch.Size([100])-范围0->7, None, torch.Size([100])-范围-1-2  torch.Size([100, 7])\n        return AssignResult(num_gts, assigned_gt_inds, None, labels=assigned_labels), order_index  \n\'> </span>', 'children': [{'type': 'heading', 'depth': 8, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/maptr/assigners/maptr_assigner.py</p><span class=\'hidden-code\' data-code=\'def normalize_2d_bbox(bboxes, pc_range):  # torch.Size([7, 4])\n    patch_h = pc_range[4]-pc_range[1]     # 60\n    patch_w = pc_range[3]-pc_range[0]     # 30\n    cxcywh_bboxes = bbox_xyxy_to_cxcywh(bboxes)     # torch.Size([7, 4])\n    cxcywh_bboxes[...,0:1] = cxcywh_bboxes[..., 0:1] - pc_range[0]\n    cxcywh_bboxes[...,1:2] = cxcywh_bboxes[...,1:2] - pc_range[1]\n    factor = bboxes.new_tensor([patch_w, patch_h,patch_w,patch_h])\n    normalized_bboxes = cxcywh_bboxes / factor\n    return normalized_bboxes     # 归一化到了0-1\n\'> </span>'}]}]}]}]}]}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
