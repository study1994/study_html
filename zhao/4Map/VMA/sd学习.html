<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>sd学习</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.hidden-code {
  display: none !important;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/mycss/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/myjs/d3@6.7.0.js"></script>
    <script src="https://study1994.github.io/study_html/npm/myjs/markmap-view@0.13.5.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/custom_infer.py推理vma_res152_e80_line.py</p><span class=\'hidden-code\' data-code=\'def main():\n    args = get_args()\n    cfg = Config.fromfile(args.config)                          &amp;#39;projects/configs/vma_res152_e80_line.py&amp;#39;\n    if hasattr(cfg, &amp;#39;plugin&amp;#39;):\n        if cfg.plugin:                                          True\n            import importlib\n            if hasattr(cfg, &amp;#39;plugin_dir&amp;#39;):\n                plugin_dir = cfg.plugin_dir                     &amp;#39;projects/mmdet3d_plugin/&amp;#39;\n                _module_dir = dirname(plugin_dir)               [&amp;#39;projects&amp;#39;, &amp;#39;mmdet3d_plugin&amp;#39;]\n                _module_dir = _module_dir.split(&amp;#39;/&amp;#39;)            [&amp;#39;projects&amp;#39;, &amp;#39;mmdet3d_plugin&amp;#39;]\n                _module_path = _module_dir[0]                   &amp;#39;projects&amp;#39;\n                for m in _module_dir[1:]:\n                    _module_path = _module_path + &amp;#39;.&amp;#39; + m       &amp;#39;projects.mmdet3d_plugin&amp;#39;\n                print(_module_path)\n                plg_lib = importlib.import_module(_module_path)\n            else:\n                ......\n    data_list = os.listdir(args.root_path)                      &amp;#39;data/sd_data/line/origin_data/image_data&amp;#39;-->[.jpg,,...]  5张\n    data_list.sort()    \n    infer_combine_function = infer_combine_func[args.element_type]\n    `infer_combine_function`(args, data_list)                   demo/inference_and_combine_function.py\n    print(&amp;#39;completed&amp;#39;)                  \n\'> </span>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><span class=\'hidden-code\' data-code=\'def driving_line_infer_and_save(args, data_list):                       # [&amp;#39;sample_1.jpg&amp;#39;, &amp;#39;sample_2.jpg&amp;#39;, &amp;#39;sample_3.jpg&amp;#39;, &amp;#39;sample_4.jpg&amp;#39;, &amp;#39;sample_5.jpg&amp;#39;]\n    config = args.config                                                # &amp;#39;projects/configs/vma_res152_e80_line.py&amp;#39;\n    checkpoint = args.checkpoint                                        # &amp;#39;ckpts/sd_line.pth&amp;#39;\n    model = init_model(config, checkpoint, device=args.device)\n    print(&amp;#39;driving line model initiated&amp;#39;)\n    prog_bar = mmcv.ProgressBar(len(data_list))\n    for idx, image_name in enumerate(data_list): \n        upload_img_path = os.path.join(args.root_path, image_name)                                            # &amp;#39;data/sd_data/line/origin_data/image_data/sample_1.jpg&amp;#39;\n        trajectory_path = upload_img_path.replace(&amp;#39;image_data&amp;#39;, &amp;#39;trajectory_data&amp;#39;).replace(&amp;#39;jpg&amp;#39;, &amp;#39;json&amp;#39;)     # &amp;#39;data/sd_data/line/origin_data/trajectory_data/sample_1.json&amp;#39;\n        results_list, left_top = `inference_detector_forcurb_bigimg`(model, get_sub_data_driving, upload_img_path, trajectory_path, args.trajectory_sample_num, get_traj_pad=True, pad=False, attr=True)\n        if not (len(results_list)):\n            print(&amp;#39;no data&amp;#39;)\n            continue\n        `combine_results_and_save_driving_line`(model.cfg, upload_img_path, results_list, out_dir=args.out_dir, left_top=left_top, visualize_flag=args.visualize)\n        prog_bar.update()\n    print(&amp;#39;driving line infer and combine completed&amp;#39;)\n\'> </span>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><span class=\'hidden-code\' data-code=\'def inference_detector_forcurb_bigimg(model, get_sub_data_func, imgs, trajectory_path, sample_num, get_traj_pad=False, pad=False, attr=False):\n    device = next(model.parameters()).device      # model device   device(type=&amp;#39;cuda&amp;#39;, index=0)\n    if pad:                                       # False\n        test_pipeline = [LoadBigImagePad()]\n    else:\n        test_pipeline = [LoadBigImage()]          # build the data pipeline\n    test_pipeline = Compose(test_pipeline)\n    # prepare data\n    data = dict(img=imgs)\n    data = test_pipeline(data)\n    # add get_sub_img func\n    trajectory_data_list = mmcv.load(trajectory_path)        # &amp;#39;data/sd_data/line/origin_data/trajectory_data/sample_1.json&amp;#39; --> len(trajectory_data_list[0])=126\n    sub_datas, left_top = `get_sub_data_func`(data, trajectory_data_list, sample_num)  # demo/inference_and_combine_function.py --> def get_sub_data_driving\n    if get_traj_pad:                                         # True           \n        from utils import TrajPadSampler\n        pad_sampler =  `TrajPadSampler`(imgs, left_top)      # demo/utils.py:\n        more_sub_datas, more_left_top = pad_sampler.`sample`(data)\n        sub_datas += more_sub_datas\n        left_top += more_left_top\n    if not len(sub_datas):\n        return [], []\n    results = []\n    for sub_data in sub_datas:                               # {&amp;#39;img_metas&amp;#39;:{&amp;#39;img_shape&amp;#39;: (1000, 1000), &amp;#39;batch_input_shape&amp;#39;: (1000, 1000)},&amp;#39;img&amp;#39;:torch.Size([3, 1000, 1000])}\n        data = collate([sub_data], samples_per_gpu=1)\n        if next(model.parameters()).is_cuda:\n            data = scatter(data, [device])[0]                # scatter to specified GPU\n        else:\n            data[&amp;#39;img_metas&amp;#39;] = [i.data[0] for i in data[&amp;#39;img_metas&amp;#39;]]\n        with torch.no_grad():\n            result = `model`(return_loss=False, rescale=True, **data)      # 每张 torch.Size([1, 3, 1000, 1000]) 直接从head那里看\n            results.append(result)\n    print(&amp;#39;Start to convert detection format...&amp;#39;)\n    results_list = []\n    if not attr:                         # False\n        ......\n    else: \n        for batch_result in results:                  # [[{&amp;#39;pts_bbox&amp;#39;: {...}}],[{&amp;#39;pts_bbox&amp;#39;: {...}}],....]\n            for result in batch_result:\n                result = result[&amp;#39;pts_bbox&amp;#39;]\n                pred_instances = []\n                pred_scores = result[&amp;#39;scores_3d&amp;#39;].numpy()              # (12,)\n                pred_data = result[&amp;#39;pts_3d&amp;#39;].numpy()                   # (12, 50, 2)\n                pred_labels = result[&amp;#39;labels_3d&amp;#39;].numpy()              # (12,)\n                pred_attrs_labels = torch.stack(result[&amp;#39;attrs_3d&amp;#39;][&amp;#39;attrs_preds&amp;#39;], 1)      # torch.Size([12, 6])\n                pred_attrs_scores = torch.cat(result[&amp;#39;attrs_3d&amp;#39;][&amp;#39;attrs_scores&amp;#39;], 1)       # torch.Size([12, 28])\n                for idx in range(len(pred_scores)):                    # 12\n                    pred_instances.append({&amp;#39;class&amp;#39;:pred_labels[idx], &amp;#39;data&amp;#39;:pred_data[idx], &amp;#39;attrs_labels&amp;#39;:pred_attrs_labels[idx],\n                                        &amp;#39;attrs_scores&amp;#39;:pred_attrs_scores[idx], &amp;#39;confidence_level&amp;#39;:pred_scores[idx]})\n            results_list.append(pred_instances)\n    print(&amp;#39;convert completed&amp;#39;)\n    return results_list, left_top\n\'> </span>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><span class=\'hidden-code\' data-code=\'def get_sub_data_driving(data_dict, trajectory_data_list, sample_num):\n    sub_datas = []\n    img = data_dict[&amp;#39;img&amp;#39;]                             # torch.Size([3, 6000, 6000])\n    img_name = data_dict[&amp;#39;filename&amp;#39;]                   # &amp;#39;data/sd_data/line/origin_data/image_data/sample_1.jpg&amp;#39;\n    sub_imgs, left_top = `get_sub_img_driving`(img, trajectory_data_list, sample_num)   # demo/inference_and_combine_function.py --> def get_sub_img_driving\n    for i in range(len(sub_imgs)):\n        sub_data_dict = dict()\n        sub_data_dict[&amp;#39;img_metas&amp;#39;] = data_dict[&amp;#39;img_metas&amp;#39;]                                          # {}\n        sub_data_dict[&amp;#39;img_metas&amp;#39;][0][&amp;#39;img_shape&amp;#39;] = (sub_imgs[i].shape[1], sub_imgs[i].shape[2])    # {&amp;#39;img_shape&amp;#39;: (1000, 1000)}\n        sub_data_dict[&amp;#39;img&amp;#39;] = sub_imgs[i]\n        sub_datas.append(sub_data_dict)\n    return sub_datas, left_top\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><span class=\'hidden-code\' data-code=\'def get_sub_img_driving(img, trajectory_data_list, sample_num):         # 15\n    sub_img_list = []\n    left_top_list = []\n    for i, trajectory_data in enumerate(trajectory_data_list):          # 对每条轨迹采样sample_num个点作为中心点在大图上进行裁切\n        if len(trajectory_data) < 2:\n            print(&amp;#39;trajectory {} is not long enough&amp;#39;.format(i))\n            continue\n        trajectory_shapely = LineString(np.array(trajectory_data))\n        # trajectory_length = trajectory_shapely.length\n        # sample_num = round((trajectory_length // 1000)*2 + 1)\n        distances = np.linspace(0, trajectory_shapely.length, sample_num)\n        sampled_points = np.array([list(trajectory_shapely.interpolate(distance).coords) for distance in distances]).reshape(-1, 2)  # shape:(15, 2)\n        for sampled_point in sampled_points:                            # 拆分成1000x1000的大小\n            x, y = sampled_point\n            if x < 500:\n                x = 500\n            elif x > 5500:\n                x = 5500\n            if y < 500:\n                y = 500\n            elif y > 5500:\n                y = 5500\n            bbox_minx, bbox_miny, bbox_maxx, bbox_maxy = round(x - 500), round(y - 500), round(x + 499), round(y + 499)\n            sub_img_list.append(img[:, bbox_miny:bbox_maxy+1, bbox_minx:bbox_maxx+1])  # (left, upper, right, lower)\n            left_top_list.append([bbox_minx, bbox_miny])\n    return sub_img_list, left_top_list                 # 切割出来的500x500以及，对应的左上角(x1,y1)-->len=15  torch.Size([3, 1000, 1000])\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/utils.py</p><span class=\'hidden-code\' data-code=\'class TrajPadSampler(object):\n    SIZE = 1000\n    def __init__(self, imgpath, left_top):\n        self.imgpath = imgpath                            # &amp;#39;data/sd_data/line/origin_data/image_data/sample_1.jpg&amp;#39;\n        self.left_top = left_top                          # [[2415, 0], [2438, 0], ....]\n        img = cv2.imread(self.imgpath)\n        self.img = img\n        canvas = np.zeros_like(img)\n        for lt in self.left_top:\n            cv2.rectangle(canvas ,lt, [lt[0]+self.SIZE, lt[1]+self.SIZE], (1, 1, 1), -1)  # 相当于被采集区域的mask\n        self.canvas = canvas\n    def sample(self, data_dict):\n        img = data_dict[&amp;#39;img&amp;#39;]                                # torch.Size([3, 6000, 6000])\n        infer_subimg_resolution = 1000                        # 采样轨迹之外的区域\n        bigimgsize=6000\n        subimg_interval = infer_subimg_resolution // 2        # 500\n        sub_img_list,left_top_list,sampled_points = [],[],[]\n        for offsetx in range(subimg_interval, bigimgsize+1-subimg_interval, subimg_interval):        # （500,6000-500+1,500)\n            for offsety in range(subimg_interval, bigimgsize+1-subimg_interval, subimg_interval):    # （500,6000-500+1,500)\n                sampled_points.append([offsetx, offsety])     # 121 11x11个点                 \n        for point in sampled_points:\n            x, y = point                                      # 点限制在500->6000-500\n            if x < subimg_interval:\n                x = subimg_interval\n            elif x > (bigimgsize - subimg_interval):\n                x = (bigimgsize - subimg_interval)\n            if y < subimg_interval:\n                y = subimg_interval\n            elif y > (bigimgsize - subimg_interval):\n                y = (bigimgsize - subimg_interval)\n            bbox_minx, bbox_miny, bbox_maxx, bbox_maxy = round(x - subimg_interval), round(y - subimg_interval), round(x + subimg_interval - 1), round(y + subimg_interval - 1)\n            subimg = img[:, bbox_miny: bbox_maxy+1, bbox_minx: bbox_maxx+1]     # torch.Size([3, 1000, 1000])\n            img_ratio = self.`get_img_mask_raio`([bbox_minx, bbox_miny])          # 有一般值小于130的continue        \n            if img_ratio < 0.5:\n                continue\n            if self.`get_traj_mask_ratio`([bbox_minx, bbox_miny]) > 0.5:          # [bbox_minx, bbox_miny]=[2000,2500]  有一半是原先要训练的，continue\n                continue\n            if not subimg.shape[-2:] == (1000, 1000):\n                subimg = F.resize(subimg, (1000, 1000))\n            sub_img_list.append(subimg)                                         # (left, upper, right, lower)\n            left_top_list.append([bbox_minx, bbox_miny])\n        sub_datas = []\n        for i in range(len(sub_img_list)):\n            sub_data_dict = dict()\n            sub_data_dict[&amp;#39;img_metas&amp;#39;] = data_dict[&amp;#39;img_metas&amp;#39;]\n            sub_data_dict[&amp;#39;img_metas&amp;#39;][0][&amp;#39;batch_input_shape&amp;#39;] = (sub_img_list[i].shape[1], sub_img_list[i].shape[2])\n            sub_data_dict[&amp;#39;img&amp;#39;] = sub_img_list[i]\n            sub_datas.append(sub_data_dict)\n        return sub_datas, left_top_list\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/utils.py</p><span class=\'hidden-code\' data-code=\'class TrajPadSampler(object):\n    def get_img_mask_raio(self, left_top):\n        x, y = left_top\n        roi = self.img[y: y+self.SIZE, x: x+self.SIZE, :]          # (1000, 1000, 3)\n        return (roi.sum(axis=2) > 130).mean()                      # (roi.sum(axis=2) > 130).shape=(1000, 1000)的True和False  mean()=0\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/utils.py</p><span class=\'hidden-code\' data-code=\'class TrajPadSampler(object):\n    def get_traj_mask_ratio(self, left_top):\n        x, y = left_top\n        return self.canvas[y: y+self.SIZE, x: x+self.SIZE, 0].mean()\n\'> </span>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/detectors/vma.py</p><span class=\'hidden-code\' data-code=\'class VMA(MVXTwoStageDetector):\n    def simple_test_pts(self, x, img_metas, ):                           # Test function\n        outs = self.`pts_bbox_head`(x, img_metas)\n        bbox_list = self.pts_bbox_head.`get_bboxes`(outs,img_metas,)     # projects/mmdet3d_plugin/vma/dense_heads/vmahead.py: def get_bboxes\n        bbox_results = [self.pred2result(scores, labels, pts, attrs) for scores, labels, pts, attrs in bbox_list]\n        return bbox_results\n\'> </span>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/dense_heads/vmahead.py</p><span class=\'hidden-code\' data-code=\'@HEADS.register_module()\nclass VMAHead(DETRHead):\n    @force_fp32(apply_to=(&amp;#39;mlvl_feats&amp;#39;, &amp;#39;prev_bev&amp;#39;))\n    def forward(self, mlvl_feats, img_metas=None, ):\n        bs, _, H, W = mlvl_feats[0].shape       # [torch.Size([1, 256, 125, 125]), [1, 256, 63, 63], [1, 256, 32, 32], [1, 256, 16, 16]]  1000/125=8  输入大小为1000*1000\n        dtype = mlvl_feats[0].dtype\n        if self.query_embed_type == &amp;#39;all_pts&amp;#39;:\n            object_query_embeds = self.query_embedding.weight.to(dtype)\n        elif self.query_embed_type == &amp;#39;instance_pts&amp;#39;:                                      # True\n            pts_embeds = self.pts_embedding.weight.unsqueeze(0)                            # Embedding(50, 512) --> torch.Size([1, 50, 512])\n            instance_embeds = self.instance_embedding.weight.unsqueeze(1)                  # Embedding(50, 512) --> torch.Size([50, 1, 512])\n            object_query_embeds = (pts_embeds + instance_embeds).flatten(0, 1).to(dtype)   # torch.Size([50, 50, 512])->torch.Size([2500, 512])\n        if hasattr(self.transformer, &amp;#39;encoder&amp;#39;):                                # True\n            input_img_h, input_img_w = img_metas[0][&amp;#39;img_shape&amp;#39;]                # tensor([1000], device=&amp;#39;cuda:0&amp;#39;),tensor([1000], device=&amp;#39;cuda:0&amp;#39;)\n            img_masks = mlvl_feats[0].new_ones((bs, input_img_h, input_img_w))  # torch.Size([1, 1000, 1000])\n            for img_id in range(bs):\n                img_h, img_w = img_metas[img_id][&amp;#39;img_shape&amp;#39;]                   # 1000,1000\n                img_masks[img_id, :img_h, :img_w] = 0\n            mlvl_masks = []                    # [torch.Size([1, 125, 125]),      [1, 63, 63],      [1, 32, 32],      [1, 16, 16]]     原先mask到对应特征大小上\n            mlvl_positional_encodings = []     # [torch.Size([1, 256, 125, 125]), [1, 256, 63, 63], [1, 256, 32, 32], [1, 256, 16, 16]]\n            for feat in mlvl_feats:\n                mlvl_masks.append(F.interpolate(img_masks[None],size=feat.shape[-2:]).to(torch.bool).squeeze(0))\n                mlvl_positional_encodings.append(self.positional_encoding(mlvl_masks[-1])) \n        else:\n            mlvl_masks=None\n            mlvl_positional_encodings=None\n        outputs = self.transformer(mlvl_feats, mlvl_masks, object_query_embeds, mlvl_positional_encodings,\n            reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501\n            cls_branches=self.cls_branches if self.as_two_stage else None)\n    \n        hs, init_reference, inter_references, _, _ = outputs     # torch.Size([6, 2500, 1, 256])，torch.Size([1, 2500, 2])， torch.Size([6, 1, 2500, 2])\n        hs = hs.permute(0, 2, 1, 3)                              # torch.Size([6, 1, 2500, 256])\n        outputs_classes = []\n        outputs_pts_coords = [] \n        outputs_attrs = []\n        for lvl in range(hs.shape[0]):\n            if lvl == 0:\n                reference = init_reference\n            else:\n                reference = inter_references[lvl - 1]\n            reference = inverse_sigmoid(reference)               # torch.Size([1, 2500, 2])  log(x/(1-x))将(0,1)区间内的值映射回实数轴的函数\n            outputs_class = self.cls_branches[lvl](hs[lvl]       # (torch.Size([1, 2500, 256])--[1, 50, 50, 256]--[1,50,256])->torch.Size([1, 50, 3])\n                                            .view(bs,self.num_vec, self.num_pts_per_vec,-1).mean(2)) \n            if self.attr_head_cfg is not None:\n                outputs_attr = self.attr_head_branches[lvl](hs[lvl].view(bs, self.num_vec, self.num_pts_per_vec,-1)\n                                                .mean(2))        # ...-> [torch.Size([1, 50, 3]), [1, 50, 6], [1, 50, 6], [1, 50, 4], [1, 50, 3], [1, 50, 6]]\n            else:\n                outputs_attr = None\n            tmp = self.reg_branches[lvl](hs[lvl])                # torch.Size([1, 2500, 2])\n            assert reference.shape[-1] == 2\n            tmp[..., 0:2] += reference[..., 0:2]                 # torch.Size([1, 2500, 2])\n            tmp = tmp.sigmoid()                                  # torch.Size([1, 2500, 2])\n            outputs_pts_coord = tmp.view(tmp.shape[0], self.num_vec, self.num_pts_per_vec,2)  # torch.Size([1, 50, 50, 2])\n            outputs_pts_coord = torch.clamp(outputs_pts_coord, min=0, max=0.999)              # torch.Size([1, 50, 50, 2])\n            outputs_classes.append(outputs_class)\n            outputs_pts_coords.append(outputs_pts_coord)\n            outputs_attrs.append(outputs_attr)\n        outputs_classes = torch.stack(outputs_classes)           # torch.Size([6, 1, 50, 3])\n        outputs_pts_coords = torch.stack(outputs_pts_coords)     # torch.Size([6, 1, 50, 50, 2])\n        outs = {\n            &amp;#39;all_cls_scores&amp;#39;: outputs_classes,&amp;#39;all_pts_preds&amp;#39;: outputs_pts_coords,&amp;#39;all_attrs_preds&amp;#39;:outputs_attrs,&amp;#39;enc_cls_scores&amp;#39;: None,&amp;#39;enc_pts_preds&amp;#39;: None\n        }\n        return outs\n\'> </span>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/dense_heads/vmahead.py</p><span class=\'hidden-code\' data-code=\'@HEADS.register_module()\nclass VMAHead(DETRHead):\n    @force_fp32(apply_to=(&amp;#39;preds_dicts&amp;#39;))\n    def get_bboxes(self, preds_dicts, img_metas, rescale=False):\n        preds_dicts = self.bbox_coder.`decode`(preds_dicts, img_metas)   # projects.mmdet3d_plugin.vma.coder.vmacoder.VMANMSFreeCoder\n        num_samples = len(preds_dicts)           # 1\n        ret_list = []\n        for i in range(num_samples):\n            preds = preds_dicts[i]\n            scores = preds[&amp;#39;scores&amp;#39;]             # torch.Size([12])\n            labels = preds[&amp;#39;labels&amp;#39;]             # torch.Size([12])\n            pts = preds[&amp;#39;pts&amp;#39;]                   # torch.Size([12, 50, 2])\n            attrs = preds[&amp;#39;attrs&amp;#39;]               # &amp;#39;attrs_scores&amp;#39;:[torch.Size([12, 3]),[12, 6],[12, 6],[12, 4],[12, 3],[12, 6]], &amp;#39;attrs_preds&amp;#39;:[torch.Size([12]),[12],[12],[12],[12],[12]]\n            ret_list.append([scores, labels, pts, attrs])\n        return ret_list\n\'> </span>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/coder/vmacoder.py</p><span class=\'hidden-code\' data-code=\'@BBOX_CODERS.register_module()\nclass VMANMSFreeCoder(BaseBBoxCoder):\n    def decode(self, preds_dicts, img_metas):\n        all_cls_scores = preds_dicts[&amp;#39;all_cls_scores&amp;#39;][-1]             # torch.Size([1, 50, 3])\n        # all_bbox_preds = preds_dicts[&amp;#39;all_bbox_preds&amp;#39;][-1]\n        all_pts_preds = preds_dicts[&amp;#39;all_pts_preds&amp;#39;][-1]               # torch.Size([1, 50, 50, 2])\n        all_attrs_preds = preds_dicts[&amp;#39;all_attrs_preds&amp;#39;][-1]           # [torch.Size([1, 50, 3]), ......, [1, 50, 6]]\n        batch_size = all_cls_scores.size()[0]\n        predictions_list = []\n        for i in range(batch_size):\n            predictions_list.append(self.`decode_single`(all_cls_scores[i], all_pts_preds[i], all_attrs_preds, img_metas[i]))\n        return predictions_list\n    def decode_single(self, cls_scores, pts_preds, attrs_preds, img_metas):\n        # use score threshold to get mask\n        max_num = self.max_num                                           # 50\n        cls_scores = cls_scores.sigmoid()                                # torch.Size([50, 3])\n        scores, indexs = cls_scores.view(-1).topk(max_num)               # torch.Size([50]),torch.Size([50])  index最大值148\n        labels = indexs % self.num_classes                               # torch.Size([50])值的范围0->2,(0,1,2)\n        bbox_index = indexs // self.num_classes                          # 最大值49\n        pts_preds = pts_preds[bbox_index]                                # torch.Size([50, 50, 2])\n        img_shape = img_metas[&amp;#39;img_shape&amp;#39;]                               # [tensor([1000]), tensor([1000])]\n        final_pts_preds = pts_preds.clone()\n        final_pts_preds[..., 0:1] = pts_preds[..., 0:1] * img_shape[0]\n        final_pts_preds[..., 1:2] = pts_preds[..., 1:2] * img_shape[1]   # num_q,num_p,2=50,50,2\n        final_scores = scores \n        final_preds = labels \n        if self.score_threshold is not None:\n            thresh_mask = final_scores > self.score_threshold            # torch.Size([50])\n            tmp_score = self.score_threshold\n            while thresh_mask.sum() == 0:                                # False\n                tmp_score *= 0.9\n                if tmp_score < 0.01:\n                    thresh_mask = final_scores > -1\n                    break\n                thresh_mask = final_scores >= tmp_score\n        else:\n            thresh_mask = torch.ones(final_scores.shape, dtype=bool)\n        scores = final_scores[thresh_mask]                 # torch.Size([12])\n        pts = final_pts_preds[thresh_mask]                 # torch.Size([12, 50, 2])\n        labels = final_preds[thresh_mask]                  # torch.Size([12])\n        if attrs_preds is not None:                        # [torch.Size([1, 50, 3]), [1, 50, 6], [1, 50, 6], [1, 50, 4], [1, 50, 3], [1, 50, 6]]\n            all_attrs_scores = []\n            all_attrs_labels = []\n            for attr_scores in attrs_preds:                                             # [1, 50, 3]\n                attr_scores = attr_scores.squeeze()                                     # [50, 3]\n                attr_scores = attr_scores.softmax(1)                                    # [50, 3]\n                _, attr_labels = torch.max(attr_scores[bbox_index, :], 1)               # 里面相当于排了序？->([50, 3])经过max->torch.Size([50])值范围0->2\n                all_attrs_labels.append(attr_labels[thresh_mask].cpu())\n                all_attrs_scores.append(attr_scores[bbox_index][thresh_mask].cpu())\n            final_attrs = {&amp;#39;attrs_scores&amp;#39;:all_attrs_scores, &amp;#39;attrs_preds&amp;#39;:all_attrs_labels}   # [torch.Size([12, 3]),[12, 6],[12, 6],[12, 4],[12, 3],[12, 6]]  + [torch.Size([12]),[12],[12],[12],[12],[12]]\n        else:\n            final_attrs = None         \n        predictions_dict = {\n            &amp;#39;scores&amp;#39;: scores,               # torch.Size([12])\n            &amp;#39;labels&amp;#39;: labels,               # torch.Size([12])\n            &amp;#39;pts&amp;#39;: pts,                     # torch.Size([12, 50, 2])         有12段？每段50个？\n            &amp;#39;attrs&amp;#39;:final_attrs\n        }\n        return predictions_dict\n\'> </span>'}]}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/combine_and_save_driving_line.py</p><span class=\'hidden-code\' data-code=\'def combine_results_and_save_driving_line(config, img, results, out_dir=None, left_top=None, visualize_flag=False):\n    print(&amp;#39;Start to aggregate driving line instances&amp;#39;)\n    image_name = img.split(&amp;#39;/&amp;#39;)[-1]                         # &amp;#39;sample_1.jpg&amp;#39;\n    line_class = config[&amp;#39;map_classes&amp;#39;]                      # [&amp;#39;lane&amp;#39;, &amp;#39;curb&amp;#39;, &amp;#39;stopline&amp;#39;]\n    lane_direction = config[&amp;#39;lane_direction&amp;#39;]               # [&amp;#39;unidirectional&amp;#39;, &amp;#39;bidirectional&amp;#39;, &amp;#39;unknown&amp;#39;]\n    lane_type = config[&amp;#39;lane_type&amp;#39;]                         # [&amp;#39;solid&amp;#39;, &amp;#39;dotted&amp;#39;, &amp;#39;solid_fishbone&amp;#39;, &amp;#39;ldotted_fishbone&amp;#39;, &amp;#39;unknown&amp;#39;, &amp;#39;no&amp;#39;]\n    lane_properties = config[&amp;#39;lane_properties&amp;#39;]             # [&amp;#39;general&amp;#39;, &amp;#39;stay&amp;#39;, &amp;#39;tide&amp;#39;, &amp;#39;bus&amp;#39;, &amp;#39;three_color&amp;#39;, &amp;#39;unknown&amp;#39;]\n    lane_flag = config[&amp;#39;lane_flag&amp;#39;]                         # [&amp;#39;single&amp;#39;, &amp;#39;double&amp;#39;, &amp;#39;triple&amp;#39;, &amp;#39;unknown&amp;#39;]\n    lane_width = config[&amp;#39;lane_width&amp;#39;]                       # [&amp;#39;normal&amp;#39;, &amp;#39;wide&amp;#39;, &amp;#39;unknown&amp;#39;]\n    curb_type = config[&amp;#39;curb_type&amp;#39;]                         # [&amp;#39;groundside&amp;#39;, &amp;#39;roadside&amp;#39;, &amp;#39;cone&amp;#39;, &amp;#39;water_horse&amp;#39;, &amp;#39;guardrail&amp;#39;, &amp;#39;unknown&amp;#39;]\n    lane_start_idx = 0\n    curb_start_idx = len(lane_direction) + len(lane_type) + len(lane_properties) + len(lane_flag) + len(lane_width)        # 22\n    distinct_pred_instances_big_image = []\n    metric = &amp;#39;chamfer&amp;#39;\n    # first step: dedulipcate in one 1k image\n    for pred_instances in results:                          # 长度为16\n        cls_gens = format_res_by_classes_line_resample(pred_instances,\n                                                        cls_names=line_class,\n                                                        num_sample=600,\n                                                        num_pred_pts_per_instance=600,\n                                                        eval_use_same_gt_sample_num_flag=True, \n                                                        fix_interval=False)\n    ......\n\'> </span>'}]}]}]})</script><script src='https://study1994.github.io/study_html/npm/myjs/tooltip.js'></script>
</body>
</html>
