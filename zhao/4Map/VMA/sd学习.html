<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>sd学习</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://study1994.github.io/study_html/npm/prism.css"><link rel="stylesheet" href="https://study1994.github.io/study_html/npm/markmap-toolbar@0.13.5/dist/style.css">
</head>
<body>
    <svg id="mindmap"></svg>
    <script src="https://study1994.github.io/study_html/npm/d3@6.7.0"></script>
    <script src="https://study1994.github.io/study_html/npm/markmap-view@0.13.5"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    <script>
        (r => {
            setTimeout(r);
        })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{'type': 'root', 'depth': 0, 'content': '', 'children': [{'type': 'heading', 'depth': 1, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/custom_infer.py推理vma_res152_e80_line.py</p><font size="0"><pre class="language-python"><code class="language-python">def main():\n    args = get_args()\n    cfg = Config.fromfile(args.config)                          <span style=\'color: red\'>\'projects/configs/vma_res152_e80_line.py\'</span>\n    if hasattr(cfg, \'plugin\'):\n        if cfg.plugin:                                          <span style=\'color: red\'>True</span>\n            import importlib\n            if hasattr(cfg, \'plugin_dir\'):\n                plugin_dir = cfg.plugin_dir                     <span style=\'color: red\'>\'projects/mmdet3d_plugin/\'</span>\n                _module_dir = dirname(plugin_dir)               <span style=\'color: red\'>[\'projects\', \'mmdet3d_plugin\']</span>\n                _module_dir = _module_dir.split(\'/\')            <span style=\'color: red\'>[\'projects\', \'mmdet3d_plugin\']</span>\n                _module_path = _module_dir[0]                   <span style=\'color: red\'>\'projects\'</span>\n                for m in _module_dir[1:]:\n                    _module_path = _module_path + \'.\' + m       <span style=\'color: red\'>\'projects.mmdet3d_plugin\'</span>\n                print(_module_path)\n                plg_lib = importlib.import_module(_module_path)\n            else:\n                ......\n    data_list = os.listdir(args.root_path)                      <span style=\'color: red\'>\'data/sd_data/line/origin_data/image_data\'-->[.jpg,,...]  5张</span>\n    data_list.sort()    \n    infer_combine_function = infer_combine_func[args.element_type]\n    <span style=\'color: green;font-weight: bold;\'>infer_combine_function</span>(args, data_list)                   <span style=\'color: red\'>demo/inference_and_combine_function.py</span>\n    print(\'completed\')                  \n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 2, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><font size="0"><pre class="language-python"><code class="language-python">def driving_line_infer_and_save(args, data_list):                       <span style=\'color: red\'># [\'sample_1.jpg\', \'sample_2.jpg\', \'sample_3.jpg\', \'sample_4.jpg\', \'sample_5.jpg\']</span>\n    config = args.config                                                <span style=\'color: red\'># \'projects/configs/vma_res152_e80_line.py\'</span>\n    checkpoint = args.checkpoint                                        <span style=\'color: red\'># \'ckpts/sd_line.pth\'</span>\n    model = init_model(config, checkpoint, device=args.device)\n    print(\'driving line model initiated\')\n    prog_bar = mmcv.ProgressBar(len(data_list))\n    for idx, image_name in enumerate(data_list): \n        upload_img_path = os.path.join(args.root_path, image_name)                                            <span style=\'color: red\'># \'data/sd_data/line/origin_data/image_data/sample_1.jpg\'</span>\n        trajectory_path = upload_img_path.replace(\'image_data\', \'trajectory_data\').replace(\'jpg\', \'json\')     <span style=\'color: red\'># \'data/sd_data/line/origin_data/trajectory_data/sample_1.json\'</span>\n        results_list, left_top = <span style=\'color: green;font-weight: bold;\'>inference_detector_forcurb_bigimg</span>(model, get_sub_data_driving, upload_img_path, trajectory_path, args.trajectory_sample_num, get_traj_pad=True, pad=False, attr=True)\n        if not (len(results_list)):\n            print(\'no data\')\n            continue\n        <span style=\'color: green;font-weight: bold;\'>combine_results_and_save_driving_line</span>(model.cfg, upload_img_path, results_list, out_dir=args.out_dir, left_top=left_top, visualize_flag=args.visualize)\n        prog_bar.update()\n    print(\'driving line infer and combine completed\')\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><font size="0"><pre class="language-python"><code class="language-python">def inference_detector_forcurb_bigimg(model, get_sub_data_func, imgs, trajectory_path, sample_num, get_traj_pad=False, pad=False, attr=False):\n    device = next(model.parameters()).device      <span style=\'color: red\'># model device   device(type=\'cuda\', index=0)</span>\n    if pad:                                       <span style=\'color: red\'># False</span>\n        test_pipeline = [LoadBigImagePad()]\n    else:\n        test_pipeline = [LoadBigImage()]          <span style=\'color: red\'># build the data pipeline</span>\n    test_pipeline = Compose(test_pipeline)\n    <span style=\'color: red\'># prepare data</span>\n    data = dict(img=imgs)\n    data = test_pipeline(data)\n    <span style=\'color: red\'># add get_sub_img func</span>\n    trajectory_data_list = mmcv.load(trajectory_path)        <span style=\'color: red\'># \'data/sd_data/line/origin_data/trajectory_data/sample_1.json\' --> len(trajectory_data_list[0])=126</span>\n    sub_datas, left_top = <span style=\'color: green;font-weight: bold;\'>get_sub_data_func</span>(data, trajectory_data_list, sample_num)  <span style=\'color: red\'># demo/inference_and_combine_function.py --> def get_sub_data_driving</span>\n    if get_traj_pad:                                         <span style=\'color: red\'># True           </span>\n        from utils import TrajPadSampler\n        pad_sampler =  <span style=\'color: green;font-weight: bold;\'>TrajPadSampler</span>(imgs, left_top)      <span style=\'color: red\'># demo/utils.py:</span>\n        more_sub_datas, more_left_top = pad_sampler.<span style=\'color: green;font-weight: bold;\'>sample</span>(data)\n        sub_datas += more_sub_datas\n        left_top += more_left_top\n    if not len(sub_datas):\n        return [], []\n    results = []\n    for sub_data in sub_datas:\n        data = collate([sub_data], samples_per_gpu=1)\n        if next(model.parameters()).is_cuda:\n            data = scatter(data, [device])[0]                <span style=\'color: red\'># scatter to specified GPU</span>\n        else:\n            data[\'img_metas\'] = [i.data[0] for i in data[\'img_metas\']]\n        with torch.no_grad():\n            result = <span style=\'color: green;font-weight: bold;\'>model</span>(return_loss=False, rescale=True, **data)      <span style=\'color: red\'># 每张 torch.Size([1, 3, 1000, 1000]) 直接从head那里看</span>\n            results.append(result)\n    print(\'Start to convert detection format...\')\n    results_list = []\n    if not attr:                         <span style=\'color: red\'># False</span>\n        ......\n    else: \n        for batch_result in results:                  <span style=\'color: red\'># [[{\'pts_bbox\': {...}}],[{\'pts_bbox\': {...}}],....]</span>\n            for result in batch_result:\n                result = result[\'pts_bbox\']\n                pred_instances = []\n                pred_scores = result[\'scores_3d\'].numpy()              <span style=\'color: red\'># (12,)</span>\n                pred_data = result[\'pts_3d\'].numpy()                   <span style=\'color: red\'># (12, 50, 2)</span>\n                pred_labels = result[\'labels_3d\'].numpy()              <span style=\'color: red\'># (12,)</span>\n                pred_attrs_labels = torch.stack(result[\'attrs_3d\'][\'attrs_preds\'], 1)      <span style=\'color: red\'># torch.Size([12, 6])</span>\n                pred_attrs_scores = torch.cat(result[\'attrs_3d\'][\'attrs_scores\'], 1)       <span style=\'color: red\'># torch.Size([12, 28])</span>\n                for idx in range(len(pred_scores)):                    <span style=\'color: red\'># 12</span>\n                    pred_instances.append({\'class\':pred_labels[idx], \'data\':pred_data[idx], \'attrs_labels\':pred_attrs_labels[idx],\n                                        \'attrs_scores\':pred_attrs_scores[idx], \'confidence_level\':pred_scores[idx]})\n            results_list.append(pred_instances)\n    print(\'convert completed\')\n    return results_list, left_top\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_sub_data_driving(data_dict, trajectory_data_list, sample_num):\n    sub_datas = []\n    img = data_dict[\'img\']                             <span style=\'color: red\'># torch.Size([3, 6000, 6000])</span>\n    img_name = data_dict[\'filename\']                   <span style=\'color: red\'># \'data/sd_data/line/origin_data/image_data/sample_1.jpg\'</span>\n    sub_imgs, left_top = <span style=\'color: green;font-weight: bold;\'>get_sub_img_driving</span>(img, trajectory_data_list, sample_num)   <span style=\'color: red\'># demo/inference_and_combine_function.py --> def get_sub_img_driving</span>\n    for i in range(len(sub_imgs)):\n        sub_data_dict = dict()\n        sub_data_dict[\'img_metas\'] = data_dict[\'img_metas\']                                          <span style=\'color: red\'># {}</span>\n        sub_data_dict[\'img_metas\'][0][\'img_shape\'] = (sub_imgs[i].shape[1], sub_imgs[i].shape[2])    <span style=\'color: red\'># {\'img_shape\': (1000, 1000)}</span>\n        sub_data_dict[\'img\'] = sub_imgs[i]\n        sub_datas.append(sub_data_dict)\n    return sub_datas, left_top\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/inference_and_combine_function.py</p><font size="0"><pre class="language-python"><code class="language-python">def get_sub_img_driving(img, trajectory_data_list, sample_num):         <span style=\'color: red\'># 15</span>\n    sub_img_list = []\n    left_top_list = []\n    for i, trajectory_data in enumerate(trajectory_data_list):          <span style=\'color: red\'># 对每条轨迹采样sample_num个点作为中心点在大图上进行裁切</span>\n        if len(trajectory_data) < 2:\n            print(\'trajectory {} is not long enough\'.format(i))\n            continue\n        trajectory_shapely = LineString(np.array(trajectory_data))\n        <span style=\'color: red\'># trajectory_length = trajectory_shapely.length</span>\n        <span style=\'color: red\'># sample_num = round((trajectory_length // 1000)*2 + 1)</span>\n        distances = np.linspace(0, trajectory_shapely.length, sample_num)\n        sampled_points = np.array([list(trajectory_shapely.interpolate(distance).coords) for distance in distances]).reshape(-1, 2)  <span style=\'color: red\'># shape:(15, 2)</span>\n        for sampled_point in sampled_points:                            <span style=\'color: red\'># 拆分成500x500的大小</span>\n            x, y = sampled_point\n            if x < 500:\n                x = 500\n            elif x > 5500:\n                x = 5500\n            if y < 500:\n                y = 500\n            elif y > 5500:\n                y = 5500\n            bbox_minx, bbox_miny, bbox_maxx, bbox_maxy = round(x - 500), round(y - 500), round(x + 499), round(y + 499)\n            sub_img_list.append(img[:, bbox_miny:bbox_maxy+1, bbox_minx:bbox_maxx+1])  <span style=\'color: red\'># (left, upper, right, lower)</span>\n            left_top_list.append([bbox_minx, bbox_miny])\n    return sub_img_list, left_top_list                 <span style=\'color: red\'># 切割出来的500x500以及，对应的左上角(x1,y1)-->len=15  torch.Size([3, 1000, 1000])</span>\n</code></pre></font>'}]}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/utils.py</p><font size="0"><pre class="language-python"><code class="language-python">class TrajPadSampler(object):\n    SIZE = 1000\n    def __init__(self, imgpath, left_top):\n        self.imgpath = imgpath                            <span style=\'color: red\'># \'data/sd_data/line/origin_data/image_data/sample_1.jpg\'</span>\n        self.left_top = left_top                          <span style=\'color: red\'># [[2415, 0], [2438, 0], ....]</span>\n        img = cv2.imread(self.imgpath)\n        self.img = img\n        canvas = np.zeros_like(img)\n        for lt in self.left_top:\n            cv2.rectangle(canvas ,lt, [lt[0]+self.SIZE, lt[1]+self.SIZE], (1, 1, 1), -1)\n        self.canvas = canvas\n    def sample(self, data_dict):\n        img = data_dict[\'img\']                                <span style=\'color: red\'># torch.Size([3, 6000, 6000])</span>\n        <span style=\'color: red\'># 采样轨迹之外的区域</span>\n        infer_subimg_resolution = 1000\n        bigimgsize=6000\n        subimg_interval = infer_subimg_resolution // 2        <span style=\'color: red\'># 500</span>\n        sub_img_list = []\n        left_top_list = []\n        sampled_points = []\n        for offsetx in range(subimg_interval, bigimgsize+1-subimg_interval, subimg_interval):        <span style=\'color: red\'># （500,6000-500+1,500)</span>\n            for offsety in range(subimg_interval, bigimgsize+1-subimg_interval, subimg_interval):    <span style=\'color: red\'># （500,6000-500+1,500)</span>\n                sampled_points.append([offsetx, offsety])     <span style=\'color: red\'># 121 11x11个点                 </span>\n        for point in sampled_points:\n            x, y = point                                      <span style=\'color: red\'># 点限制在500->6000-500</span>\n            if x < subimg_interval:\n                x = subimg_interval\n            elif x > (bigimgsize - subimg_interval):\n                x = (bigimgsize - subimg_interval)\n            if y < subimg_interval:\n                y = subimg_interval\n            elif y > (bigimgsize - subimg_interval):\n                y = (bigimgsize - subimg_interval)\n            bbox_minx, bbox_miny, bbox_maxx, bbox_maxy = round(x - subimg_interval), round(y - subimg_interval), round(x + subimg_interval - 1), round(y + subimg_interval - 1)\n            subimg = img[:, bbox_miny: bbox_maxy+1, bbox_minx: bbox_maxx+1]     <span style=\'color: red\'># torch.Size([3, 1000, 1000])</span>\n            img_ratio = self.get_img_mask_raio([bbox_minx, bbox_miny])          <span style=\'color: red\'># 有一般值小于130的continue        </span>\n            if img_ratio < 0.5:\n                continue\n            if self.get_traj_mask_ratio([bbox_minx, bbox_miny]) > 0.5:          <span style=\'color: red\'># [bbox_minx, bbox_miny]=[2000,2500]  有一半是原先要训练的，continue</span>\n                continue\n            if not subimg.shape[-2:] == (1000, 1000):\n                subimg = F.resize(subimg, (1000, 1000))\n            sub_img_list.append(subimg)                                         <span style=\'color: red\'># (left, upper, right, lower)</span>\n            left_top_list.append([bbox_minx, bbox_miny])\n        sub_datas = []\n        for i in range(len(sub_img_list)):\n            sub_data_dict = dict()\n            sub_data_dict[\'img_metas\'] = data_dict[\'img_metas\']\n            sub_data_dict[\'img_metas\'][0][\'batch_input_shape\'] = (sub_img_list[i].shape[1], sub_img_list[i].shape[2])\n            sub_data_dict[\'img\'] = sub_img_list[i]\n            sub_datas.append(sub_data_dict)\n        return sub_datas, left_top_list\n</code></pre></font>'}, {'type': 'heading', 'depth': 4, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/detectors/vma.py</p><font size="0"><pre class="language-python"><code class="language-python">class VMA(MVXTwoStageDetector):\n    def simple_test_pts(self, x, img_metas, ):                           <span style=\'color: red\'># Test function</span>\n        outs = self.<span style=\'color: green;font-weight: bold;\'>pts_bbox_head</span>(x, img_metas)\n        bbox_list = self.pts_bbox_head.<span style=\'color: green;font-weight: bold;\'>get_bboxes</span>(outs,img_metas,)     <span style=\'color: red\'># projects/mmdet3d_plugin/vma/dense_heads/vmahead.py: def get_bboxes</span>\n        bbox_results = [self.pred2result(scores, labels, pts, attrs) for scores, labels, pts, attrs in bbox_list]\n        return bbox_results\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/dense_heads/vmahead.py</p><font size="0"><pre class="language-python"><code class="language-python">@HEADS.register_module()\nclass VMAHead(DETRHead):\n    @force_fp32(apply_to=(\'mlvl_feats\', \'prev_bev\'))\n    def forward(self, mlvl_feats, img_metas=None, ):\n        bs, _, H, W = mlvl_feats[0].shape       <span style=\'color: red\'># [torch.Size([1, 256, 125, 125]), [1, 256, 63, 63], [1, 256, 32, 32], [1, 256, 16, 16]]  1000/125=8  输入大小为1000*1000</span>\n        dtype = mlvl_feats[0].dtype\n        if self.query_embed_type == \'all_pts\':\n            object_query_embeds = self.query_embedding.weight.to(dtype)\n        elif self.query_embed_type == \'instance_pts\':                                      <span style=\'color: red\'># True</span>\n            pts_embeds = self.pts_embedding.weight.unsqueeze(0)                            <span style=\'color: red\'># Embedding(50, 512) --> torch.Size([1, 50, 512])</span>\n            instance_embeds = self.instance_embedding.weight.unsqueeze(1)                  <span style=\'color: red\'># Embedding(50, 512) --> torch.Size([50, 1, 512])</span>\n            object_query_embeds = (pts_embeds + instance_embeds).flatten(0, 1).to(dtype)   <span style=\'color: red\'># torch.Size([50, 50, 512])->torch.Size([2500, 512])</span>\n        if hasattr(self.transformer, \'encoder\'):                                <span style=\'color: red\'># True</span>\n            input_img_h, input_img_w = img_metas[0][\'img_shape\']                <span style=\'color: red\'># tensor([1000], device=\'cuda:0\'),tensor([1000], device=\'cuda:0\')</span>\n            img_masks = mlvl_feats[0].new_ones((bs, input_img_h, input_img_w))  <span style=\'color: red\'># torch.Size([1, 1000, 1000])</span>\n            for img_id in range(bs):\n                img_h, img_w = img_metas[img_id][\'img_shape\']                   <span style=\'color: red\'># 1000,1000</span>\n                img_masks[img_id, :img_h, :img_w] = 0\n            mlvl_masks = []                    <span style=\'color: red\'># [torch.Size([1, 125, 125]),      [1, 63, 63],      [1, 32, 32],      [1, 16, 16]]     原先mask到对应特征大小上</span>\n            mlvl_positional_encodings = []     <span style=\'color: red\'># [torch.Size([1, 256, 125, 125]), [1, 256, 63, 63], [1, 256, 32, 32], [1, 256, 16, 16]]</span>\n            for feat in mlvl_feats:\n                mlvl_masks.append(F.interpolate(img_masks[None],size=feat.shape[-2:]).to(torch.bool).squeeze(0))\n                mlvl_positional_encodings.append(self.positional_encoding(mlvl_masks[-1])) \n        else:\n            mlvl_masks=None\n            mlvl_positional_encodings=None\n        outputs = self.transformer(mlvl_feats, mlvl_masks, object_query_embeds, mlvl_positional_encodings,\n            reg_branches=self.reg_branches if self.with_box_refine else None,  <span style=\'color: red\'># noqa:E501</span>\n            cls_branches=self.cls_branches if self.as_two_stage else None)\n    \n        hs, init_reference, inter_references, _, _ = outputs     <span style=\'color: red\'># torch.Size([6, 2500, 1, 256])，torch.Size([1, 2500, 2])， torch.Size([6, 1, 2500, 2])</span>\n        hs = hs.permute(0, 2, 1, 3)                              <span style=\'color: red\'># torch.Size([6, 1, 2500, 256])</span>\n        outputs_classes = []\n        outputs_pts_coords = [] \n        outputs_attrs = []\n        for lvl in range(hs.shape[0]):\n            if lvl == 0:\n                reference = init_reference\n            else:\n                reference = inter_references[lvl - 1]\n            reference = inverse_sigmoid(reference)               <span style=\'color: red\'># torch.Size([1, 2500, 2])  log(x/(1-x))将(0,1)区间内的值映射回实数轴的函数</span>\n            outputs_class = self.cls_branches[lvl](hs[lvl]       <span style=\'color: red\'># (torch.Size([1, 2500, 256])--[1, 50, 50, 256]--[1,50,256])->torch.Size([1, 50, 3])</span>\n                                            .view(bs,self.num_vec, self.num_pts_per_vec,-1).mean(2)) \n            if self.attr_head_cfg is not None:\n                outputs_attr = self.attr_head_branches[lvl](hs[lvl].view(bs, self.num_vec, self.num_pts_per_vec,-1)\n                                                .mean(2))        <span style=\'color: red\'># ...-> [torch.Size([1, 50, 3]), [1, 50, 6], [1, 50, 6], [1, 50, 4], [1, 50, 3], [1, 50, 6]]</span>\n            else:\n                outputs_attr = None\n            tmp = self.reg_branches[lvl](hs[lvl])                <span style=\'color: red\'># torch.Size([1, 2500, 2])</span>\n            assert reference.shape[-1] == 2\n            tmp[..., 0:2] += reference[..., 0:2]                 <span style=\'color: red\'># torch.Size([1, 2500, 2])</span>\n            tmp = tmp.sigmoid()                                  <span style=\'color: red\'># torch.Size([1, 2500, 2])</span>\n            outputs_pts_coord = tmp.view(tmp.shape[0], self.num_vec, self.num_pts_per_vec,2)  <span style=\'color: red\'># torch.Size([1, 50, 50, 2])</span>\n            outputs_pts_coord = torch.clamp(outputs_pts_coord, min=0, max=0.999)              <span style=\'color: red\'># torch.Size([1, 50, 50, 2])</span>\n            outputs_classes.append(outputs_class)\n            outputs_pts_coords.append(outputs_pts_coord)\n            outputs_attrs.append(outputs_attr)\n        outputs_classes = torch.stack(outputs_classes)           <span style=\'color: red\'># torch.Size([6, 1, 50, 3])</span>\n        outputs_pts_coords = torch.stack(outputs_pts_coords)     <span style=\'color: red\'># torch.Size([6, 1, 50, 50, 2])</span>\n        outs = {\n            \'all_cls_scores\': outputs_classes,\'all_pts_preds\': outputs_pts_coords,\'all_attrs_preds\':outputs_attrs,\'enc_cls_scores\': None,\'enc_pts_preds\': None\n        }\n        return outs\n</code></pre></font>'}, {'type': 'heading', 'depth': 5, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/dense_heads/vmahead.py</p><font size="0"><pre class="language-python"><code class="language-python">@HEADS.register_module()\nclass VMAHead(DETRHead):\n    @force_fp32(apply_to=(\'preds_dicts\'))\n    def get_bboxes(self, preds_dicts, img_metas, rescale=False):\n        preds_dicts = self.bbox_coder.<span style=\'color: green;font-weight: bold;\'>decode</span>(preds_dicts, img_metas)   <span style=\'color: red\'># projects.mmdet3d_plugin.vma.coder.vmacoder.VMANMSFreeCoder</span>\n        num_samples = len(preds_dicts)           <span style=\'color: red\'># 1</span>\n        ret_list = []\n        for i in range(num_samples):\n            preds = preds_dicts[i]\n            scores = preds[\'scores\']             <span style=\'color: red\'># torch.Size([12])</span>\n            labels = preds[\'labels\']             <span style=\'color: red\'># torch.Size([12])</span>\n            pts = preds[\'pts\']                   <span style=\'color: red\'># torch.Size([12, 50, 2])</span>\n            attrs = preds[\'attrs\']               <span style=\'color: red\'># \'attrs_scores\':[torch.Size([12, 3]),[12, 6],[12, 6],[12, 4],[12, 3],[12, 6]], \'attrs_preds\':[torch.Size([12]),[12],[12],[12],[12],[12]]</span>\n            ret_list.append([scores, labels, pts, attrs])\n        return ret_list\n</code></pre></font>', 'children': [{'type': 'heading', 'depth': 6, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">projects/mmdet3d_plugin/vma/coder/vmacoder.py</p><font size="0"><pre class="language-python"><code class="language-python">@BBOX_CODERS.register_module()\nclass VMANMSFreeCoder(BaseBBoxCoder):\n    def decode(self, preds_dicts, img_metas):\n        all_cls_scores = preds_dicts[\'all_cls_scores\'][-1]             <span style=\'color: red\'># torch.Size([1, 50, 3])</span>\n        <span style=\'color: red\'># all_bbox_preds = preds_dicts[\'all_bbox_preds\'][-1]</span>\n        all_pts_preds = preds_dicts[\'all_pts_preds\'][-1]               <span style=\'color: red\'># torch.Size([1, 50, 50, 2])</span>\n        all_attrs_preds = preds_dicts[\'all_attrs_preds\'][-1]           <span style=\'color: red\'># [torch.Size([1, 50, 3]), ......, [1, 50, 6]]</span>\n        batch_size = all_cls_scores.size()[0]\n        predictions_list = []\n        for i in range(batch_size):\n            predictions_list.append(self.<span style=\'color: green;font-weight: bold;\'>decode_single</span>(all_cls_scores[i], all_pts_preds[i], all_attrs_preds, img_metas[i]))\n        return predictions_list\n    def decode_single(self, cls_scores, pts_preds, attrs_preds, img_metas):\n        <span style=\'color: red\'># use score threshold to get mask</span>\n        max_num = self.max_num                                           <span style=\'color: red\'># 50</span>\n        cls_scores = cls_scores.sigmoid()                                <span style=\'color: red\'># torch.Size([50, 3])</span>\n        scores, indexs = cls_scores.view(-1).topk(max_num)               <span style=\'color: red\'># torch.Size([50]),torch.Size([50])  index最大值148</span>\n        labels = indexs % self.num_classes                               <span style=\'color: red\'># torch.Size([50])值的范围0->2,(0,1,2)</span>\n        bbox_index = indexs // self.num_classes                          <span style=\'color: red\'># 最大值49</span>\n        pts_preds = pts_preds[bbox_index]                                <span style=\'color: red\'># torch.Size([50, 50, 2])</span>\n        img_shape = img_metas[\'img_shape\']                               <span style=\'color: red\'># [tensor([1000]), tensor([1000])]</span>\n        final_pts_preds = pts_preds.clone()\n        final_pts_preds[..., 0:1] = pts_preds[..., 0:1] * img_shape[0]\n        final_pts_preds[..., 1:2] = pts_preds[..., 1:2] * img_shape[1]   <span style=\'color: red\'># num_q,num_p,2=50,50,2</span>\n        final_scores = scores \n        final_preds = labels \n        if self.score_threshold is not None:\n            thresh_mask = final_scores > self.score_threshold            <span style=\'color: red\'># torch.Size([50])</span>\n            tmp_score = self.score_threshold\n            while thresh_mask.sum() == 0:                                <span style=\'color: red\'># False</span>\n                tmp_score *= 0.9\n                if tmp_score < 0.01:\n                    thresh_mask = final_scores > -1\n                    break\n                thresh_mask = final_scores >= tmp_score\n        else:\n            thresh_mask = torch.ones(final_scores.shape, dtype=bool)\n        scores = final_scores[thresh_mask]                 <span style=\'color: red\'># torch.Size([12])</span>\n        pts = final_pts_preds[thresh_mask]                 <span style=\'color: red\'># torch.Size([12, 50, 2])</span>\n        labels = final_preds[thresh_mask]                  <span style=\'color: red\'># torch.Size([12])</span>\n        if attrs_preds is not None:                        <span style=\'color: red\'># [torch.Size([1, 50, 3]), [1, 50, 6], [1, 50, 6], [1, 50, 4], [1, 50, 3], [1, 50, 6]]</span>\n            all_attrs_scores = []\n            all_attrs_labels = []\n            for attr_scores in attrs_preds:                                             <span style=\'color: red\'># [1, 50, 3]</span>\n                attr_scores = attr_scores.squeeze()                                     <span style=\'color: red\'># [50, 3]</span>\n                attr_scores = attr_scores.softmax(1)                                    <span style=\'color: red\'># [50, 3]</span>\n                _, attr_labels = torch.max(attr_scores[bbox_index, :], 1)               <span style=\'color: red\'># 里面相当于排了序？->([50, 3])经过max->torch.Size([50])值范围0->2</span>\n                all_attrs_labels.append(attr_labels[thresh_mask].cpu())\n                all_attrs_scores.append(attr_scores[bbox_index][thresh_mask].cpu())\n            final_attrs = {\'attrs_scores\':all_attrs_scores, \'attrs_preds\':all_attrs_labels}   <span style=\'color: red\'># [torch.Size([12, 3]),[12, 6],[12, 6],[12, 4],[12, 3],[12, 6]]  + [torch.Size([12]),[12],[12],[12],[12],[12]]</span>\n        else:\n            final_attrs = None         \n        predictions_dict = {\n            \'scores\': scores,               <span style=\'color: red\'># torch.Size([12])</span>\n            \'labels\': labels,               <span style=\'color: red\'># torch.Size([12])</span>\n            \'pts\': pts,                     <span style=\'color: red\'># torch.Size([12, 50, 2])         有12段？每段50个？</span>\n            \'attrs\':final_attrs\n        }\n        return predictions_dict\n</code></pre></font>'}]}]}]}, {'type': 'heading', 'depth': 3, 'payload': {'lines': [0, 1]}, 'content': '<p style="color: blue;font-weight: bold;">demo/combine_and_save_driving_line.py</p><font size="0"><pre class="language-python"><code class="language-python">def combine_results_and_save_driving_line(config, img, results, out_dir=None, left_top=None, visualize_flag=False):\n    print(\'Start to aggregate driving line instances\')\n    image_name = img.split(\'/\')[-1]                         <span style=\'color: red\'># \'sample_1.jpg\'</span>\n    line_class = config[\'map_classes\']                      <span style=\'color: red\'># [\'lane\', \'curb\', \'stopline\']</span>\n    lane_direction = config[\'lane_direction\']               <span style=\'color: red\'># [\'unidirectional\', \'bidirectional\', \'unknown\']</span>\n    lane_type = config[\'lane_type\']                         <span style=\'color: red\'># [\'solid\', \'dotted\', \'solid_fishbone\', \'ldotted_fishbone\', \'unknown\', \'no\']</span>\n    lane_properties = config[\'lane_properties\']             <span style=\'color: red\'># [\'general\', \'stay\', \'tide\', \'bus\', \'three_color\', \'unknown\']</span>\n    lane_flag = config[\'lane_flag\']                         <span style=\'color: red\'># [\'single\', \'double\', \'triple\', \'unknown\']</span>\n    lane_width = config[\'lane_width\']                       <span style=\'color: red\'># [\'normal\', \'wide\', \'unknown\']</span>\n    curb_type = config[\'curb_type\']                         <span style=\'color: red\'># [\'groundside\', \'roadside\', \'cone\', \'water_horse\', \'guardrail\', \'unknown\']</span>\n    lane_start_idx = 0\n    curb_start_idx = len(lane_direction) + len(lane_type) + len(lane_properties) + len(lane_flag) + len(lane_width)        <span style=\'color: red\'># 22</span>\n    distinct_pred_instances_big_image = []\n    metric = \'chamfer\'\n    <span style=\'color: red\'># first step: dedulipcate in one 1k image</span>\n    for pred_instances in results:                          <span style=\'color: red\'># 长度为16</span>\n        cls_gens = format_res_by_classes_line_resample(pred_instances,\n                                                        cls_names=line_class,\n                                                        num_sample=600,\n                                                        num_pred_pts_per_instance=600,\n                                                        eval_use_same_gt_sample_num_flag=True, \n                                                        fix_interval=False)\n    ......\n</code></pre></font>'}]}]}]})</script></body>
</html>
